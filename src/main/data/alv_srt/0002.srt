1
00:00:00,000 --> 00:00:02,610
exact and that are with different shapes

2
00:00:05,520 --> 00:00:08,730
these approaches do contain

3
00:00:08,750 --> 00:00:11,650
some phenomenological

4
00:00:11,710 --> 00:00:15,110
parameters especially if you want to describe the

5
00:00:15,130 --> 00:00:19,400
so a lot of work will be done in the future to better describe

6
00:00:19,420 --> 00:00:24,480
the interaction between alpha particles and neutrons and protons surrounding them

7
00:00:24,500 --> 00:00:26,860
but it's very it's really something

8
00:00:27,230 --> 00:00:29,610
that is important

9
00:00:30,480 --> 00:00:35,940
some experiments are experiments three are related to add close to ground state

10
00:00:35,960 --> 00:00:37,550
because i was talking here

11
00:00:37,570 --> 00:00:41,730
what i find clusters in that state close to the threshold of the and five

12
00:00:42,550 --> 00:00:45,670
but there are also looking at the heart of condensate

13
00:00:45,690 --> 00:00:49,750
in the ground state

14
00:00:50,800 --> 00:00:55,230
now i will talk about superheavy elements so

15
00:00:55,260 --> 00:00:57,960
the search for a very very old

16
00:00:57,980 --> 00:00:59,000
for example

17
00:00:59,570 --> 00:01:06,400
when people before they were discovering fish and they were using actinides and collision with

18
00:01:06,610 --> 00:01:07,750
the night and this

19
00:01:07,760 --> 00:01:09,520
i have seen your raised

20
00:01:09,530 --> 00:01:14,920
but the chances said all we have discovered a new element superheavy elements

21
00:01:14,940 --> 00:01:20,340
in fact there were on the fission fragments range from fission fragment but it's really

22
00:01:20,340 --> 00:01:25,110
really search there's really for the superheavy elements

23
00:01:25,130 --> 00:01:26,420
so if you do

24
00:01:26,440 --> 00:01:32,000
classic physic with the fluid so you consider an equilibrium so then you have

25
00:01:32,230 --> 00:01:35,650
the balance between the coulomb repulsion

26
00:01:35,650 --> 00:01:37,400
and the strong interaction

27
00:01:37,420 --> 00:01:39,650
so what because your face takes so

28
00:01:39,670 --> 00:01:45,880
and then you have maximum charge allowed which in one o four so you cannot

29
00:01:46,130 --> 00:01:48,920
elements heavier years and one of four

30
00:01:49,170 --> 00:01:52,440
with these liquid drop model

31
00:01:52,440 --> 00:01:57,960
and experimentally there are many experiments back in their place

32
00:01:57,960 --> 00:02:00,320
i don't know

33
00:02:00,360 --> 00:02:08,690
guinea that in france in genocide germany where they have found that elements two

34
00:02:08,710 --> 00:02:11,150
and right eighteen have been seen

35
00:02:11,150 --> 00:02:15,840
so much more than what is predicted to limit predicted by basically with

36
00:02:15,860 --> 00:02:19,250
so i mean that here you've really the stability

37
00:02:19,250 --> 00:02:24,710
which is due to microscopic effects so this is really because of this existence of

38
00:02:24,710 --> 00:02:26,550
microscopic effect that you

39
00:02:26,570 --> 00:02:30,150
you have these superheavy element that are

40
00:02:30,420 --> 00:02:32,320
that do exist

41
00:02:33,320 --> 00:02:38,360
in order to information and approaches and the first is very important to know

42
00:02:38,380 --> 00:02:42,880
which nuclei very giving you know you can

43
00:02:46,380 --> 00:02:47,230
and then

44
00:02:47,230 --> 00:02:49,710
you will tell me OK but

45
00:02:49,750 --> 00:02:52,530
what i presented before was about them

46
00:02:52,550 --> 00:02:56,320
there are magic numbers two eight twenty twenty eight and they were the same for

47
00:02:56,320 --> 00:02:59,320
proton and neutron so why don't you have the same

48
00:02:59,340 --> 00:03:02,520
magic number four

49
00:03:02,530 --> 00:03:04,150
one hundred twenty six

50
00:03:04,150 --> 00:03:08,920
so this is really a question but in fact is due to the different theories

51
00:03:08,940 --> 00:03:14,210
because many calculations have been there from to all the conditions to find something else

52
00:03:14,230 --> 00:03:16,550
he starts with typical example

53
00:03:16,570 --> 00:03:17,820
for this is

54
00:03:18,030 --> 00:03:22,610
the number of false stability for proton or neutron number

55
00:03:22,630 --> 00:03:26,290
and you see that one hundred fourteen one hundred twenty and one with twenty six

56
00:03:26,290 --> 00:03:27,710
is predicted

57
00:03:27,710 --> 00:03:32,800
and what is predicted also that this number really depends on the number of neutrons

58
00:03:32,800 --> 00:03:37,820
so here we have a real real disagreement between the different areas

59
00:03:37,840 --> 00:03:41,610
that's why the search is very important

60
00:03:41,630 --> 00:03:44,210
to know what is this

61
00:03:44,250 --> 00:03:47,820
new magic

62
00:03:47,840 --> 00:03:51,280
i will finish with something that

63
00:03:51,300 --> 00:03:55,420
concern fundamental symmetries and interactions soul

64
00:03:55,440 --> 00:03:57,520
so this is something

65
00:03:57,520 --> 00:04:02,050
a little bit special from the user use of nuclear physics community which is more

66
00:04:02,050 --> 00:04:05,340
dealing with such structure exotic nuclei

67
00:04:05,360 --> 00:04:09,530
but you would see that in the future a lot can be done

68
00:04:09,550 --> 00:04:13,130
two have some elements concerning fundamental symmetries

69
00:04:14,480 --> 00:04:19,420
when you are weak interaction it is it is all these was a bit added

70
00:04:19,440 --> 00:04:23,690
to the new trend is brought and placed electrons and antineutrinos

71
00:04:24,860 --> 00:04:28,420
the experimental data that get can contribute to

72
00:04:28,440 --> 00:04:32,130
first test unitarity of the CKM matrix

73
00:04:32,260 --> 00:04:35,780
is fundamental symmetries of the weak interaction investigates

74
00:04:35,780 --> 00:04:38,030
the structure of the weak interaction

75
00:04:38,030 --> 00:04:39,420
for that

76
00:04:39,440 --> 00:04:44,550
you need very precise measurements of the two values to vitality the energy really so

77
00:04:44,550 --> 00:04:48,650
you really precise measurements of the mass of the nuclei

78
00:04:48,670 --> 00:04:52,340
branching ratio between the different processes and fly

79
00:04:52,360 --> 00:04:56,590
so far that moves among the riverside do you need these tracks

80
00:04:56,590 --> 00:04:58,480
i penning trap paul trap

81
00:04:58,480 --> 00:05:02,130
so to really to cool down the the elements and to two

82
00:05:02,150 --> 00:05:05,500
and they almost that phrase to measure the mass

83
00:05:05,520 --> 00:05:07,670
and even more complicated for that

84
00:05:07,710 --> 00:05:11,320
unit correlations between the spins and momenta of the particles involved

85
00:05:11,400 --> 00:05:14,570
so that's why you need polarized nuclei

86
00:05:14,590 --> 00:05:20,110
so in some ways are don't think that the right some sorry some progress under

87
00:05:20,150 --> 00:05:23,400
the direction here at his older again

88
00:05:23,400 --> 00:05:26,400
i can promise

89
00:05:26,690 --> 00:05:28,440
thanks everybody for

90
00:05:28,450 --> 00:05:32,220
coming to the last hurdle that separates it from the

91
00:05:32,240 --> 00:05:34,610
my name is alaska

92
00:05:35,280 --> 00:05:39,200
the remaining our soul two

93
00:05:39,220 --> 00:05:44,260
machine learning applications very exciting field of computer security where we have been working

94
00:05:44,280 --> 00:05:45,670
that article from the first

95
00:05:45,700 --> 00:05:48,510
about four five years

96
00:05:48,540 --> 00:05:49,740
so i will

97
00:05:51,040 --> 00:05:55,060
focus on the topic of network intrusion detection which is one

98
00:05:55,060 --> 00:05:57,450
very important problem that you will see

99
00:05:57,460 --> 00:06:02,990
before i proceed i would like to ask something for audience how many people consider

100
00:06:02,990 --> 00:06:09,670
themselves how many people here consider themselves computer science

101
00:06:11,700 --> 00:06:13,070
more than half

102
00:06:13,100 --> 00:06:14,300
OK so

103
00:06:14,310 --> 00:06:15,920
i was starting and to talk with it

104
00:06:15,930 --> 00:06:18,270
short program included

105
00:06:18,310 --> 00:06:23,490
one of the following program do that you can see on the slide

106
00:06:25,030 --> 00:06:26,350
relatively short

107
00:06:26,410 --> 00:06:28,350
c programs

108
00:06:28,370 --> 00:06:31,150
a few minutes is located and and

109
00:06:31,170 --> 00:07:14,210
we'll have some majority voting

110
00:07:26,900 --> 00:07:32,940
i guess guess guess ready to justified so approach of what the program does allow

111
00:07:32,940 --> 00:07:39,770
for the you know what you think and who thinks the right answer is a

112
00:07:39,860 --> 00:07:43,440
you think that that this one of the great

113
00:07:43,650 --> 00:07:49,090
one of the

114
00:07:49,160 --> 00:07:51,210
he will be OK

115
00:07:51,270 --> 00:07:56,680
one and he was see

116
00:07:56,770 --> 00:07:58,140
let's see

117
00:08:05,680 --> 00:08:13,780
think that you know what's going to happen depending on the pop

118
00:08:17,870 --> 00:08:23,680
i can see let's see what's going to happen understood

119
00:08:28,840 --> 00:08:31,120
in fact this

120
00:08:35,680 --> 00:08:38,610
i cannot say

121
00:08:38,650 --> 00:08:50,620
but this is going sl it's this right something this file

122
00:08:51,590 --> 00:08:54,840
he received complaints something that's there

123
00:08:54,860 --> 00:08:57,990
and that's the problem of

124
00:08:57,990 --> 00:09:01,870
really not

125
00:09:01,930 --> 00:09:06,960
going to sleep for a while in the second no more

126
00:09:07,060 --> 00:09:09,340
do something so

127
00:09:09,340 --> 00:09:12,020
basically this you know read the files

128
00:09:12,030 --> 00:09:14,680
and text in print out of you know more

129
00:09:14,740 --> 00:09:16,430
diagnostic information

130
00:09:16,440 --> 00:09:19,030
we'll see what we're

131
00:09:19,340 --> 00:09:26,990
this is something else

132
00:09:27,870 --> 00:09:31,900
there are no

133
00:09:33,840 --> 00:09:37,020
so i started the program and sense of the the file

134
00:09:37,030 --> 00:09:39,480
while the program

135
00:09:39,500 --> 00:09:40,940
and as you know

136
00:09:40,950 --> 00:09:42,580
show something else

137
00:09:42,590 --> 00:09:45,900
so it's really sort of waiting and

138
00:09:45,960 --> 00:09:50,030
what's going to happen to the files going to see what holland

139
00:09:50,050 --> 00:09:54,260
after twenty years after the waiting period has elapsed we the files

140
00:09:54,350 --> 00:09:57,020
and this is what happened

141
00:09:57,080 --> 00:10:00,530
OK let's try something else

142
00:10:02,090 --> 00:10:03,470
now what

143
00:10:03,510 --> 00:10:09,080
disappeared from the map

144
00:10:09,090 --> 00:10:10,650
one the cycle

145
00:10:11,700 --> 00:10:13,750
that's one

146
00:10:13,850 --> 00:10:16,460
that's right

147
00:10:16,650 --> 00:10:26,350
in a sense that it was predictable so it's funny here

148
00:10:26,470 --> 00:10:29,690
o thing happened

149
00:10:29,730 --> 00:10:31,020
one down

150
00:10:31,250 --> 00:10:37,880
well OK

151
00:10:37,890 --> 00:10:40,210
we don't know something

152
00:10:40,220 --> 00:10:42,470
there's a difference

153
00:10:52,970 --> 00:10:56,400
private sector that you can see from the

154
00:10:56,400 --> 00:10:59,600
from the

155
00:10:59,610 --> 00:11:03,940
the text that you my login prompts

156
00:11:03,950 --> 00:11:05,710
now for something else

157
00:11:05,900 --> 00:11:23,780
and writers programmers who

158
00:11:24,020 --> 00:11:32,480
nothing happened

159
00:11:32,520 --> 00:11:39,900
so nothing happened

160
00:11:39,910 --> 00:11:43,250
again have a little more difficult to do this the

161
00:11:43,260 --> 00:11:45,170
so that's the problem is not really

162
00:11:45,220 --> 00:11:47,190
that's reliable is that

163
00:11:47,270 --> 00:11:48,750
this act

164
00:11:48,860 --> 00:11:52,210
to guess the return address of the program

165
00:11:52,260 --> 00:11:54,710
and that's pretty tricky business

166
00:11:57,820 --> 00:12:00,840
unintentional so if you see what it's trying to do

167
00:12:00,860 --> 00:12:05,440
this allows us here

168
00:12:10,540 --> 00:12:14,000
that we need some process information from the running process and

169
00:12:14,420 --> 00:12:17,900
find where the fact of this process is located

170
00:12:17,910 --> 00:12:19,600
and then

171
00:12:19,670 --> 00:12:21,980
called the program actually exploit

172
00:12:22,030 --> 00:12:25,450
that uses factor information to

173
00:12:25,630 --> 00:12:29,110
well that rights information into the file

174
00:12:29,130 --> 00:12:30,400
it's going to to read

175
00:12:30,410 --> 00:12:34,140
the program that we seen contains about vulnerability

176
00:12:35,950 --> 00:12:37,600
is the place from

177
00:12:37,690 --> 00:12:39,170
but the problem

178
00:12:39,760 --> 00:12:40,860
we can look at this

179
00:12:47,110 --> 00:12:49,710
actually doing exploits

180
00:12:49,720 --> 00:12:52,700
it contains something that will help

181
00:12:52,750 --> 00:12:57,290
which is binary code used for system over

182
00:12:57,300 --> 00:12:58,320
and then

183
00:12:58,540 --> 00:13:03,590
yes the return address

184
00:13:03,590 --> 00:13:07,150
build the barbarism from jump address the bar is not

185
00:13:08,280 --> 00:13:09,900
except information

186
00:13:09,900 --> 00:13:13,410
and while the rights information file

187
00:13:13,440 --> 00:13:17,730
and this this this information to get overflowed into about

188
00:13:17,750 --> 00:13:21,190
and the way it works is that you know if you recall how

189
00:13:21,200 --> 00:13:23,220
now parameters are stored in the stack

190
00:13:23,280 --> 00:13:26,080
the above still together with the return

191
00:13:26,090 --> 00:13:27,730
overflow buffer

192
00:13:27,760 --> 00:13:30,410
information in this graph is going to be allowed

193
00:13:30,530 --> 00:13:33,400
into the area where the return address is stored

194
00:13:33,400 --> 00:13:36,420
and if we are lucky enough to guess the right return address

195
00:13:36,440 --> 00:13:38,470
you know this the

196
00:13:38,500 --> 00:13:40,170
when returning from the function

197
00:13:40,190 --> 00:13:42,090
this command is john back

198
00:13:42,110 --> 00:13:43,890
two the return address

199
00:13:43,900 --> 00:13:46,230
which is somewhere in the middle of the buffer

200
00:13:46,280 --> 00:13:49,110
and this is what the shell code

201
00:13:49,210 --> 00:13:52,680
at the end of the programme instead of returning to the main function to the

202
00:13:52,680 --> 00:13:55,100
operating system we're jumping back to

203
00:13:55,660 --> 00:14:00,390
executable code that has been written into this buffer and open the show

204
00:14:00,440 --> 00:14:01,380
this is

205
00:14:01,390 --> 00:14:04,720
in general how does whole flow words that actually

206
00:14:04,770 --> 00:14:05,880
not easy to prove

207
00:14:06,390 --> 00:14:07,910
the problem nine

208
00:14:07,920 --> 00:14:10,950
exploits of this code publicly available

209
00:14:11,010 --> 00:14:14,530
so i basically had to program during meeting for

210
00:14:14,540 --> 00:14:15,800
a couple of hours

211
00:14:15,980 --> 00:14:21,440
although i'm not really tacky outfits

212
00:14:21,500 --> 00:14:22,490
OK so

213
00:14:22,750 --> 00:14:34,620
what is about seven more alarming to think about it is

214
00:14:36,750 --> 00:14:38,110
the hackers are

215
00:14:38,130 --> 00:14:41,480
very good at distributing this information to the community

216
00:14:41,490 --> 00:14:44,790
there some websites

217
00:14:44,790 --> 00:14:48,060
the rank in l

218
00:14:49,640 --> 00:14:51,270
OK where

219
00:14:51,270 --> 00:14:56,380
r where the two here has two parts one is the access

220
00:14:57,720 --> 00:14:59,230
and the other is the

221
00:15:03,950 --> 00:15:10,180
so that's sort of what they did and one of the nice properties of this

222
00:15:10,180 --> 00:15:14,040
is that if it turns out that there's locality in the access patterns

223
00:15:14,080 --> 00:15:15,450
if once you

224
00:15:15,470 --> 00:15:18,590
it's not just the static distribution

225
00:15:18,590 --> 00:15:23,180
a rather one-sided access something if it's more likely and access it again

226
00:15:23,210 --> 00:15:25,660
which tends to be the case for many

227
00:15:25,680 --> 00:15:30,370
input types of patterns this responds well to locality

228
00:15:30,410 --> 00:15:32,720
because it's going to be up near the front

229
00:15:32,730 --> 00:15:37,620
if i access it very soon after i that of this what's called temporal locality

230
00:15:37,620 --> 00:15:42,140
meaning the entire might tend access thing so maybe the axis some things very hard

231
00:15:42,140 --> 00:15:44,620
for a while and get very cold

232
00:15:44,670 --> 00:15:48,580
this type of album responds very well to the hotness

233
00:15:49,530 --> 00:15:51,470
of the accessing

234
00:15:51,590 --> 00:15:55,040
OK so responds well to locality

235
00:16:08,280 --> 00:16:11,360
so i

236
00:16:11,380 --> 00:16:13,620
so this is sort of what was known

237
00:16:13,650 --> 00:16:17,120
up to the point that are very famous paper was written

238
00:16:17,160 --> 00:16:18,630
by r

239
00:16:18,640 --> 00:16:22,110
dennis later and bob tarjan

240
00:16:23,450 --> 00:16:25,950
they took a totally different approach

241
00:16:25,990 --> 00:16:28,150
looking at this kind of problem

242
00:16:28,200 --> 00:16:33,470
OK in approach that now you see everywhere from analysis of cashing in

243
00:16:36,760 --> 00:16:41,730
high-performance processors to analyses of disk paging

244
00:16:42,510 --> 00:16:46,370
just a huge number of applications of this basic technique

245
00:16:46,390 --> 00:16:48,960
and that's the technique of comparative analysis

246
00:16:49,010 --> 00:16:58,100
so here's the definition

247
00:17:01,380 --> 00:17:05,440
on line

248
00:17:17,140 --> 00:17:19,910
if there exists

249
00:17:19,920 --> 00:17:22,650
constant k

250
00:17:22,700 --> 00:17:24,960
such that

251
00:17:24,970 --> 00:17:27,660
for any sequence

252
00:17:27,680 --> 00:17:32,140
as of operations

253
00:17:32,260 --> 00:17:37,100
of s

254
00:17:37,120 --> 00:17:40,630
using algorithm a

255
00:17:40,670 --> 00:17:44,340
is bounded by alpha times the cost

256
00:17:50,650 --> 00:17:56,890
bounded by alpha times the cost of of that work is the optimal

257
00:18:03,620 --> 00:18:08,970
OK so the optimal offline the one that knows the whole sequence

258
00:18:09,010 --> 00:18:14,270
and does the does the absolute best it could do on that sequence

259
00:18:14,310 --> 00:18:18,540
OK that's the that's this cost here this is sometimes called

260
00:18:19,110 --> 00:18:20,980
god's algorithm

261
00:18:21,010 --> 00:18:25,610
not to bring religion into the classroom

262
00:18:25,620 --> 00:18:29,190
or to offend anybody but that is what people sometimes call it case of the

263
00:18:29,190 --> 00:18:30,910
fully on missions

264
00:18:30,940 --> 00:18:34,990
knows absolutely the best thing that could be differences in the future

265
00:18:35,010 --> 00:18:36,880
the whole works OK

266
00:18:36,930 --> 00:18:41,400
just apply that that's what ops algorithm is

267
00:18:41,450 --> 00:18:46,380
and what we're saying is that the cost is basically

268
00:18:46,420 --> 00:18:50,850
whatever the alpha factor it could be a function of things you could be a

269
00:18:51,940 --> 00:18:54,400
OK times whatever

270
00:18:54,410 --> 00:18:57,170
whatever the best algorithm is

271
00:18:57,230 --> 00:18:58,810
k clusters of

272
00:18:58,810 --> 00:19:01,930
potential for constant out here

273
00:19:01,990 --> 00:19:07,640
so for example if alpha is too

274
00:19:07,690 --> 00:19:09,650
then we say it's too competitive

275
00:19:09,690 --> 00:19:13,110
that means you're going to do and worst twice

276
00:19:13,150 --> 00:19:16,960
the album has all the information but you're doing it online

277
00:19:16,970 --> 00:19:18,530
for example

278
00:19:19,420 --> 00:19:22,160
so really pretty pretty powerful

279
00:19:22,180 --> 00:19:24,850
notion and what's interesting about this

280
00:19:24,850 --> 00:19:28,590
it's not even clear these things should exist to my mind

281
00:19:29,460 --> 00:19:30,540
what what's

282
00:19:30,540 --> 00:19:32,070
which pretty

283
00:19:32,120 --> 00:19:39,960
remarkable about about this i think is that there's no assumption of distribution of probability

284
00:19:39,960 --> 00:19:42,210
distribution or anything

285
00:19:42,240 --> 00:19:45,830
it's whatever the sequences that you give it

286
00:19:45,920 --> 00:19:49,150
within a factor of alpha essentially of

287
00:19:49,210 --> 00:19:51,640
the best our

288
00:19:51,660 --> 00:19:53,550
which is pretty remarkable

289
00:19:53,560 --> 00:19:58,350
and so we prove the following theorem

290
00:19:58,350 --> 00:20:04,360
which is the one that's later encourage improved

291
00:20:04,370 --> 00:20:07,330
and that is the MTF

292
00:20:07,420 --> 00:20:10,410
this work compared

293
00:20:10,510 --> 00:20:16,880
for self organizing lists

294
00:20:23,900 --> 00:20:29,390
so the idea here is that

295
00:20:30,480 --> 00:20:34,950
the adversary says oh i'm always going to access the thing at the end of

296
00:20:34,950 --> 00:20:36,580
the list

297
00:20:36,620 --> 00:20:38,430
like we said in the beginning

298
00:20:38,430 --> 00:20:40,340
it's actually equal to

299
00:20:40,390 --> 00:20:44,180
this sigmoidal function here or the

300
00:20:45,280 --> 00:20:47,620
parlance the logit function

301
00:20:47,640 --> 00:20:49,410
right so this is the

302
00:20:52,180 --> 00:20:55,100
i believe it that as the log function

303
00:20:55,120 --> 00:20:57,890
OK so i can just some simple

304
00:20:57,930 --> 00:21:01,990
one all primary school algebra will get you to that the

305
00:21:02,010 --> 00:21:05,470
so we want to define the likelihood function

306
00:21:05,490 --> 00:21:09,660
so the likelihood clearly for each point is going to be the probability of the

307
00:21:09,660 --> 00:21:10,950
class label

308
00:21:11,590 --> 00:21:13,700
target values

309
00:21:13,720 --> 00:21:15,720
given x and

310
00:21:15,740 --> 00:21:17,640
and this will define

311
00:21:17,660 --> 00:21:20,010
a binomial distribution

312
00:21:20,090 --> 00:21:24,800
so we can write the component of the likelihood for each of the n data

313
00:21:26,760 --> 00:21:30,120
simple is this unnormalized binomial

314
00:21:30,140 --> 00:21:32,390
so when two zero

315
00:21:33,280 --> 00:21:38,760
the likelihood component is one minus the probability of class one in other words the

316
00:21:38,760 --> 00:21:40,100
probability that the

317
00:21:40,160 --> 00:21:42,430
target values classes

318
00:21:42,450 --> 00:21:44,100
and when it's one

319
00:21:44,930 --> 00:21:47,240
the likelihood component will just be

320
00:21:47,280 --> 00:21:49,030
this here

321
00:21:49,050 --> 00:21:53,590
so given the definition of our log it functions which we obtained

322
00:21:53,870 --> 00:21:56,600
the previous slide we just plug that

323
00:21:57,870 --> 00:21:58,930
and here

324
00:21:59,640 --> 00:22:02,510
some simple schoolboy algebra shows that

325
00:22:03,590 --> 00:22:07,220
the component of our likelihood is just going to be

326
00:22:07,570 --> 00:22:11,890
this exponential of our linear response

327
00:22:11,910 --> 00:22:14,010
this to the power of the binary

328
00:22:14,240 --> 00:22:15,760
variable tn

329
00:22:15,760 --> 00:22:17,990
and then normalized by

330
00:22:18,010 --> 00:22:20,010
this here

331
00:22:20,030 --> 00:22:22,700
so the total likelihood

332
00:22:22,700 --> 00:22:27,410
it's just going to be the product over all end of this year

333
00:22:28,410 --> 00:22:33,360
we are all bayesians no overall business for the next twenty minutes

334
00:22:35,370 --> 00:22:38,030
let's just take a bayesian viewpoint

335
00:22:40,640 --> 00:22:43,950
as we are given in the previous lecture

336
00:22:43,950 --> 00:22:48,820
we'll just encoder preference for simple models

337
00:22:48,910 --> 00:22:51,720
well a lot of the w is going to be close to zero

338
00:22:51,720 --> 00:22:55,620
so we will use the same kind used previously

339
00:22:55,660 --> 00:22:57,680
as i said of likelihood

340
00:22:57,720 --> 00:22:59,490
it's not just

341
00:22:59,510 --> 00:23:02,450
standard product four

342
00:23:02,550 --> 00:23:07,430
so the overall joint likelihood of our target values discrete target values given the model

343
00:23:09,640 --> 00:23:11,620
is just going to be

344
00:23:11,680 --> 00:23:13,530
the product of likelihood

345
00:23:13,550 --> 00:23:14,820
on the

346
00:23:14,820 --> 00:23:18,890
the product of of prior

347
00:23:19,740 --> 00:23:24,680
because of bayesians what we want is the posterior

348
00:23:24,700 --> 00:23:29,090
so we need the marginal likelihood

349
00:23:29,120 --> 00:23:32,240
and the marginal likelihood requires to

350
00:23:32,240 --> 00:23:35,910
former multi dimensional integral over w

351
00:23:36,200 --> 00:23:38,570
of this function is here

352
00:23:38,570 --> 00:23:40,640
so this is where the party ends

353
00:23:40,640 --> 00:23:41,800
i'm afraid

354
00:23:43,370 --> 00:23:44,300
for all

355
00:23:44,320 --> 00:23:48,030
really reasonably interesting models this integral

356
00:23:48,070 --> 00:23:50,760
is not analytic

357
00:23:50,820 --> 00:24:01,410
and the nice analytic representations of the posterior and of the posterior predictive

358
00:24:03,590 --> 00:24:06,390
all falls down

359
00:24:07,760 --> 00:24:11,950
how do make any progress

360
00:24:13,340 --> 00:24:15,010
we could

361
00:24:15,050 --> 00:24:19,280
here's a monte carlo estimate

362
00:24:21,010 --> 00:24:25,160
basically sampling from this which we can do

363
00:24:25,180 --> 00:24:28,090
and then just taking a discrete something we know that

364
00:24:28,090 --> 00:24:30,050
as the number of samples

365
00:24:30,070 --> 00:24:34,740
of monte carlo estimate increases then we will converge

366
00:24:35,180 --> 00:24:36,450
to this integral

367
00:24:36,470 --> 00:24:41,640
but if we know sampling in c one thousand dimensions as we will be doing

368
00:24:41,640 --> 00:24:47,180
this afternoon seven hundred then we're gonna need an awful lot of samples before we

369
00:24:47,180 --> 00:24:52,550
can get a reasonable estimate of this and it becomes totally feasible

370
00:24:52,550 --> 00:24:56,910
and so we could use markov chain monte carlo

371
00:24:56,930 --> 00:25:03,860
to obtain samples from the posterior p of w given x of t

372
00:25:03,890 --> 00:25:06,570
and there are methods like

373
00:25:06,620 --> 00:25:11,780
well in this example here we could use the metropolis hastings algorithm which basically requires

374
00:25:11,780 --> 00:25:12,740
us to

375
00:25:12,740 --> 00:25:20,180
i define a markov chain for the markov chain states are the values of our

376
00:25:21,910 --> 00:25:25,460
and although we need to do is they compute

377
00:25:25,510 --> 00:25:27,240
the ratio

378
00:25:27,240 --> 00:25:29,530
of joint likelihood

379
00:25:29,550 --> 00:25:35,160
because computing every ratio the joint likelihood for one set of parameters and a new

380
00:25:35,160 --> 00:25:41,180
set of proposed parameters the troublesome marginal likelihood the disappears

381
00:25:41,180 --> 00:25:42,820
so we could use

382
00:25:42,870 --> 00:25:46,240
very standard markov chain monte carlo methods

383
00:25:47,410 --> 00:25:49,430
two sample from the posterior

384
00:25:49,490 --> 00:25:54,470
and then use those samples from the posterior to do you wanted to make monte

385
00:25:55,570 --> 00:25:58,740
approximations to predictive

386
00:26:01,120 --> 00:26:02,360
another way

387
00:26:02,860 --> 00:26:07,780
and problem we switch on vault was the machine learning community is that we make

388
00:26:07,780 --> 00:26:11,950
an approximation of the posterior with a tractable distribution

389
00:26:11,990 --> 00:26:18,840
the tomorrow and you mention variational methods distance available into a moment to finish

390
00:26:19,870 --> 00:26:21,010
ten so

391
00:26:21,030 --> 00:26:25,180
and the five minutes left

392
00:26:25,260 --> 00:26:27,840
is that attains a quarter two

393
00:26:27,840 --> 00:26:30,160
OK i think you should be able to get through this

394
00:26:30,180 --> 00:26:32,030
well maybe not

395
00:26:32,050 --> 00:26:37,970
tomorrow i'll talk in more detail about variational methods

396
00:26:39,180 --> 00:26:44,160
someone straightforward problems but another way of

397
00:26:44,180 --> 00:26:48,640
making a deterministic approximation is to just say well let's

398
00:26:48,640 --> 00:26:52,880
every engineer will tell you yes every engineer will tell you that as a sort

399
00:26:52,910 --> 00:26:56,820
for voice should not be the same as the sense for images

400
00:26:56,890 --> 00:26:58,340
OK that's why

401
00:26:58,360 --> 00:27:00,500
i'm using this as the microphone

402
00:27:00,540 --> 00:27:03,080
and the ladies using that to filming

403
00:27:03,090 --> 00:27:05,290
OK so the other way around

404
00:27:06,240 --> 00:27:09,590
so this issue of universality is kind of

405
00:27:09,660 --> 00:27:14,100
mathematically very interesting but in practice is very clear to everybody that we should be

406
00:27:14,100 --> 00:27:15,980
able to do better

407
00:27:16,720 --> 00:27:18,710
and this has been addressed

408
00:27:18,750 --> 00:27:21,870
of optimizing this sensing

409
00:27:21,930 --> 00:27:25,890
not taking random this has been addressed by making labarbera

410
00:27:25,910 --> 00:27:28,240
five address that an our ago goal

411
00:27:28,260 --> 00:27:29,810
his previous talk

412
00:27:29,810 --> 00:27:35,410
there are more so upper bounded sum very nice work about and the sensing that

413
00:27:35,410 --> 00:27:38,600
this actually optimise computationally

414
00:27:38,640 --> 00:27:43,810
it does as good as as the random but computationally much more efficient

415
00:27:43,810 --> 00:27:46,520
and k

416
00:27:46,540 --> 00:27:49,890
we are going to try to optimize the sensing

417
00:27:49,910 --> 00:27:54,200
and addiction both of them at the same time

418
00:27:54,200 --> 00:27:56,600
that's what we're going to try to do

419
00:27:56,870 --> 00:28:01,330
so here's the formulation and i change notation to be in agreement more with the

420
00:28:01,330 --> 00:28:04,970
notation used in the compressed sensing bank explained to you

421
00:28:04,990 --> 00:28:07,620
it's all my data

422
00:28:07,680 --> 00:28:08,910
vector form

423
00:28:13,010 --> 00:28:15,430
is now my dictionary

424
00:28:15,450 --> 00:28:18,310
this site is what was the

425
00:28:19,640 --> 00:28:21,950
and these are my qualifications

426
00:28:21,970 --> 00:28:24,280
that i'm going to request to be sparse

427
00:28:24,330 --> 00:28:27,580
so this is what i had before

428
00:28:27,640 --> 00:28:29,600
and this is what they had before

429
00:28:29,620 --> 00:28:32,640
this to tax

430
00:28:32,640 --> 00:28:33,810
is that OK

431
00:28:33,830 --> 00:28:37,760
so i'm trying to represent my image all my images in as far as fashion

432
00:28:37,760 --> 00:28:40,220
with elements of my diction

433
00:28:40,220 --> 00:28:43,280
but i have to make two more requirements

434
00:28:43,290 --> 00:28:44,660
one is

435
00:28:44,660 --> 00:28:47,350
i'm not going to see the image again in my life

436
00:28:47,370 --> 00:28:49,740
because i'm in a sense the image

437
00:28:49,790 --> 00:28:52,040
with a matrix multiplication

438
00:28:52,080 --> 00:28:55,350
so this is what i going actually see

439
00:28:55,370 --> 00:28:57,680
and it's going to be multiplied by

440
00:28:57,740 --> 00:29:00,640
this sense in matrix

441
00:29:01,470 --> 00:29:04,930
during the training i do have access to both

442
00:29:05,600 --> 00:29:09,290
during the reconstruction this term will disappear

443
00:29:10,330 --> 00:29:12,700
so this is my sense matrix

444
00:29:12,810 --> 00:29:15,600
forget about this for one second case

445
00:29:15,620 --> 00:29:17,280
look only at this

446
00:29:17,410 --> 00:29:21,040
and now optimise for my dictionary

447
00:29:21,040 --> 00:29:22,850
four michael efficiency

448
00:29:22,890 --> 00:29:23,850
as before

449
00:29:23,870 --> 00:29:25,640
and for the same semantics

450
00:29:25,660 --> 00:29:28,290
all at the same time

451
00:29:28,350 --> 00:29:32,160
in kind of according to the same type of approach

452
00:29:33,310 --> 00:29:37,220
if i decide not to optimise for my sensing

453
00:29:37,280 --> 00:29:39,260
i mean the previous case

454
00:29:39,290 --> 00:29:42,720
but and better than the previous case i'm going to show you because i'm taking

455
00:29:42,720 --> 00:29:44,740
into account the sensing

456
00:29:44,820 --> 00:29:48,100
if for some reason you fix the dictionary

457
00:29:48,100 --> 00:29:49,780
i'm still going to go

458
00:29:49,790 --> 00:29:55,180
and optimize the sense in which is related to a work and you advise what

459
00:29:55,220 --> 00:29:57,470
by the optimized for everything

460
00:29:58,580 --> 00:30:00,470
there's one more conditions

461
00:30:00,490 --> 00:30:04,700
that appears here which is related to the uncertainty problem before

462
00:30:04,720 --> 00:30:09,060
these are that condition comes from the theory of compressed sensing it says that these

463
00:30:09,060 --> 00:30:10,810
two guys

464
00:30:10,830 --> 00:30:16,430
can now be arbitrary they should be related among themselves

465
00:30:17,100 --> 00:30:21,450
and that's pretty intuitive you can not sense in the same direction of your dictionary

466
00:30:21,450 --> 00:30:23,580
are not getting knocked

467
00:30:24,970 --> 00:30:26,490
you're getting enough

468
00:30:26,510 --> 00:30:28,720
and that expressed

469
00:30:28,740 --> 00:30:30,450
as there are IP

470
00:30:30,470 --> 00:30:32,700
restricted isometry property

471
00:30:32,790 --> 00:30:37,490
and that's related to making the gram matrix corresponding to this

472
00:30:38,780 --> 00:30:41,390
as close as possible to the identity matrix

473
00:30:41,410 --> 00:30:43,810
so that also shows up here

474
00:30:43,810 --> 00:30:46,510
so we have two new types

475
00:30:46,520 --> 00:30:48,950
and we have an optimized for

476
00:30:49,760 --> 00:30:51,970
before i show you the pictures

477
00:30:52,020 --> 00:30:56,290
this is why we get forget about this this is just a parameters this alpha

478
00:30:57,870 --> 00:31:01,580
this is classic compressed sensing mean square error

479
00:31:01,620 --> 00:31:06,120
what i mean by classical compressed sensing do around the matrix

480
00:31:06,180 --> 00:31:11,350
and one of the shared dictionary that's you somehow learned in this case we learn

481
00:31:11,350 --> 00:31:14,470
a dictionary with case b

482
00:31:14,490 --> 00:31:16,120
this is what we are

483
00:31:16,140 --> 00:31:17,720
this is where we are

484
00:31:17,740 --> 00:31:22,080
if we optimise for the dictionary and sensing matrix

485
00:31:22,100 --> 00:31:24,180
about four times improvement

486
00:31:24,180 --> 00:31:26,410
in our mean square

487
00:31:26,430 --> 00:31:28,870
and these are intermediate cases

488
00:31:28,890 --> 00:31:29,870
for example

489
00:31:29,910 --> 00:31:31,870
you fix this same semantics

490
00:31:31,870 --> 00:31:33,890
by optimizing dictionary

491
00:31:35,040 --> 00:31:37,520
that you're not going to seeks you can see why

492
00:31:37,540 --> 00:31:39,580
so it's a different dictionary

493
00:31:39,620 --> 00:31:41,390
then you end up in the media

494
00:31:41,410 --> 00:31:45,930
or you fix the dictionary and you optimize for sensing matrix i forgot which one

495
00:31:45,930 --> 00:31:48,390
is the two because they are very similar

496
00:31:53,600 --> 00:31:55,560
for the learning stage

497
00:31:55,580 --> 00:31:57,100
i know it

498
00:31:57,120 --> 00:32:01,240
why assimilated i multiply by feel

499
00:32:03,180 --> 00:32:05,370
say that again sorry

500
00:32:05,390 --> 00:32:09,890
why is fy x

501
00:32:11,810 --> 00:32:14,620
why sphere x y is equal phi x

502
00:32:14,640 --> 00:32:19,240
now they had this i know because i can see more like i know it

503
00:32:19,260 --> 00:32:23,350
and i know what the sensing will see because i'm simulating the same sort

504
00:32:23,390 --> 00:32:25,870
in the reconstruction you don't know this

505
00:32:25,890 --> 00:32:28,140
so all the images i show you

506
00:32:28,180 --> 00:32:31,290
after we have learned and you only do this

507
00:32:31,310 --> 00:32:32,490
with this

508
00:32:32,580 --> 00:32:34,790
that's a classic compressed sensing

509
00:32:35,910 --> 00:32:37,660
OK but in the learning

510
00:32:37,720 --> 00:32:42,700
i assume for this case that i know both there will be applications that people

511
00:32:42,700 --> 00:32:46,240
say on all you never have access to the original image

512
00:32:47,010 --> 00:32:50,490
then i will have this but we have this i we still value to optimize

513
00:32:50,490 --> 00:32:54,200
for addiction sort

514
00:32:59,540 --> 00:33:07,410
you see that

515
00:33:07,510 --> 00:33:13,850
he said he said for

516
00:33:13,870 --> 00:33:19,850
was all because i learned from images

517
00:33:19,870 --> 00:33:22,740
i'm going to reconstruct images

518
00:33:22,740 --> 00:33:26,140
and the my images are not in the database are used for learning

519
00:33:26,160 --> 00:33:28,470
but it's not going to learn images

520
00:33:28,490 --> 00:33:30,060
i reconstruct speech

521
00:33:30,080 --> 00:33:34,700
i'm going to learn images to reconstruct images that that's part of the point the

522
00:33:34,700 --> 00:33:35,740
point here is

523
00:33:35,780 --> 00:33:37,600
university is fine

524
00:33:37,620 --> 00:33:39,640
but i'm trying to design a camera

525
00:33:39,680 --> 00:33:43,640
OK so let's use images to design my camera life tomorrow

526
00:33:43,680 --> 00:33:45,470
you want to use the camera

527
00:33:45,490 --> 00:33:47,160
four speech

528
00:33:47,180 --> 00:33:50,760
classic compressed sensing will do much better and we will do

529
00:33:50,790 --> 00:33:53,560
random mattresses with do much better and we will do

530
00:33:53,580 --> 00:33:55,290
but it will do much wars

531
00:33:55,310 --> 00:33:58,390
that if you tell me in advance if you're going to use speech

532
00:33:58,410 --> 00:34:01,010
OK if you're to be me than

533
00:34:01,040 --> 00:34:02,100
i followed

534
00:34:05,700 --> 00:34:07,970
just believe the images

535
00:34:07,970 --> 00:34:11,510
so i'm going to show you only these two extremes

536
00:34:12,910 --> 00:34:15,290
classical compressed sensing

537
00:34:15,310 --> 00:34:17,700
we have sensing twice

538
00:34:17,700 --> 00:34:22,010
this sparsity level which is about what the theory tells you to do

539
00:34:22,060 --> 00:34:26,780
our result with optimise dictionary and optimize sensing matrix

540
00:34:26,810 --> 00:34:28,910
so these

541
00:34:28,930 --> 00:34:33,540
it's really is not just numbers so if he was like point one db we

542
00:34:33,540 --> 00:34:36,650
properties that can be computed

543
00:34:36,860 --> 00:34:39,490
can be global property sold in

544
00:34:39,550 --> 00:34:43,720
the global that means for the whole network so the number of net votes in

545
00:34:43,720 --> 00:34:47,890
the network the number of edges the number of car parks and so on the

546
00:34:47,890 --> 00:34:52,590
number of strong components we components and then low-cost so

547
00:34:52,600 --> 00:34:53,770
this would be for

548
00:34:53,790 --> 00:34:56,860
as a single vertex for single

549
00:34:56,860 --> 00:34:58,310
line and so on

550
00:34:58,350 --> 00:35:03,150
this will be the degrees of vertices the course each other

551
00:35:03,170 --> 00:35:05,330
how we know how the court

552
00:35:05,380 --> 00:35:10,780
number to each vertex belongs to the highest score and so on

553
00:35:10,840 --> 00:35:22,150
let's look at for example at the edinburgh associative thesaurus network as one network where

554
00:35:22,150 --> 00:35:25,780
we compute something where

555
00:35:25,830 --> 00:35:28,000
what this network is about

556
00:35:28,020 --> 00:35:30,450
it's about twelve

557
00:35:30,450 --> 00:35:32,060
the survey was made

558
00:35:32,890 --> 00:35:42,600
all students each student has been given a hundred sixty million words

559
00:35:42,620 --> 00:35:44,620
and they had to

560
00:35:45,270 --> 00:35:49,290
they had to write their associations to these words as

561
00:35:49,310 --> 00:35:52,480
quick as they could so that's how we got here

562
00:35:53,640 --> 00:35:56,350
words and associations to these works

563
00:35:56,370 --> 00:35:57,640
so the

564
00:35:57,810 --> 00:35:59,890
this this would be the words and

565
00:35:59,910 --> 00:36:02,850
link the are would be associations

566
00:36:02,870 --> 00:36:06,180
and what we can do here

567
00:36:06,180 --> 00:36:10,930
we were we can first three g network and then get the general information so

568
00:36:11,730 --> 00:36:15,660
global information about the network if i do that

569
00:36:15,690 --> 00:36:17,250
and by

570
00:36:17,250 --> 00:36:20,980
i first read the network

571
00:36:21,000 --> 00:36:22,600
and this will be

572
00:36:22,620 --> 00:36:25,060
the right one

573
00:36:29,310 --> 00:36:35,080
it's quite large network so it takes a bit of time to read it

574
00:36:36,480 --> 00:36:37,770
and if you go to

575
00:36:37,790 --> 00:36:42,160
in network in general

576
00:36:42,160 --> 00:36:44,500
sorry i just close this

577
00:36:44,500 --> 00:36:48,600
so here we have in the report window we get

578
00:36:53,140 --> 00:36:58,640
general information about the network and what we see from here that there's

579
00:36:58,730 --> 00:37:01,660
how much

580
00:37:02,210 --> 00:37:03,640
thirty more than

581
00:37:03,960 --> 00:37:06,180
three hundred twenty five thousand

582
00:37:07,680 --> 00:37:10,620
and there's

583
00:37:10,640 --> 00:37:15,140
two to twenty two

584
00:37:17,120 --> 00:37:19,100
and we get

585
00:37:19,140 --> 00:37:21,980
also the densities

586
00:37:22,390 --> 00:37:24,730
the density of of the network here

587
00:37:24,730 --> 00:37:26,410
so there will be

588
00:37:26,450 --> 00:37:27,930
three main

589
00:37:28,000 --> 00:37:28,750
o thing

590
00:37:28,790 --> 00:37:31,410
helps sorry

591
00:37:31,410 --> 00:37:36,620
about this network and where is the same also here

592
00:37:36,640 --> 00:37:42,000
and what we can do next for example we can calculate the degrees

593
00:37:42,000 --> 00:37:46,180
of each registers and we see here that the

594
00:37:46,210 --> 00:37:47,980
these are

595
00:37:48,000 --> 00:37:50,210
the most

596
00:37:50,230 --> 00:37:52,210
the words with closed

597
00:37:54,140 --> 00:37:56,560
those with most links

598
00:37:56,560 --> 00:38:01,100
they are probably the words the team words

599
00:38:03,100 --> 00:38:08,410
i can do i can also show you how to

600
00:38:08,430 --> 00:38:15,460
how to do that for example if you go to network partitions degree all degrees

601
00:38:15,460 --> 00:38:16,870
so that's how you know

602
00:38:16,870 --> 00:38:22,560
you get to the really degree for each vertex has the partition

603
00:38:23,120 --> 00:38:27,480
this is already done in here's you get all the partition

604
00:38:27,500 --> 00:38:31,160
if you want if you want to see

605
00:38:31,180 --> 00:38:33,790
what what how they look

606
00:38:33,810 --> 00:38:38,250
you can of course get to info and partition but i want to that because

607
00:38:38,250 --> 00:38:40,620
i want to show you something else so

608
00:38:42,210 --> 00:38:48,120
attached is degrees to each vertex is vertex it's degree to each vertex so i

609
00:38:48,120 --> 00:38:50,710
want to get the properties of the vertices

610
00:38:50,710 --> 00:38:52,040
so what i'll do

611
00:38:52,060 --> 00:38:52,770
and go

612
00:38:52,810 --> 00:38:53,890
the partition

613
00:38:53,910 --> 00:38:55,810
and i'm going to make a vector

614
00:38:55,810 --> 00:38:56,870
that means

615
00:38:56,890 --> 00:38:58,810
the from the degree partition

616
00:39:00,500 --> 00:39:04,500
we just transform the degree partition two vectors

617
00:39:04,520 --> 00:39:09,480
and that's how that's that these that each vertex now has its well as a

618
00:39:09,480 --> 00:39:12,270
number of degree

619
00:39:13,910 --> 00:39:15,520
and there are

620
00:39:16,430 --> 00:39:18,790
what i want to really to do with this

621
00:39:19,100 --> 00:39:21,620
is i want to show that pi

622
00:39:21,640 --> 00:39:25,520
it also supports an interaction with statistical

623
00:39:25,540 --> 00:39:26,690
programs are

624
00:39:26,960 --> 00:39:30,950
and here i want to use

625
00:39:31,080 --> 00:39:35,500
this vector this degree british the degree vectors

626
00:39:35,520 --> 00:39:39,430
two it's two exported to are and two

627
00:39:40,710 --> 00:39:42,350
calculate world too

628
00:39:42,370 --> 00:39:44,460
to draw

629
00:39:46,290 --> 00:39:48,580
how do we do that we go to tools

630
00:39:48,640 --> 00:39:50,450
they are in santa are

631
00:39:50,460 --> 00:39:52,750
course we first have to locate are

632
00:39:52,810 --> 00:39:54,250
and if

633
00:39:54,310 --> 00:39:57,040
because we have to have it on our computer

634
00:39:57,060 --> 00:39:58,640
and make things

635
00:39:58,680 --> 00:40:00,160
and we

636
00:40:00,210 --> 00:40:04,080
so like current vector

637
00:40:04,100 --> 00:40:05,330
now an our

638
00:40:05,330 --> 00:40:08,290
a window opens

639
00:40:08,310 --> 00:40:11,830
and this is the director did we

640
00:40:14,410 --> 00:40:16,060
what very

641
00:40:17,390 --> 00:40:19,180
i want to do now

642
00:40:24,540 --> 00:40:34,120
i will just

643
00:40:34,250 --> 00:40:37,870
is this into our because this is

644
00:40:37,890 --> 00:40:40,540
this is what's written on the

645
00:40:40,560 --> 00:40:44,310
slide so you see

646
00:40:44,370 --> 00:40:45,580
we get this

647
00:40:45,620 --> 00:40:46,870
agree to it

648
00:40:46,890 --> 00:40:48,580
the degree distribution

649
00:40:48,580 --> 00:40:49,640
so that's how

650
00:40:49,640 --> 00:40:51,120
some kind of

651
00:40:51,140 --> 00:40:54,580
statistics can be computed with with

652
00:40:54,640 --> 00:40:57,460
by combining are so and so

653
00:40:57,480 --> 00:41:00,040
OK well that

654
00:41:00,080 --> 00:41:02,350
that's quite an unusual thing here

655
00:41:02,350 --> 00:41:05,580
but with this

656
00:41:05,600 --> 00:41:06,930
i associative

657
00:41:06,960 --> 00:41:12,450
network it can be due to that some words were similarly and also the response

658
00:41:12,680 --> 00:41:14,810
but the others which is the response

659
00:41:14,810 --> 00:41:24,830
so the question is it's intuition why it's OK to have six this bracket cut

660
00:41:24,830 --> 00:41:28,730
off the whole region we should go to that sometimes when we sample high we

661
00:41:28,730 --> 00:41:31,640
would go left and then when we extracted bracket we would go all the way

662
00:41:31,640 --> 00:41:35,730
out here and we could get that region region and potentially go up again and

663
00:41:36,020 --> 00:41:39,620
that would work

664
00:41:47,750 --> 00:41:51,580
so the question is what about applying this multiple dimensions because i drawn one d

665
00:41:51,600 --> 00:41:55,160
plot here and it's not clear what the algorithm for multiple dimensions

666
00:41:55,180 --> 00:41:57,000
this actually

667
00:41:57,890 --> 00:41:59,500
different multidimensional

668
00:41:59,500 --> 00:42:02,830
slice sampling algorithms and

669
00:42:02,830 --> 00:42:06,390
it's not at all clear which one to use but the simplest is just go

670
00:42:06,390 --> 00:42:08,220
to each variable in turn

671
00:42:08,230 --> 00:42:11,950
and move it using this algorithm that bit like if something you update each variable

672
00:42:11,950 --> 00:42:15,230
in turn do this and that is something conditional

673
00:42:15,230 --> 00:42:16,730
and that's very easy to do

674
00:42:16,850 --> 00:42:19,000
may well be better things to do

675
00:42:32,080 --> 00:42:33,310
so the question is

676
00:42:33,330 --> 00:42:36,980
it looks would that inside the vast science and i explicitly told you in the

677
00:42:36,980 --> 00:42:39,620
last lecture that you could be really careful when you that there was a step

678
00:42:39,620 --> 00:42:43,140
size because you might not get valid algorithm say

679
00:42:43,160 --> 00:42:46,080
that's the selling point of this algorithm is valid way

680
00:42:46,120 --> 00:42:50,370
of doing something that looks as they are adapting the step size

681
00:42:50,430 --> 00:42:55,950
i haven't gone through derivation precisely why this is valid but the intuition is

682
00:42:55,950 --> 00:42:59,980
back to the basics of how MCMC works you need to be able to identify

683
00:43:00,560 --> 00:43:04,960
any given move forwards from your current position the next position

684
00:43:05,020 --> 00:43:08,850
how you would get from the final position back to where you are now

685
00:43:08,870 --> 00:43:13,480
so once you've gone to in place you could imagine working at the probability of

686
00:43:13,480 --> 00:43:15,040
selecting a height again

687
00:43:15,040 --> 00:43:17,220
construction the bracket starting here

688
00:43:17,230 --> 00:43:21,500
moving back to here and you can verify explicitly that they satisfy detailed balance when

689
00:43:21,500 --> 00:43:26,120
you work out that the probabilities

690
00:43:26,140 --> 00:43:29,480
that's why this construction is

691
00:43:29,640 --> 00:43:33,450
this elaborate i didn't fail just double the width of the brackets something very specific

692
00:43:33,450 --> 00:43:40,870
designed to make that prefigured three

693
00:43:40,930 --> 00:43:46,080
OK so the paper in the annals of statistics on fly something contains many ideas

694
00:43:46,270 --> 00:43:51,830
has ways of using slice sampling to adapt to the distribution in other ways very

695
00:43:51,830 --> 00:43:56,500
different ideas of how to apply to multiple dimensions ways of using

696
00:43:56,540 --> 00:44:01,250
different information that you may have if you do have it the long paper and

697
00:44:01,250 --> 00:44:03,930
i don't think many people really exploit a lot of the stuff in they do

698
00:44:03,930 --> 00:44:08,080
the simplest version that is here and it's in some of the introductory textbooks but

699
00:44:08,080 --> 00:44:15,080
i strongly recommend the original paper

700
00:44:18,980 --> 00:44:20,520
cover one more

701
00:44:20,520 --> 00:44:25,120
deliverable method and then we'll have a very short breaks and i'll change topic

702
00:44:25,810 --> 00:44:27,890
here's a completely different way

703
00:44:27,930 --> 00:44:32,830
of introducing more variables so that your sample better questions

704
00:44:33,040 --> 00:44:37,350
his son

705
00:44:39,140 --> 00:44:44,390
what's the catch yes

706
00:44:44,390 --> 00:44:46,660
none of the algorithms described

707
00:44:46,720 --> 00:44:51,720
strictly dominate any of the others that you can always construct an example of one

708
00:44:51,720 --> 00:44:54,910
of these algorithms are going to be better and that's often useful thing to do

709
00:44:54,910 --> 00:44:56,600
with the help to understand

710
00:44:56,600 --> 00:45:00,750
so the what how they might fall down what to watch out for

711
00:45:03,000 --> 00:45:08,000
here because it doesn't have to send little information about the conditional distributions in maybe

712
00:45:08,000 --> 00:45:10,640
that doesn't mean it's good missus gibbs sampler

713
00:45:10,660 --> 00:45:15,450
if your distributions are very similar might and the main win here isn't technically will

714
00:45:15,450 --> 00:45:18,160
markov chain because it's just this is easy to implement

715
00:45:18,160 --> 00:45:20,820
so this is what i do when i don't want to think too hard and

716
00:45:20,820 --> 00:45:25,330
i want to try something out if you're interested in really pushing harder application and

717
00:45:25,430 --> 00:45:29,080
really wanted you tend be working better than you would probably try and do something

718
00:45:29,080 --> 00:45:30,350
more sophisticated

719
00:45:30,370 --> 00:45:33,540
for example the algorithm that describe

720
00:45:33,980 --> 00:45:41,350
the here's a completely different way of using deliverables and here motivation is kind of

721
00:45:41,350 --> 00:45:44,040
different the previous motivation was

722
00:45:44,060 --> 00:45:48,230
i don't want to do things the hard like setting step size parameters of computing

723
00:45:48,230 --> 00:45:52,950
conditional distribution so i'm going to introduce auxiliary variables to make the problem easy once

724
00:45:52,950 --> 00:45:57,890
that so all distributions uniform

725
00:45:57,890 --> 00:46:00,870
the method based on how opinion dynamics which they

726
00:46:00,870 --> 00:46:03,330
the opposite think you know lots of stuff say

727
00:46:03,350 --> 00:46:07,140
you derive probability distribution you can probably also derived its gradient so you've got this

728
00:46:07,140 --> 00:46:11,410
high dimensional object is good if you have gradients to tell you which direction you

729
00:46:11,410 --> 00:46:17,160
can move in which direction is the health like to areas of high probability they

730
00:46:17,250 --> 00:46:19,930
went to be good if we could exploit gradient information

731
00:46:19,950 --> 00:46:24,060
like optimizes do it would be a terrible optimizer to just ten points and accept

732
00:46:24,100 --> 00:46:28,720
reject it's much better to use gradient and maybe we should be using gradient based

733
00:46:28,730 --> 00:46:30,660
mcmc methods

734
00:46:30,730 --> 00:46:34,680
so if we have a gradients we need some way of moving in the direction

735
00:46:34,680 --> 00:46:36,460
of great

736
00:46:36,480 --> 00:46:41,580
haldane in dynamic take your original distribution which we're going to write e to the

737
00:46:41,580 --> 00:46:43,390
minus energy

738
00:46:43,430 --> 00:46:46,520
we can always do that because we just say energy is equal to minus log

739
00:46:46,520 --> 00:46:47,750
have some

740
00:46:47,760 --> 00:46:55,130
fifty or sixty hertz depending on the country powerline interferences and typically have assigned this

741
00:46:55,500 --> 00:47:00,310
and the cosine component and if you plot the science and the cosine in and

742
00:47:00,310 --> 00:47:03,290
in two d then you get a circle

743
00:47:03,310 --> 00:47:08,990
OK so this this powerline interference is intrinsically two-dimensional

744
00:47:11,280 --> 00:47:18,700
OK what is the idea that the basic idea is to go

745
00:47:18,760 --> 00:47:22,220
two to generate some sorghum

746
00:47:24,770 --> 00:47:26,200
the sample

747
00:47:26,220 --> 00:47:28,810
from the data that you have

748
00:47:28,830 --> 00:47:31,330
make an estimation

749
00:47:31,340 --> 00:47:34,590
sample again make another estimation and so on so forth

750
00:47:34,600 --> 00:47:40,560
and then with this you can see how how much these estimations actually we get

751
00:47:41,330 --> 00:47:45,290
because in the way you are trying to see what is the statistical stability of

752
00:47:45,290 --> 00:47:47,460
the estimator

753
00:47:48,060 --> 00:47:55,070
OK so clearly you estimate the mixing unmixing matrix for each so we're gonna dataset

754
00:47:55,070 --> 00:47:58,400
and then you would like to calculate the stability

755
00:47:58,410 --> 00:48:00,190
that's the program

756
00:48:00,200 --> 00:48:05,930
so there are different ways

757
00:48:07,030 --> 00:48:10,930
of doing that one this bootstrap resampling that's the first

758
00:48:10,950 --> 00:48:15,380
option but there's also other things like if you could

759
00:48:15,400 --> 00:48:22,680
to generate you're sort data by an additional applying an additional filter but this is

760
00:48:22,680 --> 00:48:28,070
just a side remark so that the filter doesn't change anything in the linear model

761
00:48:28,160 --> 00:48:30,650
as you see from the equation

762
00:48:31,340 --> 00:48:33,770
so the a is actually

763
00:48:33,820 --> 00:48:35,910
you can pull it through

764
00:48:35,930 --> 00:48:38,660
through the filter OK

765
00:48:38,680 --> 00:48:41,630
but anyway so so

766
00:48:41,850 --> 00:48:48,190
in the case of of signals that have no temporal correlation it's more less clear

767
00:48:48,190 --> 00:48:53,310
what you do so you do some research as you would always do it

768
00:48:53,430 --> 00:48:58,630
in the case of temporal signals it's not so obvious how to do it and

769
00:48:59,100 --> 00:49:02,370
the idea would be to have two resample

770
00:49:02,390 --> 00:49:09,460
the correlation matrix is the temporal correlation matrices that that you would

771
00:49:11,680 --> 00:49:18,090
the need for for actually doing this similar tennis taking the station

772
00:49:19,280 --> 00:49:22,290
his the procedure first

773
00:49:22,310 --> 00:49:24,210
OK so so

774
00:49:24,230 --> 00:49:27,540
i mentioned this in my last lecture that

775
00:49:27,550 --> 00:49:28,570
when you

776
00:49:28,580 --> 00:49:34,910
so i see a problem you always solve up to scaling and permutation

777
00:49:34,930 --> 00:49:36,320
so imagine

778
00:49:36,370 --> 00:49:39,280
if if you would now

779
00:49:39,300 --> 00:49:45,190
two doors served got then

780
00:49:45,200 --> 00:49:49,910
you get some projection directions and you always have to match them with with the

781
00:49:49,910 --> 00:49:55,560
other projection damage mentions of the other circuits this is quite painful because it's somewhat

782
00:49:57,900 --> 00:49:58,820
in in

783
00:49:58,870 --> 00:50:03,700
in fact if there are some miss estimates it's even more and more difficult

784
00:50:04,420 --> 00:50:06,240
for this reason we

785
00:50:06,260 --> 00:50:08,690
we are playing a trick

786
00:50:08,730 --> 00:50:12,190
so because it

787
00:50:12,240 --> 00:50:14,770
we can

788
00:50:14,790 --> 00:50:16,540
it's actually

789
00:50:16,550 --> 00:50:19,730
it doesn't matter whether we first

790
00:50:20,270 --> 00:50:26,870
a separation step and then compute the surrogates from the already separated data

791
00:50:26,890 --> 00:50:28,110
whether we

792
00:50:28,120 --> 00:50:30,690
computer circuits from the original data

793
00:50:30,790 --> 00:50:35,550
is no difference in that only convenience

794
00:50:35,600 --> 00:50:37,340
OK so

795
00:50:37,390 --> 00:50:43,500
this is the trick with this we avoid all the tedious combinatorics

796
00:50:43,570 --> 00:50:48,440
why is that because we assume that that are

797
00:50:48,490 --> 00:50:55,030
independent component analysis algorithm already gets on cuts is a decent solution

798
00:50:55,040 --> 00:51:01,190
so this means that certain gods from the solution which is already on mixed

799
00:51:01,200 --> 00:51:04,890
if we can if we draw surrogate from that

800
00:51:04,930 --> 00:51:07,520
and if we

801
00:51:07,530 --> 00:51:08,700
then do one

802
00:51:08,710 --> 00:51:14,600
an independent component analysis again on the already and circuits it would almost be the

803
00:51:14,600 --> 00:51:22,310
identity matrix up to some statistical fluctuations so there's no need for actually matching directions

804
00:51:22,380 --> 00:51:24,150
this is

805
00:51:25,920 --> 00:51:27,640
o thing

806
00:51:27,660 --> 00:51:29,510
OK so

807
00:51:29,520 --> 00:51:32,300
the first step is to

808
00:51:32,310 --> 00:51:36,720
two blind source separation as i said the second step this also for convenience you

809
00:51:36,720 --> 00:51:38,670
whiten the data

810
00:51:39,350 --> 00:51:40,250
and then

811
00:51:40,270 --> 00:51:41,490
you do

812
00:51:41,500 --> 00:51:49,910
you draw some sobering surrogate datasets you do blind source separation and this produces some

813
00:51:49,920 --> 00:51:55,820
rotation matrices only because you have widened the data so this is almost

814
00:51:56,780 --> 00:52:00,580
o only irritation that that you know that you left

815
00:52:00,600 --> 00:52:04,620
and this rotation is very similar to the unit matrix

816
00:52:04,640 --> 00:52:12,050
and then basically you decompose dissertation into some

817
00:52:12,060 --> 00:52:18,300
portation angles and you compute some standard deviation from the rotation angle so in order

818
00:52:18,300 --> 00:52:21,990
to get this is zero boxing

819
00:52:27,600 --> 00:52:30,420
so just have some some definitions

820
00:52:30,440 --> 00:52:34,380
so you have to define samarra

821
00:52:34,430 --> 00:52:37,850
i mean slightly more precise than this hand-waving thing

822
00:52:37,870 --> 00:52:40,130
and so

823
00:52:40,170 --> 00:52:45,690
we take the angle deviation between the source signal subspace because it could be of

824
00:52:45,690 --> 00:52:52,620
a more dimensional subspace space component not the one-dimensional ICA components

825
00:52:53,230 --> 00:52:58,590
and then you have this this matrix which we call separability in a matrix with

826
00:52:58,590 --> 00:53:00,500
all these angles

827
00:53:00,510 --> 00:53:02,780
OK and now our

828
00:53:02,790 --> 00:53:06,140
definition of uncertainty is to take the max

829
00:53:06,150 --> 00:53:09,070
with respect to each other

830
00:53:09,090 --> 00:53:10,640
it's that column

831
00:53:10,660 --> 00:53:12,200
i think

832
00:53:16,690 --> 00:53:21,540
i mean this separability in the matrix page basically measures

833
00:53:21,560 --> 00:53:22,520
how are you

834
00:53:22,530 --> 00:53:24,410
DV eight and all

835
00:53:24,420 --> 00:53:26,880
directions and

836
00:53:26,890 --> 00:53:31,200
the uncertainty is the maximum of one you could also take the mean or the

837
00:53:31,200 --> 00:53:37,890
median or whatever but maybe this is the most pessimistic one

838
00:53:40,870 --> 00:53:44,730
and just to show you how we get this

839
00:53:45,660 --> 00:53:49,360
we take the exponential

840
00:53:49,730 --> 00:53:57,490
the rotation group representation which is quite convenient for this purpose and then this is

841
00:53:57,510 --> 00:53:58,630
the are

842
00:54:00,880 --> 00:54:05,530
this is just a convenience it's it's because

843
00:54:05,540 --> 00:54:08,700
otherwise we have to work too hard

844
00:54:08,720 --> 00:54:10,160
OK so

845
00:54:10,210 --> 00:54:14,200
one of the things that is important if you do bootstrap

846
00:54:14,220 --> 00:54:17,200
it is to that you prove that actually when you

847
00:54:17,220 --> 00:54:20,280
when you do the bootstrap that you

848
00:54:20,290 --> 00:54:25,750
that year the estimated that you approximating is actually converging to the right seeing things

849
00:54:25,750 --> 00:54:28,000
so that it's consistent

850
00:54:29,550 --> 00:54:30,920
when you do

851
00:54:33,010 --> 00:54:38,380
with temporal information this is not at all clear that it would be a consistent

852
00:54:41,320 --> 00:54:46,230
so in the case of all these other all these algorithm that whose temporal information

853
00:54:46,720 --> 00:54:52,200
such bootstrap procedure might not actually estimate the right statistical thing

854
00:54:53,620 --> 00:54:56,980
and for this reason we try to bound

855
00:54:56,990 --> 00:54:58,310
the error

856
00:54:58,330 --> 00:55:00,860
of our method

857
00:55:00,880 --> 00:55:02,850
and we do this

858
00:55:02,860 --> 00:55:08,490
on the basis of the correlation matrices and we see that it's actually bounded

859
00:55:08,500 --> 00:55:12,920
and we also see that this

860
00:55:12,930 --> 00:55:18,140
with strep estimator is actually biased so it doesn't actually

861
00:55:18,140 --> 00:55:18,910
understood this

862
00:55:19,400 --> 00:55:24,110
general purpose recipe for backprop you can apply to more complicated objects in the traditional

863
00:55:24,690 --> 00:55:25,900
for example you can apply it to

864
00:55:29,070 --> 00:55:34,750
of layers stack which correspond to be unfolding in time recurrent network where you can

865
00:55:34,750 --> 00:55:38,010
apply to recursive networks which are shown in the bottom right here

866
00:55:38,480 --> 00:55:39,890
where are you constructed tree

867
00:55:42,420 --> 00:55:42,920
in some way

868
00:55:45,760 --> 00:55:48,640
inputs or at least and at each internal node

869
00:55:49,190 --> 00:55:50,360
you compute a representation

870
00:55:50,910 --> 00:55:51,880
which summarizes

871
00:55:52,260 --> 00:55:53,380
these subtrees below

872
00:55:54,590 --> 00:55:56,510
this has been used on

873
00:55:56,970 --> 00:56:00,150
by richard soldiering chris manning in a number of

874
00:56:00,220 --> 00:56:01,730
applications of these that's to

875
00:56:02,290 --> 00:56:04,120
natural language and also the computer vision

876
00:56:05,180 --> 00:56:06,190
and with great success

877
00:56:07,910 --> 00:56:10,840
in in lab with views recurrent nets fore

878
00:56:14,290 --> 00:56:17,930
structured output problem where you want to help with something of high dimension

879
00:56:18,410 --> 00:56:22,600
in it's not like classification where you have one choice out of ten out of

880
00:56:22,690 --> 00:56:26,490
thousand you have one choice over an exponential number of states sequences of words

881
00:56:27,130 --> 00:56:29,490
so you can use recurrent nets here

882
00:56:29,990 --> 00:56:30,870
in aid

883
00:56:34,650 --> 00:56:36,740
way meaning that you can use them to

884
00:56:38,430 --> 00:56:39,240
one of these

885
00:56:39,620 --> 00:56:40,910
high probability answers

886
00:56:42,290 --> 00:56:44,570
the way that you can do without is first

887
00:56:46,790 --> 00:56:51,690
the problem of modelling the conditional distribution of a sequence of outputs given some inputs

888
00:56:53,850 --> 00:56:55,980
through the decomposition of the joint

889
00:56:56,770 --> 00:56:58,520
in two conditionals wear

890
00:56:59,070 --> 00:57:00,580
he avoid one to

891
00:57:01,150 --> 00:57:06,850
he is decomposed into the product of the probability of each word given the previous works

892
00:57:07,840 --> 00:57:08,810
so you're gonna have

893
00:57:10,850 --> 00:57:14,510
the previous here were represented here by the state of the recurrent nett

894
00:57:15,100 --> 00:57:17,160
the previous slide that vertical

895
00:57:17,790 --> 00:57:18,370
o thing here

896
00:57:19,830 --> 00:57:23,960
and this is is gonna be a deterministic function of the previous words that have been seen through

897
00:57:24,590 --> 00:57:25,460
in mapping

898
00:57:25,870 --> 00:57:26,570
after which

899
00:57:26,970 --> 00:57:29,560
depends on the previous state and the current observation

900
00:57:31,210 --> 00:57:35,820
and now what you do with this state is you use it to predict the

901
00:57:35,820 --> 00:57:38,880
distribution over the next word once you have a distribution

902
00:57:39,530 --> 00:57:39,840
you can

903
00:57:40,240 --> 00:57:44,080
compute the probability of the next word if your training and you maximize the probability

904
00:57:44,660 --> 00:57:46,560
or you can sample the next word so

905
00:57:47,540 --> 00:57:52,670
recursively sampling the next word and then using data as input for something the next one and the next one

906
00:57:53,200 --> 00:57:55,400
so what you can generate a sequence out of nothing

907
00:57:55,860 --> 00:57:57,220
more out of the input here

908
00:57:57,740 --> 00:57:58,600
so this is how we

909
00:57:59,120 --> 00:58:00,420
we take an input sequence

910
00:58:01,570 --> 00:58:03,680
at the bottom of the figure are here

911
00:58:04,490 --> 00:58:09,660
and we summarize it with one kind of sort of simple recurrent nett which is just taking

912
00:58:10,440 --> 00:58:11,240
the previous

913
00:58:11,760 --> 00:58:15,080
summary and the current observation into the next summary

914
00:58:15,500 --> 00:58:17,430
and then we take the summary for the whole sequence

915
00:58:18,480 --> 00:58:23,710
two condition and other recurrent nett which is a generative recurrent which gives us a probability fore

916
00:58:24,150 --> 00:58:26,360
the next element of the output sequence given

917
00:58:26,980 --> 00:58:28,230
the elements that we already

918
00:58:28,760 --> 00:58:29,520
have sampled

919
00:58:31,200 --> 00:58:34,620
all right so i i you know i can i can imagine that if for

920
00:58:34,650 --> 00:58:38,130
the first time you are seeing these concepts that might be a bit

921
00:58:38,540 --> 00:58:39,220
obscure but

922
00:58:40,780 --> 00:58:43,790
i encourage you to to read about this one shot one thing i forgot to

923
00:58:43,790 --> 00:58:45,370
say is that i'm reading a book

924
00:58:46,230 --> 00:58:48,190
another one called deep learning

925
00:58:49,110 --> 00:58:49,910
not a big surprise

926
00:58:52,870 --> 00:58:55,260
and some draft chapters already available

927
00:58:55,850 --> 00:59:01,310
on and you can find those on my web page and there's already a chapter on recurrent nets which contains

928
00:59:01,790 --> 00:59:04,180
a lot more detailed information about these these beasts

929
00:59:09,050 --> 00:59:11,290
get these things o one interesting

930
00:59:13,060 --> 00:59:14,670
consideration when your

931
00:59:15,120 --> 00:59:18,550
when you're replying backdrop to simple things like the traditional

932
00:59:19,360 --> 00:59:23,540
neural net with one layer you can write down the equations for derivative yourself

933
00:59:24,340 --> 00:59:28,490
and-or the chances of making a mistake a pretty small but once you get to

934
00:59:28,490 --> 00:59:34,230
these complicated graphs like we had war recursive mansoor other kinds of complicated

935
00:59:34,690 --> 00:59:40,420
accommodations are doing through which like compute derivatives it becomes more and more error-prone and

936
00:59:40,450 --> 00:59:42,840
humans make errors and little errors are

937
00:59:43,310 --> 00:59:44,500
hard to catch their

938
00:59:45,820 --> 00:59:47,270
so to make the life of the

939
00:59:47,700 --> 00:59:49,700
machine learning design are easier

940
00:59:50,650 --> 00:59:53,400
we've introduce automatic differentiation

941
00:59:54,330 --> 00:59:55,180
this this world

942
00:59:55,910 --> 00:59:57,550
so automatic differentiation is just

943
00:59:58,200 --> 01:00:04,110
software that they forward graph that maps your value inputs and you parameters you to loss

944
01:00:08,280 --> 01:00:09,460
softwar busy

945
01:00:09,950 --> 01:00:12,440
create backward backdrop graph

946
01:00:13,000 --> 01:00:15,130
from about and then you can write about

947
01:00:15,590 --> 01:00:17,520
so different ways of doing

948
01:00:18,150 --> 01:00:19,900
we have a binary code for you know

949
01:00:20,440 --> 01:00:21,490
which is uh

950
01:00:23,030 --> 01:00:26,850
um four pi expressions into siu chiu expressions

951
01:00:27,790 --> 01:00:29,300
will only compound but also

952
01:00:31,160 --> 01:00:32,500
the automatic differentiation

953
01:00:32,970 --> 01:00:33,880
as well as

954
01:00:35,390 --> 01:00:37,400
are optimizing code optimization

955
01:00:38,050 --> 01:00:39,740
in order to save memory and computation

956
01:00:40,680 --> 01:00:41,020
all right

957
01:00:42,770 --> 01:00:44,000
so let's go back to

958
01:00:46,030 --> 01:00:46,960
basic notion

959
01:00:47,860 --> 01:00:49,020
try to understand

960
01:00:50,970 --> 01:00:52,180
representation learning

961
01:00:53,950 --> 01:00:55,660
in deep learning can give us

962
01:00:57,220 --> 01:00:59,370
somewhat surprising an amazing

963
01:01:00,050 --> 01:01:03,650
statistical power so how it can help us to generalize better

964
01:01:05,060 --> 01:01:06,250
more traditional techniques

965
01:01:07,650 --> 01:01:13,060
be basic reason why this is gonna happen it's something i've talked about already is that we're gonna be

966
01:01:13,540 --> 01:01:19,300
composing pieces together it's thanks to compositionality not we can have this statistical game

967
01:01:20,450 --> 01:01:23,100
all right so let's let's let's go into this

968
01:01:25,190 --> 01:01:26,560
so before i tell you to

969
01:01:27,710 --> 01:01:31,650
how we can get this game let me tell you know how a lot of machine learning is done

970
01:01:32,860 --> 01:01:36,220
a lot of machine learning that's i call non distributed

971
01:01:36,920 --> 01:01:37,320
and later

972
01:01:37,960 --> 01:01:38,480
i'll talk about

973
01:01:39,260 --> 01:01:40,480
distributed representations

974
01:01:40,940 --> 01:01:41,340
which is

975
01:01:43,340 --> 01:01:46,420
parents are bins and all these things but first of all

976
01:01:47,530 --> 01:01:49,430
most machine learning about you see

977
01:01:49,990 --> 01:01:51,150
in machine learning books

978
01:01:51,780 --> 01:01:52,380
are things

979
01:01:53,110 --> 01:01:53,850
i think

980
01:01:54,500 --> 01:01:55,450
he input space

981
01:01:56,560 --> 01:01:58,210
and break it into regions

982
01:02:00,500 --> 01:02:05,180
like clustering so each region corresponds to the regions assigned to cluster or nearest neighbours

983
01:02:05,350 --> 01:02:08,280
where each region corresponds to one example and andy

984
01:02:08,980 --> 01:02:15,050
configurations that get mapped to it because the nearest neighbour or r are we have our kernel machines

985
01:02:15,600 --> 01:02:16,870
oral or or

986
01:02:17,520 --> 01:02:21,500
nonparametric density estimation classical nonparametric density estimation or engrams

987
01:02:22,490 --> 01:02:27,320
in like in language modeling or decision trees so all these and more

988
01:02:28,070 --> 01:02:30,950
like nonparametric graph-based manifold learning methods

989
01:02:31,420 --> 01:02:37,140
they all work by taking the input space and breaking into the regions now those regions will have to be

990
01:02:38,220 --> 01:02:42,960
hard here that could be solved you can have of interpolation between neighboring regions

991
01:02:43,600 --> 01:02:47,090
but the crucial point is it commenting argument

992
01:02:47,090 --> 01:02:54,150
of Newton steps to solve an LP with an accuracy epsilon is bounded by

993
01:02:54,150 --> 01:02:58,910
an upper bound like this it depends on the epsilon on the accuracy and on the

994
01:02:58,910 --> 01:03:03,540
square root of the number of inequalities in a problem so that's why it's

995
01:03:03,720 --> 01:03:09,550
polynomial time worst-case complexity that's this gives you a bound on the number of

996
01:03:09,550 --> 01:03:15,770
Netonw steps number of systems updates like this so if you multiply this with

997
01:03:15,770 --> 01:03:21,510
the cost of computing this Newton step you get a polynomial upper bound

998
01:03:21,520 --> 01:03:29,050
on the total number of operations right so this is a very simple type

999
01:03:29,050 --> 01:03:34,930
of interior point method but this complexity is still the best known complexity for linear

1000
01:03:34,930 --> 01:03:40,090
programming worst-case complexity if you multiply this with the cost of solving a Newton system

1001
01:03:40,090 --> 01:03:50,930
you get actually the best worst-case complexity that's known for linear programming is that clear so all of the

1002
01:03:50,950 --> 01:03:55,110
of this is linear for linear programming so now we can see how

1003
01:03:55,130 --> 01:04:01,330
we extend this to general conic optimization with in the linear format so first

1004
01:04:01,330 --> 01:04:09,130
we'll have to extend the this simple logarithmic barrier to a general cone not

1005
01:04:09,130 --> 01:04:11,070
necessary orthant

1006
01:04:11,450 --> 01:04:15,370
then we can if you have that we can define a central path in

1007
01:04:15,370 --> 01:04:22,250
exactly the same way and then another interesting question would be this M over T

1008
01:04:22,250 --> 01:04:27,610
M was the number of inequalities in the LP will re will be replaced by some

1009
01:04:27,610 --> 01:04:34,190
other parameter and then that will affect the worst-case complexity here so let's start with the

1010
01:04:34,200 --> 01:04:43,370
barrier function so in a very influential book Nesterov and Nemirovski have showed that

1011
01:04:43,370 --> 01:04:50,770
any for every proper cone K one can find an barrier function that satisfies these three

1012
01:04:50,770 --> 01:04:58,690
properties and it's called a normal barrier function and it really can be interpreted as

1013
01:04:58,690 --> 01:05:02,750
the extension of the of the simple logarithmic barrier function of the nonnegative orthant to

1014
01:05:02,750 --> 01:05:08,550
a general nonpolyhedral cone so the three properties are that first it's a barrier function so I

1015
01:05:08,550 --> 01:05:14,170
mean it's differentiable it's convex its domain is the interior of the polyhedron

1016
01:05:14,650 --> 01:05:20,830
and it goes to infinity as you approach the boundary right then the second definition

1017
01:05:20,830 --> 01:05:26,470
actually extends the logarithmic property of the cone so it says if you take an

1018
01:05:26,470 --> 01:05:33,250
X in the cone K and you multiply it with a positive T then the value of

1019
01:05:33,250 --> 01:05:38,750
this barrier function changes like this it's the value at X minus data log

1020
01:05:38,870 --> 01:05:46,170
T so in other words the barrier function if you restrict it to a ray in the convex

1021
01:05:46,170 --> 01:05:52,010
cone at the origin on that ray it looks like a negative logarithm multiplied with

1022
01:05:52,010 --> 01:05:56,470
theta and this theta is a parameter of the normal cone of the n

1023
01:05:56,650 --> 01:06:03,690
barrier function so it's called a theta normal barrier function if this coefficient is theta

1024
01:06:04,130 --> 01:06:07,870
so you can look at this definition as just an extension of a logarithm to a cone to

1025
01:06:07,870 --> 01:06:15,940
a vector in a cone and then the last condition will be needed for Newton's method

1026
01:06:15,960 --> 01:06:23,310
or the analysis of Newton's method so if you restrict this function phi to an a line

1027
01:06:23,310 --> 01:06:27,770
so you pick some vector X you pick a direction V and then you st restrict it

1028
01:06:27,790 --> 01:06:32,930
you look at this function as a function of the point on that line it

1029
01:06:32,940 --> 01:06:39,250
satisfies this inequality so the third derivative with the function of alpha is bounded by

1030
01:06:39,310 --> 01:06:45,010
this function of the second derivative and the reason why that's needed is that the

1031
01:06:45,010 --> 01:06:51,090
analysis of these methods depends on results for Newton's method that's already true for

1032
01:06:51,090 --> 01:07:00,690
linear programming because these complexity results really exploit very properties of the logarithmic barrier

1033
01:07:00,690 --> 01:07:05,410
function when you apply Newton's method to it so to extend this type of analysis

1034
01:07:05,410 --> 01:07:14,590
to a more general cones you need this condition so and this intuitively makes sense

1035
01:07:14,590 --> 01:07:23,200
at least because this restricts the third derivative of the function by a local limit defined

1036
01:07:23,310 --> 01:07:29,070
by the second derivative so it restricts how fast second derivative can change so it makes sense

1037
01:07:29,070 --> 01:07:34,910
that this is useful if you analyze Newton's method right so if a barrier function satisfies

1038
01:07:34,910 --> 01:07:40,970
these properties we call it normal and this parameter theta will be important so let's look at some

1039
01:07:40,970 --> 01:07:47,070
examples so for the non-negative orthant this standard barrier that we use before is

1040
01:07:47,080 --> 01:07:53,150
normal and the parameter theta is just M if you replace X with T X then

1041
01:07:53,150 --> 01:07:58,930
that's the same as substracting subtracting M log T from this right so this parameter theta

1042
01:07:58,940 --> 01:08:05,610
here is M for the non-negative orthant in R M for the second order cone you

1043
01:08:05,610 --> 01:08:10,370
can show a define like this so scalar Y that's an upper bound on the

1044
01:08:10,370 --> 01:08:17,350
Euclidean norm of X that this is a normal barrier and the parameter is two

1045
01:08:17,350 --> 01:08:23,110
if you multiply Y and X by T then it's same as substract subtracting two log T from this

1046
01:08:23,110 --> 01:08:30,290
barrier function for the semi-definite cone you can use the negative of the log of

1047
01:08:30,290 --> 01:08:36,770
the determinant of the matrix as a normal barrier and the corresponding theta is M if

1048
01:08:36,770 --> 01:08:44,570
this is an M by M matrix so that's just a log that determinant for the exponential

1049
01:08:44,570 --> 01:08:50,270
cone you have a complicated barrier that you can show is normal with parameter three and

1050
01:08:50,270 --> 01:09:00,350
also for the normal power cone so that's the definition of a normal barrier so

1051
01:09:00,350 --> 01:09:04,270
then we can apply that to define a central path in exactly the same way

1052
01:09:04,270 --> 01:09:10,550
as for linear programming so now we take an LP with conic inequalities we use a normal barrier

1053
01:09:10,550 --> 01:09:15,410
as our barrier function for the inequalities and then we define the central path as

1054
01:09:15,410 --> 01:09:22,970
exactly the same way we construct these unconstrained problems with a weighted sum of

1055
01:09:22,980 --> 01:09:28,350
the objective and the barrier function and then the set of minimizers of this convex

1056
01:09:28,350 --> 01:09:34,010
function as with parameter T is called the central path and it has exactly the same

1057
01:09:34,010 --> 01:09:47,630
properties as for linear programming so the motivation for introducing the definitions or the

1058
01:09:47,630 --> 01:09:52,730
properties of normal barriers have to do with the convergence analysis of Newton's method when

1059
01:09:52,760 --> 01:09:57,870
you apply it to this function a function of this type where the first term is linear and

1060
01:09:57,870 --> 01:10:05,030
the second is a normal barrier applied to a linear function of X so

1061
01:10:05,030 --> 01:10:10,090
if you wanted to minimize this for fixed T then at each iteration you will have to compute

1062
01:10:10,100 --> 01:10:15,630
a Newton step like this so the inverse of the Hessian times the gradient and then

1063
01:10:15,630 --> 01:10:23,510
so that's the Newton step at X and then in the convergence analysis it's very convenient

1064
01:10:23,510 --> 01:10:30,470
to use to define an function lambda of X that's called a Newton decrement

1065
01:10:30,470 --> 01:10:37,330
at X so it's basically the norm of the Newton step delta X in local

1066
01:10:37,330 --> 01:10:43,110
norm defined by the Hessian of the function of X or if you use the definition of the

1067
01:10:43,110 --> 01:10:47,240
Newton step you can also write it like this it's the inner product of the gradient

1068
01:10:47,260 --> 01:10:52,700
the negative gradient with the Newton step that should be a positive number because the Newton step

1069
01:10:52,710 --> 01:10:56,930
will be the same direction and then the square root of that number so we'll use

1070
01:10:56,930 --> 01:11:05,930
this as a proximity measure to evaluate the proximity of X to the central point

1071
01:11:05,930 --> 01:11:10,910
with parameter T so this will be zero if you're on the central path with

1072
01:11:10,910 --> 01:11:16,490
parameter T because in that case the gradients will be zero of this function if you're on

1073
01:11:16,490 --> 01:11:18,390
and intuition

1074
01:11:18,400 --> 01:11:24,000
let's derive gasoline processes starting from linear regression

1075
01:11:24,010 --> 01:11:28,900
so let's think of linear regression imagine you have a linear regression with input

1076
01:11:28,920 --> 01:11:30,430
x i

1077
01:11:30,450 --> 01:11:32,640
and output

1078
01:11:32,650 --> 01:11:34,720
t i

1079
01:11:35,360 --> 01:11:39,470
so in a linear regression model t i is

1080
01:11:39,530 --> 01:11:43,020
linear function of the exercise was some noise

1081
01:11:43,040 --> 01:11:47,020
OK where the coefficients of the linear function are the WP's

1082
01:11:47,070 --> 01:11:49,330
so we're going to get the gas processes by

1083
01:11:49,360 --> 01:11:51,280
taking a few steps

1084
01:11:51,290 --> 01:11:54,700
the first step is that instead of thinking of linear regression

1085
01:11:54,710 --> 01:12:01,000
we can think of a linear regression with some basis functions OK so think of

1086
01:12:01,020 --> 01:12:03,460
david lowe's right so

1087
01:12:03,480 --> 01:12:05,510
we have a linear regression

1088
01:12:05,520 --> 01:12:09,480
but instead of being on the wrong inputs we're going to have these fixed basis

1089
01:12:10,680 --> 01:12:12,200
five of the

1090
01:12:12,220 --> 01:12:13,680
functions of the input

1091
01:12:13,730 --> 01:12:18,440
OK so this is also known as the kernel trick we're basically mapping from access

1092
01:12:18,440 --> 01:12:20,830
to some feature space phi

1093
01:12:22,340 --> 01:12:26,610
and this happens to be maybe a finite dimensional feature space that doesn't matter

1094
01:12:26,800 --> 01:12:30,490
so t i is this what some gas nice

1095
01:12:30,510 --> 01:12:37,220
now instead of maximizing the likelihood with respect to these W's instead of optimizing these

1096
01:12:37,220 --> 01:12:41,120
W's we're going to try to be bayesian about this we're going to say

1097
01:12:41,170 --> 01:12:44,440
if we put a prior on the w

1098
01:12:44,460 --> 01:12:47,280
then this gives us a prior over functions

1099
01:12:47,300 --> 01:12:50,510
and it turns out that the guassian process prior over functions

1100
01:12:51,860 --> 01:12:54,370
you know family guassian process priors

1101
01:12:54,380 --> 01:12:59,550
and the way you do that is you just give these coefficients gaussians prior

1102
01:13:00,330 --> 01:13:01,870
so w b

1103
01:13:01,910 --> 01:13:05,250
is normally distributed with mean zero

1104
01:13:05,260 --> 01:13:07,950
and variance of data d

1105
01:13:07,960 --> 01:13:12,760
and is independent of the other coefficients in some other wl it's a new make

1106
01:13:12,770 --> 01:13:16,460
the noise be gaussians as well

1107
01:13:17,220 --> 01:13:21,640
if you give this the gas in distribution and you make the noise the gas

1108
01:13:21,640 --> 01:13:27,240
in then you can compute things like the expected value of t i

1109
01:13:27,250 --> 01:13:28,500
the output

1110
01:13:28,540 --> 01:13:32,530
and well we gave this means zero also the expected value of the function at

1111
01:13:32,530 --> 01:13:34,020
any point is going to be

1112
01:13:35,840 --> 01:13:38,600
but you can also compute

1113
01:13:38,650 --> 01:13:43,640
the covariance between p i j this is that element of the covariance matrix sigma

1114
01:13:43,640 --> 01:13:44,770
i j

1115
01:13:44,790 --> 01:13:47,380
so you can compute that there has exactly

1116
01:13:47,380 --> 01:13:48,620
this form

1117
01:13:49,290 --> 01:13:52,250
it's a sum over the data the

1118
01:13:52,260 --> 01:13:55,480
and then it's basically the dot product of

1119
01:13:57,040 --> 01:14:01,530
and exciting fight xj but weighted by the state of these

1120
01:14:01,540 --> 01:14:03,960
that's what this does

1121
01:14:07,370 --> 01:14:12,300
you assume that each individual data point had some gas in noise so there's

1122
01:14:12,320 --> 01:14:15,630
delta function here that says if i cause j at

1123
01:14:15,650 --> 01:14:19,280
noise sigma school very with variance sigma squared

1124
01:14:20,010 --> 01:14:24,600
that's just like a couple of lines about to drive this but this is exactly

1125
01:14:24,610 --> 01:14:27,250
a covariance function

1126
01:14:27,620 --> 01:14:32,030
four guassian process missus guassian process

1127
01:14:32,050 --> 01:14:35,590
with zero mean and covariance function c

1128
01:14:35,610 --> 01:14:38,440
but the exciting xj

1129
01:14:38,610 --> 01:14:43,040
which we can write out as the i j element of the covariance matrix so

1130
01:14:43,110 --> 01:14:45,080
just to make it absolutely clear

1131
01:14:45,090 --> 01:14:46,470
the covariance

1132
01:14:47,590 --> 01:14:49,780
measures the covariance between

1133
01:14:50,110 --> 01:14:53,580
the target t i and the target TJ

1134
01:14:53,600 --> 01:14:55,880
and it's the a function of

1135
01:14:57,440 --> 01:14:59,320
input location x i

1136
01:14:59,370 --> 01:15:01,450
and in the location xj

1137
01:15:01,470 --> 01:15:11,430
so clearly you want the covariance matrix to be positive definite now if you define

1138
01:15:11,430 --> 01:15:13,430
any basis functions

1139
01:15:13,450 --> 01:15:15,110
regression like this

1140
01:15:15,110 --> 01:15:18,060
then you get a positive definite covariance

1141
01:15:18,080 --> 01:15:22,040
function but there are other positive definite covariance functions that might not be simple to

1142
01:15:22,040 --> 01:15:23,930
express in this one

1143
01:15:23,950 --> 01:15:28,060
the same sorts of ideas you might be familiar with the CMC carry over here

1144
01:15:28,070 --> 01:15:29,030
in terms of

1145
01:15:29,030 --> 01:15:31,760
kernel make valid kernel matrices

1146
01:15:31,790 --> 01:15:36,680
now in this particular case this casting process has a finite number of basis functions

1147
01:15:36,680 --> 01:15:39,930
like we just find some finite number of the basis functions

1148
01:15:40,340 --> 01:15:46,000
but many useful gases cross covariance functions correspond to infinitely many such basis functions

1149
01:15:46,030 --> 01:15:51,370
and in those cases you can just write out this function is positive definite covariance

1150
01:15:51,390 --> 01:15:56,910
function you don't have to represented as this infinite expansion in terms of basis functions

1151
01:15:58,350 --> 01:16:03,150
that's how you go from linear regression to get processes

1152
01:16:05,750 --> 01:16:11,830
so what do you do for regression well let's just recap a little bit against

1153
01:16:12,910 --> 01:16:17,140
places of prior directly on the space of functions

1154
01:16:17,550 --> 01:16:22,650
such that at any finite selection of points x one through x and the course

1155
01:16:22,650 --> 01:16:28,700
and the corresponding function values t one through t and have a multivariate gaussians distribution

1156
01:16:28,700 --> 01:16:32,530
that's the sort of the basic definition of guassian process

1157
01:16:34,070 --> 01:16:36,650
and in learning just like i showed you the devil

1158
01:16:36,680 --> 01:16:40,790
you might start out with some prior over functions that should encode your beliefs about

1159
01:16:40,790 --> 01:16:44,740
first how much does the mean move but more importantly

1160
01:16:44,750 --> 01:16:48,800
how does it behave this tail of the distribution here

1161
01:16:48,820 --> 01:16:50,910
because we were aiming to

1162
01:16:50,910 --> 01:16:55,330
cut off the tail that's what that's what the analysis was trying to do questions

1163
01:16:55,330 --> 01:16:59,960
did succeed well you know here it looks looks positive

1164
01:17:00,010 --> 01:17:03,110
and and so so does it here

1165
01:17:03,210 --> 01:17:07,210
means are pretty darn close and there is much in it you know so actually

1166
01:17:07,210 --> 01:17:11,310
the parzen window estimator at this point seems to be on average pretty good

1167
01:17:12,230 --> 01:17:14,520
you know the tail of the distribution

1168
01:17:14,530 --> 01:17:16,240
it is is being cut off

1169
01:17:16,250 --> 01:17:21,290
and as things get worse it seems to be you know still able to even

1170
01:17:21,290 --> 01:17:25,310
was quite small training set sizes to keep this sort of

1171
01:17:25,320 --> 01:17:27,760
the worst case performance to be

1172
01:17:27,770 --> 01:17:31,710
quite well controlled so we're getting out point two

1173
01:17:31,750 --> 01:17:35,040
maybe if we cut off maybe one percent of the distribution would still be a

1174
01:17:35,040 --> 01:17:36,210
point two

1175
01:17:37,470 --> 01:17:41,130
and so on so you can see you know the tail is growing but much

1176
01:17:41,130 --> 01:17:42,750
slower than the ones

1177
01:17:42,880 --> 01:17:44,600
of the past window

1178
01:17:44,660 --> 01:17:49,480
sort of nice feeling is cutting off at very nasty but there are you know

1179
01:17:49,480 --> 01:17:52,580
this already quite bad performances size twenty

1180
01:17:52,620 --> 01:17:54,270
and so on

1181
01:17:54,330 --> 01:17:57,240
OK so that just to generate OK

1182
01:17:57,250 --> 01:18:03,040
so that's that's in the case of the input you know just linear kernels were

1183
01:18:03,080 --> 01:18:06,570
if we use a gas in can of course we give the extra power needs

1184
01:18:06,710 --> 01:18:08,150
and then it can actually

1185
01:18:08,150 --> 01:18:11,150
for instance using this this kernel here

1186
01:18:11,160 --> 01:18:12,440
as encounter with it

1187
01:18:12,450 --> 01:18:13,900
appropriate sigma

1188
01:18:15,260 --> 01:18:20,130
we would offend you know effectively be far worse danger of overfitting but in fact

1189
01:18:20,130 --> 01:18:22,310
it makes use of that extra power

1190
01:18:22,330 --> 01:18:24,360
and gets the

1191
01:18:26,380 --> 01:18:31,610
the much better but also the tail is still controlled and even the worst case

1192
01:18:31,610 --> 01:18:34,710
performance is better than the average performance

1193
01:18:34,730 --> 01:18:37,670
in the in the most simple feature space

1194
01:18:38,040 --> 01:18:40,210
and this is where the slightly smaller

1195
01:18:40,270 --> 01:18:42,650
training set

1196
01:18:45,240 --> 01:18:49,920
so this was the conclusions of day one right so that so i give an

1197
01:18:49,920 --> 01:18:55,320
outline central results touched on this covering numbers and showed you this application to SVM

1198
01:18:57,680 --> 01:19:01,070
so now i'd like to launching today too

1199
01:19:01,100 --> 01:19:06,310
with some delay but my main aim here is to try and bring you

1200
01:19:06,330 --> 01:19:09,370
some of the ideas behind rademacher complexity

1201
01:19:09,410 --> 01:19:14,210
i think it's a nice theory it's quite nicely general in that can be applied

1202
01:19:14,210 --> 01:19:16,820
to lots of things not just

1203
01:19:16,830 --> 01:19:24,870
you know the classification can be applied to sort of regression and even things like

1204
01:19:24,870 --> 01:19:28,140
PCA and so on so there's sort of quite general

1205
01:19:28,140 --> 01:19:33,380
it has some weaknesses but it actually gives a lot of insight i think into

1206
01:19:33,380 --> 01:19:34,710
what's going on

1207
01:19:34,720 --> 01:19:40,020
in some learning scenarios but also into the proof techniques that we revisited so you'll

1208
01:19:40,020 --> 01:19:42,030
see some of the proof techniques we we've

1209
01:19:42,080 --> 01:19:43,380
i already looked at

1210
01:19:43,380 --> 01:19:47,820
kind of more often reappearing in the rademacher proof

1211
01:19:48,970 --> 01:19:51,280
and all gain given application two

1212
01:19:51,310 --> 01:19:53,680
svm so you can actually see that working

1213
01:19:53,680 --> 01:19:58,330
where you are allowed to take into account the slack variables so in the result

1214
01:19:58,330 --> 01:19:59,180
i gave here

1215
01:19:59,200 --> 01:20:02,610
i assume the hard margin SVM with a clear separation

1216
01:20:02,630 --> 01:20:05,300
with the rademacher proof i'll show you

1217
01:20:05,820 --> 01:20:10,970
the case where you allow you know soft margin SVM with slack variables

1218
01:20:11,000 --> 01:20:11,950
OK so

1219
01:20:12,240 --> 01:20:19,010
so the the starting point for the rademacher proof is really something that is

1220
01:20:19,030 --> 01:20:20,210
studied in

1221
01:20:20,220 --> 01:20:23,380
probability theory and concentration inequalities

1222
01:20:23,600 --> 01:20:29,470
and this is a quite natural because in statistical learning is really about the reliability

1223
01:20:29,470 --> 01:20:34,120
and stability of inferences made from a random sample as indicated the beginning

1224
01:20:35,050 --> 01:20:39,210
and this is really what concentration inequalities study so

1225
01:20:39,220 --> 01:20:41,100
let me start with the

1226
01:20:41,110 --> 01:20:42,580
a relatively

1227
01:20:42,620 --> 01:20:45,560
simple example of a concentration inequality

1228
01:20:45,620 --> 01:20:50,060
our very soon give you a generalisation of this this example of this is known

1229
01:20:50,060 --> 01:20:52,100
as her things inequality

1230
01:20:54,130 --> 01:20:55,940
what it looks at is the

1231
01:20:58,610 --> 01:20:59,470
so far

1232
01:20:59,490 --> 01:21:03,630
a random sequence set of random variables and random variables

1233
01:21:03,640 --> 01:21:05,750
and you sample each of them

1234
01:21:05,810 --> 01:21:06,870
and then you

1235
01:21:06,880 --> 01:21:08,690
compute the average

1236
01:21:08,710 --> 01:21:09,900
of that

1237
01:21:09,950 --> 01:21:13,080
set of values that you get

1238
01:21:13,150 --> 01:21:15,080
and the question is

1239
01:21:15,110 --> 01:21:20,080
how far is that average that you compute from its expected value

1240
01:21:20,100 --> 01:21:22,750
you can imagine if you do lots and lots of times you get an average

1241
01:21:22,750 --> 01:21:26,480
of that some of the story an average of an average

1242
01:21:26,530 --> 01:21:31,850
and what you're interested in how likely in the particular case

1243
01:21:31,860 --> 01:21:37,780
how far will you expect that the average to be from that expected value

1244
01:21:40,480 --> 01:21:41,710
basically the

1245
01:21:41,720 --> 01:21:46,080
probability that is bigger than epsilon is bounded by this quantity

1246
01:21:46,080 --> 01:21:47,890
which is an exponential

1247
01:21:47,890 --> 01:21:50,270
with notice the end here

1248
01:21:50,290 --> 01:21:53,450
twenty eight m squared but this is sort of an EM implicitly on the bottom

1249
01:21:53,450 --> 01:21:56,140
here so it's really an m r

1250
01:21:56,160 --> 01:21:57,950
epsilon squared

1251
01:21:57,950 --> 01:22:00,790
so there's an exponential decay

1252
01:22:00,790 --> 01:22:02,480
of this probability

1253
01:22:02,500 --> 01:22:04,020
with the

1254
01:22:04,040 --> 01:22:05,620
sample size

1255
01:22:05,660 --> 01:22:09,310
so this is what we expect to see in concentration what it means is that

1256
01:22:09,310 --> 01:22:15,350
the distribution is concentrating around the mean value so that the chances of it being

1257
01:22:15,350 --> 01:22:17,580
significantly away from the mean

1258
01:22:17,580 --> 01:22:21,520
falls off very fast in fact exponentially

1259
01:22:22,750 --> 01:22:25,580
the critical quantities inbounding this are

1260
01:22:25,930 --> 01:22:31,600
the range of the individual random variables so these are assumed to have potentially different

1261
01:22:31,600 --> 01:22:33,370
ranges AI BI i

1262
01:22:33,410 --> 01:22:38,700
and you some of the squares of the differences between the upper bound and lower

1263
01:22:38,700 --> 01:22:40,200
bound of the range

1264
01:22:40,250 --> 01:22:41,470
over the

1265
01:22:41,520 --> 01:22:45,120
random variables so this sort of an implicit and i mean if you think of

1266
01:22:45,150 --> 01:22:49,970
being dating the same values may be one zero then this would just be and

1267
01:22:50,020 --> 01:22:54,350
on the bottom and you get one counseling e to the minus two and epsilon

1268
01:22:55,970 --> 01:22:58,890
the important point is you get this exponential decay

1269
01:22:58,910 --> 01:23:01,930
and so you get a very tight concentration around the mean

1270
01:23:01,950 --> 01:23:07,450
and that arises essentially from this taking the mean of a set of independent random

1271
01:23:07,450 --> 01:23:10,810
variables it's important to these independent

1272
01:23:10,850 --> 01:23:14,560
OK so sampled independently

1273
01:23:14,600 --> 01:23:15,700
OK so

1274
01:23:15,790 --> 01:23:23,120
we'd like to derive statistical learning theory results from concentration inequalities and to do that

1275
01:23:23,120 --> 01:23:27,330
we actually need a slightly more general version of a concentration inequality which is due

1276
01:23:27,330 --> 01:23:28,600
to diamond

1277
01:23:28,660 --> 01:23:32,020
so it has a very similar favour to this thing

1278
01:23:32,060 --> 01:23:35,930
but is slightly more general and therefore i'm afraid a bit more

1279
01:23:35,950 --> 01:23:37,600
a bit messy

1280
01:23:37,600 --> 01:23:40,410
so here it is messy as it is OK

1281
01:23:41,700 --> 01:23:46,080
what we're looking at here is some function so it more general because it's the

1282
01:23:46,100 --> 01:23:51,390
general function it's not just the average of those random variables so it some function

1283
01:23:51,390 --> 01:23:53,540
of the random variables that we compute

1284
01:23:53,580 --> 01:23:56,580
so the idea is we take a sample one you know if x one x

1285
01:23:56,580 --> 01:24:00,930
two x two xn and we compute that function of that sample and here it

1286
01:24:01,680 --> 01:24:03,540
and we're asking how close

1287
01:24:03,540 --> 01:24:04,700
is it too

1288
01:24:04,710 --> 01:24:08,020
the expected value that we would have if we did that

1289
01:24:10,270 --> 01:24:14,810
and the same kind of bound holds basically the

1290
01:24:14,850 --> 01:24:19,680
the probability that this is further than epsilon from the mean

1291
01:24:19,700 --> 01:24:23,370
is bounded again by an exponentially decaying quantity

1292
01:24:23,480 --> 01:24:26,480
and the

1293
01:24:26,500 --> 01:24:28,500
o point here is that the

1294
01:24:28,520 --> 01:24:31,950
it's slightly differently formulated because i've got CI i

1295
01:24:31,950 --> 01:24:35,450
it is the amount by which the function can change remember

1296
01:24:35,500 --> 01:24:36,250
in the

1297
01:24:36,250 --> 01:24:38,750
through it and i do it so many times

1298
01:24:39,940 --> 01:24:41,240
so it

1299
01:24:41,290 --> 01:24:43,890
and then i have to multiply the value zero

1300
01:24:43,930 --> 01:24:48,940
and then i i e to find the book through i have current i

1301
01:24:48,950 --> 01:24:51,430
so what you see now

1302
01:24:51,450 --> 01:24:53,930
is that b

1303
01:24:53,950 --> 01:24:57,640
twelve new zero

1304
01:24:57,640 --> 01:24:59,810
and xi i

1305
01:24:59,830 --> 01:25:02,390
find and

1306
01:25:02,480 --> 01:25:04,100
divided by l

1307
01:25:04,120 --> 01:25:06,080
so that's our prediction

1308
01:25:06,930 --> 01:25:09,860
approximately constant magnetic field inside

1309
01:25:09,910 --> 01:25:11,210
so no it

1310
01:25:11,220 --> 01:25:12,390
and this actually

1311
01:25:12,400 --> 01:25:14,350
it's a very good approximation

1312
01:25:14,370 --> 01:25:16,120
as long as l

1313
01:25:16,170 --> 01:25:18,150
the length of the solenoid

1314
01:25:18,280 --> 01:25:20,050
is substantially larger

1315
01:25:20,070 --> 01:25:22,210
and the radius of the solar

1316
01:25:22,220 --> 01:25:23,890
the radius would be

1317
01:25:23,990 --> 01:25:34,050
the radius of these looks

1318
01:25:34,250 --> 01:25:35,790
work out

1319
01:25:35,830 --> 01:25:38,660
numerical example

1320
01:25:38,690 --> 01:25:41,770
which is aimed at the demonstration that comes

1321
01:25:43,510 --> 01:25:45,440
we have here is solenoidal

1322
01:25:45,450 --> 01:25:48,360
well by and is about twenty eight hundred

1323
01:25:48,390 --> 01:25:52,660
and l is about sixty centimeters is o point six metres

1324
01:25:52,670 --> 01:25:56,250
and i'm going to run through their current which i really don't know yet what

1325
01:25:56,260 --> 01:25:59,260
is going to be close to four and a half and you will see

1326
01:25:59,360 --> 01:26:02,580
when we do the demonstration

1327
01:26:02,620 --> 01:26:06,960
so i can calculate now with the magnetic field is going to be

1328
01:26:06,960 --> 01:26:09,270
so the magnetic field strength

1329
01:26:09,330 --> 01:26:12,090
it's going to be four pi

1330
01:26:12,100 --> 01:26:14,270
times ten to the minus seven

1331
01:26:14,370 --> 01:26:16,700
that's what new zero is

1332
01:26:16,700 --> 01:26:18,400
and then i have to multiply

1333
01:26:19,010 --> 01:26:21,290
twenty eight hundred

1334
01:26:21,410 --> 01:26:24,580
you have to multiply divided by point six

1335
01:26:24,620 --> 01:26:27,190
and then i multiplied by the current

1336
01:26:27,220 --> 01:26:29,210
four point five

1337
01:26:29,270 --> 01:26:31,680
and when i do that i find about

1338
01:26:31,720 --> 01:26:34,080
o point o two six test life

1339
01:26:34,140 --> 01:26:36,080
o point o two six

1340
01:26:36,170 --> 01:26:37,350
that's why

1341
01:26:37,410 --> 01:26:40,390
that is about two hundred and sixty gun

1342
01:26:40,470 --> 01:26:43,450
the duty experiment the current will be a little different

1343
01:26:43,460 --> 01:26:47,000
but you will see that indeed if you will be very close

1344
01:26:47,010 --> 01:26:50,920
two hundred and sixty gauss

1345
01:26:50,960 --> 01:26:52,420
why is is it

1346
01:26:52,440 --> 01:26:56,520
that the magnetic field is not proportional to the number of loops

1347
01:26:56,580 --> 01:26:59,010
but proportional to the number of look

1348
01:26:59,010 --> 01:27:01,250
per unit length

1349
01:27:01,260 --> 01:27:04,650
you may say well if i have one loop forever so the magnetic field two

1350
01:27:04,650 --> 01:27:08,600
loops i have twice the magnetic field had look i have had times the magnetic

1351
01:27:11,390 --> 01:27:14,180
imagine that we start sort or

1352
01:27:14,230 --> 01:27:16,010
it be seven

1353
01:27:16,060 --> 01:27:18,500
so here's is the sort of his lobby seven

1354
01:27:18,690 --> 01:27:23,600
here is that solenoid goes

1355
01:27:23,670 --> 01:27:24,790
all the way

1356
01:27:24,840 --> 01:27:26,010
all the way

1357
01:27:26,020 --> 01:27:29,710
thousands and thousands and thousands and thousands of books

1358
01:27:29,710 --> 01:27:31,070
and we end up here

1359
01:27:31,090 --> 01:27:34,840
twenty six one hundred

1360
01:27:34,890 --> 01:27:35,760
look at this

1361
01:27:35,780 --> 01:27:38,490
look i think of it that the first look

1362
01:27:38,490 --> 01:27:41,150
creates a magnetic field

1363
01:27:41,190 --> 01:27:44,480
one is the shape of the magnetic field qualities of current loop

1364
01:27:44,490 --> 01:27:46,240
we discussed last time

1365
01:27:46,250 --> 01:27:51,000
the magnetic field that one little produces is like a dipole field

1366
01:27:51,010 --> 01:27:54,700
so do you really think that you in twenty six one hundred we can sense

1367
01:27:54,720 --> 01:27:59,440
the magnetic field there is produced by this one doing it or look

1368
01:27:59,460 --> 01:28:01,550
practically nothing

1369
01:28:01,570 --> 01:28:06,260
it falls off so rapidly the magnetic field that we don't notice notice here

1370
01:28:06,270 --> 01:28:11,020
so it's immediately obvious that the magnetic field is not proportional to how many of

1371
01:28:11,100 --> 01:28:11,980
you have

1372
01:28:11,980 --> 01:28:15,660
if however you put all those links on top of of each other

1373
01:28:15,740 --> 01:28:18,720
then of course you can add the magnetic field

1374
01:28:18,760 --> 01:28:20,340
so it is natural

1375
01:28:20,390 --> 01:28:22,750
that you get how many windings you have

1376
01:28:22,750 --> 01:28:28,250
so that you take full advantage of a lot of information taking advantage of both

1377
01:28:28,260 --> 01:28:33,790
the gradient knowledge of the gradients analysis you taking advantage of 1st and 2nd derivative

1378
01:28:33,800 --> 01:28:35,850
information about the function

1379
01:28:36,190 --> 01:28:47,130
because that being a good thing they are so tonight I remark at the top

1380
01:28:47,130 --> 01:28:55,560
of page 3 today's price the method actually assumes that the Hessian is invertible at

1381
01:28:55,560 --> 01:29:01,930
each iteration which might not be so we look for classes of problems in which

1382
01:29:01,930 --> 01:29:10,730
we can guarantee that for we need some mechanism for enforcing or modifying that whatever

1383
01:29:10,730 --> 01:29:11,530
we do

1384
01:29:11,560 --> 01:29:16,640
we we've got something that's invertible and has a remark at the top of the

1385
01:29:20,210 --> 01:29:25,360
right there is no there there's no guarantee is written as method and my objective

1386
01:29:25,360 --> 01:29:31,530
function decreases as 1 would think in any intelligent algorithms as I go from in

1387
01:29:31,530 --> 01:29:34,540
the area of improving my objective function

1388
01:29:36,110 --> 01:29:40,750
but so far I have no guarantee that innocent

1389
01:29:41,430 --> 01:29:43,550
and we'll talk more about that later

1390
01:29:44,590 --> 01:29:48,250
and then as stated the next point is that

1391
01:29:48,710 --> 01:29:52,920
is step 2 could be augmented by a line search what I mean by that

1392
01:29:52,940 --> 01:30:00,910
so that if I think of method they here I am I came here

1393
01:30:01,710 --> 01:30:09,870
in a sense 1 can think of an algorithm as just being something at each

1394
01:30:09,870 --> 01:30:15,350
iteration each iteration the 1st computer direction in which to go

1395
01:30:15,360 --> 01:30:21,010
so I have they have this direction decay

1396
01:30:21,190 --> 01:30:26,880
and I need a mechanism for figuring out how far should go in that direction

1397
01:30:27,120 --> 01:30:33,880
and if I go my step length is 1 that means take full Newton step

1398
01:30:34,250 --> 01:30:43,490
I could and trying to solve the following one-dimensional problems clearly minimize over alpha xk

1399
01:30:43,640 --> 01:30:48,270
+ alpha decay so this whole here

1400
01:30:48,730 --> 01:30:53,800
I'm going to look for a one-dimensional minimum of my function

1401
01:30:54,270 --> 01:31:01,490
and actually there's a homework exercises we decided not to assign you where I could

1402
01:31:01,490 --> 01:31:08,850
have asked you to prove the convergence of Newton's method with all of inexact lines

1403
01:31:10,010 --> 01:31:16,110
so that that's the key features of Newton's method

1404
01:31:21,290 --> 01:31:23,300
even before I go any further

1405
01:31:23,790 --> 01:31:28,370
why is Newton's method was was developed

1406
01:31:28,510 --> 01:31:33,140
what is the most well then you end up into the following questions what is

1407
01:31:33,140 --> 01:31:39,140
the most well-known very attractive feature of Newton's method at least from a theoretical point

1408
01:31:39,140 --> 01:31:41,030
of view

1409
01:31:41,040 --> 01:31:42,820
you know

1410
01:31:43,800 --> 01:31:49,280
OK actually not necessarily

1411
01:31:49,430 --> 01:31:55,030
but if it does converge it converges at an extremely fast rate

1412
01:31:55,650 --> 01:31:59,390
and extremely festering will see that shortly

1413
01:31:59,410 --> 01:32:08,470
but before I do we get to that let me go over other features

1414
01:32:08,530 --> 01:32:09,600
of the method

1415
01:32:13,610 --> 01:32:18,570
so that you can read this

1416
01:32:19,550 --> 01:32:31,910
it is that is the direction

1417
01:32:32,730 --> 01:32:37,130
and the 1st thing that approved you before we talk about convergence rates Proposition 1

1418
01:32:37,130 --> 01:32:45,090
. 1 and find out where recall sold right we see matrix is SPD

1419
01:32:45,100 --> 01:32:49,320
that is symmetric and positive definite and what is positive definite me

1420
01:32:52,850 --> 01:32:57,570
I wanted to say about people so ingenuity is the person next to you is

1421
01:32:57,670 --> 01:33:02,040
I like land what is positive definite meaning

1422
01:33:02,830 --> 01:33:13,970
is multiplied by a factor of of the group as long as the vector is

1423
01:33:13,970 --> 01:33:20,450
nonzero very much so that means symmetric positive definite and Proposition 1 . 1

1424
01:33:20,850 --> 01:33:23,290
says that

1425
01:33:23,810 --> 01:33:27,280
get existing

1426
01:33:28,490 --> 01:33:33,330
then the Newton step

1427
01:33:33,450 --> 01:33:48,430
and the if 0 that is not 0 then D is the set of X

1428
01:33:48,510 --> 01:33:52,620
and Y and recall from the informally what it means for a direction to be

1429
01:33:52,630 --> 01:33:53,510
a descent direction

1430
01:33:53,990 --> 01:33:56,330
this is a descent direction

1431
01:33:56,570 --> 01:34:05,170
X was going to be a descent direction at x yeah function

1432
01:34:05,250 --> 01:34:13,840
OK for a long as 1 as go enough distances right this is the direction

1433
01:34:13,840 --> 01:34:19,780
of decreasing the function of the however proved that well 1st

1434
01:34:19,780 --> 01:34:24,050
is order t one over p

1435
01:34:28,430 --> 01:34:33,430
you the proof here

1436
01:34:35,150 --> 01:34:39,200
he said he

1437
01:34:39,300 --> 01:34:41,220
is most t one

1438
01:34:41,260 --> 01:34:42,380
over p

1439
01:34:42,470 --> 01:34:45,400
as t infinity

1440
01:34:45,450 --> 01:34:47,760
well this is or t

1441
01:34:47,780 --> 01:34:50,760
one over p the whole thing is for t one of

1442
01:35:00,130 --> 01:35:01,220
and so

1443
01:35:01,240 --> 01:35:02,680
now i

1444
01:35:02,700 --> 01:35:03,950
i have p

1445
01:35:03,990 --> 01:35:06,030
is or t one over p

1446
01:35:06,070 --> 01:35:11,150
and what we need is that to compute t one over

1447
01:35:12,550 --> 01:35:16,410
and that's going to be or p

1448
01:35:21,510 --> 01:35:24,700
everybody that so what that says is that if i have a certain amount of

1449
01:35:26,910 --> 01:35:32,490
if i run essentially on fewer processors than that heroism i get linear speed

1450
01:35:32,530 --> 01:35:36,470
phase rescheduling

1451
01:35:37,240 --> 01:35:40,570
one or more processors than the parallelism

1452
01:35:40,610 --> 01:35:45,090
some sense and being wasteful because i can't possibly get enough speed up to justify

1453
01:35:45,090 --> 01:35:47,550
those extra processes

1454
01:35:47,570 --> 01:35:50,820
so understanding the parallelism of job says

1455
01:35:50,970 --> 01:35:54,070
that's sort of a limit on the number of processors i want to have in

1456
01:35:54,130 --> 01:35:57,130
fact i can achieve that question

1457
01:36:02,200 --> 01:36:05,630
in this

1458
01:36:05,880 --> 01:36:07,930
and so

1459
01:36:07,950 --> 01:36:09,950
if you see

1460
01:36:10,180 --> 01:36:17,570
really in some sense this is saying it should be omega p yes so that

1461
01:36:17,570 --> 01:36:18,630
that's fine

1462
01:36:18,670 --> 01:36:23,450
it's a question of so so so i ask again because

1463
01:36:30,490 --> 01:36:40,800
if you know it's only if is bounded above by a constant is not

1464
01:36:40,840 --> 01:36:44,430
t one and t infinity are constants

1465
01:36:44,450 --> 01:36:46,740
the variables in this

1466
01:36:46,780 --> 01:36:51,220
so this is more you know we're doing multivariable

1467
01:36:51,220 --> 01:36:53,150
asymptotic analysis so

1468
01:36:53,150 --> 01:36:56,590
any of these things can be a function of anything else and can be

1469
01:36:56,610 --> 01:36:58,610
growing as much as we want

1470
01:36:58,610 --> 01:37:01,950
the fact that we say we're given it for particular thing we're really not given

1471
01:37:01,950 --> 01:37:04,280
that number we give a whole class of

1472
01:37:04,360 --> 01:37:08,930
dags or whatever of various sizes is really what we're talking about so i can

1473
01:37:08,930 --> 01:37:10,650
look at the growth

1474
01:37:10,650 --> 01:37:12,900
he was talking about the growth of

1475
01:37:12,910 --> 01:37:16,050
you know of the

1476
01:37:16,380 --> 01:37:20,760
of the parallelism sorry the growth of the

1477
01:37:21,010 --> 01:37:26,340
you know of the runtime TCP as a function of

1478
01:37:26,360 --> 01:37:28,130
t one and t infinity

1479
01:37:29,110 --> 01:37:32,740
so i am talking about things that are that are growing here

1480
01:37:38,510 --> 01:37:40,530
so let's

1481
01:37:40,550 --> 01:37:42,470
let's put this to work

1482
01:37:43,490 --> 01:37:46,010
in fact number

1483
01:37:46,010 --> 01:37:47,860
let's go back to here

1484
01:37:47,880 --> 01:37:52,650
it's a little bit of my own research

1485
01:37:52,670 --> 01:37:57,170
and how we use this in some of the work that we did

1486
01:37:57,180 --> 01:38:01,990
so we developed a

1487
01:38:02,010 --> 01:38:07,550
dynamic multi threaded language called silk spelled c because it's based on the language c

1488
01:38:08,650 --> 01:38:10,910
it's not an acronym

1489
01:38:11,050 --> 01:38:15,800
it's because silk is like nice threads

1490
01:38:15,890 --> 01:38:24,340
and although we at one point my students had a competition for

1491
01:38:24,400 --> 01:38:26,200
what the acronym so

1492
01:38:26,280 --> 01:38:29,180
could mean

1493
01:38:29,200 --> 01:38:34,680
and they came up with the the winner turns out with charles' idiotic linguistic clues

1494
01:38:35,300 --> 01:38:38,510
so anyway

1495
01:38:38,510 --> 01:38:40,610
you want to take a look

1496
01:38:40,680 --> 01:38:43,930
and you can find some stuff on here

1497
01:38:55,490 --> 01:38:56,970
and what uses

1498
01:38:56,990 --> 01:39:00,010
it is actually one of these more complicated schedulers

1499
01:39:02,130 --> 01:39:05,610
online scheduler

1500
01:39:11,050 --> 01:39:14,320
and if you look at the expected run time

1501
01:39:14,470 --> 01:39:17,180
on p processors

1502
01:39:17,260 --> 01:39:21,180
it gets the sack effectively t one over p

1503
01:39:22,680 --> 01:39:24,200
o as t infinity

1504
01:39:32,610 --> 01:39:38,820
and empirically if you actually look at what kind of times you get

1505
01:39:38,820 --> 01:39:42,510
to find out what in the big o there it turns out

1506
01:39:42,510 --> 01:39:46,150
in fact it's t one people plus t infinity

1507
01:39:46,150 --> 01:39:49,590
with the constants here being very close to one empirical

1508
01:39:49,640 --> 01:39:54,220
no guarantees

1509
01:39:54,240 --> 01:39:57,010
but this turns out to be a pretty good bound

1510
01:39:57,050 --> 01:40:02,700
you know sometimes you see it coefficient infinity that's up maybe close to four something

1511
01:40:02,760 --> 01:40:04,950
but generally you don't see something

1512
01:40:04,950 --> 01:40:08,340
it's much bigger than that and mostly it tends to be around if you do

1513
01:40:09,820 --> 01:40:16,200
linear regression curve that you get the constant here is close to one

1514
01:40:16,240 --> 01:40:21,670
and so you get with this you get near perfect if used

1515
01:40:22,680 --> 01:40:29,510
this formalism model here run time you get near perfect linear speedup

1516
01:40:35,090 --> 01:40:40,450
the number of processors to running on is much less than your average girls

1517
01:40:40,470 --> 01:40:44,630
which of course is the same thing as if t

1518
01:40:44,670 --> 01:40:51,260
infinity is much less than t one over

1519
01:40:51,320 --> 01:40:54,070
so what happens here is that

1520
01:40:54,950 --> 01:40:59,090
when p is much less than p infinity as t infinity is much less than

1521
01:40:59,130 --> 01:40:59,950
t one over p

1522
01:40:59,950 --> 01:41:02,950
and then we reformulate the original problem

1523
01:41:02,990 --> 01:41:07,800
so this is not problem is formulated in terms of

1524
01:41:07,890 --> 01:41:11,270
let's get the match is defined based on white

1525
01:41:12,140 --> 01:41:14,960
constant is also revised accordingly

1526
01:41:15,090 --> 01:41:22,400
thank you wanted and the constant throughout revised weights according to the

1527
01:41:22,460 --> 01:41:24,370
it and that is why

1528
01:41:24,380 --> 01:41:26,120
and now

1529
01:41:26,200 --> 01:41:29,600
to solve this problem we propose we

1530
01:41:29,620 --> 01:41:31,800
robert the theorem in the paper

1531
01:41:31,850 --> 01:41:36,870
this theorem states that the solution to this new NPC pollen that typically for

1532
01:41:36,890 --> 01:41:38,500
people one

1533
01:41:38,510 --> 01:41:41,180
if the you need less i can better

1534
01:41:41,200 --> 01:41:43,460
corresponding to the largest eigen value

1535
01:41:45,830 --> 01:41:47,730
i had a problem

1536
01:41:48,600 --> 01:41:50,870
the single

1537
01:41:50,880 --> 01:41:53,970
c is the defined

1538
01:41:54,070 --> 01:41:58,180
as follows the it the division is basically

1539
01:41:58,200 --> 01:41:59,260
based on the

1540
01:41:59,280 --> 01:42:00,770
the fermat

1541
01:42:02,970 --> 01:42:06,120
the according to the majesty

1542
01:42:06,130 --> 01:42:07,350
because each of the

1543
01:42:07,380 --> 01:42:10,110
coordinate vector

1544
01:42:10,120 --> 01:42:11,430
i'm not going to

1545
01:42:11,440 --> 01:42:15,810
give the details of derivation and you can read the paper all come to the

1546
01:42:15,810 --> 01:42:18,710
poster just outside of these

1547
01:42:19,690 --> 01:42:23,320
so by now i have proposed to the UN CC angry

1548
01:42:23,340 --> 01:42:28,550
so it's time to evaluate the problem that the proposed angry them

1549
01:42:28,570 --> 01:42:33,460
so this is the right from the top which was surrounded the face recognition problem

1550
01:42:33,760 --> 01:42:36,100
of sublime face recognition problem

1551
01:42:36,210 --> 01:42:38,590
that we have to is

1552
01:42:38,630 --> 01:42:41,350
the feret database and which was the subset

1553
01:42:41,400 --> 01:42:42,930
so that the maximum

1554
01:42:42,940 --> 01:42:46,230
pose variation in fifteen degrees

1555
01:42:46,250 --> 01:42:49,730
and the minimum number of face images per subject in is

1556
01:42:50,960 --> 01:42:52,780
this results in

1557
01:42:52,810 --> 01:42:56,140
seven hundred and twenty one thousand images

1558
01:42:56,170 --> 01:42:58,610
from seventy seven

1559
01:42:58,620 --> 01:42:59,980
and we have to the other

1560
01:42:59,990 --> 01:43:01,520
feature extraction so it

1561
01:43:01,550 --> 01:43:03,440
preprocessing by

1562
01:43:03,510 --> 01:43:06,230
many parking and alignment

1563
01:43:06,270 --> 01:43:11,000
and it is all the images are normalized to a by itself with two hundred

1564
01:43:11,000 --> 01:43:15,460
fifty fifth grade three level but because

1565
01:43:15,470 --> 01:43:17,280
and for the classification

1566
01:43:17,340 --> 01:43:20,720
we feel that features and is simple classification method

1567
01:43:20,730 --> 01:43:22,490
nearest neighbour classifiers

1568
01:43:22,690 --> 01:43:24,570
euclidean distance measure

1569
01:43:24,580 --> 01:43:29,580
and for the performance evaluation of classification we use the report

1570
01:43:29,670 --> 01:43:34,780
rank one identification rate

1571
01:43:34,840 --> 01:43:37,350
so here are the results

1572
01:43:37,350 --> 01:43:40,520
first of all i want here

1573
01:43:40,960 --> 01:43:45,490
you know the number of training examples presented to have good one is

1574
01:43:45,510 --> 01:43:48,050
the only one thing is simple

1575
01:43:48,110 --> 01:43:53,230
subject this this is extremely small sample size scenario

1576
01:43:53,300 --> 01:43:56,010
it's very difficult and

1577
01:43:56,020 --> 01:43:59,760
the estimated that the supplies mesenchymal kind of cells

1578
01:44:00,610 --> 01:44:05,450
the mission of supplies but cannot solve this problem because we have only one sample

1579
01:44:05,450 --> 01:44:08,970
you can now estimate the within class scatter

1580
01:44:09,010 --> 01:44:12,770
and here the results the first one is for them metadata different one

1581
01:44:14,330 --> 01:44:16,090
right from fifteen

1582
01:44:16,120 --> 01:44:18,620
two up to eighty

1583
01:44:18,630 --> 01:44:21,900
and the proposed method is marked by

1584
01:44:24,480 --> 01:44:25,510
i meant

1585
01:44:26,820 --> 01:44:28,970
from the results we had

1586
01:44:28,970 --> 01:44:31,690
i think that this proposed new entity

1587
01:44:31,800 --> 01:44:35,140
eight outperforms the other three methods

1588
01:44:35,150 --> 01:44:36,560
fifty and can see

1589
01:44:37,580 --> 01:44:40,120
he out

1590
01:44:40,180 --> 01:44:41,280
and the we

1591
01:44:41,320 --> 01:44:42,340
for the domain

1592
01:44:43,070 --> 01:44:45,450
l you could two seven

1593
01:44:45,460 --> 01:44:48,370
here are the results in the show

1594
01:44:48,380 --> 01:44:51,100
and we have a similar observation

1595
01:44:51,200 --> 01:44:55,130
the UN CC outperform the other three methods as well

1596
01:44:55,190 --> 01:44:58,480
and for the other results from two to six

1597
01:44:58,530 --> 01:45:00,840
it outside of the cell

1598
01:45:00,850 --> 01:45:04,240
you can come to the poster

1599
01:45:04,250 --> 01:45:05,810
and since we have

1600
01:45:05,830 --> 01:45:07,560
this method maximizes

1601
01:45:08,590 --> 01:45:12,900
and actually the observation from the previous results we of that

1602
01:45:12,970 --> 01:45:16,500
you can see it saturates

1603
01:45:16,520 --> 01:45:20,270
earlier than the other methods the problem is set route

1604
01:45:20,330 --> 01:45:26,450
earlier around ten t thirty eight thirty eight very fast so

1605
01:45:26,460 --> 01:45:33,140
this motivates us to examine the various captured by space here at the recently

1606
01:45:33,220 --> 01:45:34,460
plot the

1607
01:45:34,470 --> 01:45:37,180
variation captured managed by scatter

1608
01:45:37,180 --> 01:45:40,330
love you i love you

1609
01:45:40,350 --> 01:45:41,950
it is for one

1610
01:45:41,960 --> 01:45:44,080
at the two

1611
01:45:45,200 --> 01:45:48,680
the problem is that must by diamonds

1612
01:45:48,690 --> 01:45:50,930
OK although from the

1613
01:45:53,330 --> 01:45:56,780
that the british captured by UN is much

1614
01:45:58,060 --> 01:46:01,860
the other methods right

1615
01:46:01,860 --> 01:46:03,620
and this is due to

1616
01:46:03,640 --> 01:46:04,520
the very

1617
01:46:04,540 --> 01:46:05,800
constant solution

1618
01:46:05,840 --> 01:46:10,810
so we have their correlation and we also have the content of

1619
01:46:10,830 --> 01:46:14,740
it must be a tensor to vector direct projection

1620
01:46:14,760 --> 01:46:15,880
and this

1621
01:46:16,730 --> 01:46:18,810
parishes space

1622
01:46:18,830 --> 01:46:24,230
the saturation because we is to low the variation it too limited contribution

1623
01:46:24,240 --> 01:46:26,430
if the recognition

1624
01:46:26,440 --> 01:46:32,700
so it clearly is uncorrelated with mathematical proof here as a simulation study we

1625
01:46:32,750 --> 01:46:38,110
of the partition the average correlation between a pair i feature

1626
01:46:38,150 --> 01:46:39,470
and he had be done

1627
01:46:39,520 --> 01:46:42,240
the above is PC and you the

1628
01:46:42,250 --> 01:46:42,980
the other thing

1629
01:46:42,980 --> 01:46:47,650
today I'll be talking about Bayesian parametrics and I'll I guess I'll start

1630
01:46:47,650 --> 01:46:50,930
off with a bit of introduction to

1631
01:46:51,030 --> 01:46:55,090
Bayesian machine learning first I guess in in the last week you have

1632
01:46:55,090 --> 01:47:01,600
heard a lot about various Bayesian approaches to machine learning and statistics so

1633
01:47:01,600 --> 01:47:09,560
this this is a bit of a recapping it so let's start with probabilistic modeling okay well

1634
01:47:09,670 --> 01:47:13,490
as we all know machine learning is all about data and we know data has lots

1635
01:47:13,490 --> 01:47:18,380
of different has lots of noise and uncertainty and so forth and a big part

1636
01:47:18,380 --> 01:47:23,430
of learning from data is about handling of being able to reason with the

1637
01:47:23,430 --> 01:47:26,580
noise in the data all right and

1638
01:47:26,800 --> 01:47:32,580
the idea of probabilistic modeling is that the this language of probability theory is

1639
01:47:32,580 --> 01:47:38,480
have a very reach language to express all of these uncertainties about about data and and

1640
01:47:38,480 --> 01:47:42,900
allows us to to reason coherently

1641
01:47:43,570 --> 01:47:49,210
about this data and the so of a models that we use to express

1642
01:47:49,240 --> 01:47:52,460
these uncertainties about data are called probabilistic models

1643
01:47:52,850 --> 01:47:59,410
okay so we'll see a number of these examples later and I think probabilistic

1644
01:47:59,410 --> 01:48:04,800
models has been very successful especially in the last ten years in in both

1645
01:48:04,800 --> 01:48:11,600
statistics and machine learning and the reason is because well there's kind of a few reasons the first one

1646
01:48:11,600 --> 01:48:13,660
is that it allows us to

1647
01:48:13,710 --> 01:48:17,430
kind of like visualize

1648
01:48:17,460 --> 01:48:21,300
all models of the data at a high level

1649
01:48:21,350 --> 01:48:25,710
without actually worrying about all the little details of

1650
01:48:26,320 --> 01:48:31,800
about a model okay another reason is that we can do very complex models from

1651
01:48:31,800 --> 01:48:36,010
lots of simple parts and and we can do both of these using

1652
01:48:36,010 --> 01:48:41,570
the using the language of graphical models and which I which I guess you're

1653
01:48:41,570 --> 01:48:44,630
probably familiar with

1654
01:48:44,660 --> 01:48:46,740
and another reason is that

1655
01:48:46,770 --> 01:48:53,050
it allows us to separate out the modeling questions from the computational questions right so it

1656
01:48:53,050 --> 01:48:57,680
it allows the modeler to build a comma with a model and then it it allows somebody

1657
01:48:57,680 --> 01:49:00,120
else to build to derive algorithms

1658
01:49:00,150 --> 01:49:06,090
for doing inference and learning in the models given the models that the modeler

1659
01:49:06,090 --> 01:49:11,300
has come out with and this I think this separation of the modeling question

1660
01:49:11,300 --> 01:49:17,830
from the algorithmic questions is very important basically because

1661
01:49:17,890 --> 01:49:20,400
there's a few reasons

1662
01:49:20,450 --> 01:49:25,450
one of the reasons is that the people who known their domains well

1663
01:49:25,610 --> 01:49:29,170
may may not be the same people who can program well and the people who can

1664
01:49:29,170 --> 01:49:30,610
program well may not build to know

1665
01:49:30,840 --> 01:49:35,610
biology or linguistics domain well so that allows us

1666
01:49:35,650 --> 01:49:41,230
a separation of efforts into the modelings of efforts and the algorithmics of

1667
01:49:41,230 --> 01:49:46,800
effort and this allows the field to to advance a lot faster another reason

1668
01:49:46,840 --> 01:49:52,930
for for this separation of modeling and al algorithmic questions is because

1669
01:49:53,020 --> 01:49:58,520
when you do that you're modeling your data you'd like to to think about

1670
01:49:58,950 --> 01:50:04,150
the statistical questions without having to worry about all the different approximations that you have

1671
01:50:04,150 --> 01:50:07,780
to do to get the model to work on your data and this is very

1672
01:50:07,780 --> 01:50:12,170
nice because it allows us to think about in the ideal situation what should be

1673
01:50:12,800 --> 01:50:16,730
and that should be separated from what could be done on your data so that

1674
01:50:16,730 --> 01:50:18,430
you can so that

1675
01:50:18,500 --> 01:50:21,900
if it works if your

1676
01:50:21,990 --> 01:50:26,270
algorithms will orders and what you can count like localize

1677
01:50:26,350 --> 01:50:30,730
and figure out where is it that it's gone wrong

1678
01:50:32,870 --> 01:50:38,270
so what is probabilistic modeling basically a probabilistic model is simply a

1679
01:50:38,270 --> 01:50:45,400
joint's distribution over a set of random variables okay so so here's our joint's distribution and you

1680
01:50:45,400 --> 01:50:51,180
would typically be parametrized by some parameter theta and typically we can think about this

1681
01:50:51,230 --> 01:50:53,170
probabilistic models as

1682
01:50:53,210 --> 01:50:58,230
generative models so the idea is that we can postulate a particle of generative

1683
01:50:58,230 --> 01:51:04,230
process that could have led to the observations that we actually do observe

1684
01:51:04,530 --> 01:51:05,940
right so

1685
01:51:08,640 --> 01:51:11,300
all right okay I see so

1686
01:51:11,350 --> 01:51:13,290
and the idea is that

1687
01:51:13,320 --> 01:51:19,310
with this joint's distribution you can then do things like inference which is to

1688
01:51:19,310 --> 01:51:22,730
basically estimate what were the

1689
01:51:22,760 --> 01:51:30,300
likely states of our unobserved random variables given the states of our observed data

1690
01:51:30,300 --> 01:51:33,910
okay so this allows us to infer what we did not observe okay

1691
01:51:34,050 --> 01:51:41,240
and of course this conditional distribution of the unobserved variables given the observed

1692
01:51:41,240 --> 01:51:47,170
ones is simply given by base rule all right so this is just ratio of the

1693
01:51:47,180 --> 01:51:52,380
joint's distribution divided by the marginal distribution of the observed data

1694
01:51:52,550 --> 01:51:55,760
and of course

1695
01:51:56,640 --> 01:52:02,590
don't typically know the values for for the parameters for our model and we'd like

1696
01:52:02,590 --> 01:52:04,170
to learn about those as well

1697
01:52:08,180 --> 01:52:13,640
the process of finding the likely settings of the parameteres which could have

1698
01:52:13,680 --> 01:52:18,820
led to our data is called learning and this is typically done by maximum

1699
01:52:18,820 --> 01:52:24,090
likelihood right so we'd like to find the parameter setting which gives highest probably

1700
01:52:24,110 --> 01:52:25,300
D to our

1701
01:52:27,000 --> 01:52:32,710
okay there's lots of different things that you could do with probabilistic models you

1702
01:52:32,710 --> 01:52:38,970
could do things like prediction so prediction this is can be phrased as basically computing

1703
01:52:38,970 --> 01:52:43,090
the conditional distribution of some unobserved

1704
01:52:43,110 --> 01:52:47,490
data from the observed ones so you can think of this as your test set

1705
01:52:47,490 --> 01:52:49,090
and as your training set

1706
01:52:49,170 --> 01:52:53,500
we can do classification so

1707
01:52:53,610 --> 01:52:57,850
if we have a bunch of different models each of them indexed by C

1708
01:52:57,990 --> 01:53:02,440
and given by a different parameter then you could decide

1709
01:53:02,490 --> 01:53:07,350
given tested item which class it belongs to buy basically assigning it to

1710
01:53:07,350 --> 01:53:12,170
the to the category or to the class that gives at highest probably the under

1711
01:53:12,170 --> 01:53:15,260
its probabilistic model okay

1712
01:53:16,440 --> 01:53:20,640
and in addition to this of course there's lots of different other things that you could do

1713
01:53:20,640 --> 01:53:23,060
I guess these are a bit

1714
01:53:23,180 --> 01:53:28,870
these are things like visualization so you can you'd like to visualize

1715
01:53:28,880 --> 01:53:35,050
degenerative process that have could have led to your data and this is also very useful

1716
01:53:35,050 --> 01:53:39,940
in things like interpreting the data and summarizing the data and so forth okay

1717
01:53:40,320 --> 01:53:45,370
and as I say before there's this separation of the modeling question from the algorithmic questions

1718
01:53:45,370 --> 01:53:46,440
and the

1719
01:53:46,490 --> 01:53:50,620
one of the nice things with probabilistic models is that there are lots of

1720
01:53:51,620 --> 01:53:57,640
there're lots of standard ways in which you could derive algorithms for your models okay so you can derive

1721
01:53:57,640 --> 01:54:03,160
inference algorithms learning algorithms and so forth and this algorithms things like you know

1722
01:54:03,180 --> 01:54:10,170
the expectation propagation the expe expectation maximization algorithm junction tree there's variational

1723
01:54:10,170 --> 01:54:14,790
inference expectation propagation Markov chain Monte Carlo sampling and so forth

1724
01:54:14,790 --> 01:54:17,820
but i that's the count parameters

1725
01:54:17,840 --> 01:54:20,980
because i can't get publicity theory

1726
01:54:23,400 --> 01:54:24,070
but o

1727
01:54:24,090 --> 01:54:27,120
i did check should i guess that would be a good thing to do

1728
01:54:27,940 --> 01:54:30,870
actually cost you do that was not the original

1729
01:54:31,610 --> 01:54:32,990
remember the error rate

1730
01:54:33,110 --> 01:54:37,210
ninety five per cent

1731
01:54:37,240 --> 01:54:40,040
have to mine OK

1732
01:54:41,050 --> 01:54:45,440
one thing people who have been doing that algorithms seem to not which i don't

1733
01:54:46,290 --> 01:54:48,720
like so much myself the swiss roll data

1734
01:54:49,600 --> 01:54:52,570
additional dataset now what you have here is to say well this is a non

1735
01:54:52,570 --> 01:54:57,810
linear embedding is the swiss roll you can see that clearly have done this coloring

1736
01:54:57,810 --> 01:55:02,920
trick they use and to i get is a natural question to ask what happens

1737
01:55:05,070 --> 01:55:10,470
in this example you got three dimension example is in the phi is not initialisation

1738
01:55:10,470 --> 01:55:15,340
aside this is an initialisation aside if you see what i mean it'll be on

1739
01:55:16,960 --> 01:55:20,260
the unit shot PCA

1740
01:55:20,270 --> 01:55:23,450
you get what you might say all these embedding guys would say it is of

1741
01:55:23,450 --> 01:55:24,980
course solution

1742
01:55:25,000 --> 01:55:26,430
i could be seen

1743
01:55:26,440 --> 01:55:32,130
in what criteria judging things is the cornish isation what the sort

1744
01:55:32,140 --> 01:55:38,410
this is the map which is the sort of multidimensional scaling algorithm and the examples

1745
01:55:38,410 --> 01:55:43,530
taken from a nice man so obviously works very nicely friedman so initialize with isomap

1746
01:55:43,720 --> 01:55:47,570
of solution but many local minima solution

1747
01:55:48,760 --> 01:55:53,550
and that one can be dismissed much when you at least

1748
01:55:53,600 --> 01:55:57,840
i think interpretation this one so i can things like PCA

1749
01:55:57,850 --> 01:56:03,700
or isomap or anything else by using smart initialisation i don't use PCA for argument

1750
01:56:03,700 --> 01:56:06,770
is i not see any spiral structure

1751
01:56:06,790 --> 01:56:11,760
in this visualisation and this is because of his talk about visualizations now i you

1752
01:56:11,760 --> 01:56:15,700
can see spider this three d parts

1753
01:56:16,170 --> 01:56:18,660
but if this was really fifty six dimensional

1754
01:56:18,710 --> 01:56:24,400
then i would much of it seems to be this one is almost think someone

1755
01:56:24,460 --> 01:56:27,590
from the end of the swiss roll and a scorched on the floor

1756
01:56:27,600 --> 01:56:33,000
so you this spiral structure here which is telling you that some former spiral structures

1757
01:56:33,000 --> 01:56:38,910
are the best solution there so is that solution seems to be careful in saying

1758
01:56:39,760 --> 01:56:41,730
how well you've done

1759
01:56:41,750 --> 01:56:45,440
so i'm going to talk about

1760
01:56:45,460 --> 01:56:47,800
it is

1761
01:56:47,990 --> 01:56:51,800
for example people like to do is digits that's good example because it's much more

1762
01:56:51,800 --> 01:56:55,210
high dimensional than the oil data we talked about before

1763
01:56:55,210 --> 01:56:58,510
this is what it is is far from the the paper

1764
01:56:58,960 --> 01:57:04,670
zero is read the one is green that are associated with one very small is

1765
01:57:04,670 --> 01:57:08,860
RBF kernel MLP kernel to the blue one

1766
01:57:08,870 --> 01:57:15,420
three years in service of the between the ability using the threes and fours are

1767
01:57:16,360 --> 01:57:19,280
so again visualizations of those

1768
01:57:19,300 --> 01:57:22,510
sort of things and you can see with PCA

1769
01:57:22,510 --> 01:57:26,400
you get a lot of overlap GGM again we get a good separation

1770
01:57:26,420 --> 01:57:30,170
but we get this grading effect but once again if we look at the classification

1771
01:57:30,170 --> 01:57:36,090
rates GTM does the best services twenty five digits from USPS

1772
01:57:36,110 --> 01:57:40,900
dataset and that's what we needed space to take on the data to get training

1773
01:57:40,900 --> 01:57:42,510
data half the data

1774
01:57:42,530 --> 01:57:47,840
and take test data but due to performs the GPLVM

1775
01:57:48,280 --> 01:57:49,920
i it would be interesting to do that

1776
01:57:49,920 --> 01:57:54,070
gplvm rather my one and see what the results of and but i haven't done

1777
01:57:56,470 --> 01:57:58,220
how done

1778
01:57:58,220 --> 01:57:59,240
you mean

1779
01:57:59,320 --> 01:58:02,760
am i doing multi class

1780
01:58:02,840 --> 01:58:04,710
this is my idea

1781
01:58:04,720 --> 01:58:09,690
this the one i have code for is the same processes talking about same process

1782
01:58:09,690 --> 01:58:16,510
classifier is the same classifier in each case i guess the topic use isn't that

1783
01:58:17,590 --> 01:58:20,440
so non-linear one yet

1784
01:58:20,470 --> 01:58:24,470
so thanks for that clarification in case we do some down because one of the

1785
01:58:24,470 --> 01:58:26,550
things we can do

1786
01:58:27,780 --> 01:58:31,070
the sort of algorithms you can't do with kernel PCA

1787
01:58:31,090 --> 01:58:33,820
basically because we have

1788
01:58:33,840 --> 01:58:35,130
a mapping

1789
01:58:36,130 --> 01:58:38,490
the latent space we observe space

1790
01:58:39,570 --> 01:58:43,950
is we can take things in that space and quickly ask what it looks like

1791
01:58:43,950 --> 01:58:46,440
in the observe space

1792
01:58:46,490 --> 01:58:50,030
one was a real passion for calling this fantasy data

1793
01:58:50,110 --> 01:58:52,170
so what

1794
01:58:52,190 --> 01:58:53,630
this data set here

1795
01:58:53,650 --> 01:58:58,120
is the dataset you might be familiar with the attendant a video sequence of moving

1796
01:58:58,210 --> 01:59:02,630
brenda moving space around so these fantasies of brendan frey

1797
01:59:05,170 --> 01:59:10,320
it and this is the RBF kernel is the visualization is no associated label because

1798
01:59:10,320 --> 01:59:11,590
it's just to move it

1799
01:59:11,590 --> 01:59:15,650
now as i move around you can see i can make friends move around doing

1800
01:59:15,650 --> 01:59:17,220
different things

1801
01:59:18,420 --> 01:59:21,210
now clear is there twenty by twenty eight

1802
01:59:21,220 --> 01:59:22,440
he's up there

1803
01:59:22,440 --> 01:59:26,010
i should point out three twenty by twenty eight images so i hope it's clear

1804
01:59:26,010 --> 01:59:27,590
that you would to make it bigger

1805
01:59:30,970 --> 01:59:37,110
OK so i can move around it seems to kiss you

1806
01:59:37,370 --> 01:59:41,010
two point four

1807
01:59:44,630 --> 01:59:47,090
so you see there a lot of

1808
01:59:47,110 --> 01:59:53,260
where is on one side and then kind of movie five interpolated something wrong sign

1809
01:59:53,280 --> 01:59:59,900
that kind of thing that i've

1810
01:59:59,950 --> 02:00:04,860
so i mean there's no data to try to interpolate you see the uncertainty

1811
02:00:04,880 --> 02:00:06,090
yes hi

1812
02:00:06,190 --> 02:00:07,210
thing here

1813
02:00:07,210 --> 02:00:11,840
massive which is why you can't visualize the uncertainty so on

1814
02:00:11,860 --> 02:00:16,030
i don't because it's an RBF kernels i talked to just go back to his

1815
02:00:16,030 --> 02:00:17,550
mean phrase

1816
02:00:25,490 --> 02:00:31,510
i mean just move around and in doing and again that it has no problem

1817
02:00:31,530 --> 02:00:34,380
and it's not quite smooth

1818
02:00:35,720 --> 02:00:38,150
so they make shape

1819
02:00:40,110 --> 02:00:42,030
because we can

1820
02:00:42,070 --> 02:00:46,780
action is a little things you can think about these things are unlikely

1821
02:00:46,800 --> 02:00:51,900
one thing that if they to show you what i pointed leaves shade and side

1822
02:00:51,900 --> 02:00:57,170
he's facing left and on the right-hand side is facing my body is here because

1823
02:00:57,670 --> 02:01:02,110
across the system and there's no global correlation of that time

1824
02:01:02,490 --> 02:01:06,860
just to show the difference in effect you get if you don't use that covariance

1825
02:01:06,860 --> 02:01:08,740
if you use the MLP one

1826
02:01:08,740 --> 02:01:13,820
and of course you know this thousands of covariance matrices waiting to be discovered

1827
02:01:13,840 --> 02:01:16,820
all of which you can use

1828
02:01:16,920 --> 02:01:20,300
so this is the same visualization again

1829
02:01:20,320 --> 02:01:24,720
in this case we're using the p stock variance this one the saturates as you

1830
02:01:24,860 --> 02:01:29,110
back to the data space so if you the space he'll stay

1831
02:01:29,110 --> 02:01:37,060
he was

1832
02:01:43,300 --> 02:01:45,200
primary emphasis

1833
02:01:45,220 --> 02:01:48,060
for the next few minutes

1834
02:01:48,110 --> 02:01:50,720
in the mean

1835
02:01:50,950 --> 02:01:53,200
she said

1836
02:01:53,860 --> 02:01:55,340
it's important

1837
02:01:55,360 --> 02:01:58,660
so you want to get

1838
02:01:58,720 --> 02:02:04,660
it's it's it's good it's much more

1839
02:02:04,690 --> 02:02:06,840
you start with

1840
02:02:07,060 --> 02:02:10,670
these bonds are you not to

1841
02:02:10,730 --> 02:02:14,230
forty four

1842
02:02:14,240 --> 02:02:20,160
one of the distributions within each

1843
02:02:20,190 --> 02:02:22,880
the key

1844
02:02:22,890 --> 02:02:26,700
so we

1845
02:02:26,760 --> 02:02:31,100
and this is just one of these

1846
02:02:35,630 --> 02:02:43,130
so this is picture

1847
02:02:43,140 --> 02:02:44,860
o point of this

1848
02:02:44,890 --> 02:02:54,860
c is sort of a compliment from which is was written

1849
02:02:56,050 --> 02:02:58,890
so the company from which was

1850
02:02:58,910 --> 02:03:03,080
the main difference in each of these models

1851
02:03:03,090 --> 02:03:07,580
lord which features was

1852
02:03:21,170 --> 02:03:23,550
what makes

1853
02:03:25,520 --> 02:03:30,590
the said

1854
02:03:54,580 --> 02:03:55,800
three days

1855
02:04:10,640 --> 02:04:14,170
if you

1856
02:04:29,110 --> 02:04:31,800
you know

1857
02:04:33,730 --> 02:04:35,460
so this is

1858
02:04:53,140 --> 02:04:55,680
i don't know

1859
02:05:18,430 --> 02:05:20,240
for every one

1860
02:05:20,860 --> 02:05:23,580
not as

1861
02:05:53,110 --> 02:05:54,510
it is

1862
02:05:57,260 --> 02:05:58,040
it was

1863
02:05:58,050 --> 02:06:00,530
he was

1864
02:06:08,490 --> 02:06:15,170
as you know

1865
02:06:15,250 --> 02:06:19,260
so of course this is

1866
02:06:22,360 --> 02:06:27,670
one is that

1867
02:06:27,670 --> 02:06:31,190
she the well all these are some

1868
02:06:31,230 --> 02:06:34,630
is the way the way scoring and support

1869
02:06:34,710 --> 02:06:38,500
what will also each term match

1870
02:06:38,530 --> 02:06:42,730
phrases are little special scored phrase results in one to one score

1871
02:06:43,130 --> 02:06:44,610
four for the phrase match

1872
02:06:44,610 --> 02:06:47,150
and then we we some of us

1873
02:06:47,230 --> 02:06:51,940
so it's it's this in fact is treated as a vector

1874
02:06:51,980 --> 02:06:54,300
then each of these

1875
02:06:57,570 --> 02:06:59,590
is is a

1876
02:06:59,590 --> 02:07:02,880
is a component of the vector in the search term and this is just something

1877
02:07:02,880 --> 02:07:06,690
is multiplied into that component of the vector it's a little bit more complicated than

1878
02:07:06,690 --> 02:07:10,480
that in the nested trees are between a little differently

1879
02:07:10,480 --> 02:07:13,710
but that's the basic idea

1880
02:07:17,210 --> 02:07:18,780
when you pass the query

1881
02:07:18,860 --> 02:07:21,460
there is a couple of

1882
02:07:21,570 --> 02:07:24,030
special things that happened

1883
02:07:24,070 --> 02:07:27,050
is that a certain carey characters

1884
02:07:27,170 --> 02:07:31,130
form implicit racism is actually the same as that of web search engines do

1885
02:07:31,150 --> 02:07:34,070
because they don't have to describe this very much but

1886
02:07:34,110 --> 02:07:38,550
you may not be aware in this seems to be the set of characters that

1887
02:07:38,550 --> 02:07:39,710
everyone uses

1888
02:07:39,840 --> 02:07:42,900
so that if you take into your l

1889
02:07:42,980 --> 02:07:45,670
it's implicitly is exactly the same is searching for

1890
02:07:45,710 --> 02:07:47,230
this this phrase

1891
02:07:47,230 --> 02:07:49,420
because these characters

1892
02:07:49,420 --> 02:07:51,550
for all implicit phase operators

1893
02:07:51,570 --> 02:07:53,820
and same with email addresses

1894
02:07:53,840 --> 02:07:56,380
these researchers

1895
02:07:56,400 --> 02:07:59,590
i believe in the search engines will return exactly the same documents

1896
02:08:00,420 --> 02:08:01,340
so that's

1897
02:08:01,340 --> 02:08:02,480
something we do as well

1898
02:08:02,500 --> 02:08:05,270
we index also was established

1899
02:08:05,280 --> 02:08:06,590
the next time

1900
02:08:08,190 --> 02:08:12,280
let's start with a stop order in the query is normally removed this is again

1901
02:08:12,280 --> 02:08:15,710
a standard practice in search engines has as good

1902
02:08:16,320 --> 02:08:22,830
performance properties as well that unless someone explicitly requires with a plus sign you it

1903
02:08:23,280 --> 02:08:25,570
in the query or put it in a phrase

1904
02:08:25,570 --> 02:08:28,570
it's not

1905
02:08:28,710 --> 02:08:36,150
the international edition of this stuff is

1906
02:08:36,170 --> 02:08:38,860
evolving i believe

1907
02:08:38,940 --> 02:08:41,900
you have to look and see what the latest state of things and i think

1908
02:08:41,900 --> 02:08:44,270
which should be should different samples per language

1909
02:08:44,320 --> 02:08:48,940
and there but the thing is is usually

1910
02:08:48,940 --> 02:08:52,320
searching you declare your language at query time you don't you don't

1911
02:08:52,360 --> 02:08:58,590
inferred automatically so documents you and the language automatically so when searching you have different

1912
02:08:58,590 --> 02:09:00,900
stop list ending with the declared search

1913
02:09:02,480 --> 02:09:03,840
so it makes sense

1914
02:09:09,130 --> 02:09:12,770
let's see was so you can also use

1915
02:09:12,780 --> 02:09:16,320
so the the stop words can can be a performance problem because they are very

1916
02:09:16,320 --> 02:09:20,090
common in lots of documents that are not search for the convoy so

1917
02:09:20,670 --> 02:09:25,250
this this gets a lot of them but then they do end up phrases and

1918
02:09:25,250 --> 02:09:29,030
how can you help phrases well do n grams of its upwards

1919
02:09:29,070 --> 02:09:31,960
occur in

1920
02:09:34,000 --> 02:09:37,360
he it was that was occurring phrases we can

1921
02:09:38,900 --> 02:09:44,000
before hand and n grams of you if we say the word is is a

1922
02:09:44,000 --> 02:09:46,460
stopword if we have some some

1923
02:09:46,570 --> 02:09:47,820
running text

1924
02:09:47,890 --> 02:09:52,480
well also in the if in those words stop words then we get index in

1925
02:09:52,510 --> 02:09:56,320
phrase as a single word and even in a single word

1926
02:09:56,360 --> 02:09:59,860
and the grandson of a single word is well known that way

1927
02:09:59,880 --> 02:10:04,630
i'm automatically search for those of us and when you don't have to search for

1928
02:10:04,630 --> 02:10:08,450
the high frequency words ever because you combine them with their neighbors and make them

1929
02:10:08,450 --> 02:10:10,320
low frequency words

1930
02:10:10,340 --> 02:10:11,630
that makes sense

1931
02:10:11,670 --> 02:10:16,860
again this is all some stuff that you can extend and change this is the

1932
02:10:17,710 --> 02:10:21,400
the same with what is about how things are indexed as the default

1933
02:10:21,500 --> 02:10:27,090
the way things are this is the default queries translated but applications can extend this

1934
02:10:27,090 --> 02:10:30,820
modifier arthur presented here

1935
02:10:30,860 --> 02:10:36,380
subjects for performance another dimension by the

1936
02:10:36,460 --> 02:10:38,280
and rams

1937
02:10:38,320 --> 02:10:42,630
and so we can convert is automatically to ten gram queries

1938
02:10:44,380 --> 02:10:46,690
we also can convert common binary terms

1939
02:10:46,780 --> 02:10:51,590
two bit vectors surface if you search for documents in a particular language we can

1940
02:10:51,590 --> 02:10:55,550
accelerate that very easily by by catching a bit vector the matches

1941
02:10:55,670 --> 02:10:58,530
and the same with particular content types

1942
02:10:58,610 --> 02:11:06,190
and so on otherwise this can be very expensive if used frequently

1943
02:11:06,300 --> 02:11:11,250
the biggest probably speed that we get is by

1944
02:11:11,270 --> 02:11:14,210
sorting indexes and

1945
02:11:14,320 --> 02:11:17,130
this is the a technique has been described in the literature

1946
02:11:17,510 --> 02:11:20,130
it's been used in industry for

1947
02:11:20,130 --> 02:11:21,860
the longer i think

1948
02:11:22,030 --> 02:11:24,420
and the idea is that if you have some

1949
02:11:24,440 --> 02:11:30,730
objective measure page quality like pagerank or some similar things when you sort

1950
02:11:30,840 --> 02:11:35,440
the posting lists in the indexed by that so that the

1951
02:11:35,480 --> 02:11:40,400
in the ordered you see documents when you're searching when you when you're conducting searches

1952
02:11:40,400 --> 02:11:42,770
is see the higher scoring pages first

