1
00:00:00,000 --> 00:00:02,930
i mean this is what you would get

2
00:00:02,940 --> 00:00:03,900
if you do then

3
00:00:06,050 --> 00:00:11,450
and it's clear that if you test distribution is different from the training distribution then

4
00:00:11,450 --> 00:00:14,000
the estimator is biased

5
00:00:14,020 --> 00:00:16,290
there's no way wrong

6
00:00:16,820 --> 00:00:19,600
and so so we would like to

7
00:00:19,610 --> 00:00:25,370
reduce the bias

8
00:00:25,670 --> 00:00:32,630
one way of doing that is was proposed version with

9
00:00:34,320 --> 00:00:39,980
in fact the idea here is that you get yourself density estimates

10
00:00:40,030 --> 00:00:42,680
i mean they don't need to be very precise

11
00:00:42,690 --> 00:00:47,460
but you know it so every data point that you use for learning

12
00:00:47,520 --> 00:00:52,170
is actually weighted by the ratio of the densities of the training and test distributions

13
00:00:52,340 --> 00:00:57,830
so this is very similar to importance something although importance sampling also assumes that the

14
00:00:57,830 --> 00:00:59,590
distribution c

15
00:01:04,000 --> 00:01:05,180
so the idea

16
00:01:05,190 --> 00:01:06,710
here would be

17
00:01:06,760 --> 00:01:12,930
that all the data points that are close to the test distribution which is here

18
00:01:12,940 --> 00:01:15,410
would be weighted much higher

19
00:01:15,420 --> 00:01:17,580
this is what the ratio is about

20
00:01:17,600 --> 00:01:18,730
so essentially

21
00:01:18,740 --> 00:01:21,110
all the data points here in this area

22
00:01:21,130 --> 00:01:23,360
the training data

23
00:01:23,380 --> 00:01:24,950
only this will be used

24
00:01:24,970 --> 00:01:28,340
for training and the rest is ignored

25
00:01:28,350 --> 00:01:31,200
this would give you a prediction like that

26
00:01:31,240 --> 00:01:33,360
or or

27
00:01:37,930 --> 00:01:40,690
the distribution of the same name

28
00:01:43,640 --> 00:01:46,870
very close to the

29
00:01:46,880 --> 00:01:50,710
well if it has the simple

30
00:01:50,720 --> 00:01:54,920
and the problem is with that before

31
00:01:58,570 --> 00:02:02,860
you can prove this is an unbiased asymptotically

32
00:02:02,860 --> 00:02:05,220
which is very nice

33
00:02:05,270 --> 00:02:08,820
still has a large variance

34
00:02:09,490 --> 00:02:12,680
we should be trading

35
00:02:12,720 --> 00:02:14,770
the bias of a little bit

36
00:02:14,780 --> 00:02:19,240
with the very with reducing the variance which means that we basically

37
00:02:19,280 --> 00:02:20,560
put the land up here

38
00:02:20,570 --> 00:02:23,820
and this this

39
00:02:23,880 --> 00:02:28,270
allows us to balance and then the whole story becomes the model selection problem

40
00:02:31,550 --> 00:02:35,780
OK anyway so we can do that in the context of weighted linear regression are

41
00:02:38,280 --> 00:02:41,260
also linear discriminant analysis

42
00:02:41,310 --> 00:02:45,340
and basically introduce some weighting

43
00:02:45,360 --> 00:02:48,910
and some some regularisation we have anyway

44
00:02:52,980 --> 00:02:57,500
so this is just the updated

45
00:02:57,590 --> 00:03:04,460
in LDA equation on the weighted regression equation that we using now to to adapt

46
00:03:04,460 --> 00:03:07,020
to this change of distribution so

47
00:03:21,310 --> 00:03:30,030
there is some

48
00:03:30,050 --> 00:03:32,110
there is some

49
00:03:32,140 --> 00:03:33,630
so we have some

50
00:03:33,680 --> 00:03:36,450
something that test give

51
00:03:37,320 --> 00:03:40,150
it doesn't exist i mean

52
00:03:40,760 --> 00:03:42,060
with the believe

53
00:03:42,220 --> 00:03:45,460
we to against

54
00:03:45,460 --> 00:03:51,910
estimate one i mean we we actually in this experiment showing you don't

55
00:03:56,200 --> 00:04:01,820
since suppose you don't have to see you assuming that you

56
00:04:01,840 --> 00:04:03,040
i mean this is

57
00:04:03,050 --> 00:04:07,020
but i mean in the papers and so on

58
00:04:07,040 --> 00:04:08,020
it's not

59
00:04:08,030 --> 00:04:09,980
it's not like this

60
00:04:10,730 --> 00:04:18,180
i mean i talked about clearly hear alex and this

61
00:04:18,200 --> 00:04:19,190
i mean

62
00:04:26,830 --> 00:04:32,140
OK i think it's philosophical issues

63
00:04:32,140 --> 00:04:36,620
very happy to discuss about because i think it's

64
00:04:36,640 --> 00:04:38,340
he really

65
00:04:38,370 --> 00:04:40,960
paid enough attention

66
00:04:43,040 --> 00:04:44,990
if we look at the

67
00:04:45,010 --> 00:04:47,900
the difference between the distribution

68
00:04:47,900 --> 00:04:54,720
the training and the feedback session c four was some imagination

69
00:04:54,740 --> 00:05:00,160
then we see that there's a difference mostly in the in the visual cortex which

70
00:05:00,160 --> 00:05:01,420
makes some sense

71
00:05:01,430 --> 00:05:03,940
in the alpha activity

72
00:05:04,980 --> 00:05:08,740
and which makes some sense because it's just

73
00:05:08,750 --> 00:05:12,180
much more exciting to play

74
00:05:16,360 --> 00:05:17,210
OK so

75
00:05:18,490 --> 00:05:20,400
i will not go into this but

76
00:05:20,410 --> 00:05:24,440
in principle one could ask the question why just

77
00:05:24,490 --> 00:05:25,520
you know

78
00:05:25,540 --> 00:05:29,950
correct this one's only from training to test what is it

79
00:05:29,950 --> 00:05:32,780
that's the other of this region

80
00:05:32,870 --> 00:05:37,860
so if you want to make predictions small you put down

81
00:05:37,910 --> 00:05:38,890
next year

82
00:05:38,900 --> 00:05:40,600
and you see all

83
00:05:40,600 --> 00:05:44,700
from this curve design this one this one

84
00:05:44,730 --> 00:05:47,020
is also

85
00:05:47,060 --> 00:05:48,970
one theory holds that

86
00:05:49,000 --> 00:05:54,480
here and then given to the right of you sample from the conditional distribution which

87
00:05:54,480 --> 00:05:55,700
is again

88
00:05:55,720 --> 00:05:57,620
this means that

89
00:05:59,090 --> 00:06:03,910
in terms of the slide here is the input space

90
00:06:03,930 --> 00:06:05,370
here's the

91
00:06:05,380 --> 00:06:10,230
the variables that which depends on the input from the conditional distributions of this thing

92
00:06:10,280 --> 00:06:12,130
here is the outline

93
00:06:12,150 --> 00:06:17,980
and then z y depends on the x axis here

94
00:06:20,130 --> 00:06:25,130
any questions about the simple idea to make sure that seems

95
00:06:25,150 --> 00:06:30,170
while this makes you in your efforts

96
00:06:30,190 --> 00:06:31,130
now it time is

97
00:06:31,240 --> 00:06:33,050
four a

98
00:06:39,830 --> 00:06:44,480
i'm going to try

99
00:06:47,450 --> 00:06:51,610
i promise you we will see the algorithm but before we get there i just

100
00:06:51,610 --> 00:06:53,870
want to write down the gradient of the line

101
00:06:54,160 --> 00:06:59,130
small because it very interesting for me and

102
00:06:59,140 --> 00:07:05,140
and i think it helps to understand what is really happening when we define so

103
00:07:05,140 --> 00:07:07,530
let's say that you

104
00:07:07,540 --> 00:07:12,090
you were interested in the sense the amount algorithm you just want to optimize the

105
00:07:12,090 --> 00:07:16,500
likelihood and you can see here is just

106
00:07:16,530 --> 00:07:17,650
despite the director

107
00:07:17,680 --> 00:07:22,320
first wrote down this like it's a lot of the probability mass

108
00:07:22,330 --> 00:07:25,110
the probability of accidents so

109
00:07:25,290 --> 00:07:27,140
you take the derivative

110
00:07:27,160 --> 00:07:29,710
because has a lot to get one billion

111
00:07:30,840 --> 00:07:35,680
the mixture green one gradient for each expert k

112
00:07:35,790 --> 00:07:39,400
OK and if you must cite this for a little bit

113
00:07:39,430 --> 00:07:47,390
way you start with marginal density inside here in one iteration

114
00:07:47,950 --> 00:07:51,670
what you find is the gradient is

115
00:07:51,690 --> 00:07:56,430
the responsibility weighted sum of the individual breeds

116
00:07:56,470 --> 00:07:58,890
so the overall gradient

117
00:07:58,930 --> 00:08:03,230
is a weighted sum of the gradient but each expert

118
00:08:04,380 --> 00:08:05,570
weighted by

119
00:08:05,600 --> 00:08:08,880
the posterior probability that experts is

120
00:08:09,470 --> 00:08:13,070
so when we think about it is

121
00:08:13,070 --> 00:08:16,880
if learning in the small i shall appoint

122
00:08:16,890 --> 00:08:22,140
all of the experts decide between who will most likely be responsible for that data

123
00:08:22,140 --> 00:08:29,440
point an expert who will most likely does not need to be likely to be

124
00:08:30,120 --> 00:08:35,310
if something happens to your problem you really have just parameters something happens in your

125
00:08:35,310 --> 00:08:39,990
backyard your data from the office three

126
00:08:40,040 --> 00:08:42,630
so great is weighted by

127
00:08:42,700 --> 00:08:45,580
the posterior probability that it was our fault

128
00:08:46,110 --> 00:08:47,940
that's not the way

129
00:08:50,750 --> 00:08:56,520
OK so just quick recap related variables the probability contains some

130
00:08:56,530 --> 00:09:01,550
so the log probability the likelihood function as the greatest

131
00:09:01,640 --> 00:09:05,500
it's the variables were observed

132
00:09:05,520 --> 00:09:07,390
then we can do define the

133
00:09:09,600 --> 00:09:13,160
and what is just ignore this fact

134
00:09:13,190 --> 00:09:16,110
go back to this compute the gradient to do this

135
00:09:17,250 --> 00:09:19,460
three whatever

136
00:09:21,210 --> 00:09:22,810
here's another idea

137
00:09:22,820 --> 00:09:28,830
and the idea is to use the current parameters so some parameter optimisation you could

138
00:09:28,830 --> 00:09:34,480
get the parameters to estimate the values of the latent variables

139
00:09:35,310 --> 00:09:41,610
the estimates standing circuits the real values and do fully observed

140
00:09:41,630 --> 00:09:47,620
and this back-and-forth you might make optimisation in some sense make it easy

141
00:09:49,390 --> 00:09:51,070
that's exactly what

142
00:09:51,080 --> 00:09:56,590
so this general strategy is known as the expectation maximisation algorithm

143
00:09:56,600 --> 00:10:01,380
it is most often credited as to their

144
00:10:02,560 --> 00:10:05,120
from the late seventies

145
00:10:05,140 --> 00:10:07,720
journal of the royal statistical society

146
00:10:08,240 --> 00:10:14,880
although this idea was developed much earlier by parliament through the people working in statistics

147
00:10:14,980 --> 00:10:18,440
not to mention

148
00:10:18,450 --> 00:10:26,810
the canonical citation details so we have here is two steps in the east village

149
00:10:26,820 --> 00:10:30,620
values variables using this here

150
00:10:30,630 --> 00:10:33,670
and instead of going to parameters

151
00:10:33,680 --> 00:10:36,630
in some sense taking advantage of its

152
00:10:37,320 --> 00:10:40,750
so he's involved is crucial

153
00:10:40,750 --> 00:10:44,980
it would tend to decrease and then at some point would start to swing up

154
00:10:45,000 --> 00:10:48,340
and why is it swing up well that's exactly

155
00:10:48,360 --> 00:10:53,110
due to this variance that variance term that i just derived in the two cases

156
00:10:53,110 --> 00:10:57,310
is exactly what's pushing this up because the overall risk is bounded below by the

157
00:10:58,540 --> 00:10:59,980
so that various

158
00:10:59,980 --> 00:11:04,170
it's what's pushing the true risk up and so what we like to do somehow

159
00:11:04,190 --> 00:11:08,560
minimize empirical risk up to a point in the point where we probably want stop

160
00:11:08,560 --> 00:11:12,610
is where this variance term starts to exceed the value of the empirical risk because

161
00:11:12,610 --> 00:11:16,360
at that point of the empirical risk is really are are sort of guideline our

162
00:11:16,360 --> 00:11:20,060
surrogate for the risk we know the true risk can be less than the variance

163
00:11:20,060 --> 00:11:23,560
we should never try to push empirical risk down below that level

164
00:11:23,610 --> 00:11:26,330
and in this region here we can think of this is where we started to

165
00:11:30,920 --> 00:11:32,750
this suggests

166
00:11:32,810 --> 00:11:37,340
following procedure to guard against overfitting

167
00:11:37,340 --> 00:11:42,870
we would select a tree not just based on minimizing empirical risk which is here

168
00:11:42,870 --> 00:11:43,840
but also

169
00:11:43,860 --> 00:11:49,310
as you add to that the variance so what this does exactly when empirical risk

170
00:11:49,310 --> 00:11:53,650
is lower than the variance for the variance starts to dominate and that pushes away

171
00:11:53,650 --> 00:11:56,590
from over fitting to the data because

172
00:11:56,630 --> 00:12:00,810
at some point this guy starts to increase in among the larger value here

173
00:12:00,840 --> 00:12:05,840
four four very large and complicated trees

174
00:12:06,730 --> 00:12:11,150
that's the basic idea of variances proportional to the complexity of the partition of the

175
00:12:11,150 --> 00:12:15,170
number of cells in a dish so this is often called complexity regularisation

176
00:12:15,230 --> 00:12:19,340
and the basic idea is that in principle this can help us avoid overfitting and

177
00:12:19,340 --> 00:12:22,710
give a very accurate model

178
00:12:22,790 --> 00:12:27,520
the empirical risk we can calculate very easily from the training data which is the

179
00:12:27,520 --> 00:12:32,090
sum of squared errors the sum of absolute errors that's easy to do the variance

180
00:12:32,840 --> 00:12:38,750
is something that again they these expected values of f had myself square f myself

181
00:12:38,750 --> 00:12:42,920
in absolute value in order to compute those expectations in principle we need to have

182
00:12:42,920 --> 00:12:45,860
access to this underlying probability law p

183
00:12:45,860 --> 00:12:48,730
we don't have that but it turns out we can get

184
00:12:48,790 --> 00:12:50,860
good bounds for the variance

185
00:12:50,880 --> 00:12:54,610
and rather than using various itself we could use the bound on the variance and

186
00:12:54,610 --> 00:12:57,210
so that's what i want to describe next all

187
00:12:57,210 --> 00:13:01,480
first of all i at bounding the variance and purcell basis and then once i

188
00:13:01,480 --> 00:13:06,770
have various bounds for cells of pulled us together again variance found for an overall

189
00:13:09,520 --> 00:13:10,880
so here's

190
00:13:10,900 --> 00:13:13,880
per cell variance bounds in regression

191
00:13:13,900 --> 00:13:19,270
alice according to denote only for selling the tree this grey when they have indicated

192
00:13:19,270 --> 00:13:20,440
for example

193
00:13:20,480 --> 00:13:25,690
as well is the number of training examples that landed in that cell

194
00:13:25,710 --> 00:13:31,070
i have had in regression is the empirical mean is the true mean

195
00:13:31,130 --> 00:13:35,920
the variance is equal to the expected squared error between

196
00:13:35,920 --> 00:13:39,330
and in this situation already doing is fitting

197
00:13:39,330 --> 00:13:46,360
of single parameter mean two to data was just one dimensional

198
00:13:46,420 --> 00:13:52,190
parameter estimation problem you should know from your undergraduate statistics that air will decay like

199
00:13:52,190 --> 00:13:55,900
one of the number of samples we have one over and help

200
00:13:55,960 --> 00:13:58,380
and about the minus one so this is

201
00:13:58,400 --> 00:14:02,630
i'm assuming everybody is comfortable with that idea that the variance of

202
00:14:02,710 --> 00:14:06,000
of the parameter estimates decays like one over here

203
00:14:08,790 --> 00:14:14,000
in the classification case is very similar same set up except that had is the

204
00:14:14,000 --> 00:14:15,270
empirical medium

205
00:14:15,310 --> 00:14:16,830
f is the true medium

206
00:14:16,840 --> 00:14:22,710
and the variance in this case is behaving like the absolute error of mine

207
00:14:22,750 --> 00:14:24,880
i have an expectation

208
00:14:24,940 --> 00:14:28,330
and because we're looking at absolute error rather than swear here

209
00:14:28,340 --> 00:14:29,560
it's a very

210
00:14:29,590 --> 00:14:34,230
fairly easy way calculation to see that this should decay like one over the square

211
00:14:34,230 --> 00:14:37,860
root of n rather than one over and as it did in the regression case

212
00:14:37,880 --> 00:14:39,150
so we get this

213
00:14:39,210 --> 00:14:40,000
and the

214
00:14:40,160 --> 00:14:41,790
and for the minus one half

215
00:14:41,790 --> 00:14:46,440
decay of this this parameter estimates of the median to the sample

216
00:14:47,590 --> 00:14:53,770
everybody by that because is kind of important little pointed to by two interesting

217
00:14:53,790 --> 00:14:55,860
OK right

218
00:14:55,880 --> 00:15:00,900
now we can convert those purcell balanced overall violence for a full tree and to

219
00:15:00,900 --> 00:15:05,340
do this you use it will be that have

220
00:15:05,520 --> 00:15:09,400
maybe will lose here but let's just think about this for the first of all

221
00:15:09,400 --> 00:15:13,310
and civil the number of training samples that fall to sell l

222
00:15:13,380 --> 00:15:17,460
will be approximately like and the total number of training samples

223
00:15:17,520 --> 00:15:21,960
times the probability that any sample what happened fall in

224
00:15:22,290 --> 00:15:26,440
the leaf for self-help that's the problem x one to l

225
00:15:26,460 --> 00:15:30,170
and l denote the probability by piece about

226
00:15:30,170 --> 00:15:33,380
so and about the number of samples and leaf l

227
00:15:33,440 --> 00:15:37,690
should be about like and so there are n times piece about

228
00:15:37,710 --> 00:15:42,110
so by looking at the overall variance for the regression case

229
00:15:42,130 --> 00:15:46,170
i can write that is the sum of the purcell variances

230
00:15:46,230 --> 00:15:48,810
times the probability falling into each cell

231
00:15:48,860 --> 00:15:52,440
these purcell variances behave like one over and times pl

232
00:15:52,500 --> 00:15:55,630
so the peels cancel each other and just left with

233
00:15:55,670 --> 00:15:58,730
the number of leaves in the tree divided by

234
00:15:58,750 --> 00:16:00,380
so that's the variance

235
00:16:00,380 --> 00:16:02,460
bound for tree in regression

236
00:16:02,460 --> 00:16:06,380
and this should also look familiar it's essentially like the degrees of freedom in your

237
00:16:06,380 --> 00:16:08,610
model which is the number of leaves in the tree

238
00:16:08,630 --> 00:16:11,670
divided by the number of samples

239
00:16:11,710 --> 00:16:16,270
yes but

240
00:16:18,690 --> 00:16:22,210
so we didn't get it right and so i'm assuming right now fixed to give

241
00:16:22,210 --> 00:16:24,900
an idea for a fixed rate how does

242
00:16:24,920 --> 00:16:26,110
that the

243
00:16:26,110 --> 00:16:31,770
fitting of sample means are medians affect the variability of that that the resulting

244
00:16:31,770 --> 00:16:36,390
we can use markov chain monte carlo method is to try to sample from both

245
00:16:36,390 --> 00:16:39,660
data and x

246
00:16:39,700 --> 00:16:44,100
we can use variational bayesian methods and i'll try to talk about that

247
00:16:45,430 --> 00:16:47,790
in the next lecture little bit

248
00:16:49,470 --> 00:16:52,270
these are just different ways of dealing with intractability

249
00:16:52,290 --> 00:16:56,020
so let me just talk about a summary of the parameter learning

250
00:16:59,450 --> 00:17:05,950
i talked about maximum likelihood and bayesian methods for both complete other words fully observed

251
00:17:05,950 --> 00:17:10,120
data incomplete data with hidden or missing variables

252
00:17:10,160 --> 00:17:14,750
for complete data bayesian learning procedure is not any more costly than the maximum likelihood

253
00:17:14,750 --> 00:17:19,870
procedure just involves treating the council differently

254
00:17:19,870 --> 00:17:23,100
for incomplete data all talk about this a bit later

255
00:17:23,100 --> 00:17:28,410
we have yes an algorithm but there's this variational bayesian the EM algorithm that is

256
00:17:28,410 --> 00:17:31,330
about the same time complexity is

257
00:17:31,330 --> 00:17:33,370
it has some nice properties

258
00:17:33,390 --> 00:17:41,220
other parameter priors are possible but there's laser pretty flexible and intuitive distribution for these

259
00:17:41,220 --> 00:17:44,120
conditional probability tables

260
00:17:44,160 --> 00:17:50,890
for binary data other very well known parametrizations have been used

261
00:17:50,910 --> 00:17:53,580
these include the sigmoid

262
00:17:55,270 --> 00:18:01,350
where you have a whole bunch of parents the probability that your notice on given

263
00:18:01,350 --> 00:18:05,870
its parents is one over one plus e to the minus some linear function of

264
00:18:05,870 --> 00:18:07,040
the parents

265
00:18:07,100 --> 00:18:09,080
this is very analogous to

266
00:18:09,100 --> 00:18:10,470
the way we

267
00:18:10,500 --> 00:18:15,660
i have neural networks with sigmoid units but just the generalization of that too

268
00:18:15,680 --> 00:18:16,700
the discrete

269
00:18:17,790 --> 00:18:20,770
graphical models with binary variables

270
00:18:20,770 --> 00:18:26,910
there is also very nice noisy or parametrisation that makes sense for situations where you

271
00:18:26,910 --> 00:18:30,310
have a whole bunch of parents and you want to include the idea that the

272
00:18:30,310 --> 00:18:35,200
child is going to be on if any of the parents is on

273
00:18:35,200 --> 00:18:39,220
but you want to have some noise in that in that or function so this

274
00:18:39,220 --> 00:18:43,870
makes sense for modeling things like diseases where you have

275
00:18:44,870 --> 00:18:49,450
diseases and you have a symptom and the symptom is on if any of those

276
00:18:49,450 --> 00:18:53,430
diseases that could cause the symptoms are all roughly speaking

277
00:18:53,470 --> 00:18:58,020
OK for non discrete data we can use similar ideas but generally is harder to

278
00:18:58,020 --> 00:19:00,930
do inference and learning

279
00:19:01,910 --> 00:19:06,200
so now let's talk about structure learning

280
00:19:06,250 --> 00:19:09,970
given a dataset with observations of

281
00:19:10,000 --> 00:19:13,080
so these variables a b c d and e can we learn the structure of

282
00:19:13,080 --> 00:19:14,290
the graphical model

283
00:19:14,310 --> 00:19:20,500
by that i mean let let m land the graph structure by that i mean

284
00:19:20,520 --> 00:19:23,580
the set of edges in the graph can we learn whether this is a good

285
00:19:23,580 --> 00:19:27,350
model where nothing is connected or this is a good model we have lots of

286
00:19:27,350 --> 00:19:29,390
connections et cetera

287
00:19:31,200 --> 00:19:37,480
so there are two general classes of structure learning problem algorithms

288
00:19:39,470 --> 00:19:44,680
one class of algorithms is called constraint based algorithms

289
00:19:44,720 --> 00:19:51,120
and the idea of these algorithms is to use statistical tests of marginal and conditional

290
00:19:51,120 --> 00:19:53,040
independence c take

291
00:19:53,100 --> 00:19:56,180
pairs of variables are sets of variables and you say

292
00:19:56,180 --> 00:19:57,080
can i do

293
00:19:57,100 --> 00:20:01,950
statistical test for whether these variables are independent or not

294
00:20:02,000 --> 00:20:05,020
condition these variables in my observed data

295
00:20:05,040 --> 00:20:10,600
and the standard of frequencies test that you can use like for example kai square

296
00:20:10,600 --> 00:20:14,410
base test for assessing whether something is independent or not

297
00:20:14,450 --> 00:20:19,540
then what you get is a whole bunch of conditional independence statements

298
00:20:19,540 --> 00:20:24,950
and what you do with the same as you try to find a DAG or

299
00:20:24,980 --> 00:20:31,700
set of directed acyclic graphs whose d separation relations match the results of those conditional

300
00:20:31,700 --> 00:20:36,330
independence tests and in general the answer to that procedure will not be a single

301
00:20:36,350 --> 00:20:42,100
DAG will be a family of dags with equivalent conditional independence properties

302
00:20:43,060 --> 00:20:47,470
so i won't talk a lot more about that the algorithm for example like the

303
00:20:47,470 --> 00:20:52,000
PC algorithm is the classical way of doing constraint based learning

304
00:20:52,020 --> 00:20:54,330
these graphical models

305
00:20:54,350 --> 00:21:01,350
the other family of methods are score based methods and what these methods do is

306
00:21:01,350 --> 00:21:03,540
they compute a global score

307
00:21:03,560 --> 00:21:07,310
of the particular graph given the data on that score could be something like the

308
00:21:07,310 --> 00:21:09,640
bayesian information criterion score

309
00:21:09,760 --> 00:21:14,890
the bayesian marginal likelihood and they try to find the structure or structures that maximize

310
00:21:14,890 --> 00:21:16,200
this score

311
00:21:17,040 --> 00:21:27,200
how do we know when we're doing well is the question that's a great question

312
00:21:27,330 --> 00:21:29,830
is it depends on your application domain

313
00:21:29,850 --> 00:21:34,540
there might be ground truth in that we might be able to go collect more

314
00:21:34,540 --> 00:21:36,870
data to do some experiments

315
00:21:36,870 --> 00:21:41,930
for example to evaluate a particular structure or another but is the same as any

316
00:21:41,930 --> 00:21:46,620
other learning problem another way to test that is to leave out some test data

317
00:21:46,980 --> 00:21:49,600
and evaluate performance on the test data

318
00:21:49,600 --> 00:21:52,680
but in general it's a good question difficult to know

319
00:21:52,720 --> 00:21:54,430
the ground truth of course

320
00:21:54,430 --> 00:21:58,880
it's just another formula that we e we associate

321
00:22:02,200 --> 00:22:06,990
maybe i want to name for that school that BI

322
00:22:07,030 --> 00:22:14,770
so the formula

323
00:22:14,790 --> 00:22:18,490
we assume that we've chosen the constant c is so that for example

324
00:22:18,520 --> 00:22:23,390
caesar by doesn't occur anywhere in this previously also

325
00:22:23,480 --> 00:22:28,220
like that so it's it's far in its new with his best known possibility of

326
00:22:29,390 --> 00:22:33,790
special pleading for it

327
00:22:33,840 --> 00:22:37,850
and then we can let gamma double prime

328
00:22:37,900 --> 00:22:41,880
gamma prime

329
00:22:48,240 --> 00:22:49,530
this debate

330
00:22:58,510 --> 00:23:00,710
OK with throwing all of those

331
00:23:04,590 --> 00:23:09,210
gamma prime

332
00:23:19,390 --> 00:23:20,370
the brain is

333
00:23:20,370 --> 00:23:24,200
taking here because

334
00:23:24,300 --> 00:23:30,800
there's there's some slippage i think

335
00:23:30,810 --> 00:23:33,210
these guys must be

336
00:23:33,270 --> 00:23:35,990
yet more constance

337
00:23:36,100 --> 00:23:38,820
they can't be the same things as we used to get rid of the free

338
00:23:40,870 --> 00:23:44,090
we have to throwing yet more but we can always throwing yet more i mean

339
00:23:44,090 --> 00:23:45,480
there's always yet more

340
00:23:45,540 --> 00:23:47,780
through in this is throwing the days and the

341
00:23:48,100 --> 00:23:53,930
what we want is that absolutely none of this stuff interferes with anything else they'll

342
00:23:53,930 --> 00:23:57,110
just they're all just one off

343
00:23:57,190 --> 00:24:04,820
and now we can appeal to this stuff look

344
00:24:04,870 --> 00:24:06,410
there's a theorem

345
00:24:06,460 --> 00:24:09,330
of the logic

346
00:24:09,370 --> 00:24:11,230
that's a is

347
00:24:11,330 --> 00:24:15,550
there is an x satisfying this

348
00:24:15,600 --> 00:24:17,600
this implication

349
00:24:17,680 --> 00:24:20,370
all right

350
00:24:20,380 --> 00:24:22,900
so here's the left hand side of it

351
00:24:22,960 --> 00:24:25,460
there is some x satisfying this

352
00:24:25,480 --> 00:24:26,960
thing is a theorem of logic

353
00:24:27,210 --> 00:24:29,290
by the rule of choice

354
00:24:29,290 --> 00:24:33,710
we can pick one and we can call it seems by

355
00:24:33,800 --> 00:24:37,470
right whatever follows from that

356
00:24:37,530 --> 00:24:39,380
followed anyway

357
00:24:39,410 --> 00:24:41,680
from the existential

358
00:24:41,690 --> 00:24:45,870
from the existential fact which was a theorem of the logic

359
00:24:45,990 --> 00:24:49,140
for that reason

360
00:24:49,190 --> 00:24:53,400
if gamma double prime or inconsistent

361
00:24:53,410 --> 00:24:55,170
so would

362
00:24:55,220 --> 00:24:59,400
gamma prime being consistent and so would gamma being

363
00:25:00,400 --> 00:25:08,200
gamma double prime is still consistent

364
00:25:08,410 --> 00:25:15,590
all right

365
00:25:17,620 --> 00:25:21,220
we can use linden baum's lemma

366
00:25:21,240 --> 00:25:23,370
it's this big theory

367
00:25:23,380 --> 00:25:25,270
that we apply the a

368
00:25:25,310 --> 00:25:27,780
the linden down hankin

369
00:25:32,060 --> 00:25:35,990
we're gonna blow that

370
00:25:36,020 --> 00:25:39,610
to a maximal consistent theory

371
00:25:39,620 --> 00:25:41,930
right so let delta

372
00:25:41,980 --> 00:25:44,720
the maximal consistent theory

373
00:25:44,770 --> 00:25:47,300
maximum the and extension

374
00:25:47,470 --> 00:25:52,580
of that big

375
00:25:54,380 --> 00:25:57,460
by the same construction is this morning

376
00:25:57,540 --> 00:25:59,810
or if you want to be quick

377
00:25:59,820 --> 00:26:01,760
by zorn's lemma

378
00:26:02,100 --> 00:26:05,270
because this is a consistent set

379
00:26:05,340 --> 00:26:09,900
so the set of consistent sets of it is nonempty it's got a gamma double

380
00:26:09,900 --> 00:26:11,600
prime interest

381
00:26:13,120 --> 00:26:16,600
partially ordered by inclusion

382
00:26:16,600 --> 00:26:18,340
and every chain

383
00:26:18,390 --> 00:26:19,530
in that sense

384
00:26:19,540 --> 00:26:22,870
is bounded about its union

385
00:26:22,920 --> 00:26:26,090
OK because of the chain of sets is consistent

386
00:26:26,290 --> 00:26:29,960
if you have a chain of consistent sets the union

387
00:26:30,050 --> 00:26:35,120
is also consistent set by the same arguement we had before if there were direct

388
00:26:35,120 --> 00:26:39,060
derivation of a contradiction from the union it would have to be a derivation would

389
00:26:39,060 --> 00:26:43,040
have to be finite and therefore derivation from something in the chain

390
00:26:45,010 --> 00:26:47,970
we can use on islam and get linden

391
00:26:48,070 --> 00:26:50,380
in one hit

392
00:26:50,400 --> 00:26:54,160
static i through the construction of delta zero delta one delta two delta three all

393
00:26:54,160 --> 00:26:56,770
we can do it we could do about constructions

394
00:26:56,780 --> 00:27:01,900
i'm going to us i get away my hands and assume that does

395
00:27:01,930 --> 00:27:05,020
so we have the maximal consistent extension

396
00:27:05,020 --> 00:27:06,340
of gamma

397
00:27:06,360 --> 00:27:10,270
double prime now

398
00:27:10,320 --> 00:27:20,380
it's a maximal consistent sets has nice properties of maximal consistent sets like being closed

399
00:27:22,020 --> 00:27:26,270
and consequence

400
00:27:26,310 --> 00:27:28,720
for every x extensional

401
00:27:28,770 --> 00:27:31,100
in the language it contains

402
00:27:31,280 --> 00:27:35,310
a formula of this sort of witness formula

403
00:27:35,360 --> 00:27:40,310
and so in particular for any existential formulas in delta

404
00:27:40,380 --> 00:27:44,620
the analogous thing on the right hand side of this conditional is also going to

405
00:27:44,620 --> 00:27:46,820
be in delta

406
00:27:46,830 --> 00:27:53,220
and that's great because that means whenever delta contains an existential it contains eyewitness

407
00:27:53,280 --> 00:27:54,090
we just

408
00:27:54,100 --> 00:27:57,100
constructed it to be like that

409
00:27:58,610 --> 00:28:03,790
a theory that has that property is called a henkin theory

410
00:28:03,830 --> 00:28:08,110
after another logician anking

411
00:28:11,780 --> 00:28:15,900
so the theory saturated something about sort

412
00:28:15,900 --> 00:28:17,950
explanation of why things keep getting better

413
00:28:18,180 --> 00:28:25,660
now the question how many layers should you actually use

414
00:28:25,730 --> 00:28:27,600
there is no simple answer

415
00:28:27,610 --> 00:28:31,610
yoshua bengio's group has done a lot of careful experiments showing that more layers helps up to about

416
00:28:33,610 --> 00:28:37,070
if you try messing with this stuff

417
00:28:37,080 --> 00:28:41,530
changing the number of units in a layer by fifty percent doesn't seem to make much difference

418
00:28:41,580 --> 00:28:43,280
it's very robust to that

419
00:28:43,290 --> 00:28:49,720
there are theorems that say if you make the net deep enough it can learn anything any distribution

420
00:28:49,720 --> 00:28:52,070
over binary vectors even if it's narrow

421
00:28:52,570 --> 00:28:56,600
but this is basically empirical.

422
00:28:56,610 --> 00:28:58,480
you just mess around

423
00:29:03,410 --> 00:29:07,950
this is just related to the first paper which we talked about all this

424
00:29:08,080 --> 00:29:10,190
kind of learning

425
00:29:10,280 --> 00:29:15,120
when i learn those weights in higher layers, the inference that i'm doing at the bottom level is

426
00:29:15,120 --> 00:29:16,740
no longer correct

427
00:29:20,610 --> 00:29:23,910
if i use the transpose of these weights to do inference

428
00:29:23,970 --> 00:29:26,060
that's the correct way to do inference

429
00:29:26,070 --> 00:29:28,200
if these weights are all the same as these weights.

430
00:29:28,220 --> 00:29:30,770
as soon as i start changing these

431
00:29:30,780 --> 00:29:34,870
the prior up here no longer exactly cancels the likelihood term here

432
00:29:34,930 --> 00:29:37,270
but at least i started from the right way of doing inference,

433
00:29:37,360 --> 00:29:40,850
as opposed to something that's hopelessly wrong like variational inference

434
00:29:40,940 --> 00:29:43,730
and now as i change these weights

435
00:29:43,730 --> 00:29:47,790
they're actually going to be very unhappy if they make this be doing inference wrong

436
00:29:47,820 --> 00:29:50,780
so this'll stay pretty good

437
00:29:50,780 --> 00:29:54,270
and it turns out that what you win by changing these weights

438
00:29:54,280 --> 00:29:55,980
to get a better model of this

439
00:29:55,990 --> 00:29:58,640
is always going to be more than you lose by the fact that this isn't

440
00:29:58,640 --> 00:30:00,530
doing correct inference anymore

441
00:30:01,940 --> 00:30:05,280
i'm not gonna to go into that any more

442
00:30:05,540 --> 00:30:09,430
in the last couple of minutes i want to talk about

443
00:30:09,430 --> 00:30:11,870
how to improve contrastive divergence

444
00:30:11,890 --> 00:30:19,190
basically by doing something else instead. but we'll still call it contrastive divergence.

445
00:30:19,200 --> 00:30:21,480
where contrastive divergence goes wrong,

446
00:30:21,520 --> 00:30:25,190
is if i have say an energy landscape like this

447
00:30:25,280 --> 00:30:28,730
so work

448
00:30:28,910 --> 00:30:31,110
i have my data on some manifold like this

449
00:30:31,140 --> 00:30:33,140
here's the whole space

450
00:30:33,190 --> 00:30:36,280
and this is what i want to be low energy

451
00:30:36,370 --> 00:30:39,490
and so what we do in contrastive divergence is, we pick a data point

452
00:30:39,580 --> 00:30:40,950
we start the model there,

453
00:30:40,990 --> 00:30:43,560
we say, where would you like to go? it says i'd like to go here

454
00:30:43,570 --> 00:30:44,400
and we say,

455
00:30:44,400 --> 00:30:47,940
don't go there, go here. so make this higher, make this lower,

456
00:30:47,990 --> 00:30:51,150
and that's going to create a ravine along the data

457
00:30:51,190 --> 00:30:54,980
but suppose that when we do this, we say make this higher and make this lower,

458
00:30:55,000 --> 00:30:58,530
we accidentally make some point over here lower,

459
00:30:58,570 --> 00:31:02,480
because these parameters are used for all sorts of things

460
00:31:02,500 --> 00:31:05,220
and suppose that when we learned here we also make this lower.

461
00:31:05,230 --> 00:31:08,320
we could end up with a big hole here that we never see

462
00:31:08,330 --> 00:31:11,430
so we don't realize that actually we haven't got a very good model because the model

463
00:31:11,430 --> 00:31:13,640
really believes in this stuff

464
00:31:13,650 --> 00:31:17,000
we have to run the markov chain for a long time to see that

465
00:31:17,030 --> 00:31:19,490
so is there any way we can run the markov chain for longer to see

466
00:31:19,490 --> 00:31:21,410
this kind of thing?

467
00:31:21,440 --> 00:31:26,190
well that's what this is about

468
00:31:26,280 --> 00:31:28,720
what we're going to do is

469
00:31:28,730 --> 00:31:30,500
we're gonna take the data

470
00:31:30,520 --> 00:31:33,930
we're gonna activate the hidden units, and we're going to measure the statistics

471
00:31:33,940 --> 00:31:37,950
and that's only one term in the learning algorithm

472
00:31:37,970 --> 00:31:39,370
but in addition

473
00:31:39,400 --> 00:31:43,430
we can have a whole bunch of particles, i call them fantasy particles,

474
00:31:43,470 --> 00:31:46,520
that are sort of wandering around

475
00:31:46,570 --> 00:31:48,680
in the data space

476
00:31:48,680 --> 00:31:53,560
and each time you update the weights, you move all these fantasy particles by one step

477
00:31:53,560 --> 00:31:55,100
of the markov chain

478
00:31:55,110 --> 00:31:57,850
but you don't reinitialize them at the data

479
00:31:57,900 --> 00:32:04,600
and that initially sounds like a bad idea

480
00:32:05,650 --> 00:32:08,220
here's a concrete example you take

481
00:32:08,280 --> 00:32:10,640
a hundred examples of data. you activate the hidden

482
00:32:11,190 --> 00:32:14,070
units, you estimate the gradient, the first term.

483
00:32:14,070 --> 00:32:16,400
you estimate the first term of the gradient

484
00:32:16,480 --> 00:32:18,620
you also take a hundred fantasies

485
00:32:18,650 --> 00:32:20,730
and use those to estimate the second term

486
00:32:20,780 --> 00:32:24,040
but that's very tricky because the second term

487
00:32:24,060 --> 00:32:27,220
should sort of model what's going on in the whole space

488
00:32:27,230 --> 00:32:28,910
and these particles

489
00:32:28,980 --> 00:32:32,890
can't find all the modes in the space, you'd have thought.

490
00:32:32,940 --> 00:32:37,100
so it's very strange that with just one hundred twenty particles you can model this

491
00:32:37,100 --> 00:32:44,720
whole partition function

492
00:32:44,730 --> 00:32:49,860
and the issue is why does, why are these fantasy particles able

493
00:32:49,900 --> 00:32:55,430
to find all the modes and model the partition function accurately?

494
00:32:55,470 --> 00:32:58,280
i'm going quite fast here, but you'll get the slides

495
00:32:58,290 --> 00:32:59,900
i'll put all this on the web

496
00:32:59,910 --> 00:33:06,560
and the answer is that the learning makes the fantasy particles move around

497
00:33:06,610 --> 00:33:11,790
so i have an analogy here. suppose i want to get 3d shape from images

498
00:33:11,850 --> 00:33:15,400
and i said well i'm gonna study single images and figure out how to get

499
00:33:15,400 --> 00:33:16,860
the three d shape

500
00:33:16,910 --> 00:33:21,500
and a friend says, you know, it might be better to think about video sequences, 'cause that sort of

501
00:33:21,570 --> 00:33:25,850
might be easy--easier problem. i say, no, no, no, i don't even want to think about video sequences

502
00:33:25,850 --> 00:33:28,440
until i can get shape from single images

503
00:33:28,480 --> 00:33:31,070
well it's obvious that's a mistake, right?

504
00:33:31,200 --> 00:33:33,540
the equivalent mistake here is to say

505
00:33:33,580 --> 00:33:37,290
i'm gonna get the gradient and i'm gonna do some learning.

506
00:33:37,310 --> 00:33:41,040
and i really want get the gradient before do the learning

507
00:33:41,070 --> 00:33:46,680
well it's going to turn out that because your gradient is the difference of statistics

508
00:33:46,730 --> 00:33:51,260
actually the learning is very helpful in getting the gradient

509
00:33:51,270 --> 00:33:53,820
and the reason the learning is helpful is

510
00:33:53,870 --> 00:33:55,890
if you think about it

511
00:33:55,900 --> 00:34:00,430
if you have an energy landscape

512
00:34:00,480 --> 00:34:05,940
that looks like this

513
00:34:05,950 --> 00:34:08,760
suppose on the energy landscape actually

514
00:34:09,310 --> 00:34:11,480
i've got some fantasy particles

515
00:34:11,520 --> 00:34:16,370
and my fantasy particles are sort of sitting here

516
00:34:16,400 --> 00:34:20,770
i'll draw those in red for fantasy particles

517
00:34:20,820 --> 00:34:23,810
my data

518
00:34:23,860 --> 00:34:27,220
is sitting over here

519
00:34:27,290 --> 00:34:32,540
a markov chain has a hell of a job getting these guys out of there and into here

520
00:34:32,560 --> 00:34:34,910
what the data is going to do is it's going to pull down there. it's

521
00:34:34,910 --> 00:34:39,540
gonna say, make this deeper, because i want to be have more belief in the data

522
00:34:39,610 --> 00:34:44,700
but the learning is also going to say, because fantasy particles outnumber data here

523
00:34:44,760 --> 00:34:48,200
it's going to make this go up

524
00:34:48,230 --> 00:34:51,530
so rather than getting the particles to jump out of the minimum,

525
00:34:51,530 --> 00:34:52,230
in parallel

526
00:34:52,710 --> 00:34:54,190
and then have the computation

527
00:34:54,660 --> 00:35:01,510
follow the data that's how paralyzed competition so the basic idea is to partition your data logically

528
00:35:01,900 --> 00:35:03,530
using the mapping function that is

529
00:35:04,040 --> 00:35:05,400
well the partitioning key is

530
00:35:07,790 --> 00:35:09,220
you take the partitions

531
00:35:09,990 --> 00:35:10,900
i do

532
00:35:11,540 --> 00:35:15,150
but partition processing on then there is the so-called reduced at

533
00:35:15,750 --> 00:35:19,720
and in between the my spilled it at a local this

534
00:35:20,160 --> 00:35:23,570
and this is all the network and a very expensive shuffle step

535
00:35:24,030 --> 00:35:25,490
the set things up for the reduced

536
00:35:26,850 --> 00:35:28,720
the reduce tasks work on this

537
00:35:29,340 --> 00:35:33,250
andy leave their results in a local flight that's mapreduce

538
00:35:35,120 --> 00:35:36,740
thank you things i would like you to

539
00:35:37,220 --> 00:35:37,920
think about here

540
00:35:38,710 --> 00:35:40,490
this factor of implementation

541
00:35:41,090 --> 00:35:46,810
this early june four scans it's early june four r appends sequential writes

542
00:35:47,200 --> 00:35:47,770
can see

543
00:35:50,020 --> 00:35:53,120
the scaling as i said is true partitioning data partitioning

544
00:35:54,710 --> 00:35:56,720
on the scale these systems work

545
00:35:57,160 --> 00:35:58,400
tens of thousands of machines

546
00:35:59,030 --> 00:36:03,100
this is orders of magnitude more than your typical power database systems right

547
00:36:04,110 --> 00:36:05,190
forwards are given

548
00:36:05,760 --> 00:36:09,410
and while the unique aspects of these systems is in architect for

549
00:36:11,010 --> 00:36:13,570
this turns out to be non trivial in terms of cost

550
00:36:16,130 --> 00:36:18,160
i don't have to be one of the reasons why

551
00:36:19,290 --> 00:36:25,300
mapreduce paradigm doesn't lend itself with things like iterative computation graph complications machine learning

552
00:36:27,000 --> 00:36:29,730
and then there is no concept of iteration as a first class

553
00:36:32,010 --> 00:36:33,540
the first generation of signal

554
00:36:37,320 --> 00:36:39,260
there were implemented in a straightforward way

555
00:36:39,830 --> 00:36:43,900
by translating programs into my previous statements why

556
00:36:44,680 --> 00:36:48,370
implementing large-scale distributed systems is not something you start

557
00:36:49,140 --> 00:36:49,570
you know like

558
00:36:50,320 --> 00:36:51,490
and given that you had

559
00:36:54,320 --> 00:36:56,190
that occur value system scaling issues

560
00:36:56,650 --> 00:36:59,400
the first generation sequel concentrate on functionality

561
00:37:02,200 --> 00:37:04,460
today story is what happens beyond this

562
00:37:07,290 --> 00:37:08,560
what happens next

563
00:37:10,020 --> 00:37:10,450
is this

564
00:37:13,730 --> 00:37:14,270
that people

565
00:37:14,960 --> 00:37:16,200
are going to insist

566
00:37:16,660 --> 00:37:19,210
on storing all the data in a place where

567
00:37:19,650 --> 00:37:22,010
they can bring all the analytic tools to bear on

568
00:37:22,870 --> 00:37:28,340
and so i call this the social box you when the next data management war

569
00:37:28,830 --> 00:37:32,920
by building a sequel system that's twenty percent faster than the previous generation

570
00:37:33,470 --> 00:37:35,630
or by system that's twenty percent faster

571
00:37:36,780 --> 00:37:37,900
i think we're gonna win

572
00:37:40,880 --> 00:37:45,180
the most flawed experience in terms of having more data owner diverse data in one

573
00:37:45,180 --> 00:37:48,090
place and being able to create composite workflows

574
00:37:48,710 --> 00:37:53,910
i that stitch together all these different kinds of data over the appropriate kinds of analysis

575
00:37:54,360 --> 00:37:56,650
everyone applications agave you earlier

576
00:37:57,480 --> 00:38:00,200
has this characteristic you stock for example

577
00:38:00,620 --> 00:38:02,330
with blogs and web pages

578
00:38:03,200 --> 00:38:04,570
through a process of extraction

579
00:38:05,540 --> 00:38:07,550
you may end up with tables along the way

580
00:38:08,090 --> 00:38:09,940
you do everything from machine learning

581
00:38:10,490 --> 00:38:11,300
from clustering

582
00:38:14,790 --> 00:38:15,560
role on

583
00:38:16,290 --> 00:38:17,450
visualization kinds of

584
00:38:18,190 --> 00:38:19,260
conventional analytics

585
00:38:19,750 --> 00:38:22,510
but the point is you want to be able to do all of it on

586
00:38:22,510 --> 00:38:26,540
one surface you don't wanna be moving your data across different platforms

587
00:38:28,390 --> 00:38:30,160
so this ubiquitous store

588
00:38:30,590 --> 00:38:32,490
i called the shoebox style

589
00:38:33,440 --> 00:38:34,230
has data is

590
00:38:34,900 --> 00:38:39,320
captured from different sources typically want to be able to do complex event processing on it

591
00:38:40,190 --> 00:38:41,590
and persist the results at all

592
00:38:44,830 --> 00:38:46,910
store today doesn't mean simply desk

593
00:38:48,000 --> 00:38:49,050
on windows azure

594
00:38:49,730 --> 00:38:51,460
that is a separate data storage

595
00:38:52,040 --> 00:38:56,350
i on which the machines don't allow you to run any computation

596
00:38:57,040 --> 00:38:59,560
well if you look at amazon history is like back

597
00:39:00,440 --> 00:39:04,130
there you will be giving this is specialized storage systems

598
00:39:04,520 --> 00:39:12,350
andy intron dataset center networks are getting fast enough that the distance between local and remote discuss different

599
00:39:13,750 --> 00:39:17,380
on the local machines yes you have local disks

600
00:39:17,780 --> 00:39:19,760
which is what age actually first managers four year

601
00:39:20,710 --> 00:39:23,410
four main memories are getting to be significantly large

602
00:39:23,880 --> 00:39:25,990
for many many interactive applications

603
00:39:26,360 --> 00:39:29,060
if you don't have to staged in main memory

604
00:39:29,740 --> 00:39:31,610
you simply can't make the performance criteria

605
00:39:33,210 --> 00:39:36,090
if you looked at work like i

606
00:39:37,310 --> 00:39:38,010
it's essentially

607
00:39:38,480 --> 00:39:41,430
trying to be clever about what the cache in main memory and how

608
00:39:41,430 --> 00:39:43,370
i have

609
00:39:43,430 --> 00:39:49,200
touched up and some of them already

610
00:39:49,290 --> 00:39:51,910
we'll do diagnostics

611
00:39:51,930 --> 00:39:53,350
we've done some

612
00:39:53,350 --> 00:39:57,560
i spotting of plot already

613
00:39:57,620 --> 00:40:01,790
we will study residual structures

614
00:40:01,830 --> 00:40:03,660
we will talk about

615
00:40:03,660 --> 00:40:04,160
how to

616
00:40:04,540 --> 00:40:06,080
specify the model

617
00:40:06,140 --> 00:40:08,770
modelling problems

618
00:40:09,250 --> 00:40:11,470
here we can draw a line

619
00:40:11,560 --> 00:40:14,930
say if you know this

620
00:40:14,950 --> 00:40:16,250
you are

621
00:40:16,270 --> 00:40:18,540
already pretty

622
00:40:18,580 --> 00:40:22,140
experienced regression analyst

623
00:40:22,180 --> 00:40:23,120
after this

624
00:40:23,120 --> 00:40:25,430
we go into topics

625
00:40:25,470 --> 00:40:28,020
and i suggest to study

626
00:40:28,020 --> 00:40:29,410
two topics

627
00:40:29,410 --> 00:40:31,830
that are

628
00:40:31,870 --> 00:40:33,910
quite important in political science

629
00:40:36,750 --> 00:40:38,830
we talk about categorical variables

630
00:40:38,850 --> 00:40:40,520
think about voting decisions

631
00:40:40,540 --> 00:40:43,080
this is a categorical variable

632
00:40:43,160 --> 00:40:44,830
think about gender

633
00:40:45,810 --> 00:40:51,580
at least to the extent that we needed is a categorical variable

634
00:40:53,370 --> 00:40:54,720
so study

635
00:40:54,720 --> 00:40:59,950
different sorts of variables

636
00:40:59,950 --> 00:41:02,850
and then we'll talk about

637
00:41:02,870 --> 00:41:06,870
specific ordering of the data

638
00:41:06,970 --> 00:41:10,410
namely data that this water it

639
00:41:10,430 --> 00:41:11,910
in the sequence

640
00:41:11,910 --> 00:41:15,220
time series

641
00:41:15,290 --> 00:41:17,790
and then we'll put everything together

642
00:41:17,830 --> 00:41:19,790
and we'll combine

643
00:41:19,830 --> 00:41:22,980
cross sectional data like the european social survey

644
00:41:23,040 --> 00:41:28,640
with time series data like presidential approval

645
00:41:28,640 --> 00:41:31,250
which over time is changing

646
00:41:31,270 --> 00:41:32,890
if you combine the two

647
00:41:32,910 --> 00:41:34,350
we get something

648
00:41:34,390 --> 00:41:37,140
we call a panel data set

649
00:41:37,250 --> 00:41:39,330
panel data sets data that

650
00:41:40,200 --> 00:41:41,540
across units

651
00:41:41,540 --> 00:41:49,080
and within the units over time

652
00:41:49,120 --> 00:41:50,540
depending on

653
00:41:50,560 --> 00:41:56,640
the number of questions that you will be asking how fast you were que

654
00:41:56,720 --> 00:41:58,770
distribution of

655
00:41:59,520 --> 00:42:03,160
topics of the base will be different and

656
00:42:03,220 --> 00:42:05,700
i might add small modules or

657
00:42:05,700 --> 00:42:08,540
about the small modules depending on

658
00:42:08,600 --> 00:42:11,600
your interests

659
00:42:11,640 --> 00:42:14,240
but the starting a year

660
00:42:14,750 --> 00:42:17,890
is the following

661
00:42:17,980 --> 00:42:21,430
the first two days will talk about the model

662
00:42:21,450 --> 00:42:24,850
and the assumptions underlying

663
00:42:24,850 --> 00:42:27,850
the mall

664
00:42:27,870 --> 00:42:31,600
the basic assumption is linearity

665
00:42:31,660 --> 00:42:35,100
we will talk about linear models

666
00:42:35,180 --> 00:42:36,790
and you will see

667
00:42:36,810 --> 00:42:40,540
was this example here was the square root in the log

668
00:42:40,560 --> 00:42:43,060
i have given you hints

669
00:42:43,060 --> 00:42:46,640
it's possibilities to linear lies variables

670
00:42:46,660 --> 00:42:50,370
if you have an idea why you might be generalised

671
00:42:52,470 --> 00:42:57,500
so that you can stick in the linear world

672
00:42:57,520 --> 00:43:00,790
which is the simplest of all worlds

673
00:43:01,930 --> 00:43:06,390
studying data clouds that are nonlinear in nature

674
00:43:06,390 --> 00:43:08,870
this is a different approach from

675
00:43:09,350 --> 00:43:16,830
the more sophisticated approach that has been discussed quite a bit in mythological

676
00:43:16,970 --> 00:43:24,290
the basic conferences namely non-linear estimation techniques

677
00:43:25,540 --> 00:43:27,660
technique immediately

678
00:43:27,700 --> 00:43:30,080
estimates such a function

679
00:43:30,080 --> 00:43:34,790
without first being ualizing it all the right

680
00:43:34,810 --> 00:43:37,120
you can do that

681
00:43:40,430 --> 00:43:43,040
i have hesitations

682
00:43:43,040 --> 00:43:44,770
to what extent

683
00:43:44,830 --> 00:43:46,370
you should do that

684
00:43:46,410 --> 00:43:52,120
my idea is that before embarking on nonlinear estimation techniques

685
00:43:52,120 --> 00:43:54,160
it might make sense

686
00:43:54,220 --> 00:43:55,330
the first

687
00:43:56,790 --> 00:43:59,720
possibilities still in their lives

688
00:44:01,250 --> 00:44:04,390
because the linear model is so simple

689
00:44:04,600 --> 00:44:08,080
so powerful

690
00:44:08,100 --> 00:44:09,750
and well

691
00:44:09,810 --> 00:44:11,700
research well developed

692
00:44:11,770 --> 00:44:13,720
and there are lots of

693
00:44:13,770 --> 00:44:17,180
bells and whistles you can attach to the linear model

694
00:44:17,520 --> 00:44:21,370
it's a very straightforward interpretation

695
00:44:21,450 --> 00:44:24,370
it has a very straightforward

696
00:44:24,470 --> 00:44:27,770
a way of dealing with residuals

697
00:44:27,870 --> 00:44:29,470
so there's a lot of

698
00:44:29,520 --> 00:44:34,140
that gnostics you can do in linear models the much much more difficult to do

699
00:44:34,140 --> 00:44:36,750
in the non linear context

700
00:44:36,850 --> 00:44:38,040
for that reason

701
00:44:38,470 --> 00:44:45,330
my suggestion is generally to first check all the options you

702
00:44:45,350 --> 00:44:48,410
fourteen ualizing association

703
00:44:48,520 --> 00:44:54,390
don't forget this is what natural scientists are doing all the time the first develops

704
00:44:54,480 --> 00:44:56,850
a theoretical model

705
00:44:56,970 --> 00:45:00,770
that's where the economists got the idea from

706
00:45:00,810 --> 00:45:02,180
and then

707
00:45:02,330 --> 00:45:08,680
and this theoretical model tells

708
00:45:08,700 --> 00:45:12,390
the association between variables but they do that by

709
00:45:17,830 --> 00:45:20,970
employing a mathematical transformation of the data

710
00:45:21,020 --> 00:45:22,560
i will give you exactly

711
00:45:24,100 --> 00:45:26,680
set up of the data set in order

712
00:45:26,790 --> 00:45:27,930
to find

713
00:45:27,950 --> 00:45:29,870
nonlinear associations

714
00:45:30,480 --> 00:45:35,100
linear estimation techniques

715
00:45:35,100 --> 00:45:37,160
you will see if you do

716
00:45:37,970 --> 00:45:42,160
the course

717
00:45:42,180 --> 00:45:46,240
the second problem i will discuss is identifiability

718
00:45:49,770 --> 00:45:50,850
i will be able

719
00:45:50,870 --> 00:45:52,200
to distinguish

720
00:45:52,220 --> 00:45:54,810
between different

721
00:46:01,000 --> 00:46:03,020
if two variables

722
00:46:03,080 --> 00:46:08,750
are conceptually too close

723
00:46:09,540 --> 00:46:12,500
let's say

724
00:46:13,520 --> 00:46:15,620
and a number of years

725
00:46:15,680 --> 00:46:17,740
a student has studied

726
00:46:17,770 --> 00:46:22,830
the age of students and the number of years student studies

727
00:46:22,910 --> 00:46:24,270
these two variables

728
00:46:24,270 --> 00:46:26,720
this is most students start their

729
00:46:26,750 --> 00:46:32,470
student career at the age of about eighteen nineteen twenty

730
00:46:32,980 --> 00:46:35,000
there will be some variations

731
00:46:36,330 --> 00:46:39,640
the number of years someone has study

732
00:46:39,700 --> 00:46:40,770
i will

733
00:46:40,810 --> 00:46:44,140
closely connected to the age student

734
00:46:44,160 --> 00:46:47,370
there will be some outliers there always people start late

735
00:46:47,790 --> 00:46:51,430
some countries to people start very very young

736
00:46:54,240 --> 00:46:54,980
but these

737
00:46:55,000 --> 00:46:56,220
two variables

738
00:46:57,950 --> 00:47:00,580
cope very much

739
00:47:00,640 --> 00:47:02,270
to be able

740
00:47:02,350 --> 00:47:05,520
to distinguish effects

741
00:47:05,580 --> 00:47:09,950
this is what we call identifiability

742
00:47:10,000 --> 00:47:11,390
if we add

743
00:47:12,540 --> 00:47:17,640
the errors that are conceptually very close and empirically highly correlated

744
00:47:17,700 --> 00:47:18,580
o to one

745
00:47:18,580 --> 00:47:20,270
regression model

746
00:47:20,330 --> 00:47:22,870
then you will get quite odd results

747
00:47:22,870 --> 00:47:26,940
we get exactly of past winners estimator back

748
00:47:26,950 --> 00:47:29,520
the question

749
00:47:32,420 --> 00:47:42,900
depending on how you cities just like variables

750
00:47:44,150 --> 00:47:46,120
not exactly because

751
00:47:46,120 --> 00:47:51,040
you using would actually try to push points out of the margin on both sides

752
00:47:51,090 --> 00:47:53,870
so it wouldn't be exactly the same hyperplane but

753
00:47:53,890 --> 00:47:58,650
i presume it will be extremely similar

754
00:47:58,690 --> 00:48:04,440
because here for novelty detection you're not making any assumption of separability

755
00:48:05,070 --> 00:48:08,580
they just saying what i want to ensure that points

756
00:48:08,760 --> 00:48:12,600
by and large lie on the right side of the hyperplane rather than the wrong

757
00:48:14,080 --> 00:48:21,210
for a large margin separator you actually want to have a large margin

758
00:48:21,920 --> 00:48:25,710
any other questions

759
00:48:26,470 --> 00:48:27,710
let's move on

760
00:48:27,740 --> 00:48:31,580
is a simple online algorithm for it

761
00:48:32,220 --> 00:48:34,200
what you do is here

762
00:48:34,400 --> 00:48:35,860
director function

763
00:48:35,930 --> 00:48:37,040
the squared

764
00:48:37,100 --> 00:48:39,350
so the best possible the y

765
00:48:39,400 --> 00:48:41,980
so all i am doing is just replacing this

766
00:48:41,990 --> 00:48:43,870
average by some

767
00:48:43,920 --> 00:48:47,260
that are single sample drawn from it plus one half the square

768
00:48:47,330 --> 00:48:49,780
and i can just to those updates

769
00:48:49,870 --> 00:48:54,130
three just like the perceptron algorithm essentially when the point lies on the wrong side

770
00:48:54,130 --> 00:48:55,260
of the boundary

771
00:48:55,320 --> 00:48:58,630
you just update the weight vector otherwise example anything

772
00:48:58,670 --> 00:49:02,560
with thresholds just make the will be part of if the point was normal

773
00:49:02,560 --> 00:49:03,610
otherwise you

774
00:49:04,110 --> 00:49:07,820
you relax

775
00:49:07,870 --> 00:49:10,380
hi implement things

776
00:49:10,430 --> 00:49:11,310
of course

777
00:49:11,310 --> 00:49:15,320
you don't really want to do this explicit multiplication all the way through

778
00:49:15,370 --> 00:49:20,690
you want to pull it and then multiplying being that's much faster

779
00:49:22,940 --> 00:49:25,120
that's the training ground

780
00:49:25,160 --> 00:49:28,420
and these are points that are considered notable as we go through and you can

781
00:49:30,870 --> 00:49:33,310
well it's fairly common digits

782
00:49:33,350 --> 00:49:37,040
as we go down here will see that it actually starts to focus on

783
00:49:37,090 --> 00:49:40,050
kelly pathological looking ones

784
00:49:40,100 --> 00:49:43,260
i think they all have slightly from those that stand here

785
00:49:43,360 --> 00:49:44,820
but basically

786
00:49:44,840 --> 00:49:48,570
what you can see is that

787
00:49:48,580 --> 00:49:52,930
initially it it it considers pretty much everything or even typical beach just because i

788
00:49:52,930 --> 00:49:55,350
haven't seen that in many of them yet

789
00:49:55,360 --> 00:49:58,580
but as it gets more and more data it will start picking up fairly unusual

790
00:50:00,990 --> 00:50:03,210
this the first training examples

791
00:50:03,220 --> 00:50:06,190
after looking at the same time gain

792
00:50:06,230 --> 00:50:08,070
now it's from the first

793
00:50:08,070 --> 00:50:10,820
least pathological

794
00:50:10,870 --> 00:50:14,920
and that's on it is that started it's never seen before and you would agree

795
00:50:14,920 --> 00:50:16,880
that there free of

796
00:50:16,940 --> 00:50:19,560
that's just one after other

797
00:50:19,690 --> 00:50:24,830
so let others and can only be good

798
00:50:25,730 --> 00:50:27,380
what we did so far

799
00:50:27,430 --> 00:50:28,530
we looked at

800
00:50:28,570 --> 00:50:31,080
novelty detection by this information

801
00:50:32,340 --> 00:50:33,530
we first

802
00:50:33,540 --> 00:50:38,190
look the parzen windows and then we realized that it's maybe not so effective

803
00:50:38,290 --> 00:50:40,160
then we looked at this and

804
00:50:40,210 --> 00:50:41,960
in the end we just

805
00:50:42,010 --> 00:50:44,220
the fourth places the founding

806
00:50:44,240 --> 00:50:45,620
well basically

807
00:50:45,650 --> 00:50:49,370
on one side the dots on the other side no points

808
00:50:49,410 --> 00:50:52,670
and we discussed the brief stochastic gradient descent version

809
00:50:53,570 --> 00:50:55,790
in the practical sessions of fishing

810
00:50:55,800 --> 00:50:59,330
you actually get to implement the stochastic gradient descent algorithm

811
00:50:59,380 --> 00:51:00,720
this very simple

812
00:51:00,770 --> 00:51:04,510
it will be very useful for you

813
00:51:06,550 --> 00:51:09,720
any questions before we move on

814
00:51:09,910 --> 00:51:13,290
OK good

815
00:51:27,740 --> 00:51:32,970
i don't understand british sorry

816
00:51:34,350 --> 00:51:35,570
let's say

817
00:51:35,580 --> 00:51:40,370
and some statistician at the australian bureau of statistics or some other boring place

818
00:51:40,630 --> 00:51:44,110
i hope we don't have anybody from the here to be

819
00:51:44,110 --> 00:51:46,920
now good

820
00:51:46,940 --> 00:51:52,800
so and let's say i'm just recording the weight and the height of people

821
00:51:52,830 --> 00:51:55,830
and maybe these are really tall people anyway

822
00:51:55,850 --> 00:51:56,860
and knots

823
00:51:56,880 --> 00:51:59,960
the reason is can in any case

824
00:52:00,090 --> 00:52:06,280
what you will get is if you like get the plot like this i don't

825
00:52:06,280 --> 00:52:09,370
know whether that's true have just made it up and i just use the normal

826
00:52:09,370 --> 00:52:12,260
distribution and generated something like this

827
00:52:13,540 --> 00:52:15,130
what would be nice

828
00:52:15,210 --> 00:52:16,720
be able to say well

829
00:52:16,760 --> 00:52:18,420
let's assume somebody

830
00:52:18,450 --> 00:52:23,870
is one way to ninety centimeters tall well what reasonable distribution of his life

831
00:52:23,910 --> 00:52:26,610
given that is that all

832
00:52:27,630 --> 00:52:32,090
maybe you know given all those possible situations can i think therefore making the prediction

833
00:52:32,090 --> 00:52:35,650
of how to the guy might be

834
00:52:38,230 --> 00:52:42,610
i'm going to tell you that this distribution here was a normal distribution

835
00:52:42,650 --> 00:52:44,470
so that appeared in city

836
00:52:44,480 --> 00:52:48,130
so what i could do is i could simply as well the probability of the

837
00:52:48,130 --> 00:52:50,570
weight given the height

838
00:52:50,570 --> 00:52:55,560
of course is the bayes rule p of five white to have appeared five

839
00:52:55,910 --> 00:52:57,520
this this is fixed

840
00:52:57,530 --> 00:53:02,150
just proportional to that of course also gained gaussians

841
00:53:02,170 --> 00:53:06,110
it's actually the really nice thing about constancy conditional software variable to get the calcium

842
00:53:06,110 --> 00:53:07,370
back to his

843
00:53:08,370 --> 00:53:10,160
if i go in slice

844
00:53:10,220 --> 00:53:12,600
this density here on this line

845
00:53:12,610 --> 00:53:14,620
we get some but like this

846
00:53:14,630 --> 00:53:18,240
if i sliced over here to get the public this and so on

847
00:53:18,360 --> 00:53:20,550
so these are all gaussians

848
00:53:20,590 --> 00:53:23,640
and what you could could actually see if you were to draw straight line through

849
00:53:23,640 --> 00:53:29,400
this is i could draw straight line through all the maximum

850
00:53:30,400 --> 00:53:32,370
well what's happened here

851
00:53:32,440 --> 00:53:34,620
i've basically done in linear regression

852
00:53:36,440 --> 00:53:37,650
what i've done is

853
00:53:37,660 --> 00:53:39,520
well given the height

854
00:53:39,560 --> 00:53:43,850
i can estimate the mean of the white they can also estimate the variance of

855
00:53:45,470 --> 00:53:49,870
and all else is well hey this is a linear dependency between

856
00:53:49,920 --> 00:53:55,180
the means of weight height

857
00:53:57,220 --> 00:54:00,550
this is inside debate

858
00:54:00,560 --> 00:54:04,490
let's get some divisional one why fronts otherwise

859
00:54:04,540 --> 00:54:07,170
trained on what promised to start

860
00:54:07,390 --> 00:54:09,670
of those locations

861
00:54:09,720 --> 00:54:12,110
and so i could just go in the world

862
00:54:14,150 --> 00:54:15,580
what prime

863
00:54:15,630 --> 00:54:17,350
one of them so

864
00:54:17,730 --> 00:54:19,090
i can just

865
00:54:19,100 --> 00:54:23,650
look at this one here as long as they get the proper normalisation everything's fine

866
00:54:23,710 --> 00:54:28,910
so i cannot integrate out there i can use nine districts and just sample

867
00:54:28,920 --> 00:54:31,630
i'm going to be lazy here so i'm going to look at only cases where

868
00:54:31,630 --> 00:54:35,510
i can integrate out

869
00:54:36,720 --> 00:54:42,790
now let's assume that the normal distribution david agus has seen one before his life

870
00:54:46,290 --> 00:54:48,390
this is a multivariate normal distribution

871
00:54:48,590 --> 00:54:50,470
the random variable x

872
00:54:50,510 --> 00:54:52,090
the mean

873
00:54:52,140 --> 00:54:56,940
cause you can see it's maximized for that this is inverse covariance matrix not just

874
00:54:56,940 --> 00:54:58,530
some normalization

875
00:54:58,630 --> 00:55:00,560
well forget about that very much

876
00:55:00,570 --> 00:55:03,800
but this will be interesting lecture on

877
00:55:05,570 --> 00:55:10,010
obviously this sigma has only non negative like

878
00:55:10,020 --> 00:55:10,670
i mean

879
00:55:10,760 --> 00:55:15,830
the that's random variables in a negative thinking combination of random variables also can only

880
00:55:15,830 --> 00:55:18,590
have a negative i

881
00:55:18,590 --> 00:55:20,140
people are interested in

882
00:55:20,230 --> 00:55:22,290
you know radiation in

883
00:55:22,390 --> 00:55:25,090
a particular feature value for example

884
00:55:26,570 --> 00:55:29,260
they tend to be univariate

885
00:55:29,470 --> 00:55:34,740
you know the main reason is for efficiency reasons

886
00:55:34,900 --> 00:55:39,940
finding when changes occur for example if you have a stationary stream if something

887
00:55:41,240 --> 00:55:44,510
then you want to know it could be and normally event and you have to

888
00:55:44,510 --> 00:55:46,740
detect that

889
00:55:46,760 --> 00:55:50,640
and also you're interested in finding the magnitude of the change for example not just

890
00:55:50,640 --> 00:55:51,670
to change

891
00:55:51,710 --> 00:55:53,030
but it has to be

892
00:55:53,110 --> 00:55:55,360
sufficiently significant

893
00:55:55,380 --> 00:55:57,380
for us to be worried about

894
00:55:57,390 --> 00:55:58,850
in which case

895
00:55:58,880 --> 00:56:01,920
you wanted to see the magnitude

896
00:56:01,920 --> 00:56:04,590
so for example it could be used for

897
00:56:04,600 --> 00:56:09,680
you know detecting some binding event so for example it's a there is a huge

898
00:56:10,780 --> 00:56:13,170
in the in the distribution of something

899
00:56:13,240 --> 00:56:17,410
then that could be construed as a critical events so for example it's a

900
00:56:17,420 --> 00:56:20,450
you know somebody watching internet traffic

901
00:56:20,460 --> 00:56:22,910
there is huge variation took place

902
00:56:23,410 --> 00:56:26,860
and that could be a signal of something so for example

903
00:56:26,880 --> 00:56:32,690
let's say if the packet rate on it could be somebody attacking the network or

904
00:56:32,690 --> 00:56:35,080
if the traffic drop substantially

905
00:56:35,920 --> 00:56:37,850
you know one of the line being cut

906
00:56:38,750 --> 00:56:41,020
you know one of the

907
00:56:42,530 --> 00:56:45,160
communications we just might have broken down

908
00:56:45,160 --> 00:56:50,410
so it is very important to detect these kinds of events

909
00:56:51,100 --> 00:56:52,830
so what are we going to

910
00:56:52,850 --> 00:56:54,440
measure these

911
00:56:55,220 --> 00:56:56,940
changes for example

912
00:56:56,940 --> 00:56:58,520
can we use

913
00:56:59,810 --> 00:57:01,580
you know some kind of measure

914
00:57:01,580 --> 00:57:03,050
to say that

915
00:57:03,060 --> 00:57:05,160
there is a i

916
00:57:08,060 --> 00:57:10,440
it's interesting is in the in the in the medical

917
00:57:10,520 --> 00:57:11,720
the main

918
00:57:11,810 --> 00:57:15,100
they continue to use these things like for example

919
00:57:15,110 --> 00:57:18,330
if you look at any typical medical papers

920
00:57:18,360 --> 00:57:20,880
that that have some

921
00:57:20,890 --> 00:57:22,330
i don't to look at

922
00:57:22,380 --> 00:57:24,800
unsurprisingly most cases

923
00:57:24,810 --> 00:57:27,200
they have some data to the

924
00:57:27,240 --> 00:57:34,870
and the reason i say surprising is like medical work is experimentally nature always the

925
00:57:34,870 --> 00:57:36,210
case studies

926
00:57:36,210 --> 00:57:40,270
i'm and sometimes the case studies are very small populations like

927
00:57:40,990 --> 00:57:42,360
ten twenty

928
00:57:42,570 --> 00:57:48,800
and the reason is that it's very difficult to have large large scale test for

929
00:57:49,210 --> 00:57:51,740
and also some of these cases

930
00:57:51,760 --> 00:57:53,900
run for twenty years

931
00:57:53,920 --> 00:57:55,360
and they want to study

932
00:57:55,620 --> 00:58:00,550
so it's not non-trivial is a real problem in certain domains

933
00:58:01,240 --> 00:58:04,090
so the language they try to use in in understanding

934
00:58:04,110 --> 00:58:06,110
these kinds of patterns

935
00:58:06,120 --> 00:58:11,090
they use the notions of our iteration risk ratios and i'll talk about little over

936
00:58:11,140 --> 00:58:12,390
for those things

937
00:58:12,400 --> 00:58:16,340
generally they useful binary data the mean many medical cases

938
00:58:16,360 --> 00:58:19,680
the situation is generally of

939
00:58:19,740 --> 00:58:24,710
binary you have the disease or doesn't have the disease

940
00:58:24,770 --> 00:58:27,890
can also be used for quality evaluation

941
00:58:27,890 --> 00:58:33,140
for multivariate contrasts and we see that a simple example is given x

942
00:58:33,210 --> 00:58:37,330
suppose let's say we have collective small datasets

943
00:58:37,340 --> 00:58:39,710
i and we're looking at

944
00:58:39,730 --> 00:58:41,330
but the gender

945
00:58:41,390 --> 00:58:47,490
it is a an indication of you know exposed to some kind of

946
00:58:47,510 --> 00:58:52,180
so we collect this data and and then we wanted to

947
00:58:52,220 --> 00:58:55,700
ask questions suppose let's say in our population

948
00:58:55,760 --> 00:58:58,570
we have one hundred men and women

949
00:58:58,580 --> 00:59:03,300
and and we found that seventy of of men were exposed

950
00:59:03,360 --> 00:59:05,010
and ten women

951
00:59:05,020 --> 00:59:06,510
are not exposed

952
00:59:06,610 --> 00:59:09,480
so now we want to give some kind of measures to show

953
00:59:09,520 --> 00:59:11,460
how this particular feature

954
00:59:11,490 --> 00:59:14,240
the gender value can determine

955
00:59:14,270 --> 00:59:16,700
these kinds of patterns

956
00:59:16,740 --> 00:59:19,640
so for example on this dataset we found

957
00:59:19,650 --> 00:59:22,740
the odds of exposure for male he's

958
00:59:22,760 --> 00:59:25,830
o point seven two point three so

959
00:59:27,080 --> 00:59:28,990
two twenty three

960
00:59:29,040 --> 00:59:32,520
and for female is point one one

961
00:59:32,540 --> 00:59:35,150
then in the in the medical domain they

962
00:59:35,180 --> 00:59:40,660
we use this notion of odds ratio is the ratio of these exposure or

963
00:59:40,700 --> 00:59:42,850
which is twenty one two

964
00:59:42,890 --> 00:59:47,520
so males have twenty one point two times the odds of exposure than females this

965
00:59:47,520 --> 00:59:48,210
is how

966
00:59:48,250 --> 00:59:50,830
the terminology used

967
00:59:50,850 --> 00:59:54,040
so it indicates the exposure is much more likely for males

968
00:59:54,330 --> 00:59:57,850
then for female

969
00:59:57,870 --> 01:00:00,440
and there's also a notion of relative risk

970
01:00:00,450 --> 01:00:03,460
which is a bit more probabilistic model

971
01:00:03,520 --> 01:00:04,960
so in this case

972
01:00:04,980 --> 01:00:06,580
what we do is that

973
01:00:06,590 --> 01:00:12,220
the relative risk of exposure for male is basically seventy percent

974
01:00:12,230 --> 01:00:14,720
and for females is ten percent

975
01:00:14,730 --> 01:00:15,870
and then we say

976
01:00:15,880 --> 01:00:18,840
relative risk ratio e seven

977
01:00:18,950 --> 01:00:22,870
which which means that the men had seven times more likely

978
01:00:22,910 --> 01:00:27,950
to be exposed than so this kind of measures we try to do so

979
01:00:28,020 --> 01:00:33,640
the question here is that these numbers indicate whether it's significant or not

980
01:00:33,710 --> 01:00:35,130
if the ratio is

981
01:00:36,100 --> 01:00:36,980
which means

982
01:00:37,000 --> 01:00:38,660
there is a

983
01:00:38,690 --> 01:00:40,020
good indication

984
01:00:40,030 --> 01:00:43,320
this particular feature has something to say

985
01:00:43,950 --> 01:00:45,570
conclusions what

986
01:00:45,640 --> 01:00:49,340
we're going to make

987
01:00:51,200 --> 01:00:53,150
so this is a typically

988
01:00:53,160 --> 01:00:57,210
what medical people continues to use and you will see that this could be applied

989
01:00:57,220 --> 01:00:59,180
in contrast patterns as well

990
01:00:59,770 --> 01:01:04,830
in the case of pattern and rule based contrasts we're going to look at

991
01:01:04,840 --> 01:01:08,100
overview of relational contrast practice mining

992
01:01:08,210 --> 01:01:12,460
especially we're going to look at the notions of emerging patterns

993
01:01:12,520 --> 01:01:15,100
and then i'll talk about the mining issues

994
01:01:15,130 --> 01:01:20,060
this particular type of patterns we call them jumping emerging patterns

995
01:01:20,100 --> 01:01:23,350
what it means is that you know this somehow

996
01:01:24,580 --> 01:01:27,640
a special kind of patterns that make it appear

997
01:01:27,690 --> 01:01:28,570
in one

998
01:01:28,580 --> 01:01:31,100
group and never appeared

999
01:01:31,120 --> 01:01:32,730
so this could be

1000
01:01:32,760 --> 01:01:33,970
quite a strong

1001
01:01:34,000 --> 01:01:36,580
contrast patterns

1002
01:01:36,590 --> 01:01:43,000
and we'll talk about computational complexity and not going to mathematics but just giving you

1003
01:01:43,000 --> 01:01:46,920
and you haven't got a model modification and you haven't got additional covariates are all

1004
01:01:47,170 --> 01:01:49,690
information that has allowed you to anticipate the change

1005
01:01:50,900 --> 01:01:52,380
correlation pattern changes

1006
01:01:52,870 --> 01:01:54,210
that suggests that different

1007
01:01:54,810 --> 01:01:56,750
pattern zeros in the precision matrix

1008
01:01:58,290 --> 01:02:01,900
zero suddenly pops into life and takes non-zero time varying value

1009
01:02:02,600 --> 01:02:04,870
or something that was important practically

1010
01:02:05,650 --> 01:02:08,810
try not to use the term significant that appeared in the model and the data

1011
01:02:08,810 --> 01:02:11,190
like that that was relevant influence predictions

1012
01:02:12,150 --> 01:02:13,580
suddenly becomes irrelevant

1013
01:02:14,750 --> 01:02:17,790
this is the notion of time variation in sparsity and this is the picture

1014
01:02:19,000 --> 01:02:21,850
so he weekly management aggression parameters are precision matrix

1015
01:02:22,270 --> 01:02:25,540
wear different periods of time we want different patterns of zeros

1016
01:02:26,440 --> 01:02:27,150
and we want some

1017
01:02:28,170 --> 01:02:32,620
formal model time series model that allows those things to to to actually appears so

1018
01:02:32,620 --> 01:02:36,310
you can think about switching mechanisms and various kinds but as the dimension gets higher

1019
01:02:38,290 --> 01:02:42,980
the typical methodologies and switching mechanisms are really challenged

1020
01:02:44,250 --> 01:02:45,690
to represent this this notion

1021
01:02:46,870 --> 01:02:47,920
that's one topical area

1022
01:02:49,920 --> 01:02:51,750
into my earlier slides i had

1023
01:02:53,000 --> 01:02:54,730
pictures this particular are vignettes

1024
01:02:55,370 --> 01:02:56,770
cartoon very similar to this

1025
01:02:57,480 --> 01:03:00,150
this is another area where we are

1026
01:03:00,270 --> 01:03:02,620
seeing more and more interest in dynamic modelling

1027
01:03:04,730 --> 01:03:09,080
gene network cellular networks looking at networks genes within cells and also

1028
01:03:10,460 --> 01:03:11,790
networks themselves cause

1029
01:03:14,190 --> 01:03:16,080
other kind not just genes but other molecules

1030
01:03:17,170 --> 01:03:18,370
well we have to start with

1031
01:03:19,000 --> 01:03:22,640
at least notional class mathematical models of biochemical models

1032
01:03:23,210 --> 01:03:25,650
and then of course quickly abstract the statistical level

1033
01:03:26,230 --> 01:03:29,560
because everything is in discrete time anyway in terms of data

1034
01:03:30,770 --> 01:03:34,460
and one is interested in and fitting a highly non linear model which has

1035
01:03:36,330 --> 01:03:40,400
and overlay that were interested in the question of understanding sparsity patterns

1036
01:03:41,020 --> 01:03:41,880
because network

1037
01:03:42,350 --> 01:03:44,330
biological network models like this are drawn

1038
01:03:45,310 --> 01:03:49,870
very often with some of the edges and thee arrows in their well understood to be

1039
01:03:50,690 --> 01:03:53,000
to be real by chemically and potentially functionally

1040
01:03:54,960 --> 01:03:55,810
very often also

1041
01:03:56,400 --> 01:03:57,420
it's a hypothesis

1042
01:03:58,350 --> 01:03:59,730
the network structure itself

1043
01:04:00,170 --> 01:04:01,400
is also of interest

1044
01:04:02,170 --> 01:04:05,650
so the same general questions are looking at time series data

1045
01:04:06,190 --> 01:04:10,310
where things are very and time parameters that represents strength of association

1046
01:04:10,880 --> 01:04:15,440
on arrows and dioxin in pictures like this and other features

1047
01:04:15,960 --> 01:04:18,400
that will be very in time typically because these

1048
01:04:19,020 --> 01:04:22,370
context in which the cells are growing and all the other things we don't have in the model

1049
01:04:22,830 --> 01:04:24,940
but we are also interested in sparsity structure to

1050
01:04:26,520 --> 01:04:29,520
there's lots of other interesting questions here there are spatial aspects

1051
01:04:31,250 --> 01:04:36,230
i'll talk a little more about latent states missing data and in when one discretizes differentially

1052
01:04:36,850 --> 01:04:39,210
most from the continuous-to-discrete to discrete time

1053
01:04:39,710 --> 01:04:43,150
typically these models will be but built on a very fine discrete time scale

1054
01:04:43,920 --> 01:04:47,500
and data is on a much cruder scale so this turns and missing data

1055
01:04:48,040 --> 01:04:52,920
terms of latent variables translating states that become part of a these state variable that

1056
01:04:52,920 --> 01:04:54,980
when dealing with in the analysis

1057
01:04:56,120 --> 01:04:57,230
imaging is important

1058
01:04:58,730 --> 01:05:00,690
to get data that's a whole other story

1059
01:05:03,600 --> 01:05:05,020
let me say what about computation

1060
01:05:06,520 --> 01:05:07,620
this is a foundation issue

1061
01:05:10,020 --> 01:05:13,500
until i fast-forwarded always you know still back in the nineteen eighties

1062
01:05:14,310 --> 01:05:15,080
a lot of these

1063
01:05:15,440 --> 01:05:19,000
story was analytic computation to enter the nineteen eighties

1064
01:05:20,290 --> 01:05:21,370
since then as we all know

1065
01:05:21,770 --> 01:05:22,600
life has changed

1066
01:05:25,290 --> 01:05:30,850
we use simulation i got this synthetic futures i use the term synthetic rubber and simulated a lot

1067
01:05:31,480 --> 01:05:35,830
i it particularly in time series another process modeling contexts

1068
01:05:36,920 --> 01:05:38,000
don't look at prediction

1069
01:05:38,480 --> 01:05:43,850
by computing means and variances and other things just simulate and and ask questions how

1070
01:05:43,850 --> 01:05:45,870
do those synthetic futures compare with

1071
01:05:46,350 --> 01:05:48,040
the real data just subjectively

1072
01:05:49,620 --> 01:05:53,210
so we had twenty years event one twenty years them ciency and some of you

1073
01:05:53,210 --> 01:05:55,310
will know that this really had a big impact on

1074
01:05:56,100 --> 01:05:59,310
bayesian dynamic modelling as it did on online areas for

1075
01:05:59,830 --> 01:06:00,770
bayesian statistics

1076
01:06:02,140 --> 01:06:03,750
some of you will know about

1077
01:06:04,750 --> 01:06:11,980
perturbed style algorithms forward filtering backward sampling methods for simulating in linear gaps in state space models extensions event

1078
01:06:13,040 --> 01:06:16,600
to mixture models are various kinds are the two kinds i alluded to

1079
01:06:17,100 --> 01:06:22,710
mixtures in the models in parallel and models that get mixed up in different ways and each time point

1080
01:06:24,370 --> 01:06:25,600
of course it's been revolutionary

1081
01:06:26,960 --> 01:06:29,690
we've had more than twenty years sequential monte carlo

1082
01:06:31,310 --> 01:06:34,690
the foundational concept learning in time sequentially

1083
01:06:35,670 --> 01:06:39,520
we may care about the past for analysis retrospection to understand

1084
01:06:40,000 --> 01:06:41,580
and adapt a model for the future

1085
01:06:41,580 --> 01:06:45,680
you the second one was how to train an HMM and as i said there

1086
01:06:45,810 --> 01:06:50,830
the third one which is hard to find the most likely path of an HMM

1087
01:06:51,580 --> 01:06:57,100
you have that is train and you'd like to know i give you sequence and

1088
01:06:57,100 --> 01:06:59,470
that to no which sequence of states

1089
01:07:00,140 --> 01:07:04,600
this HMM followed to a to generate the sequence

1090
01:07:04,620 --> 01:07:07,850
and there's a very efficient algorithm called the viterbi algorithm

1091
01:07:07,850 --> 01:07:10,580
that does exactly that in the

1092
01:07:10,580 --> 01:07:13,970
the idea behind the viterbi algorithm is

1093
01:07:13,990 --> 01:07:15,790
is the following

1094
01:07:15,810 --> 01:07:20,080
if you know that you have you have the best let's suppose someone gives you

1095
01:07:20,100 --> 01:07:23,890
the best buy so this graph is the time

1096
01:07:23,910 --> 01:07:28,200
and the values of the state k and basically are searching for the best

1097
01:07:29,390 --> 01:07:34,760
the viterbi algorithm is based on the following simple concept

1098
01:07:34,770 --> 01:07:36,760
if you knew

1099
01:07:36,810 --> 01:07:38,330
if you knew the best

1100
01:07:38,380 --> 01:07:41,330
and let's suppose that you know that the best path

1101
01:07:42,470 --> 01:07:47,450
goes by this but at this time so you know that you just don't know

1102
01:07:47,490 --> 01:07:51,120
what's going on here and what's going on here but you know about this time

1103
01:07:51,120 --> 01:07:53,200
he has to go through that state

1104
01:07:53,220 --> 01:07:55,660
someone give you that his

1105
01:07:55,680 --> 01:07:59,600
in that case you know that the best

1106
01:07:59,600 --> 01:08:03,100
the best path to move from the beginning that

1107
01:08:03,330 --> 01:08:07,410
the best path is going to be the best off of this the best part

1108
01:08:07,410 --> 01:08:11,020
of that is going to be the communication there's no other way to that of

1109
01:08:11,020 --> 01:08:15,160
the best path if you know that that's true that needs to be the best

1110
01:08:15,160 --> 01:08:17,470
back here in the best

1111
01:08:17,470 --> 01:08:23,370
there's no other best but it's going to be the sum of these two so

1112
01:08:23,390 --> 01:08:24,680
that means that

1113
01:08:24,700 --> 01:08:28,220
i cannot simply compute the best part here

1114
01:08:28,240 --> 01:08:29,890
and decided that the

1115
01:08:29,890 --> 01:08:34,040
that now if i were to pass through that i think it would have to

1116
01:08:34,040 --> 01:08:34,890
be on the

1117
01:08:34,930 --> 01:08:38,040
the one coming from here are the ones coming from here or the one coming

1118
01:08:38,040 --> 01:08:39,950
from here on one of them

1119
01:08:39,970 --> 01:08:43,680
and that's not i can forget about all the other ones so that's the way

1120
01:08:43,680 --> 01:08:45,950
to break the complexity of the

1121
01:08:45,970 --> 01:08:48,100
one of the the problem into

1122
01:08:48,140 --> 01:08:53,850
that's marching chunk and there is no great

1123
01:08:53,870 --> 01:08:57,390
that's exactly this is here

1124
01:08:57,490 --> 01:09:00,080
and i will not explain it but basically

1125
01:09:00,200 --> 01:09:02,100
it decides to two

1126
01:09:02,120 --> 01:09:08,060
to break so you're searching for the best sequence of by that maximize joint likelihood

1127
01:09:08,060 --> 01:09:11,410
of the data and which state you back

1128
01:09:12,950 --> 01:09:14,000
breaks down

1129
01:09:14,020 --> 01:09:15,140
two searching

1130
01:09:15,160 --> 01:09:20,100
the recursive solution again so you have these variable that we define we're going to

1131
01:09:20,100 --> 01:09:24,560
have to do it recursively as we did for the i five variables and in

1132
01:09:24,560 --> 01:09:28,310
fact searching at each time step for the best

1133
01:09:28,310 --> 01:09:29,890
partial plan

1134
01:09:29,910 --> 01:09:34,580
for a given that that so if we knew the partial path the given types

1135
01:09:34,580 --> 01:09:38,180
that we can compute the partial path of the next and the one on the

1136
01:09:38,180 --> 01:09:43,270
what the when and we're going to do that recursively seen in the similar recursion

1137
01:09:43,700 --> 01:09:48,740
as the i found liable is in fact the same equations where we replace sums

1138
01:09:48,760 --> 01:09:49,990
by max

1139
01:09:49,990 --> 01:09:53,260
that's all so it's the same complexity the difference

1140
01:09:53,270 --> 01:09:55,950
and that gives you do

1141
01:09:55,950 --> 01:09:59,350
the best for sequence of states that

1142
01:09:59,350 --> 01:10:00,600
basically all

1143
01:10:00,600 --> 01:10:02,260
there is to it

1144
01:10:02,270 --> 01:10:04,220
so because

1145
01:10:04,270 --> 01:10:10,450
it remains are mainly used in very efficiently used in speech recognition i want to

1146
01:10:10,470 --> 01:10:16,990
british he explained how we use them for speech recognition because this this knowledge can

1147
01:10:16,990 --> 01:10:26,790
be extended to many other tasks that are very related to multimodal interactions

1148
01:10:26,790 --> 01:10:30,640
so the the IDE so there are several task in speech recognition i want to

1149
01:10:30,640 --> 01:10:35,410
talk about one of the most complex one which is continuous speech recognition so you

1150
01:10:36,560 --> 01:10:39,500
you have a sequence of acoustic frames

1151
01:10:39,540 --> 01:10:43,790
you'd like to transform that into a sequence of words or

1152
01:10:43,810 --> 01:10:46,140
forty let's

1153
01:10:46,160 --> 01:10:50,350
let's see the difference first between words and phrases in the given language

1154
01:10:50,390 --> 01:10:53,830
you may have hundreds of thousands of words

1155
01:10:54,260 --> 01:10:58,330
and that might be difficult to model on the other hand you have in general

1156
01:10:58,930 --> 01:11:03,180
you can decompose what can be said in that language into a set of a

1157
01:11:03,180 --> 01:11:06,890
much simpler objects which are called phonemes

1158
01:11:06,910 --> 01:11:11,540
and most languages have between twenty and fifty

1159
01:11:11,900 --> 01:11:18,830
changes such phonemes but on the other hand nobody is ready to say the exact

1160
01:11:18,830 --> 01:11:23,140
number definitive member and everyone has its own way to define what is the fourteen

1161
01:11:23,140 --> 01:11:25,830
but on average you have about thirty

1162
01:11:25,830 --> 01:11:28,310
forty four nine per language

1163
01:11:28,330 --> 01:11:29,740
and with these

1164
01:11:29,760 --> 01:11:35,740
chunks of phonemes you can create words by simply concatenating phonemes so we have a

1165
01:11:35,740 --> 01:11:39,850
very small number of phonemes you can create a very large number of words

1166
01:11:39,870 --> 01:11:43,870
so the idea now is that we're going to create an HMM for each of

1167
01:11:43,870 --> 01:11:45,580
these phonemes

1168
01:11:45,600 --> 01:11:48,240
all words if we have a very small language

1169
01:11:48,240 --> 01:11:53,760
working code everything it on this page is guaranteed to be working code

1170
01:11:53,780 --> 01:11:57,140
there were the least runs from subversion because that's how they were generated so if

1171
01:11:57,140 --> 01:11:59,350
you see figure that you want to make

1172
01:11:59,420 --> 01:12:03,670
like this annotation here for example if you click on it

1173
01:12:03,690 --> 01:12:06,780
it takes you to a higher resolution image of that

1174
01:12:06,800 --> 01:12:11,540
and syntax highlighted source code that you can just download and run so that's great

1175
01:12:11,540 --> 01:12:14,210
that's really great leeway when you're trying to learn

1176
01:12:16,710 --> 01:12:18,360
it's a great resource

1177
01:12:20,290 --> 01:12:22,470
the documentation is

1178
01:12:22,510 --> 01:12:25,070
you know i think i pdf at this point seven hundred pages and i don't

1179
01:12:25,070 --> 01:12:27,720
know maybe half of it is documented as opposed to a lot of work

1180
01:12:28,100 --> 01:12:29,670
left to be done

1181
01:12:29,670 --> 01:12:32,420
so we're using this tool called

1182
01:12:32,440 --> 01:12:36,680
sphinx which is applied on the documentation generating tool

1183
01:12:36,780 --> 01:12:39,760
and you basically right restructured text

1184
01:12:39,780 --> 01:12:42,810
and will generate output in HTML

1185
01:12:42,820 --> 01:12:44,460
or in

1186
01:12:46,040 --> 01:12:47,480
using later

1187
01:12:47,480 --> 01:12:50,970
so i'm just going to become part of our users guide working with text

1188
01:12:51,450 --> 01:12:56,500
since we're just looking in mathematical expressions we can look at contemporary writing mathematical expressions

1189
01:12:56,540 --> 01:13:00,690
so this is kind of our guide to writing method about expressions we actually use

1190
01:13:00,690 --> 01:13:02,480
our own math rendering engine

1191
01:13:02,500 --> 01:13:06,400
and in creating a user's guide to generate the output so again here's what you

1192
01:13:06,400 --> 01:13:07,900
see is what you get

1193
01:13:10,100 --> 01:13:16,610
and basically all the different kinds of math expressions different fonts and you can choose

1194
01:13:16,630 --> 01:13:20,070
the symbol tables

1195
01:13:20,110 --> 01:13:24,030
here there's also another new little thing in this rest one of developers wrote this

1196
01:13:24,030 --> 01:13:26,930
mode called plant and you can just say

1197
01:13:26,940 --> 01:13:32,380
in your best documentary structured text document you just say plug this file so

1198
01:13:32,400 --> 01:13:36,170
for example that the cold degenerate what we're looking at here

1199
01:13:36,210 --> 01:13:39,760
mpl examples

1200
01:13:41,770 --> 01:13:43,730
user's guide

1201
01:13:45,250 --> 01:13:50,050
at the bottom there just these two lines of code here they say a

1202
01:13:50,070 --> 01:13:54,420
this is an doctoring plot this file so points an external file calls matplotlib on

1203
01:13:54,430 --> 01:13:59,280
it brings the filing documentation so it's kind regression testing you know your code examples

1204
01:13:59,280 --> 01:14:05,450
are generating your fingers run because they run away documentation build time and we do

1205
01:14:05,450 --> 01:14:13,650
the same thing in our API documents

1206
01:14:13,670 --> 01:14:16,280
so for example if you have a pipeline which is one of the larger we've

1207
01:14:16,280 --> 01:14:18,730
been looking at

1208
01:14:18,880 --> 01:14:22,270
the autocorrelation function the doc strings around here

1209
01:14:22,280 --> 01:14:25,620
we have this sort of snippets which show you graphs and for really nice

1210
01:14:25,860 --> 01:14:26,890
i think it's a really nice

1211
01:14:26,950 --> 01:14:29,210
the rest is very nice tool for making

1212
01:14:29,320 --> 01:14:30,690
these kinds of

1213
01:14:30,710 --> 01:14:33,520
these documents because you can incorporate math

1214
01:14:33,570 --> 01:14:35,400
figures in the syntax

1215
01:14:35,420 --> 01:14:39,520
it's fairly simple to plain text is not is the mark of has the latex

1216
01:14:39,520 --> 01:14:40,820
document is

1217
01:14:40,860 --> 01:14:43,230
cross-platform cetera

1218
01:14:43,250 --> 01:14:47,840
so for those of you who are interested in the math the source tree we

1219
01:14:47,840 --> 01:14:52,690
actually have a template called things template that we wrote that has all basically create

1220
01:14:52,690 --> 01:14:57,050
some document that has all of these things to do documentation and to make lots

1221
01:14:57,320 --> 01:15:02,070
math text when you first start with the template and just use the project so

1222
01:15:02,070 --> 01:15:03,980
that's a great tool

1223
01:15:03,980 --> 01:15:06,650
for doing documentation

1224
01:15:06,710 --> 01:15:08,380
OK so

1225
01:15:08,540 --> 01:15:13,420
i told you i was going to talk about interactive use and some going

1226
01:15:13,440 --> 01:15:15,500
focus on that now for the

1227
01:15:15,520 --> 01:15:19,020
the rest of my demo time

1228
01:15:19,040 --> 01:15:20,730
and how much do insert

1229
01:15:21,050 --> 01:15:27,570
after quickly

1230
01:15:27,570 --> 01:15:31,980
so had this little demo here called

1231
01:15:32,000 --> 01:15:33,420
class starts

1232
01:15:33,440 --> 01:15:37,820
leon glass was professor at the center of nonlinear dynamics and mcgill

1233
01:15:37,880 --> 01:15:40,190
he was doing this experiment

1234
01:15:40,230 --> 01:15:44,420
three in the sixties when people are interested in systems

1235
01:15:44,420 --> 01:15:46,040
where he was

1236
01:15:47,360 --> 01:15:50,900
taking a piece of blank paper putting it in the xerox machine and copying a

1237
01:15:50,900 --> 01:15:54,070
piece of blank paper to the output from the xerox machine

1238
01:15:54,090 --> 01:15:55,900
but in the copy

1239
01:15:56,020 --> 01:15:59,630
then copied it again so he was iteratively copying this by piece of paper over

1240
01:15:59,630 --> 01:16:04,150
and over again and and noticed over time that these some random dot started to

1241
01:16:04,150 --> 01:16:07,020
form just dust other things that would accumulate grow

1242
01:16:07,020 --> 01:16:11,280
this is a very simple but we tell everything you have to be able to

1243
01:16:11,280 --> 01:16:12,820
look at all directions

1244
01:16:12,830 --> 01:16:14,950
since coordinate descent does not

1245
01:16:14,970 --> 01:16:18,080
it may not be convergent is global minimum

1246
01:16:18,090 --> 01:16:22,980
and funny story got on tree trying to optimize such a function at four am

1247
01:16:22,990 --> 01:16:27,450
before the of the line in the country does not converge

1248
01:16:27,510 --> 01:16:31,130
in august so

1249
01:16:31,150 --> 01:16:36,260
and in terms of confidence rated through the glass gradient

1250
01:16:36,340 --> 01:16:40,790
it's the same as gradient descent with no assumptions consider this

1251
01:16:40,880 --> 01:16:47,090
keep in mind that coordinate descent is not always convergent with those same two slide

1252
01:16:47,090 --> 01:16:52,360
is that positive so it is convergent OK so is not always converge on BT's

1253
01:16:52,360 --> 01:16:55,820
in some some settings and for the last week

1254
01:16:55,830 --> 01:16:58,090
so before i go on this is

1255
01:16:58,110 --> 01:17:03,980
good so the last point i think quite important for sparse methods is that so

1256
01:17:03,980 --> 01:17:08,690
true as the objective function for sparse problem is not differentiable

1257
01:17:08,700 --> 01:17:10,090
but it's not any

1258
01:17:10,130 --> 01:17:14,330
nondifferentiable functions so if you assume the differential

1259
01:17:14,330 --> 01:17:20,190
you get an objective function which is of the form LFW which is differentiable plus

1260
01:17:20,250 --> 01:17:23,310
lambda times norm which may not be differentiable

1261
01:17:23,410 --> 01:17:28,480
or if you the constrained version you get the minimizer you want to minimize

1262
01:17:28,760 --> 01:17:35,180
convex differentiable function with the constraint and if you assume that any smoothness and if

1263
01:17:35,180 --> 01:17:37,240
on top of each assume that you know

1264
01:17:37,380 --> 01:17:41,230
two projects on the ball the ball defined by the nom

1265
01:17:41,270 --> 01:17:45,100
all the body farm and one of its so if you don't know

1266
01:17:45,120 --> 01:17:49,440
but i do know is simply the one of the one norm is the infinity

1267
01:17:50,160 --> 01:17:54,390
OK and the he should be enough for the moment so if you know how

1268
01:17:54,390 --> 01:17:58,310
to project on the ball defined the non in these again for the l one

1269
01:17:58,310 --> 01:18:00,880
ball is again do for the infinity ward

1270
01:18:00,900 --> 01:18:06,820
then you may use similar techniques than smooth optimisation which i won't go into this

1271
01:18:06,920 --> 01:18:12,410
new statoil for because i don't have time to talk about algorithms and simple ones

1272
01:18:12,410 --> 01:18:14,880
are projected gradient descent

1273
01:18:14,930 --> 01:18:17,150
and so proximal methods

1274
01:18:17,170 --> 01:18:22,700
which i will talk about it just for reference if you want to look at

1275
01:18:22,780 --> 01:18:26,000
and the good thing is that with those types of methods to get the same

1276
01:18:26,260 --> 01:18:30,730
comment as weights than for smooth optimisation namely it will depend a lot on the

1277
01:18:30,730 --> 01:18:32,040
condition number

1278
01:18:32,110 --> 01:18:36,030
of the loss function k so if the loss function is well conditioned it will

1279
01:18:36,520 --> 01:18:41,850
converge quickly exponential rate and if it's not any coverage it will convert slowly

1280
01:18:41,900 --> 01:18:43,910
at the rate of one one or two

1281
01:18:43,930 --> 01:18:47,890
OK so this is very important in the field of communication

1282
01:18:47,900 --> 01:18:53,560
now let's look at the source and consider simple algorithms for that

1283
01:18:53,560 --> 01:18:57,110
the first one is coordinate descent so i told you i told you that you

1284
01:18:57,110 --> 01:18:58,380
should not use it

1285
01:18:58,430 --> 01:19:04,380
but in fact you can is sitting on the main reason is that the optimality

1286
01:19:04,380 --> 01:19:09,540
conditions of c problems so if you go back here

1287
01:19:09,550 --> 01:19:11,160
in this setting

1288
01:19:11,180 --> 01:19:16,720
you know that if you optimize only one viable you want that to be true

1289
01:19:16,760 --> 01:19:20,640
and you see that for all j is the condition for the available j is

1290
01:19:20,640 --> 01:19:24,110
independent from the condition from the other visibles and this is one of the reasons

1291
01:19:24,940 --> 01:19:28,090
coordinate descent is convergent

1292
01:19:28,430 --> 01:19:31,800
is convergent was

1293
01:19:31,800 --> 01:19:35,150
i was the only

1294
01:19:35,170 --> 01:19:36,550
OK so we have another

1295
01:19:36,590 --> 01:19:41,880
simple proof later businesses so the good thing is that if you optimize with respect

1296
01:19:41,880 --> 01:19:45,800
to one viable this is exactly iterative thresholding

1297
01:19:45,810 --> 01:19:49,360
because you can not get

1298
01:19:49,390 --> 01:19:51,830
two million case square loss

1299
01:19:51,830 --> 01:20:01,680
in fact so the you know what i mean you know if you have

1300
01:20:01,700 --> 01:20:04,040
i don't see your question

1301
01:20:05,760 --> 01:20:11,060
sure OK but if you not operational you're not convex anymore

1302
01:20:11,080 --> 01:20:14,690
and then i don't exactly

1303
01:20:14,750 --> 01:20:17,070
if it is not to believe it will not converge

1304
01:20:17,080 --> 01:20:21,140
it might be some in some settings in fact the same so is guilty indeed

1305
01:20:21,140 --> 01:20:25,870
on some simple UCI datasets but you take another set it doesn't converge so it's

1306
01:20:25,870 --> 01:20:27,780
sometimes it doesn't it doesn't

1307
01:20:27,800 --> 01:20:31,830
and you would want method which may not be convergence

1308
01:20:31,840 --> 01:20:35,840
this is very important because it may get stuck and you don't notice it which

1309
01:20:35,840 --> 01:20:39,330
is the problem

1310
01:20:39,370 --> 01:20:45,190
so so def arg one so good good thing about coordinate descent is that this

1311
01:20:45,190 --> 01:20:48,950
is a very simple to implement because you can optimize is one if you have

1312
01:20:48,950 --> 01:20:54,570
one viable you have to optimize acquired function of one variable plays an absolute value

1313
01:20:54,570 --> 01:20:58,590
this can be done in closed form and this is just iterative thresholding and ask

1314
01:20:58,590 --> 01:21:00,520
for example in

1315
01:21:00,560 --> 01:21:05,090
and if slides because very simple methods here you have to keep in mind that

1316
01:21:05,090 --> 01:21:07,840
this is a first order method so if

1317
01:21:07,900 --> 01:21:13,080
eg if your comments matrix is as local coalitions it we would be quite fast

1318
01:21:13,080 --> 01:21:17,260
but if you have a high correlation between between your valuables it means that you

1319
01:21:17,260 --> 01:21:19,900
have the load condition number

1320
01:21:19,980 --> 01:21:22,720
it means that it may get very small

1321
01:21:22,730 --> 01:21:26,240
a second type of technique like called that it that take

1322
01:21:26,280 --> 01:21:29,800
because this is how we refer to it in the group

1323
01:21:29,850 --> 01:21:35,170
and it is simply a method that will do it that iteratively squares to use

1324
01:21:35,200 --> 01:21:39,080
if you take the one on that and you can see again you can see

1325
01:21:39,080 --> 01:21:43,290
it as the minimum of any of the of the vector it up to it

1326
01:21:43,290 --> 01:21:44,750
thousand time of p

1327
01:21:44,820 --> 01:21:49,330
despite is so here if you have team at with respect to it down here

1328
01:21:49,360 --> 01:21:53,870
the optimal it is exactly the value is the square root of w j

1329
01:21:53,870 --> 01:22:04,040
i know

1330
01:23:17,670 --> 01:23:20,770
all right

1331
01:23:34,170 --> 01:23:35,520
i don't

1332
01:23:36,850 --> 01:23:40,800
our mission

1333
01:24:06,590 --> 01:24:08,640
now have

1334
01:24:51,270 --> 01:24:55,970
they are

1335
01:25:17,830 --> 01:25:21,910
during had

1336
01:26:04,440 --> 01:26:05,540
are o

1337
01:26:07,720 --> 01:26:09,940
now there

1338
01:26:09,940 --> 01:26:11,460
thank you very much

1339
01:26:11,480 --> 01:26:12,900
good morning everyone

1340
01:26:12,920 --> 01:26:14,890
first like to thank all the organizers

1341
01:26:14,900 --> 01:26:17,150
for over to the

1342
01:26:17,180 --> 01:26:18,990
here in this pursuit

1343
01:26:19,000 --> 01:26:21,320
the first kind

1344
01:26:22,240 --> 01:26:28,080
this is joint work we have right here on the left gobbledygook right from the

1345
01:26:29,940 --> 01:26:31,960
so far

1346
01:26:31,960 --> 01:26:38,710
and talking to the consistency of random forests and other averaging algorithms

1347
01:26:41,330 --> 01:26:43,240
thanks to

1348
01:26:44,460 --> 01:26:50,810
that right

1349
01:26:50,820 --> 01:26:52,920
OK so

1350
01:26:52,930 --> 01:26:57,280
actually many of the ideas and greenwich to explain to is

1351
01:26:59,060 --> 01:27:03,060
been inspired by the work of children so that

1352
01:27:03,170 --> 01:27:04,740
it was

1353
01:27:04,910 --> 01:27:08,310
so a three years ago one's for

1354
01:27:08,390 --> 01:27:11,020
what do you get your

1355
01:27:11,030 --> 01:27:15,170
brown triviality

1356
01:27:15,220 --> 01:27:18,580
as for his work

1357
01:27:18,600 --> 01:27:22,830
that's the biggest the classification regression trees

1358
01:27:23,000 --> 01:27:29,130
and actually the keywords of my talk today classification regression trees inspired by the work

1359
01:27:29,130 --> 01:27:30,390
of brain

1360
01:27:30,410 --> 01:27:32,440
so just before we start

1361
01:27:32,530 --> 01:27:38,800
anyway my advisor of classification and the different classification regression you're

1362
01:27:38,850 --> 01:27:41,820
so interesting observations during for

1363
01:27:43,500 --> 01:27:47,220
any you other which is white or one year

1364
01:27:47,300 --> 01:27:50,750
and what do they want to do of course is to pretty much

1365
01:27:50,790 --> 01:27:54,940
yes lot of the observation want to know what this is that

1366
01:27:54,970 --> 01:27:56,740
or reasons why

1367
01:27:56,780 --> 01:27:59,530
this is just strange type of regression

1368
01:27:59,550 --> 01:28:02,050
the difference is that we should why

1369
01:28:02,190 --> 01:28:04,210
just take some good

1370
01:28:06,880 --> 01:28:09,500
what i'm going to explain today

1371
01:28:09,500 --> 01:28:14,190
can be transposed from classification regression of ideas which

1372
01:28:14,210 --> 01:28:17,130
let me for classification of

1373
01:28:17,190 --> 01:28:20,060
all politicians in manchester

1374
01:28:21,910 --> 01:28:23,530
you can for the two

1375
01:28:29,040 --> 01:28:33,700
so at the end of his life to run promoted from the first four years

1376
01:28:33,700 --> 01:28:35,630
in this occasion and regression

1377
01:28:35,970 --> 01:28:37,820
the idea behind forces

1378
01:28:37,840 --> 01:28:41,630
simple idea is to use averaging as the sort of thing

1379
01:28:41,630 --> 01:28:48,750
that is instead of using one the classification just many independent small ones

1380
01:28:48,750 --> 01:28:50,060
which are

1381
01:28:50,070 --> 01:28:52,410
which is very simple to my

1382
01:28:52,410 --> 01:28:57,280
and just because of the first try to obtain some which is from one

1383
01:28:57,290 --> 01:29:03,160
and brown left some very interesting question unanswered regarding the consistency in the way they

1384
01:29:04,220 --> 01:29:09,200
and in this world which is kind of a different from the top

1385
01:29:09,260 --> 01:29:12,100
we're going to try to the first answers

1386
01:29:12,160 --> 01:29:18,070
on the question why do run for preventing everything just

1387
01:29:18,190 --> 01:29:21,000
start the first stones on

1388
01:29:22,650 --> 01:29:26,500
actually random forests are part of a larger project

1389
01:29:26,510 --> 01:29:29,910
national community just go and some of the project

1390
01:29:29,910 --> 01:29:35,480
and some of its ontology learning algorithm which was set of many individual classifiers

1391
01:29:37,690 --> 01:29:42,280
and this is the largest combined due to justify that

1392
01:29:42,320 --> 01:29:47,880
it is now well known which means that some methods are often much more accurate

1393
01:29:47,880 --> 01:29:53,260
the first for example to

1394
01:29:53,280 --> 01:29:57,870
was to be the reason for that

1395
01:29:57,880 --> 01:29:59,440
contraction forwards

1396
01:29:59,470 --> 01:30:02,000
application of

1397
01:30:02,000 --> 01:30:07,070
i guess you would have us some other

1398
01:30:07,100 --> 01:30:08,910
and of course

1399
01:30:09,000 --> 01:30:13,250
run for which we to pick up to

1400
01:30:13,280 --> 01:30:19,970
so what is known about the first especially now that for the first mentioned excellent

1401
01:30:19,970 --> 01:30:22,720
performance on a number of practical problems

1402
01:30:22,730 --> 01:30:25,370
in fact according to remember

1403
01:30:25,470 --> 01:30:30,790
the most accurate general i propose classifiers the market

1404
01:30:31,190 --> 01:30:33,190
it's quite difficult

1405
01:30:33,230 --> 01:30:37,830
but first just because different forest just for how

1406
01:30:37,880 --> 01:30:42,460
a round is introduced in the tree building process so around first is just a

1407
01:30:42,460 --> 01:30:46,090
collection of three which are independent and which

1408
01:30:46,140 --> 01:30:48,820
the way introduced around the tree

1409
01:30:48,870 --> 01:30:50,760
which for one reason two

1410
01:30:51,630 --> 01:30:54,510
what is sure that we can behind

1411
01:30:54,520 --> 01:30:55,990
the first is

1412
01:30:56,000 --> 01:30:58,770
difficult to analyse and remains largely unknown

1413
01:30:58,820 --> 01:31:00,010
so they are

1414
01:31:00,060 --> 01:31:01,650
some papers about

1415
01:31:01,700 --> 01:31:04,330
first i could

1416
01:31:04,400 --> 01:31:08,770
everything that just keep differences of the reference around themselves

1417
01:31:08,820 --> 01:31:13,920
two thousand four and interesting referenced by you could see

1418
01:31:13,960 --> 01:31:17,990
who have established very interesting connection between the first at

1419
01:31:18,750 --> 01:31:22,630
and i would come back to this paper is nature and of course if you're

1420
01:31:22,640 --> 01:31:24,310
interested in the software

1421
01:31:24,360 --> 01:31:27,610
you can download it consists of because OK

1422
01:31:27,630 --> 01:31:30,040
you can download the software on the web page of

1423
01:31:30,050 --> 01:31:31,430
all right

1424
01:31:33,500 --> 01:31:39,630
before we start theory that some example of what wrong for very to example which

1425
01:31:39,630 --> 01:31:40,550
is here

1426
01:31:40,560 --> 01:31:41,960
version two

1427
01:31:41,980 --> 01:31:44,820
classification of observation one

1428
01:31:44,830 --> 01:31:46,590
one the decision

1429
01:31:48,080 --> 01:31:51,000
what can we do to around tree

1430
01:31:51,620 --> 01:31:53,380
we just

1431
01:31:53,460 --> 01:31:56,890
the second we choose the cornet

1432
01:31:56,900 --> 01:32:02,020
once you have chosen according to go from a completely different form

1433
01:32:02,070 --> 01:32:03,010
so we get

1434
01:32:03,130 --> 01:32:04,710
you get yourself

1435
01:32:05,060 --> 01:32:06,640
which is also true

1436
01:32:06,690 --> 01:32:14,270
i once you have to decide according to the one chosen according to the UK

1437
01:32:15,460 --> 01:32:16,860
can think

1438
01:32:16,870 --> 01:32:19,340
three which is just partition of square

1439
01:32:19,390 --> 01:32:20,270
and this

1440
01:32:20,290 --> 01:32:24,060
there are some pretty easy for number of cells it's all a

1441
01:32:24,120 --> 01:32:25,830
in the one

1442
01:32:25,880 --> 01:32:29,880
are interested for ones come over

1443
01:32:29,900 --> 01:32:34,320
just take rich this we're going to test

1444
01:32:35,340 --> 01:32:36,640
decide one

1445
01:32:36,700 --> 01:32:38,650
so this is the result of one

1446
01:32:38,670 --> 01:32:39,880
random tree

1447
01:32:39,880 --> 01:32:41,940
this one

1448
01:32:41,990 --> 01:32:44,940
this one stands for example so

1449
01:32:44,950 --> 01:32:47,320
this could be the result of another random trees

1450
01:32:47,380 --> 01:32:49,200
exactly the same

1451
01:32:49,250 --> 01:32:50,760
because you take about

1452
01:32:50,790 --> 01:32:53,020
what decisions will take

1453
01:32:56,320 --> 01:33:00,420
you take one sequence of two one two one one two one we get our

1454
01:33:00,440 --> 01:33:01,890
results make

1455
01:33:01,900 --> 01:33:07,020
decision on this is an example of random forest which is completely of course

1456
01:33:07,140 --> 01:33:12,840
right now this is called bird for summer of tree ring tones of the

1457
01:33:12,840 --> 01:33:16,040
this is some of these researchers there was a lot of the best way you

1458
01:33:16,040 --> 01:33:17,880
can read

1459
01:33:17,900 --> 01:33:23,230
well this is the main

1460
01:33:23,310 --> 01:33:25,320
further education you know better

1461
01:33:25,400 --> 01:33:28,150
the really really good i study

1462
01:33:28,230 --> 01:33:32,150
it's obviously reading research quarterly which is one of the top meeting journal

1463
01:33:32,150 --> 01:33:34,000
we want to find out

1464
01:33:35,860 --> 01:33:37,060
which of four

1465
01:33:37,070 --> 01:33:38,590
techniques was best

1466
01:33:38,610 --> 01:33:42,020
so to the best evidence synthesis

1467
01:33:43,340 --> 01:33:46,170
and essentially

1468
01:33:46,210 --> 01:33:49,670
just on the title of the problem with the best evidence because

1469
01:33:49,690 --> 01:33:51,940
by their own admission

1470
01:33:51,960 --> 01:33:56,360
OK they left out called the qualitative studies and services

1471
01:33:56,520 --> 01:34:00,110
i'm not sure how you could be confident about saying you found the best evidence

1472
01:34:00,130 --> 01:34:02,420
just based on quality

1473
01:34:02,460 --> 01:34:05,000
that was in a town called the best because there are some out there who

1474
01:34:05,000 --> 01:34:08,000
believe that on experimental research is the

1475
01:34:08,040 --> 01:34:11,500
you know it's there a park is really really big

1476
01:34:11,520 --> 01:34:13,000
so only only really

1477
01:34:13,000 --> 01:34:14,730
very type of research

1478
01:34:14,750 --> 01:34:17,230
and so we just have to look at all the experiments data and we can

1479
01:34:17,230 --> 01:34:21,840
find the best evidence and i would disagree that strongly not-so-great talk to decide that

1480
01:34:21,840 --> 01:34:24,000
there really powerful

1481
01:34:24,270 --> 01:34:27,210
spam which is one of them

1482
01:34:27,230 --> 01:34:30,040
the thing about

1483
01:34:31,330 --> 01:34:32,630
seven research

1484
01:34:32,650 --> 01:34:35,560
is that they tend to focus on two types nificant

1485
01:34:36,840 --> 01:34:38,210
and practical

1486
01:34:38,230 --> 01:34:41,880
statistical being p values about say this was this significant

1487
01:34:41,920 --> 01:34:45,310
particle beam things like effect sizes

1488
01:34:45,340 --> 01:34:46,230
but this

1489
01:34:46,440 --> 01:34:47,880
two other types

1490
01:34:48,900 --> 01:34:51,730
i don't get much focus

1491
01:34:51,730 --> 01:34:53,270
one type is

1492
01:34:53,310 --> 01:34:55,540
clinical significance

1493
01:34:55,590 --> 01:35:01,550
o times economics significance

1494
01:35:01,570 --> 01:35:03,300
OK significance is

1495
01:35:03,390 --> 01:35:05,270
those often oftentimes the quality

1496
01:35:05,440 --> 01:35:07,420
experience quality of life

1497
01:35:07,430 --> 01:35:09,260
so for example in the medical field

1498
01:35:09,350 --> 01:35:12,960
you may have intervention you have the drug medication

1499
01:35:12,970 --> 01:35:16,470
it doesn't cure or does it doesn't improve them in terms of

1500
01:35:16,550 --> 01:35:18,110
attacking the actual

1501
01:35:18,130 --> 01:35:19,960
you know attacking the actual

1502
01:35:20,000 --> 01:35:23,060
cause of the whatever when they have

1503
01:35:23,140 --> 01:35:25,930
we improve the quality of life in some way

1504
01:35:27,810 --> 01:35:31,180
if that's the case that might be really important

1505
01:35:32,040 --> 01:35:35,300
you know years yes is gonna die from this disease but the quality of life

1506
01:35:35,300 --> 01:35:38,050
while you're die is better than if you had taken

1507
01:35:38,070 --> 01:35:39,220
i'm educational

1508
01:35:39,230 --> 01:35:40,270
well intervention

1509
01:35:40,290 --> 01:35:43,260
so that's not the idea behind clinical significance

1510
01:35:43,720 --> 01:35:46,540
you have economic significance

1511
01:35:46,550 --> 01:35:48,090
OK something that

1512
01:35:48,420 --> 01:35:52,710
it's huge surely huge political science because of politics is

1513
01:35:52,720 --> 01:35:53,680
do we money

1514
01:35:53,680 --> 01:35:55,690
in closed session time

1515
01:35:55,850 --> 01:35:57,790
that's really important so

1516
01:35:57,860 --> 01:35:59,720
a lot of studies i see

1517
01:35:59,800 --> 01:36:03,180
we don't incorporate often local political economic significance

1518
01:36:03,210 --> 01:36:05,470
into our interpretations

1519
01:36:05,640 --> 01:36:08,510
and economics is part of my

1520
01:36:09,710 --> 01:36:15,440
background so maybe what kind of media latched onto that so at cost effectiveness

1521
01:36:15,500 --> 01:36:19,880
cost benefit cost utility cost you feasibility and cost sensitivity

1522
01:36:19,890 --> 01:36:21,760
one of those combinations

1523
01:36:21,800 --> 01:36:23,590
might be really useful

1524
01:36:23,600 --> 01:36:24,850
first four

1525
01:36:24,860 --> 01:36:27,470
when you come to talk at the policy level

1526
01:36:27,470 --> 01:36:30,210
because politicians have cell bodies like anybody else

1527
01:36:30,220 --> 01:36:33,730
and they have to make decisions and after tries to make something work

1528
01:36:33,770 --> 01:36:35,520
have to give something else

1529
01:36:35,570 --> 01:36:38,960
so if you want to convince them is important to spend money on

1530
01:36:38,960 --> 01:36:41,300
you know to address global warming

1531
01:36:41,360 --> 01:36:42,850
therefore becoming thinking

1532
01:36:42,920 --> 01:36:46,130
o that means i have to give something else i i want to give

1533
01:36:46,140 --> 01:36:48,690
so in this part of our

1534
01:36:49,100 --> 01:36:54,330
discussion in articles argues that we make we have to find ways to short to

1535
01:36:56,350 --> 01:36:57,500
you know

1536
01:36:57,510 --> 01:37:00,350
by by using this technique

1537
01:37:00,350 --> 01:37:02,050
we can improve

1538
01:37:02,830 --> 01:37:05,010
so educated for example

1539
01:37:05,130 --> 01:37:09,210
well often say that this intervention works because it increases

1540
01:37:09,220 --> 01:37:12,050
and scores by harvest and aviation

1541
01:37:12,050 --> 01:37:15,270
OK that's useful to

1542
01:37:15,310 --> 01:37:17,810
but for politicians what does that mean mean

1543
01:37:18,850 --> 01:37:21,050
so what we can do is better than that we can say OK we could

1544
01:37:21,050 --> 01:37:23,040
find out

1545
01:37:23,430 --> 01:37:25,510
what your probability is

1546
01:37:25,550 --> 01:37:28,250
giving you have some education

1547
01:37:28,250 --> 01:37:31,810
avoiding going to jail was probably don't have that of going to jail

1548
01:37:31,840 --> 01:37:34,350
you can look at how many cos decided to go for a person to go

1549
01:37:34,350 --> 01:37:35,350
to jail

1550
01:37:35,350 --> 01:37:39,020
so ten seven cells are made for every ten to seven cells are die

1551
01:37:39,040 --> 01:37:43,440
if you have an excess of new cells compared with dying cells you're in bad

1552
01:37:44,390 --> 01:37:49,710
because that's really the disease cancer and that implies even without knowing any without knowing

1553
01:37:49,710 --> 01:37:51,420
anything else about cancer

1554
01:37:51,500 --> 01:37:56,350
that they have to be very careful controls over the decision on the part of

1555
01:37:56,350 --> 01:37:57,560
the cell

1556
01:37:57,580 --> 01:38:00,940
to whether as to whether can grow and divide it was not it should grow

1557
01:38:00,940 --> 01:38:02,190
and divide

1558
01:38:02,210 --> 01:38:06,210
now what does that mean well part of the decisions

1559
01:38:06,230 --> 01:38:09,250
come from the following situation let's imagine here growing

1560
01:38:09,270 --> 01:38:12,710
we're talking about human tissue each one of these things is cells

1561
01:38:12,810 --> 01:38:16,020
and as usual my art is not that splendid

1562
01:38:16,020 --> 01:38:18,620
but one of the things i want to impress on you

1563
01:38:18,640 --> 01:38:21,460
is that the body cannot give license

1564
01:38:21,480 --> 01:38:23,210
two this l

1565
01:38:23,230 --> 01:38:25,620
order this sell order the cell

1566
01:38:25,640 --> 01:38:27,580
to make the decision

1567
01:38:27,600 --> 01:38:30,440
about growing and dividing on its own

1568
01:38:30,540 --> 01:38:32,100
i still cannot go going say

1569
01:38:32,120 --> 01:38:36,060
i think i feel like dividing i'm going to go do it that's no no

1570
01:38:36,060 --> 01:38:37,440
wiser no no

1571
01:38:37,460 --> 01:38:42,560
because the moment one grants autonomy to such as cell ones in a very dangerous

1572
01:38:43,920 --> 01:38:48,140
instead what has to happen what must happen is that cells talk to one another

1573
01:38:48,600 --> 01:38:51,190
and cells reach a consensus

1574
01:38:51,230 --> 01:38:54,390
for instance maybe there's so that's missing right over here

1575
01:38:54,440 --> 01:38:57,350
in this tissue

1576
01:38:57,370 --> 01:39:02,920
and keep in mind architecturally complex issue has a precise number of cells in an

1577
01:39:02,920 --> 01:39:05,420
organised in a very precise pattern

1578
01:39:05,440 --> 01:39:06,890
and the way that

1579
01:39:06,940 --> 01:39:11,730
is maintained is that the cells are constantly looking around and seeing is their gap

1580
01:39:11,730 --> 01:39:14,690
year do we need to make and sell or not

1581
01:39:14,690 --> 01:39:18,210
it's not as if this cell says well i'm happy here and on my neighbours

1582
01:39:18,210 --> 01:39:21,870
are happy but still on the ground by because it seems like a nice thing

1583
01:39:21,870 --> 01:39:26,560
to do it again that's an invitation to disaster

1584
01:39:26,560 --> 01:39:30,650
and i'm saying that if only to impress upon you the fact that cells only

1585
01:39:30,650 --> 01:39:34,370
will make the decision to grow and divide and going to the cell cycle on

1586
01:39:34,370 --> 01:39:39,850
the basis of consultations with their neighbours they're constantly talking to one another

1587
01:39:40,600 --> 01:39:48,710
these signals the intracellular signals within living tissue represent constant ceaseless chatter

1588
01:39:48,750 --> 01:39:52,670
it's like everybody's on the cell phone talking with six or eight or ten

1589
01:39:52,710 --> 01:39:54,910
his are neighbours all the time

1590
01:39:55,000 --> 01:39:57,940
and if you can hear it it would be a real

1591
01:39:59,190 --> 01:40:04,640
so that raises the question cells with one another what messages to the exchange one

1592
01:40:04,910 --> 01:40:09,480
with the other so that they can establish some kind of consensus as to whether

1593
01:40:09,480 --> 01:40:14,370
or not it's inappropriate for one of their number to grow and divide

1594
01:40:14,370 --> 01:40:17,140
all right this is an unauthorized and you can

1595
01:40:17,190 --> 01:40:19,460
interpret the way you want anyhow so

1596
01:40:19,460 --> 01:40:22,390
how do they do that well what they do one of the most important ways

1597
01:40:22,390 --> 01:40:26,730
they communicate is the exchange proteins one another called

1598
01:40:26,750 --> 01:40:28,270
growth factors

1599
01:40:28,270 --> 01:40:31,370
and the growth factor goes from one cell

1600
01:40:31,890 --> 01:40:34,810
here's the cell releases the growth factor goes

1601
01:40:34,830 --> 01:40:40,740
two seconds so induces the second cell to begin to divide that's a mechanism of

1602
01:40:40,790 --> 01:40:43,540
transferring the signal or conveying the signal

1603
01:40:43,560 --> 01:40:50,750
and growth factor itself is the relatively low molecular weight proteins which travels through intercellular

1604
01:40:50,750 --> 01:40:54,120
inter stellar inter cellular spaces

1605
01:40:54,170 --> 01:40:56,600
it moves from one cell to the other

1606
01:40:56,620 --> 01:40:59,120
so it secreted by oneself

1607
01:40:59,170 --> 01:41:03,420
it moves here and then it goes over here the second self in fact if

1608
01:41:03,420 --> 01:41:07,500
we want to blow up the second self the second cell has on its surface

1609
01:41:07,500 --> 01:41:12,170
a specific protein which is known as a receptor or

1610
01:41:12,170 --> 01:41:15,370
or in this case we can see growth factor receptor

1611
01:41:15,540 --> 01:41:17,960
which represents a means

1612
01:41:18,020 --> 01:41:19,060
by which

1613
01:41:19,080 --> 01:41:24,560
that's still senses the presence of a growth factor in the extracellular space

1614
01:41:25,620 --> 01:41:30,190
as is suggested in this very crude and poorly drawn

1615
01:41:32,580 --> 01:41:36,540
this receptor is a transmembrane protein

1616
01:41:36,560 --> 01:41:42,290
it has an extracellular domain is an intracellular domain and it moves through

1617
01:41:42,310 --> 01:41:48,560
it protrudes through the lipid bilayer of the plasma membrane we talk very briefly about

1618
01:41:48,560 --> 01:41:50,770
these at the beginning of the semester

1619
01:41:50,830 --> 01:41:55,710
and what happens is these receptors are so configured that in the event the growth

1620
01:41:55,710 --> 01:41:59,690
so i would say it's legitimate criticism to say for this model the only reason

1621
01:41:59,690 --> 01:42:00,750
why this model

1622
01:42:00,790 --> 01:42:05,190
i found a taxonomy of clustering is because

1623
01:42:05,190 --> 01:42:08,790
of the features that are there there's more of them that supported taxonomic organisation and

1624
01:42:09,560 --> 01:42:12,980
right whereas here the whole point is to say give it as many features as

1625
01:42:12,980 --> 01:42:17,690
you want and it will discover all of the clusterings that are supported in that

1626
01:42:17,690 --> 01:42:20,420
feature set so i'm not going to talk about here but we apply this to

1627
01:42:20,420 --> 01:42:23,080
other datasets that are bigger like for example

1628
01:42:23,130 --> 01:42:27,330
data sets of how people vote in the US senate where the features might be

1629
01:42:27,330 --> 01:42:30,060
all the bills that are considered over a period of two or four years so

1630
01:42:30,060 --> 01:42:30,980
there might be like

1631
01:42:31,040 --> 01:42:35,560
you know hundreds or even thousands of features and the model might get ten

1632
01:42:35,600 --> 01:42:38,770
in the case of the senate voting picks out about ten different ways of clustering

1633
01:42:38,770 --> 01:42:44,380
senators of which eight we found politically meaningful and there's two others that are very

1634
01:42:44,380 --> 01:42:48,150
small and not very meaningful dislike here some of the features i mean i i

1635
01:42:48,150 --> 01:42:49,440
didn't talk about this

1636
01:42:49,610 --> 01:42:54,110
partition but basically it finds a lot of the features are in some sense just

1637
01:42:54,110 --> 01:42:58,270
sparse noise these are features which might be meaningful in some context but as far

1638
01:42:58,270 --> 01:43:02,290
as this data set is concerned they don't coherently recovery very with anything else so

1639
01:43:02,290 --> 01:43:07,040
it just says i'm going put all the animals together and and have have basically

1640
01:43:07,040 --> 01:43:10,100
it's you is going to use just

1641
01:43:10,110 --> 01:43:14,560
a single coin weight for each of these columns saying other probably mostly not present

1642
01:43:14,980 --> 01:43:19,080
right and ages vary in the degree of sparsity but there isn't any interesting structure

1643
01:43:19,080 --> 01:43:23,290
except for frogs which get carbon on their own and that's because basically frogs have

1644
01:43:23,610 --> 01:43:27,920
an unusually high number in this data set of idiosyncratic features so i would say

1645
01:43:27,920 --> 01:43:30,830
it's kind of an accident of this dataset that frogs

1646
01:43:30,860 --> 01:43:33,900
it's partly an active status frogs get placed on their own

1647
01:43:33,960 --> 01:43:40,170
although not complete accident because frogs are a little bit weird maybe compare two weeks

1648
01:43:40,460 --> 01:43:43,750
but for the most part you know what this is saying is as far as

1649
01:43:43,750 --> 01:43:47,920
this data set is concerned this is all this mostly junk whereas this is not

1650
01:43:47,920 --> 01:43:52,480
junk although it was treated by as junk under a conventional mixture model

1651
01:43:52,520 --> 01:43:53,480
that makes sense

1652
01:43:54,190 --> 01:43:57,210
i mean i think the the bottom line is a the kind of concerns arising

1653
01:43:57,210 --> 01:44:00,830
are important ones there but there the sort of things which affect any kind of

1654
01:44:00,830 --> 01:44:05,420
approach to unsupervised learning and exactly why we want models like this are ways of

1655
01:44:05,420 --> 01:44:08,630
actually trying to to to go beyond those kinds of concerns

1656
01:44:08,650 --> 01:44:10,810
by by recognizing that the that the

1657
01:44:10,830 --> 01:44:12,710
not just the

1658
01:44:12,730 --> 01:44:15,830
you know the whole motivation for nonparametric bayes in general is that the amount of

1659
01:44:15,830 --> 01:44:21,210
structure in the world is effectively infinite and certainly any finite the right finite approximation

1660
01:44:21,210 --> 01:44:24,730
is unknown and this is taking that to you know it to another level or

1661
01:44:24,730 --> 01:44:28,190
another dimensions and even the number of kinds of structures might not be something we

1662
01:44:28,190 --> 01:44:31,060
can commit to advance if we're going to be willing to give our system all

1663
01:44:31,060 --> 01:44:33,310
the features we can measure

1664
01:44:34,750 --> 01:44:41,670
yes so not in this and not in this model

1665
01:44:41,690 --> 01:44:43,880
but that would be an interesting extension

1666
01:44:43,900 --> 01:44:48,310
we thought it was challenging enough if you count the combinatorics of this model like

1667
01:44:48,310 --> 01:44:51,630
the height the effective hypothesis space is

1668
01:44:51,650 --> 01:44:55,080
you already for anyone nonparametric mixture

1669
01:44:55,100 --> 01:44:59,000
the effective hypothesis space of you know is all possible partitions of the number of

1670
01:44:59,000 --> 01:45:04,150
objects bell numbers or something and here we've got all possible partitions of the number

1671
01:45:04,150 --> 01:45:07,880
of objects and for each of those partitions it's one of every one of those

1672
01:45:07,880 --> 01:45:11,940
but i mean it's huge about space and

1673
01:45:11,960 --> 01:45:18,290
we were a little bit shocked we got anything meaningful and actually the kotchman singer

1674
01:45:18,380 --> 01:45:21,340
for those of you who know him might take a little bit differently but he

1675
01:45:21,790 --> 01:45:25,880
he he didn't really believe these results although he obtained himself he has very high

1676
01:45:25,880 --> 01:45:29,080
standards for for appropriate

1677
01:45:29,130 --> 01:45:30,440
you know

1678
01:45:30,460 --> 01:45:34,380
mixing and MCMC and all those sorts of things and so he spent several years

1679
01:45:34,380 --> 01:45:40,110
and and has started a company and has acquired over a million dollars in venture

1680
01:45:40,110 --> 01:45:43,770
capital funding and mostly what they seem to be doing with that is trying to

1681
01:45:43,770 --> 01:45:45,380
come up with

1682
01:45:45,400 --> 01:45:49,860
with inference algorithms for which he actually trust in this huge hypothesis space it's actually

1683
01:45:49,860 --> 01:45:53,330
mixing in genuinely sampling from the posterior for the model

1684
01:45:53,360 --> 01:45:56,710
they have this they will suppose we have a product soon you can draw samples

1685
01:45:56,710 --> 01:45:59,460
over the web for this and pay them by the

1686
01:45:59,520 --> 01:46:00,500
by the bit

1687
01:46:00,650 --> 01:46:07,130
that that's what you want or if you're academic you can use it for free

1688
01:46:07,130 --> 01:46:11,610
right so so i'll show also something later which is kind of more local clustering

1689
01:46:11,610 --> 01:46:14,770
but the main difference between this just quickly for those of you who know CO

1690
01:46:14,770 --> 01:46:18,150
clustering is is that here

1691
01:46:18,150 --> 01:46:22,420
and co clustering you cluster the rows and the columns kind independently right so co

1692
01:46:22,460 --> 01:46:24,920
clustering shops of the matrix like

1693
01:46:24,960 --> 01:46:26,580
this way and then like that way

1694
01:46:26,630 --> 01:46:32,170
right whereas here were clustering rows inside columns and you can you could imitate this

1695
01:46:32,170 --> 01:46:35,790
with the co clustering model but the co clustering model will be over parameterized because

1696
01:46:35,790 --> 01:46:38,400
you have to basically have all of these different

1697
01:46:38,440 --> 01:46:42,310
shops and actually i take that back you can even really do that because these

1698
01:46:42,310 --> 01:46:45,410
cuts here like it looks a little bit like because of the way of drawing

1699
01:46:46,000 --> 01:46:49,440
it looks like you could just extend this line over here but in fact these

1700
01:46:49,440 --> 01:46:52,980
are ordered in a different way so these cuts orthogonal to those cuts mean basically

1701
01:46:52,980 --> 01:46:56,560
you could you could kind of capture the structure with the co clustering model but

1702
01:46:56,560 --> 01:46:59,630
it would be it would it would it would look like a complete mess so

1703
01:46:59,630 --> 01:47:02,130
basically this model has to do it similar to

1704
01:47:02,170 --> 01:47:05,980
co clustering clustering both rows and columns but it has a different and much more

1705
01:47:05,980 --> 01:47:10,100
structured inductive bias that comes from first clustering the features

1706
01:47:10,150 --> 01:47:14,830
which represent since different different aspects of the data and then for each of those

1707
01:47:14,830 --> 01:47:17,630
it's trying to say OK i'm going to find a simple

1708
01:47:17,630 --> 01:47:22,110
the single simple view of the aspect which is just a single list a single

1709
01:47:22,110 --> 01:47:26,460
way of clustering to just explain those features

1710
01:47:26,480 --> 01:47:30,000
and the question about

1711
01:47:30,020 --> 01:47:35,500
OK this just says we have lots of evidence that people

1712
01:47:36,610 --> 01:47:40,940
i can do this sort of thing and i will tell you about the experiments

1713
01:47:41,020 --> 01:47:44,830
that would that would take forever but basically what we do is we give people

1714
01:47:44,830 --> 01:47:47,460
this sort of task where we have them sort of a whole bunch of objects

1715
01:47:47,460 --> 01:47:50,630
like we had sort exactly these things and then we look at how the different

1716
01:47:50,900 --> 01:47:53,750
sorts they come up with

1717
01:47:53,750 --> 01:47:56,790
it just depends upon the difference in the pixels

1718
01:47:56,820 --> 01:47:58,630
and what you get

1719
01:47:58,650 --> 01:48:00,980
this is g

1720
01:48:01,020 --> 01:48:02,900
x i

1721
01:48:02,920 --> 01:48:05,250
minus xj

1722
01:48:05,250 --> 01:48:07,840
listen equal to

1723
01:48:09,770 --> 01:48:12,320
exciting ones alpha

1724
01:48:14,790 --> 01:48:17,400
alpha minus xj

1725
01:48:17,440 --> 01:48:19,130
absolute value

1726
01:48:20,690 --> 01:48:23,110
it's x i

1727
01:48:23,150 --> 01:48:24,540
one of

1728
01:48:26,770 --> 01:48:30,250
xj myself turns out

1729
01:48:30,310 --> 01:48:34,920
without going into this race that this is exactly the opposite condition from what the

1730
01:48:35,230 --> 01:48:36,270
chicago our

1731
01:48:36,400 --> 01:48:38,750
if the function is convex

1732
01:48:38,810 --> 01:48:40,460
concave rather

1733
01:48:40,500 --> 01:48:41,810
like this

1734
01:48:41,820 --> 01:48:44,020
this condition holds

1735
01:48:44,040 --> 01:48:46,880
right so if your function g

1736
01:48:46,880 --> 01:48:49,230
is concave

1737
01:48:50,630 --> 01:48:52,230
you can use

1738
01:48:59,020 --> 01:49:00,860
concave in positive region

1739
01:49:00,920 --> 01:49:05,440
then you can use what what cos alpha expansion of if

1740
01:49:05,440 --> 01:49:06,770
it has the opposite

1741
01:49:06,770 --> 01:49:07,820
o thing

1742
01:49:07,840 --> 01:49:11,210
you can use your housing get an exact solution in this case you can't get

1743
01:49:11,230 --> 01:49:12,550
the exact solution

1744
01:49:12,610 --> 01:49:14,840
this one can

1745
01:49:14,960 --> 01:49:20,570
this one preferable summons this one nice of positive linear

1746
01:49:20,630 --> 01:49:23,840
you can use either you can use either one

1747
01:49:23,900 --> 01:49:27,070
and why would you use one rather than the other one would use

1748
01:49:27,110 --> 01:49:32,070
for expansion rather in chicago with the chicago gives you the optimal solutions of expansion

1749
01:49:33,040 --> 01:49:34,670
well the answer is

1750
01:49:34,730 --> 01:49:38,880
because of expansion is much more

1751
01:49:38,940 --> 01:49:41,400
economical in terms cost

1752
01:49:43,750 --> 01:49:45,500
lots and lots of pages

1753
01:49:46,290 --> 01:49:48,270
she chicago

1754
01:49:48,290 --> 01:49:54,310
OK that's all i can say about after expansion take these things more

1755
01:49:54,360 --> 01:49:55,980
multiple labels

1756
01:49:56,130 --> 01:49:58,730
also something about

1757
01:49:58,820 --> 01:50:03,150
about higher order cliques in the remaining twenty five minutes

1758
01:50:03,190 --> 01:50:05,050
that i have

1759
01:50:05,110 --> 01:50:07,130
and that is

1760
01:50:07,360 --> 01:50:09,210
one of the

1761
01:50:09,210 --> 01:50:11,920
perhaps problems with

1762
01:50:11,920 --> 01:50:16,980
using a clique size two is you are looking very local information about the image

1763
01:50:16,980 --> 01:50:20,020
to see how the constraint you if you look at

1764
01:50:20,070 --> 01:50:21,290
more complex

1765
01:50:21,290 --> 01:50:22,540
cost functions

1766
01:50:22,570 --> 01:50:24,960
being presumably get better results

1767
01:50:25,040 --> 01:50:27,500
now one of the things

1768
01:50:27,540 --> 01:50:35,500
about this is

1769
01:50:42,460 --> 01:50:45,570
it really depends on what the cost function is made you can't say that i

1770
01:50:46,090 --> 01:50:47,810
for instance

1771
01:50:47,820 --> 01:50:53,440
with with size to right size to cliques depending on the cost function

1772
01:50:53,480 --> 01:50:58,040
you choose your emphasizing the smoothness or abrupt change in

1773
01:50:58,190 --> 01:51:00,710
whether you

1774
01:51:00,750 --> 01:51:03,190
it depends on the particular cost function

1775
01:51:03,590 --> 01:51:05,460
if you can make your

1776
01:51:05,520 --> 01:51:06,770
cost function

1777
01:51:08,290 --> 01:51:09,770
cause large

1778
01:51:09,820 --> 01:51:11,920
changes or not was

1779
01:51:13,480 --> 01:51:18,000
sometimes people will sometimes wear

1780
01:51:18,690 --> 01:51:20,840
i'm going to

1781
01:51:20,900 --> 01:51:24,000
rather than try to go back to slide with this is which is likely to

1782
01:51:24,000 --> 01:51:26,840
have a good look cubic

1783
01:51:26,840 --> 01:51:30,340
we can turn

1784
01:51:30,420 --> 01:51:33,710
not just here on the board this slide in the

1785
01:51:33,750 --> 01:51:37,320
in my slides much more mathematical ones

1786
01:51:37,360 --> 01:51:41,050
which tells you how this works

1787
01:51:44,360 --> 01:51:46,610
so supposing you have

1788
01:51:46,670 --> 01:51:48,710
look look at just

1789
01:51:48,710 --> 01:51:52,020
we could just you know who was binary problems

1790
01:51:52,070 --> 01:51:54,400
again we looked at the

1791
01:51:54,460 --> 01:51:57,000
case where the cost linear plus

1792
01:51:57,040 --> 01:51:58,900
i j

1793
01:51:58,920 --> 01:52:01,270
x i x j

1794
01:52:01,310 --> 01:52:03,190
what if we have

1795
01:52:03,210 --> 01:52:04,690
that's why

1796
01:52:04,820 --> 01:52:06,770
i j k

1797
01:52:06,770 --> 01:52:09,150
x i x j

1798
01:52:11,820 --> 01:52:15,750
so i got QB once you get to court you can't even tell what function

1799
01:52:15,820 --> 01:52:18,150
submodular but in this case we can

1800
01:52:18,190 --> 01:52:22,400
we can work out with the function is submodular in this particular case and hence

1801
01:52:22,400 --> 01:52:24,130
would be solved

1802
01:52:26,360 --> 01:52:29,190
you can you can see

1803
01:52:29,610 --> 01:52:35,150
i want to

1804
01:52:35,250 --> 01:52:36,960
make the following observations

1805
01:52:36,980 --> 01:52:39,460
the function can always be written

1806
01:52:39,460 --> 01:52:43,630
in the following form with this is either

1807
01:52:43,710 --> 01:52:46,340
OK i j k

1808
01:52:46,340 --> 01:52:47,770
x i

1809
01:52:47,790 --> 01:52:49,900
xj by

1810
01:52:49,940 --> 01:52:54,170
xk by or x i x j or xk in which

1811
01:52:54,230 --> 01:52:56,590
OK i j k

1812
01:52:56,590 --> 01:52:59,340
this list are equal to zero

1813
01:52:59,340 --> 01:53:01,270
any function

1814
01:53:01,290 --> 01:53:03,340
right which is cubic

1815
01:53:03,340 --> 01:53:05,840
another how do you do that well

1816
01:53:05,860 --> 01:53:10,460
it's quite simple you take too well if it's the former i think it's this

1817
01:53:10,460 --> 01:53:15,610
form already been done supposing you've got you've got this where i j k is

1818
01:53:15,630 --> 01:53:18,480
greater zero what do you do that

1819
01:53:18,540 --> 01:53:21,400
i suppose you got to got to

1820
01:53:22,040 --> 01:53:23,860
x i x j

1821
01:53:24,900 --> 01:53:27,630
positive i don't want positive and one negative

1822
01:53:28,590 --> 01:53:32,250
well it's quite simple you just change them all

1823
01:53:32,250 --> 01:53:36,820
use this will two times one minus excited by

1824
01:53:36,820 --> 01:53:38,730
one minus exchanged by

1825
01:53:38,840 --> 01:53:41,810
one minus xk but

1826
01:53:41,820 --> 01:53:44,360
that equals minus two

1827
01:53:52,880 --> 01:53:54,290
plus quadratic

1828
01:53:54,340 --> 01:53:58,170
similarly if you had to like excited by our xjxk

1829
01:53:58,190 --> 01:53:59,500
to change

1830
01:53:59,570 --> 01:54:02,420
things that one change you always get

1831
01:54:02,440 --> 01:54:03,630
in this form

1832
01:54:03,670 --> 01:54:04,750
with that

1833
01:54:04,770 --> 01:54:06,710
isn't it

1834
01:54:06,750 --> 01:54:10,650
and sometimes you with new stuff down in the quadratic

1835
01:54:10,650 --> 01:54:13,290
so what about the quadratic parts

1836
01:54:13,360 --> 01:54:15,440
we also want

1837
01:54:15,460 --> 01:54:21,020
well once you've done that the important thing about the quadratic part of this AIJ

1838
01:54:21,040 --> 01:54:23,570
it also has to be less than a year

1839
01:54:23,570 --> 01:54:27,460
got rid excites j phi rho x sidebar

1840
01:54:27,520 --> 01:54:30,670
xj then changed sign here

1841
01:54:30,710 --> 01:54:32,960
and then what's left get swept out into the

1842
01:54:35,150 --> 01:54:40,340
the thing is that the theorem is if you have it in this form

1843
01:54:40,420 --> 01:54:43,130
where i j is greater than zero

1844
01:54:43,130 --> 01:54:46,460
and all you i j is alison zero

1845
01:54:46,520 --> 01:54:49,840
but when you cannot be sure if that happens

1846
01:54:51,170 --> 01:54:52,210
the function is

1847
01:54:52,230 --> 01:54:57,980
submodular and can be solved the let's just see how this can be solved

1848
01:54:58,000 --> 01:55:00,550
my graph cut

1849
01:55:00,570 --> 01:55:04,650
the question is how to deal with skewed to how we deal with human terms

1850
01:55:04,670 --> 01:55:05,920
well what we do

1851
01:55:05,940 --> 01:55:06,920
it is

1852
01:55:06,960 --> 01:55:10,440
we introduce new variable

1853
01:55:10,480 --> 01:55:15,920
we introduce new variable we look at this

1854
01:55:15,980 --> 01:55:17,520
look at x aybar

1855
01:55:17,520 --> 01:55:19,340
xj bar

1856
01:55:19,380 --> 01:55:22,400
plus xk by

1857
01:55:22,420 --> 01:55:25,360
class one

1858
01:55:25,540 --> 01:55:27,730
we introduce why

1859
01:55:27,790 --> 01:55:30,770
a new variable y

1860
01:55:31,070 --> 01:55:37,570
three should be why i j k like

