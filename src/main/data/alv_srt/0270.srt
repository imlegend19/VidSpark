1
00:00:00,000 --> 00:00:02,330
think there is clear relation

2
00:00:02,520 --> 00:00:04,600
but i think that

3
00:00:04,600 --> 00:00:07,690
so i i don't know

4
00:00:07,730 --> 00:00:11,750
well what we do know is that the training procedure is of course more complicated

5
00:00:11,770 --> 00:00:15,020
something one single as you do this you

6
00:00:15,400 --> 00:00:17,350
he said that

7
00:00:17,380 --> 00:00:19,270
why i

8
00:00:19,290 --> 00:00:22,560
so just it is the your creation

9
00:00:22,670 --> 00:00:33,460
yes it is also possible that he was still

10
00:00:33,540 --> 00:00:37,610
and how to design a kernel

11
00:00:37,630 --> 00:00:41,710
this is also still a research issue

12
00:00:41,830 --> 00:00:48,960
so i mean by saying that that we should you design quite a few kernels

13
00:00:49,110 --> 00:00:56,110
but we want to be very careful is what we have you write function you

14
00:00:56,110 --> 00:01:01,750
make sure this is a very nice products of two many functions

15
00:01:02,250 --> 00:01:05,560
four so the function is even

16
00:01:05,580 --> 00:01:07,540
if you think b

17
00:01:07,670 --> 00:01:11,960
they should be evaluated kernels but they are not

18
00:01:11,980 --> 00:01:17,020
it is also difficult to give you an example

19
00:01:17,060 --> 00:01:24,580
forty two strings strings as well as so there's something called a distance to this

20
00:01:24,580 --> 00:01:25,520
is the thing

21
00:01:26,460 --> 00:01:30,650
so that we can define could be like these

22
00:01:30,650 --> 00:01:33,170
explanation by

23
00:01:33,190 --> 00:01:36,940
it is in this sense of this one and this is in this case the

24
00:01:36,940 --> 00:01:41,630
reason for that is for the probability that the outcome is the same as is

25
00:01:42,080 --> 00:01:47,100
he said that he distance with gradient descent

26
00:01:48,230 --> 00:01:50,540
is this something

27
00:01:50,560 --> 00:01:52,440
true but is not

28
00:01:53,610 --> 00:01:55,020
so this is a

29
00:01:55,020 --> 00:01:57,580
so you want to be careful

30
00:01:57,600 --> 00:01:58,870
you're using

31
00:01:58,880 --> 00:02:03,730
let's function looks like to one but sometimes it is not

32
00:02:03,750 --> 00:02:07,000
so it is

33
00:02:07,000 --> 00:02:13,240
so that they do you have a question about how to shape with

34
00:02:13,560 --> 00:02:16,170
function is very well or not

35
00:02:16,170 --> 00:02:24,400
if you importance you mention is a condition called mercer's condition so they

36
00:02:24,500 --> 00:02:32,710
it tried tries to address the issues of kind of chaotic represented for that so

37
00:02:32,710 --> 00:02:39,190
the conditions is for function k is can be represented as using the product then

38
00:02:39,190 --> 00:02:47,690
there's no conditions say that fall function g four g such that this integration is

39
00:02:47,690 --> 00:02:53,400
finite then if you could escape into this integration should critical to zero

40
00:02:53,460 --> 00:03:00,060
this is actually a condition to develop an interesting nineteenth century has like one hundred

41
00:03:00,060 --> 00:03:00,960
years ago

42
00:03:01,000 --> 00:03:07,750
the problem is it is also easy to cheque this condition so

43
00:03:07,770 --> 00:03:08,330
four four

44
00:03:08,350 --> 00:03:11,080
the kernels this is used to show you

45
00:03:11,480 --> 00:03:18,580
is very powerful some others then you have to use other techniques to those

46
00:03:21,130 --> 00:03:27,440
OK so so so so this point and finished discussing parameter

47
00:03:27,440 --> 00:03:30,690
the kernel selection and quite a few practical issues

48
00:03:31,020 --> 00:03:35,580
so questions so far

49
00:03:38,520 --> 00:03:40,290
raise your

50
00:03:40,310 --> 00:03:44,730
it has around

51
00:03:52,480 --> 00:03:53,770
well yes and no

52
00:03:53,790 --> 00:03:56,080
if you the computational

53
00:03:56,100 --> 00:04:00,290
you can try to agree to search so that's the way it was simultaneously check

54
00:04:00,290 --> 00:04:03,770
everything and you give you good parameters set

55
00:04:03,940 --> 00:04:05,210
you don't have

56
00:04:05,210 --> 00:04:09,500
you don't have so many resources so

57
00:04:09,810 --> 00:04:13,770
i mean if you don't have many parameters you have only one parameters

58
00:04:13,790 --> 00:04:16,380
you need to try several points

59
00:04:16,400 --> 00:04:20,210
that will give you a good parameter estimation

60
00:04:20,270 --> 00:04:22,520
so in that sense he is that sort

61
00:04:22,540 --> 00:04:28,080
when you a simple program they use the discretisation of parameter

62
00:04:28,100 --> 00:04:35,170
you check cross validation accuracy so that's the program to automatically select parameters for you

63
00:04:35,230 --> 00:04:40,150
but you have more parameters than it is more difficult

64
00:04:41,500 --> 00:04:45,600
that's why is

65
00:04:45,600 --> 00:04:46,850
is it

66
00:04:48,790 --> 00:04:54,770
well little OK

67
00:04:54,790 --> 00:05:01,730
there are some studies about that stands but the problem is a series of it

68
00:05:01,730 --> 00:05:08,130
is there's ever seen that it's usually the number of support vectors is being increased

69
00:05:08,150 --> 00:05:12,230
as well as the number of training data is increased

70
00:05:12,500 --> 00:05:15,150
so so that since then

71
00:05:15,170 --> 00:05:22,170
there is no direct relationship with number of support vectors and the performance is the

72
00:05:22,170 --> 00:05:26,690
number of support vectors is related to the number of training data

73
00:05:26,710 --> 00:05:28,190
but maybe

74
00:05:28,210 --> 00:05:30,400
some kind

75
00:05:31,580 --> 00:05:39,310
so for example the first person to really think percentages

76
00:05:46,230 --> 00:05:56,040
well that's several hints for example you if you all all training data had to

77
00:05:56,040 --> 00:05:58,460
be support vectors usually

78
00:05:58,480 --> 00:06:01,420
this is the situation where we're thinking and

79
00:06:01,420 --> 00:06:03,170
so you don't that

80
00:06:03,270 --> 00:06:05,730
that means it is

81
00:06:05,870 --> 00:06:10,920
it is OK i thirty percent training data support vectors and the sort of performance

82
00:06:10,940 --> 00:06:13,060
good that we i don't know

83
00:06:13,080 --> 00:06:18,960
necessary so we can see is usually if you tried to set the parameters you

84
00:06:18,960 --> 00:06:22,310
it's just just came up with the fourth edition

85
00:06:22,360 --> 00:06:23,810
and so on

86
00:06:23,820 --> 00:06:26,320
that book was a bestseller

87
00:06:26,340 --> 00:06:29,080
i think it's all over half a million copies

88
00:06:29,120 --> 00:06:33,400
i'm not sure where it is now but it has done very well it's been

89
00:06:33,400 --> 00:06:35,190
a perennial classic

90
00:06:35,420 --> 00:06:40,960
and it's really it's it emphasizes the long-run performance of the stock market but it's

91
00:06:40,960 --> 00:06:43,630
really a

92
00:06:44,570 --> 00:06:48,160
treatise on financial markets

93
00:06:48,180 --> 00:06:52,050
and i i get a very good reaction from this

94
00:06:52,090 --> 00:06:53,900
students for this but this one is

95
00:06:53,960 --> 00:06:55,050
very readable

96
00:06:55,060 --> 00:06:58,340
it's not so intense as proposed

97
00:06:58,360 --> 00:07:00,920
jeremy siegel has the unique distinction

98
00:07:00,950 --> 00:07:04,530
business week asked poll asking

99
00:07:04,600 --> 00:07:07,400
NBA's who was their favorite professor

100
00:07:07,460 --> 00:07:09,120
this is about ten years ago

101
00:07:09,130 --> 00:07:10,180
and they rank

102
00:07:10,240 --> 00:07:11,900
business school professors

103
00:07:11,980 --> 00:07:15,750
according to their popularity came out number one in the united states

104
00:07:15,770 --> 00:07:17,630
as business school professor

105
00:07:17,660 --> 00:07:21,590
and so i think you will like this work with the next work is my

106
00:07:21,590 --> 00:07:25,800
own called irrational exuberance

107
00:07:25,850 --> 00:07:27,750
this is the last book

108
00:07:29,240 --> 00:07:30,530
that's the phrase

109
00:07:30,540 --> 00:07:32,250
that was coined by

110
00:07:32,270 --> 00:07:33,890
alan greenspan

111
00:07:33,930 --> 00:07:36,360
in nineteen ninety six

112
00:07:36,390 --> 00:07:39,560
and refers to the stock market boom

113
00:07:39,560 --> 00:07:40,380
of the

114
00:07:40,400 --> 00:07:42,390
two thousand

115
00:07:42,390 --> 00:07:47,370
one of the nineteen nineties and the the boom-and-bust but i think it's related to

116
00:07:47,370 --> 00:07:50,630
the bus that came out later after two thousand

117
00:07:50,630 --> 00:07:53,180
i wrote this book in two thousand

118
00:07:54,390 --> 00:07:58,390
right at the peak of fortunately right at the peak of the stock market

119
00:07:58,400 --> 00:08:02,820
but what i'm saying to you is the second edition which came out in two

120
00:08:02,820 --> 00:08:04,450
thousand five

121
00:08:04,680 --> 00:08:07,370
pretty much at the peak of the housing market

122
00:08:07,500 --> 00:08:10,330
and so we're going to talk about

123
00:08:10,460 --> 00:08:13,140
both the housing market and the stock market

124
00:08:13,190 --> 00:08:16,240
in these different books

125
00:08:16,310 --> 00:08:20,650
these books are all on sale labyrinth books

126
00:08:20,670 --> 00:08:22,950
which is an independent bookstore

127
00:08:22,960 --> 00:08:26,160
here in new haven i put it there because

128
00:08:26,170 --> 00:08:27,550
well i think

129
00:08:27,720 --> 00:08:31,140
the major chain bookstores

130
00:08:31,180 --> 00:08:34,640
fulfil an important function but i'd like also to support

131
00:08:34,660 --> 00:08:36,240
independent bookstores

132
00:08:36,250 --> 00:08:38,720
and if you know the story labyrinth books

133
00:08:38,730 --> 00:08:41,360
is independence not change

134
00:08:41,360 --> 00:08:47,070
and independent bookstores trying to struggling to survive this is financed in the book business

135
00:08:47,430 --> 00:08:48,640
there's something

136
00:08:49,840 --> 00:08:54,720
about maintaining an independent operations of the labyrinth was at columbia university

137
00:08:54,740 --> 00:08:56,160
and you

138
00:08:56,180 --> 00:09:01,330
for some reason they shut down or columbia bookstore but they've opened up now

139
00:09:01,370 --> 00:09:02,470
in principle

140
00:09:02,500 --> 00:09:07,230
there was this famous bookstore conference on nassau street called the commas

141
00:09:07,250 --> 00:09:09,860
which is wonderful book store in there

142
00:09:09,880 --> 00:09:11,140
number of times

143
00:09:11,170 --> 00:09:15,150
but they just went out of business so labyrinth has moved in to take their

144
00:09:16,700 --> 00:09:21,330
anyway that's where the books all the books are and

145
00:09:21,840 --> 00:09:26,700
and a i checked there are available now them

146
00:09:36,710 --> 00:09:37,910
we're going to have

147
00:09:37,920 --> 00:09:41,420
these lectures on monday wednesday we're going to have t actions

148
00:09:42,130 --> 00:09:43,770
the second part of the week

149
00:09:43,790 --> 00:09:46,710
we're going to ask you to look at your schedule

150
00:09:46,740 --> 00:09:51,890
sometime before next lectures and think about when you can come to the teaching assistant

151
00:09:53,060 --> 00:09:58,500
but they will be wednesday thursday and friday we have six problems that the six

152
00:09:58,500 --> 00:10:00,150
problem sets are due

153
00:10:00,430 --> 00:10:02,550
and generally on mondays

154
00:10:02,550 --> 00:10:06,490
so will go over the problem in the teaching

155
00:10:07,730 --> 00:10:13,980
a couple days several days after you turn the man

156
00:10:14,000 --> 00:10:17,390
this is one of the biggest classes yale but i think

157
00:10:17,520 --> 00:10:18,940
we've got

158
00:10:18,990 --> 00:10:23,370
i think we've got it so it would be good and satisfying experience with you

159
00:10:23,370 --> 00:10:24,550
we have

160
00:10:24,580 --> 00:10:26,160
very qualified

161
00:10:26,250 --> 00:10:31,050
i'm very impressed her teaching assistants the important thing is for you to

162
00:10:31,050 --> 00:10:32,360
stay with them

163
00:10:32,380 --> 00:10:34,110
and get to know them

164
00:10:34,120 --> 00:10:38,110
and i urge you to attend the interior sections

165
00:10:38,120 --> 00:10:42,580
the course is going to be graded we have

166
00:10:42,630 --> 00:10:46,640
two midterms and one final in class mid-terms

167
00:10:46,680 --> 00:10:50,860
the grateful beasts roughly ten percent problem sets

168
00:10:50,860 --> 00:10:52,870
twenty percent first midterm

169
00:10:52,920 --> 00:10:56,640
thirty percent second term forty percent final

170
00:10:56,660 --> 00:11:01,650
but nonetheless we also use judgement and i'm going to appeal to the TA's

171
00:11:01,700 --> 00:11:03,630
to help me and judging

172
00:11:09,670 --> 00:11:15,130
i also asked the teaching assistants to give me little capsule descriptions of u

173
00:11:15,150 --> 00:11:18,230
so that ten years or twenty years from now

174
00:11:18,240 --> 00:11:20,290
and i get a call from a reporter

175
00:11:20,290 --> 00:11:26,360
asking about this illustrious person who was once my students i can have some in

176
00:11:26,370 --> 00:11:28,630
the primary

177
00:11:28,680 --> 00:11:33,070
so that's why i hope you'll stay with what you need to find teaching assistant

178
00:11:33,070 --> 00:11:35,440
will stay with that person

179
00:11:38,840 --> 00:11:43,150
i want to say something about

180
00:11:43,180 --> 00:11:46,370
one particular interest of mine

181
00:11:46,390 --> 00:11:50,730
because is part of this course

182
00:11:50,730 --> 00:11:52,490
this one is

183
00:11:52,990 --> 00:11:56,400
extension on the right

184
00:11:56,440 --> 00:11:58,150
the rule gn

185
00:11:58,200 --> 00:12:02,320
is introducing existential on the left

186
00:12:02,450 --> 00:12:06,090
the is are on the right again

187
00:12:06,130 --> 00:12:07,350
and here

188
00:12:07,360 --> 00:12:09,490
this existential on the right

189
00:12:11,860 --> 00:12:16,350
together with contracting two occurrences of the same formula to one which is just set

190
00:12:16,380 --> 00:12:18,830
combinatory properties of sets

191
00:12:19,000 --> 00:12:23,480
how do i get the FAI just threw it into free

192
00:12:23,500 --> 00:12:26,650
if you like that comes from the

193
00:12:26,660 --> 00:12:30,500
be by throwing in junk on both sides

194
00:12:35,640 --> 00:12:37,340
all right

195
00:12:37,400 --> 00:12:41,430
it's not obvious but it's a it's all legitimate you know within the meaning of

196
00:12:41,430 --> 00:12:43,850
the act is proof

197
00:12:45,280 --> 00:12:49,520
and you can do it from the axioms the health system if you really want

198
00:12:50,800 --> 00:12:57,050
which i really don't

199
00:13:02,550 --> 00:13:06,130
that's the theorem of the logic

200
00:13:06,140 --> 00:13:08,630
the not fact about logic

201
00:13:08,640 --> 00:13:11,170
that i want to observe

202
00:13:12,370 --> 00:13:13,310
and that's

203
00:13:13,320 --> 00:13:16,180
something called the rule

204
00:13:16,250 --> 00:13:18,050
of choice

205
00:13:18,100 --> 00:13:22,620
absolutely nothing to do with the axiom of choice

206
00:13:22,630 --> 00:13:26,150
i was ages ago there was no office ten

207
00:13:26,160 --> 00:13:27,780
this is the rule of choice

208
00:13:27,790 --> 00:13:30,570
route choice is used a lot in mathematics

209
00:13:30,760 --> 00:13:33,690
in mathematics you get to a point where you have proved the existence of something

210
00:13:33,690 --> 00:13:35,410
you say let's call it

211
00:13:37,210 --> 00:13:41,090
and you go deduced deduced its nutritional properties of whatever

212
00:13:41,100 --> 00:13:42,030
and then you say

213
00:13:42,050 --> 00:13:44,850
so that force

214
00:13:44,870 --> 00:13:48,290
as long as what you called it didn't matter

215
00:13:48,400 --> 00:13:50,260
you can choose an a

216
00:13:51,280 --> 00:13:56,210
that's that's the logical principle suppose that from somewhere or other you're deducing you get

217
00:13:56,210 --> 00:14:00,150
an existential

218
00:14:01,400 --> 00:14:05,400
you've proved from whatever you start with such things exist

219
00:14:05,450 --> 00:14:08,060
is a let's choose one

220
00:14:08,680 --> 00:14:10,920
let's call it

221
00:14:10,930 --> 00:14:14,010
well introduce a constant

222
00:14:14,020 --> 00:14:17,960
just as erotic a function similar

223
00:14:17,970 --> 00:14:20,090
a school it's a

224
00:14:20,100 --> 00:14:24,670
and from that you get some conclusion

225
00:14:24,720 --> 00:14:26,250
you can conclude

226
00:14:26,260 --> 00:14:30,340
the b halls

227
00:14:34,770 --> 00:14:36,810
this is no funny business

228
00:14:36,900 --> 00:14:41,090
OK it provided see doesn't occur in the conclusion

229
00:14:44,910 --> 00:14:46,560
provided that neither b

230
00:14:46,570 --> 00:14:50,460
nor any other side assumptions that use made any assumptions about c

231
00:14:50,510 --> 00:14:53,570
claimed to so

232
00:14:53,630 --> 00:14:56,410
the idea is that this deduction doesn't

233
00:14:56,450 --> 00:14:58,750
in the way depend

234
00:15:00,260 --> 00:15:05,470
on what name chose

235
00:15:05,530 --> 00:15:09,700
maybe secret gamma i don't have any way i mean

236
00:15:09,760 --> 00:15:13,530
let's TV something that's completely foreign to the context maybe it doesn't even occur in

237
00:15:14,900 --> 00:15:19,130
i if you like the and you can always choose the constant completely outside the

238
00:15:20,250 --> 00:15:25,910
so if you if you join the existence of something from wherever you going you

239
00:15:25,910 --> 00:15:28,470
can always pick one give it a name

240
00:15:28,530 --> 00:15:33,290
get a conclusion as long as that the depend on what name chose

241
00:15:33,300 --> 00:15:35,810
so is see doesn't care anywhere

242
00:15:35,870 --> 00:15:39,420
then that shows this falls

243
00:15:39,430 --> 00:15:44,010
and if you need any side assumptions here will they stay as well

244
00:15:44,020 --> 00:15:47,960
this rule of choice it's plausible that holds i'm not going to establish that it

245
00:15:48,950 --> 00:15:51,250
but it does

246
00:15:51,260 --> 00:15:57,210
right i mean you can you can work

247
00:15:57,220 --> 00:16:00,700
all right that's a little bit of useful

248
00:16:03,280 --> 00:16:05,600
back in the logic

249
00:16:05,710 --> 00:16:08,130
probably the on

250
00:16:10,700 --> 00:16:13,640
was like to do with completeness

251
00:16:13,680 --> 00:16:15,080
while nothing yet

252
00:16:15,090 --> 00:16:18,790
we're going to to

253
00:16:18,800 --> 00:16:24,940
we've got a consistent set

254
00:16:25,070 --> 00:16:31,220
we could work linden baum's lemma again

255
00:16:31,280 --> 00:16:34,830
and we could blow it up to a maximal consistent set

256
00:16:34,890 --> 00:16:40,180
but that's sort of wouldn't help a lot

257
00:16:40,230 --> 00:16:42,050
because in first order

258
00:16:42,100 --> 00:16:44,400
logic maximal consistent sets

259
00:16:44,410 --> 00:16:48,480
don't always have the nice properties

260
00:16:48,560 --> 00:16:51,080
well don't have all the nice properties that we

261
00:16:51,100 --> 00:16:53,670
like to get completeness theorem

262
00:16:55,720 --> 00:17:00,230
they behave like they do in propositional logic with respect to the connectives they treat

263
00:17:00,230 --> 00:17:03,560
negation probability they treat implication of the tree

264
00:17:03,570 --> 00:17:07,820
conjunction disjunction in just the truth table way that's fine

265
00:17:07,880 --> 00:17:09,330
but they don't

266
00:17:09,340 --> 00:17:14,680
i have strong enough properties to handle the quantifiers correctly

267
00:17:14,690 --> 00:17:17,980
right what we would like it is

268
00:17:18,050 --> 00:17:19,440
that we would like

269
00:17:19,460 --> 00:17:20,190
when we

270
00:17:20,210 --> 00:17:21,680
got a big delta

271
00:17:21,730 --> 00:17:24,160
the something like for all x a

272
00:17:24,200 --> 00:17:25,620
is in delta

273
00:17:25,700 --> 00:17:27,730
if and only if

274
00:17:27,810 --> 00:17:31,270
eighty four x is in delta

275
00:17:31,290 --> 00:17:34,210
for all

276
00:17:37,650 --> 00:17:39,620
for all t such that

277
00:17:39,630 --> 00:17:40,660
good things

278
00:17:40,780 --> 00:17:43,650
quality we like

279
00:17:46,400 --> 00:17:50,120
so that's a is going to

280
00:17:50,130 --> 00:17:51,340
if and only if

281
00:17:51,350 --> 00:17:56,670
for some

282
00:17:56,680 --> 00:17:59,800
appropriate t

283
00:18:00,380 --> 00:18:03,580
some instance of it is in delta

284
00:18:04,500 --> 00:18:08,560
how big set would not only true the connectives in the truth functional way that

285
00:18:08,560 --> 00:18:12,220
we would like it will also treat quantifiers in the way that we'd like to

286
00:18:12,220 --> 00:18:13,780
be able to cook up

287
00:18:13,820 --> 00:18:17,160
an interpretation is going to do all the tricks

288
00:18:17,170 --> 00:18:21,210
but that doesn't happen it

289
00:18:21,210 --> 00:18:22,210
this is

290
00:18:24,540 --> 00:18:31,040
you see this one one one one one one one six six

291
00:18:33,420 --> 00:18:38,290
so we got this one or not is professor

292
00:18:38,290 --> 00:18:42,500
stanford was already been published but

293
00:18:42,500 --> 00:18:45,420
first of all you can see the

294
00:18:45,440 --> 00:18:49,320
similarity was presented by the united states

295
00:18:49,440 --> 00:18:51,480
this is why

296
00:18:51,500 --> 00:18:54,820
but you say i follow pass

297
00:18:55,460 --> 00:18:58,090
he was this one

298
00:18:58,110 --> 00:19:00,150
he also go through the page

299
00:19:00,210 --> 00:19:03,670
article operators will be able to be

300
00:19:03,690 --> 00:19:07,940
this is a visit each to go to the same set of conferences

301
00:19:10,630 --> 00:19:12,480
this was far from the

302
00:19:12,540 --> 00:19:15,710
most similar ones that

303
00:19:15,730 --> 00:19:20,880
i really want to be in the past before the

304
00:19:21,020 --> 00:19:24,800
conference of these we publish the same set

305
00:19:24,840 --> 00:19:26,020
they systems

306
00:19:26,130 --> 00:19:33,840
sigma here's a nice properties so there was a really hard to come by a

307
00:19:33,980 --> 00:19:40,980
lot of work to be we should also similarity that's why because of those cells

308
00:19:40,980 --> 00:19:46,000
this is very similar to the one on a

309
00:19:46,020 --> 00:19:51,590
then at the origin nature of those guys

310
00:19:51,770 --> 00:19:55,480
what's called parents

311
00:19:55,500 --> 00:19:57,750
you can see this

312
00:19:57,770 --> 00:20:01,340
alternative is a key here is

313
00:20:02,790 --> 00:20:09,150
you would found find a reason

314
00:20:09,170 --> 00:20:10,630
the reason is

315
00:20:10,650 --> 00:20:17,900
OK well before i went on frequent pattern for this is of

316
00:20:20,880 --> 00:20:24,110
there's a lot of work is so

317
00:20:24,130 --> 00:20:28,110
we share the same scene conference brochure

318
00:20:28,110 --> 00:20:31,460
since we know what this

319
00:20:32,440 --> 00:20:33,980
it can be

320
00:20:33,980 --> 00:20:35,400
the other

321
00:20:37,150 --> 00:20:41,070
so you can see that

322
00:20:41,110 --> 00:20:42,920
one on

323
00:20:42,960 --> 00:20:44,920
some of them

324
00:20:44,940 --> 00:20:48,110
so you can see the very subtle differences

325
00:20:48,170 --> 00:20:52,860
doesn't seem to be so you get into these you used

326
00:20:53,270 --> 00:20:56,380
try to find out what was about

327
00:20:56,400 --> 00:20:59,460
this very interesting because the same

328
00:20:59,460 --> 00:21:01,960
in a essentially

329
00:21:02,040 --> 00:21:03,190
you want to

330
00:21:03,210 --> 00:21:03,940
she was here

331
00:21:03,960 --> 00:21:05,460
she got this paper

332
00:21:05,480 --> 00:21:08,730
she tried to rediscover the

333
00:21:08,730 --> 00:21:12,270
i was young stars fear

334
00:21:13,840 --> 00:21:16,170
this is what is most

335
00:21:16,650 --> 00:21:22,230
then he to his own surprise surprise she

336
00:21:22,230 --> 00:21:26,710
by child child nodes also this problem

337
00:21:28,190 --> 00:21:33,110
he actually also guessing he

338
00:21:33,110 --> 00:21:35,020
the best of

339
00:21:35,040 --> 00:21:37,130
maybe that's the key

340
00:21:38,020 --> 00:21:39,860
in this region

341
00:21:41,170 --> 00:21:42,560
o hours

342
00:21:42,590 --> 00:21:43,440
you got

343
00:21:43,460 --> 00:21:44,710
five years

344
00:21:44,770 --> 00:21:48,900
this the system to mission

345
00:21:48,900 --> 00:21:54,840
OK you you can see the first phd policy they wasting money more or you

346
00:21:55,860 --> 00:21:59,110
it's probably the most one two

347
00:21:59,130 --> 00:22:00,420
stanford one

348
00:22:00,460 --> 00:22:03,210
mission was the very reason

349
00:22:03,230 --> 00:22:07,480
this is the flying to our surprise

350
00:22:07,500 --> 00:22:09,630
you can find this

351
00:22:11,790 --> 00:22:18,270
what was the matter because of the same set of conferences using you know something

352
00:22:19,900 --> 00:22:21,710
that's that's not problem

353
00:22:23,040 --> 00:22:26,500
she actually went back to school

354
00:22:27,590 --> 00:22:29,000
this flow

355
00:22:31,420 --> 00:22:33,940
she was playing flickr

356
00:22:35,150 --> 00:22:37,560
the network

357
00:22:37,570 --> 00:22:40,460
that user group

358
00:22:40,730 --> 00:22:44,110
this is a star network schema

359
00:22:44,130 --> 00:22:45,980
the schema

360
00:22:46,000 --> 00:22:48,900
she took flowers lotus flower

361
00:22:48,960 --> 00:22:52,520
so this flux in which features most

362
00:22:52,540 --> 00:22:54,070
to this block

363
00:22:54,090 --> 00:23:01,710
she used two different as well as the ones he i i i g

364
00:23:01,730 --> 00:23:05,040
our expense with this one

365
00:23:05,040 --> 00:23:06,460
i mean is

366
00:23:06,460 --> 00:23:08,170
this shows

367
00:23:09,770 --> 00:23:17,860
he's tag if they share same similar partial is i'm only one or two

368
00:23:17,880 --> 00:23:20,570
find pictures

369
00:23:20,590 --> 00:23:28,070
the picture is not that you mean by by saying this flower flower as follows

370
00:23:29,500 --> 00:23:31,500
OK that looks like

371
00:23:31,520 --> 00:23:33,840
lotus flower anyway

372
00:23:33,860 --> 00:23:39,920
just because they have to act something that some of the five

373
00:23:42,090 --> 00:23:47,360
but if you go to school i actually here is one of the things you

374
00:23:47,380 --> 00:23:51,420
go down here you go down here

375
00:23:51,420 --> 00:23:54,190
this one and this one

376
00:23:54,210 --> 00:23:57,290
you take the rule also clustering

377
00:23:57,300 --> 00:23:59,320
four users sign

378
00:24:00,460 --> 00:24:05,000
he said well i mean this was one

379
00:24:05,020 --> 00:24:06,770
many users

380
00:24:06,820 --> 00:24:10,300
many had received right away

381
00:24:10,320 --> 00:24:16,500
but like me probably more people had this one flower

382
00:24:16,500 --> 00:24:17,840
the handle

383
00:24:17,840 --> 00:24:20,050
the full analogy

384
00:24:20,050 --> 00:24:27,530
an allophone really is a predictable phone phonetic variants of a phoneme

385
00:24:27,550 --> 00:24:29,280
phonological rules

386
00:24:29,300 --> 00:24:34,530
applied to phoneme strings to produce actual pronunciations of words and sentences

387
00:24:34,570 --> 00:24:41,400
so there are a number of phonological rules that that are typically applied assimilation for

388
00:24:42,220 --> 00:24:49,300
the spreading of phonetic features across phonemes the sequence of words this wish

389
00:24:49,300 --> 00:24:50,590
for example

390
00:24:50,610 --> 00:24:52,680
i might spread

391
00:24:52,700 --> 00:24:56,590
so that might become this wish

392
00:24:56,950 --> 00:25:04,420
in it is i would pronounce it i've actually assimilated a phonetic feature into the

393
00:25:04,420 --> 00:25:06,300
last one of the first word

394
00:25:06,900 --> 00:25:12,680
this table at the bottom here represents exactly an example of flapping

395
00:25:12,700 --> 00:25:19,570
the word city here in a funny make representation i might transcribed that the dictionary

396
00:25:19,570 --> 00:25:21,030
using a t

397
00:25:21,050 --> 00:25:24,700
but the flapping rule tells me that any

398
00:25:24,760 --> 00:25:28,970
inter vocalic t or d might be pronounced as d

399
00:25:29,510 --> 00:25:35,340
and so that that would appear as phonemic representation the city

400
00:25:35,360 --> 00:25:41,510
the vowel reduction rule here i would change a of OWL two

401
00:25:42,010 --> 00:25:48,470
a reduced vowel in an unstressed syllable so in example here for the word for

402
00:25:48,470 --> 00:25:49,400
knowledge e

403
00:25:49,450 --> 00:25:54,990
i would pronounce satisfy analogy i might say phonology so this is an example of

404
00:25:55,130 --> 00:26:00,610
applying sort of vol reduction is as a phonological rule

405
00:26:03,420 --> 00:26:05,070
rushing through the archive of

406
00:26:05,090 --> 00:26:07,090
there we go that was your introduction

407
00:26:07,130 --> 00:26:09,150
two distinctive features and

408
00:26:09,170 --> 00:26:12,220
for anybody any questions

409
00:26:12,300 --> 00:26:16,340
that having raced to that so quickly any questions on that

410
00:26:19,110 --> 00:26:25,970
the big problem dictionaries i think professor why will actually alluded to the issue of

411
00:26:27,550 --> 00:26:31,610
of dealing with canonical baseform dictionaries

412
00:26:32,950 --> 00:26:35,420
and this this little figure here

413
00:26:35,740 --> 00:26:41,030
gives an illustration of the problem we have

414
00:26:41,050 --> 00:26:43,780
and using in trying to

415
00:26:43,800 --> 00:26:48,260
use canonical dictionary is in speech recognition

416
00:26:48,300 --> 00:26:56,220
and i refer to canonical dictionary is basically being those baseform dictionaries that we we

417
00:26:56,220 --> 00:26:59,920
pose as a set of rules before we start before we

418
00:26:59,940 --> 00:27:06,740
we we build our system and the surface form variants are those that we might

419
00:27:06,740 --> 00:27:07,860
consider to be

420
00:27:08,050 --> 00:27:11,170
one allophonic very variants or

421
00:27:11,170 --> 00:27:16,180
as they would actually occur in the acoustics when we were of the work

422
00:27:17,740 --> 00:27:19,610
i could think of of having

423
00:27:19,610 --> 00:27:26,530
some here expanded in terms of its canonical phonetic baseform just sequence of n phonemes

424
00:27:26,550 --> 00:27:29,840
so for any particular phoneme k

425
00:27:29,860 --> 00:27:33,530
there may be some pronunciation variants of that running

426
00:27:33,530 --> 00:27:36,630
that let's suppose i have

427
00:27:36,670 --> 00:27:40,220
dozens of examples of utterances of this work

428
00:27:40,350 --> 00:27:44,840
i have my linguists go through and label

429
00:27:44,950 --> 00:27:48,590
are label those utterances according to

430
00:27:48,670 --> 00:27:56,200
the actual pronunciation that is perhaps i in i had an utterance of the word

431
00:27:56,200 --> 00:28:00,530
city and i didn't actually other that we didn't actually say city

432
00:28:00,550 --> 00:28:01,780
i said city

433
00:28:01,780 --> 00:28:02,470
and so

434
00:28:03,260 --> 00:28:09,570
linguists with label that or perhaps i was speaking particularly fast i might have deleted

435
00:28:09,740 --> 00:28:16,170
a phoneme altogether and so we can actually label that the another is according to

436
00:28:16,170 --> 00:28:18,070
its pronunciation variants

437
00:28:18,070 --> 00:28:21,950
having done that perhaps i could then

438
00:28:21,970 --> 00:28:24,800
produced separate acoustic models

439
00:28:24,840 --> 00:28:28,530
with whatever my favourite modelling form formalism is

440
00:28:28,530 --> 00:28:33,650
that will describe these various acoustic variants so you would think is great

441
00:28:33,700 --> 00:28:35,920
now i can represent

442
00:28:36,070 --> 00:28:41,030
this word in terms of how it actually appears in in real utterances and i'll

443
00:28:41,050 --> 00:28:45,780
build a better speech recognition system the problem is is

444
00:28:45,780 --> 00:28:47,820
i don't know how

445
00:28:48,150 --> 00:28:50,450
in in

446
00:28:50,470 --> 00:28:54,450
how to predict those pronunciation variants during recognition

447
00:28:54,510 --> 00:28:57,800
given this lexical base form

448
00:28:57,820 --> 00:29:02,170
well i might have a phoneme k is one of the

449
00:29:02,760 --> 00:29:07,260
one of the phonemes in this space form expansion i don't know how to pick

450
00:29:07,280 --> 00:29:08,650
the proper

451
00:29:08,670 --> 00:29:14,220
the proper the model associated with the acoustic variables that might actually occur

452
00:29:16,820 --> 00:29:22,470
another illustration of the problems with dictionaries let's suppose i have this sequence of words

453
00:29:22,470 --> 00:29:24,760
purpose and respect

454
00:29:24,800 --> 00:29:27,530
and i had the basis for

455
00:29:27,550 --> 00:29:32,950
pronunciation that i would have pulled out of my dictionary

456
00:29:32,990 --> 00:29:38,320
and i would also help let's say the surface form

457
00:29:38,340 --> 00:29:45,420
pronunciation that my linguist would have labelled from the actual occurrence

458
00:29:45,470 --> 00:29:50,720
of these the surface form phonemes are the surface form

459
00:29:52,550 --> 00:29:54,450
phones in the

460
00:29:54,510 --> 00:29:58,360
actual utterances and so what we see whether the

461
00:29:58,360 --> 00:30:03,650
sources the difference between the two well i can see here for the word and

462
00:30:03,650 --> 00:30:08,530
i've deleted in this particular is ever deleted the d

463
00:30:08,550 --> 00:30:10,510
i might have also

464
00:30:10,530 --> 00:30:16,340
so strictly is simply had a pronunciation variants here i'm instead of pronouncing i might

465
00:30:16,340 --> 00:30:23,300
have said and i could also have various other surface acoustic information that might not

466
00:30:23,300 --> 00:30:27,630
even be represented in my baseform dictionary for example

467
00:30:27,670 --> 00:30:31,570
the linguist might label a for each

468
00:30:32,670 --> 00:30:34,200
constant the

469
00:30:34,220 --> 00:30:36,610
closure and the release

470
00:30:36,630 --> 00:30:38,280
whereas typically in my

471
00:30:38,280 --> 00:30:43,860
lexicon i would just have a phoneme there representing that stuff

472
00:30:44,740 --> 00:30:49,240
there's all sorts of things that are going to appear in my surface form representation

473
00:30:49,240 --> 00:30:51,700
that might not appear in the base form

474
00:30:51,780 --> 00:30:53,070
and you say well

475
00:30:53,090 --> 00:30:55,610
great let's just make a bigger dictionary

476
00:30:55,630 --> 00:31:01,260
and know if i got if i've got this different pronunciation of and let's slap

477
00:31:01,260 --> 00:31:05,950
it into the dictionary is just thrown in there but what i've already got

478
00:31:05,970 --> 00:31:08,200
the were and you

479
00:31:08,200 --> 00:31:10,590
that's got the same

480
00:31:10,590 --> 00:31:12,800
if i phi slapichyn

481
00:31:12,800 --> 00:31:14,590
this additional potentiation

482
00:31:14,610 --> 00:31:17,920
four and was going to get this ambiguity

483
00:31:17,940 --> 00:31:23,900
same thing if i if i have the word had and i try to throw

484
00:31:23,900 --> 00:31:30,090
in a a pronunciation variants for that well that's going to overlap with the the

485
00:31:30,090 --> 00:31:32,360
canonical pronunciation for heads

486
00:31:33,130 --> 00:31:39,490
so as i simply try and add in these pronunciation variants in my

487
00:31:41,280 --> 00:31:47,240
i'll increase the coverage but i think going to introduce the this ambiguity among my

488
00:31:47,240 --> 00:31:50,070
dictionary entries so you see the problem

489
00:31:50,090 --> 00:31:53,110
and so you have to be clever about doing that and

490
00:31:53,320 --> 00:31:58,360
waiting your pronunciations and so on our our our

491
00:31:58,380 --> 00:32:00,180
one of many ways to

492
00:32:00,200 --> 00:32:02,440
to try and deal with that problem

493
00:32:02,440 --> 00:32:05,280
well any NEC gets even tougher

494
00:32:05,300 --> 00:32:10,950
OK here's a member of the results here that that make it even more grim

495
00:32:10,950 --> 00:32:16,490
for dealing with these problems with dictionaries if i talk faster if i increase my

496
00:32:16,490 --> 00:32:21,220
speaking rate well increase the number of words per second but the number of phones

497
00:32:21,220 --> 00:32:24,970
per second might stay roughly the same so i'm going to be deleting all these

498
00:32:24,970 --> 00:32:28,450
so velocity they are easy as long as they are along the line of sight

499
00:32:28,450 --> 00:32:31,780
if you're transverse velocity in cosmology those

500
00:32:36,040 --> 00:32:38,170
so let's come back to our wobbles look

501
00:32:38,210 --> 00:32:41,840
the reality is called below but we should

502
00:32:42,660 --> 00:32:48,130
that in reality he was best the sleeve that the working with table in the

503
00:32:48,130 --> 00:32:54,680
same cemetery between nineteen twelve and nineteen twenties found that most galaxies in the light

504
00:32:54,680 --> 00:32:58,600
of most galaxies were shifted at that time there were not called galaxy they were

505
00:32:58,600 --> 00:33:02,210
called memory and people didn't really know what they were they they know if there

506
00:33:02,210 --> 00:33:06,570
were something close by that you had to avoid when he wanted to study comments

507
00:33:06,760 --> 00:33:11,070
or whether they where all the island universes

508
00:33:11,100 --> 00:33:15,440
well there no big objects like our own universe which by that time was our

509
00:33:17,170 --> 00:33:17,940
and so

510
00:33:17,950 --> 00:33:22,700
that was the first state all look all this never seems to have the light

511
00:33:22,700 --> 00:33:25,840
mostly shift rather than blue shifted

512
00:33:25,850 --> 00:33:28,280
and and why is that

513
00:33:29,100 --> 00:33:32,890
if i interpret it as if he was the velocity then it would have to

514
00:33:32,890 --> 00:33:35,040
write this kind of equation and it

515
00:33:35,060 --> 00:33:37,690
it turns out that the velocity there

516
00:33:37,730 --> 00:33:42,300
so these are the kind of images not exactly the same historical images but the

517
00:33:42,300 --> 00:33:44,350
kind of in

518
00:33:44,360 --> 00:33:47,160
images they could have got to

519
00:33:47,170 --> 00:33:50,180
first of and and edwin hubble

520
00:33:50,200 --> 00:33:54,490
in order to see it actually that's all you just have a galaxy you put

521
00:33:54,490 --> 00:33:58,210
to sleep through the galaxy the light to get started and so this is the

522
00:33:58,210 --> 00:34:02,850
image of these kind of galaxy and then you see the assumption lines

523
00:34:02,880 --> 00:34:05,860
the PS and then get move with respect to

524
00:34:06,950 --> 00:34:11,320
rest the position what it would be interesting this different galaxies

525
00:34:11,440 --> 00:34:14,650
right and this is the spectrum and is the same feature

526
00:34:15,810 --> 00:34:19,290
gets moved

527
00:34:19,300 --> 00:34:23,190
and so we came up with a velocity well OK i'm going to make this

528
00:34:23,190 --> 00:34:25,120
connection between

529
00:34:27,300 --> 00:34:33,110
and this is the galaxy the farther away they

530
00:34:33,180 --> 00:34:36,960
the fact that the the fast and this seems to be going away from me

531
00:34:37,020 --> 00:34:39,390
and so say the well she

532
00:34:39,400 --> 00:34:43,280
i interpreted like velocity and i put constant here

533
00:34:43,370 --> 00:34:47,140
and a connected to the distance because when i plot my points it looks to

534
00:34:47,140 --> 00:34:48,940
me more or less straight line

535
00:34:48,990 --> 00:34:51,660
and this is actually what are born

536
00:34:51,660 --> 00:34:55,380
the is there are no about in this blog

537
00:34:55,420 --> 00:35:00,900
in cosmology has been like that for many years and the enormously before yesterday

538
00:35:00,920 --> 00:35:02,460
right you get your

539
00:35:02,490 --> 00:35:04,940
wind in the arab world that's too hard

540
00:35:04,950 --> 00:35:06,150
so this is

541
00:35:06,160 --> 00:35:12,880
actually hubble original number and he came up with a value for this hubble constant

542
00:35:12,880 --> 00:35:15,690
of the order of five hundred now today

543
00:35:15,740 --> 00:35:20,010
the latest that tell you that is of the order of seventy four which you

544
00:35:20,010 --> 00:35:22,620
know a few percent error bars

545
00:35:22,620 --> 00:35:27,270
and so basically today the plot to look more something like that but he was

546
00:35:27,270 --> 00:35:30,840
right there is basically a linear correlation between velocity

547
00:35:30,890 --> 00:35:36,040
the recession velocity and distance so thank you to edwin hubble we can measure distances

548
00:35:36,040 --> 00:35:37,950
relative ease in cosmology

549
00:35:37,970 --> 00:35:39,490
once we measure

550
00:35:39,510 --> 00:35:40,690
well calibrated

551
00:35:40,690 --> 00:35:43,670
they what is called the cosmic distance ladder

552
00:35:44,520 --> 00:35:49,680
measuring the value for the hubble parameter

553
00:35:49,680 --> 00:35:54,940
so let me take a very brief as i mentioned something about the great debate

554
00:35:54,940 --> 00:35:59,280
in the twenty you can find more at this link so the debate was between

555
00:35:59,280 --> 00:36:01,020
sharply in court

556
00:36:01,050 --> 00:36:05,190
that they were arguing whether those who never really where

557
00:36:05,270 --> 00:36:08,810
never that you have to avoid in order for you to be able to study

558
00:36:08,810 --> 00:36:11,470
comments and they were you know like

559
00:36:11,480 --> 00:36:16,170
clouds in our own universe or whether those nepalese where

560
00:36:16,180 --> 00:36:20,200
so far away but the small enough to you but they were really big and

561
00:36:20,200 --> 00:36:24,790
they were basically like our own universe our own galaxy and they were called island

562
00:36:24,790 --> 00:36:31,480
universes and oblong in nineteen twenty four closes this debate by saying you know

563
00:36:31,500 --> 00:36:34,260
galaxy doesn't mean universe

564
00:36:34,270 --> 00:36:38,960
the universe is much bigger than one and galaxies all those nebulae or the island

565
00:36:38,960 --> 00:36:42,980
universes of the galaxies and i actually can classify them

566
00:36:43,000 --> 00:36:48,040
and he had to sons theory on how galaxies were born evolve which

567
00:36:48,050 --> 00:36:51,010
the double classification still holds today

568
00:36:51,030 --> 00:36:53,050
so you see how many things you need to do in order to get the

569
00:36:53,050 --> 00:36:56,810
telescope named after the

570
00:36:58,460 --> 00:37:03,280
they have always very useful because it allows you to determine distances

571
00:37:03,290 --> 00:37:08,930
with some caveats however because you are you are interpreting the recession velocity as these

572
00:37:08,930 --> 00:37:13,210
stands but you know the universe there is gravity acting their own galaxies are not

573
00:37:13,210 --> 00:37:17,280
isolated we've seen they live in groups and clusters and so

574
00:37:17,310 --> 00:37:19,460
if i'm an galaxy right

575
00:37:19,550 --> 00:37:23,810
that i think i should follow the hubble law but there is another massive galaxy

576
00:37:23,810 --> 00:37:25,250
nearby me there

577
00:37:25,310 --> 00:37:26,490
then you know

578
00:37:26,540 --> 00:37:30,450
and they follow a little bit into the other galaxy because of gravitational attraction and

579
00:37:30,450 --> 00:37:34,610
so you expect to see some scattered around the table and you may expect to

580
00:37:34,610 --> 00:37:38,780
see some galaxies to be blue shifted rather than shift that you know if there

581
00:37:38,780 --> 00:37:42,110
is a big master that the men this happen to offset

582
00:37:42,150 --> 00:37:44,940
what is the background hubble functional look

583
00:37:45,210 --> 00:37:49,220
but the next key point is well

584
00:37:49,240 --> 00:37:52,220
if you're not a special place in the universe

585
00:37:52,240 --> 00:37:55,590
then it means that the if i would like to sit in a different galaxy

586
00:37:55,860 --> 00:37:59,590
and repeat the operation of esther and and and harbor

587
00:37:59,690 --> 00:38:03,860
i should still find that all the galaxies are not flying away

588
00:38:03,960 --> 00:38:05,370
so basically

589
00:38:05,380 --> 00:38:10,570
the conclusion is well maybe just this will happen if the universe is expanding

590
00:38:10,610 --> 00:38:12,690
and imagine you are making a cake

591
00:38:12,800 --> 00:38:15,070
and making you know you put in the

592
00:38:15,090 --> 00:38:18,940
the k can be of any put some raising it when when when the k

593
00:38:18,950 --> 00:38:22,680
crisis will happen to the raising they all fly away from each other

594
00:38:22,700 --> 00:38:26,280
so imagine that there is that galaxies and the the cake is the universe is

595
00:38:26,280 --> 00:38:31,450
expanding then this is what you would see

596
00:38:31,470 --> 00:38:35,200
but this will bring us into our conclusion that the universe is expanding so it's

597
00:38:35,200 --> 00:38:36,500
not static

598
00:38:36,500 --> 00:38:38,630
help us to coding

599
00:38:38,650 --> 00:38:40,920
help us get rid of all the noise

600
00:38:40,950 --> 00:38:44,110
and the output noise in quotes here

601
00:38:45,910 --> 00:38:52,000
so i see a happens to be a method that can be used but rather

602
00:38:52,000 --> 00:38:55,050
for the methods that can be

603
00:38:55,130 --> 00:38:57,790
placed like and you see another others

604
00:38:57,800 --> 00:38:59,160
i will not go into

605
00:38:59,170 --> 00:39:02,630
this part in detail

606
00:39:03,320 --> 00:39:05,780
so when when you saw

607
00:39:05,800 --> 00:39:06,840
we need doing

608
00:39:06,850 --> 00:39:08,500
the brain pong game

609
00:39:08,540 --> 00:39:12,230
i was using one particular

610
00:39:15,420 --> 00:39:18,030
which is pulling the strings

611
00:39:18,040 --> 00:39:20,370
there's lots of other possibilities

612
00:39:20,370 --> 00:39:25,040
so our subjects they typically think about squeezing the ball

613
00:39:25,050 --> 00:39:26,750
kicking a ball some

614
00:39:26,750 --> 00:39:30,110
you know resilience of subjects would always kick aboard

615
00:39:30,120 --> 00:39:34,420
they feel touch feel since feel heat whatever

616
00:39:34,430 --> 00:39:40,080
so this is something some kind of imagery that you would be using

617
00:39:43,970 --> 00:39:44,780
if you

618
00:39:44,790 --> 00:39:47,010
look at this slide

619
00:39:47,070 --> 00:39:52,230
god has pointed out that there's some desynchronisation

620
00:39:52,240 --> 00:39:54,870
if you say if you close your eyes and i

621
00:39:54,960 --> 00:39:56,550
i'm doing here this is

622
00:39:56,570 --> 00:39:58,800
the picture is that the

623
00:39:58,900 --> 00:40:01,850
again not gave me

624
00:40:02,670 --> 00:40:04,900
so then this some of activity

625
00:40:04,910 --> 00:40:07,500
if you

626
00:40:07,510 --> 00:40:11,620
open your eyes again this idea rhythm is suppressed

627
00:40:11,630 --> 00:40:13,600
and this suppression

628
00:40:13,730 --> 00:40:14,980
over them

629
00:40:14,990 --> 00:40:16,380
this is something that we

630
00:40:16,400 --> 00:40:18,160
we'll be using

631
00:40:18,850 --> 00:40:22,340
most of what what comes in in this talk

632
00:40:22,360 --> 00:40:23,370
so now

633
00:40:23,370 --> 00:40:29,350
but we not be using the ideal rhythm suppression because we have all eyes open

634
00:40:29,390 --> 00:40:30,740
now i mean we are

635
00:40:30,760 --> 00:40:32,780
actually using

636
00:40:33,270 --> 00:40:35,970
the ideal rhythm

637
00:40:36,000 --> 00:40:40,090
that we have when when we have some motor

638
00:40:40,110 --> 00:40:41,850
actually or

639
00:40:41,860 --> 00:40:43,430
all motor imagery

640
00:40:43,460 --> 00:40:45,070
so because

641
00:40:45,110 --> 00:40:48,780
for places on the dressed there's neither rhythm over the

642
00:40:48,800 --> 00:40:54,240
of the motor cortex which is naturalized if i move it to its attenuated and

643
00:40:54,240 --> 00:40:57,170
the same thing is actually happening

644
00:40:57,200 --> 00:41:01,160
if i'm just imagining this

645
00:41:03,250 --> 00:41:04,610
here's one

646
00:41:04,630 --> 00:41:05,980
view of

647
00:41:06,020 --> 00:41:08,740
the problem with variance

648
00:41:08,750 --> 00:41:11,860
we we see many single trials

649
00:41:12,730 --> 00:41:13,470
i think

650
00:41:13,490 --> 00:41:19,040
fall of it's clear what the difference between single trial and averaging as

651
00:41:19,050 --> 00:41:22,720
i mean if you if it's not clear to you then you can look at

652
00:41:22,720 --> 00:41:25,320
the slide

653
00:41:25,370 --> 00:41:27,700
the solid lines are the averages

654
00:41:28,500 --> 00:41:30,750
not so solid lines

655
00:41:30,750 --> 00:41:33,630
the single trials

656
00:41:33,670 --> 00:41:38,200
if you look at this from the math perspective

657
00:41:38,220 --> 00:41:39,500
and to you

658
00:41:39,510 --> 00:41:41,670
i had an explanation from god will

659
00:41:42,620 --> 00:41:47,360
you can actually see so you

660
00:41:47,400 --> 00:41:48,870
if you imagine

661
00:41:48,910 --> 00:41:53,230
lieutenant then there should be some activity on the right hemisphere

662
00:41:53,270 --> 00:41:57,180
if you look at this on several different try as you can see that there

663
00:41:57,200 --> 00:41:59,720
is some considerable variation in this

664
00:41:59,800 --> 00:42:04,140
this is one subject imagining across different single trial

665
00:42:04,230 --> 00:42:09,540
this is right and left imagination so it's it's

666
00:42:09,600 --> 00:42:10,460
why did

667
00:42:10,490 --> 00:42:12,870
bit of the challenge

668
00:42:12,870 --> 00:42:15,250
and this is

669
00:42:15,250 --> 00:42:17,040
some more

670
00:42:17,050 --> 00:42:19,230
subjects but now

671
00:42:19,250 --> 00:42:20,970
on average

672
00:42:20,980 --> 00:42:22,750
over many trials

673
00:42:23,480 --> 00:42:27,490
you know all these two plots on one subject and you can see that no

674
00:42:27,510 --> 00:42:32,390
subjects are like and they all imagining left versus right

675
00:42:33,280 --> 00:42:36,000
i mean of course the conclusion from that is you have to deal with the

676
00:42:36,000 --> 00:42:39,340
variability and you have to to use machine learning for that

677
00:42:39,360 --> 00:42:43,540
because its primary tool that we have in our hands

678
00:42:45,370 --> 00:42:47,250
and just

679
00:42:48,380 --> 00:42:52,850
to just spell out what what we do

680
00:42:52,870 --> 00:42:57,250
first of all we have to tell the machine learning algorithm what what the brain

681
00:42:57,250 --> 00:42:58,270
is doing

682
00:42:58,270 --> 00:43:01,730
if you

683
00:44:07,430 --> 00:44:11,090
we do

684
00:45:45,520 --> 00:45:56,560
right here

685
00:46:23,090 --> 00:46:26,350
and we

686
00:46:48,320 --> 00:46:53,190
these are

687
00:47:58,550 --> 00:48:03,340
the rule

688
00:48:08,340 --> 00:48:18,300
or maybe that is not that there are

689
00:49:19,490 --> 00:49:27,480
well i

690
00:50:33,980 --> 00:50:36,020
all right

691
00:50:36,040 --> 00:50:38,090
o he

692
00:51:40,520 --> 00:51:44,870
OK a

693
00:52:12,160 --> 00:52:18,760
thank you very much

694
00:52:18,930 --> 00:52:32,420
they are

695
00:52:39,020 --> 00:52:40,340
or at

696
00:52:49,160 --> 00:52:50,130
you know

697
00:53:21,880 --> 00:53:24,590
so the

698
00:53:24,600 --> 00:53:26,410
all that

699
00:53:26,410 --> 00:53:28,200
so far

700
00:53:28,210 --> 00:53:32,410
we've seen a new representation called markov logic

701
00:53:32,630 --> 00:53:36,180
it's very nice because a it's very simple

702
00:53:36,220 --> 00:53:40,870
markov logic is just first-order logic with what's on the formulas there's almost nothing to

703
00:53:40,870 --> 00:53:43,090
learn it in the first order logic

704
00:53:43,210 --> 00:53:47,590
and it's also very powerful because a as first order logic is a special case

705
00:53:47,590 --> 00:53:48,700
as we just saw

706
00:53:48,710 --> 00:53:52,770
it also has most of the graphical models that people use a special cases so

707
00:53:52,770 --> 00:53:55,890
it's both very simple and very general and that's nice

708
00:53:55,910 --> 00:54:00,570
but without efficient and effective inference and learning algorithms this you know is only of

709
00:54:00,570 --> 00:54:04,360
theoretical interest so let's start by looking at the inference problem

710
00:54:04,380 --> 00:54:07,610
and again here what i'm going to do is i'm going to start with logic

711
00:54:07,640 --> 00:54:11,880
the kinds of you know things that techniques that used for inference in logic and

712
00:54:11,970 --> 00:54:16,360
and see how we can extend to do probabilistic inference

713
00:54:16,380 --> 00:54:21,070
and in logic because as i mentioned people to tell you one as entailment queries

714
00:54:21,510 --> 00:54:24,310
there's this clearly follow from this knowledge base

715
00:54:24,320 --> 00:54:27,990
and this is usually done using theorem proving

716
00:54:28,000 --> 00:54:33,490
the improving is one of the most intensively studied problems on earth but most theorem

717
00:54:33,490 --> 00:54:36,350
provers follow the following scheme

718
00:54:36,360 --> 00:54:39,960
so i'm given a query in a knowledge based on what i do is i

719
00:54:39,960 --> 00:54:43,550
negate the query and i added to the knowledge base

720
00:54:43,580 --> 00:54:46,710
and then i converted that to CNF

721
00:54:46,720 --> 00:54:50,770
and then i check whether the resulting CNF is satisfiable

722
00:54:50,790 --> 00:54:55,660
satisfiable means that there is at least one world one assignment of truth values to

723
00:54:55,660 --> 00:54:57,190
the atoms that makes

724
00:54:57,190 --> 00:55:00,440
all the formulas in the knowledge base true

725
00:55:00,460 --> 00:55:05,490
if there is no such a satisfying assignment that means that the negation of the

726
00:55:05,490 --> 00:55:10,250
query is incompatible with the knowledge base and therefore the query does fall from the

727
00:55:10,250 --> 00:55:11,330
knowledge base

728
00:55:11,360 --> 00:55:13,940
so this is what is called proof by refutation

729
00:55:13,990 --> 00:55:16,850
and you know most theorem provers work in this way

730
00:55:16,860 --> 00:55:21,140
notice that all the hard work is being done by by this call you to

731
00:55:21,140 --> 00:55:22,130
say that

732
00:55:22,830 --> 00:55:26,240
so that is the function that i give it CNF and it tells me where

733
00:55:26,240 --> 00:55:27,860
there is satisfiable or not

734
00:55:28,390 --> 00:55:32,790
and of course that is that when you know the quite essential NP complete problem

735
00:55:32,790 --> 00:55:37,000
so there's no guarantee there is no way to solve it efficiently guarantees but there's

736
00:55:37,000 --> 00:55:40,880
a lot of very sophisticated technology for solving SAT

737
00:55:40,900 --> 00:55:45,380
and the most widely used technique for solving SAT is what's called the DPLL algorithm

738
00:55:45,420 --> 00:55:49,730
which there are many variations DPLL is is short for the

739
00:55:49,750 --> 00:55:53,120
names of the authors davis putnam large man and loveland

740
00:55:53,160 --> 00:55:54,750
here's how it works

741
00:55:54,790 --> 00:55:58,440
and this is important to note because this is what we're going to be building

742
00:55:58,470 --> 00:56:02,790
so the the bare bones of DPLL is as follows

743
00:56:02,840 --> 00:56:04,780
you give me a CNF

744
00:56:04,790 --> 00:56:09,060
and if CNF is empty i just return true because there are no conditions so

745
00:56:09,060 --> 00:56:11,470
any state satisfies the CNN

746
00:56:11,480 --> 00:56:14,670
on the other hand if CNF contains the empty clause

747
00:56:14,690 --> 00:56:16,810
then i return false

748
00:56:16,820 --> 00:56:20,220
because it means that there is no way to satisfy the class so there's going

749
00:56:20,220 --> 00:56:22,130
to be one unsatisfied clause

750
00:56:22,150 --> 00:56:25,150
so these are the basic cases of the recursion

751
00:56:25,160 --> 00:56:26,650
now here's the recursion

752
00:56:26,660 --> 00:56:29,670
the recursion is choose an atom a

753
00:56:29,720 --> 00:56:34,060
and there's very plus various possible heuristics which is in the attic and then what

754
00:56:34,060 --> 00:56:37,790
i need to do is i'm going to set the atom to true

755
00:56:37,810 --> 00:56:39,170
in the CNS

756
00:56:39,190 --> 00:56:41,200
simplify the CMFA

757
00:56:41,220 --> 00:56:42,730
and also

758
00:56:42,750 --> 00:56:44,940
with the simplified formula

759
00:56:45,000 --> 00:56:48,430
and if that returns true then i know that there a satisfying solution and a

760
00:56:48,430 --> 00:56:49,530
return true

761
00:56:49,540 --> 00:56:52,840
if that fails then i said it to false

762
00:56:52,880 --> 00:56:55,900
simplify the CNF and calls again

763
00:56:55,940 --> 00:56:58,950
and again if that is true then i know that this is true if both

764
00:56:58,950 --> 00:57:02,760
of them false then i know that this CNF is unsatisfiable

765
00:57:02,820 --> 00:57:07,600
so this is the basic algorithm that we use for satisfiability testing

766
00:57:07,620 --> 00:57:11,590
now what we saw is just the propositional case but in general we want to

767
00:57:11,590 --> 00:57:13,880
do inference in first order logic

768
00:57:14,030 --> 00:57:16,040
so how can we do that

769
00:57:16,070 --> 00:57:21,200
well there's two basic ways the first and most obvious one is propositionalisation

770
00:57:21,220 --> 00:57:23,530
as long as we have a finite domain

771
00:57:23,540 --> 00:57:27,130
i can just form on the ground atoms by you know putting all the classes

772
00:57:27,130 --> 00:57:32,910
into all the predicates arguments have large propositional problem and i just apply propositional theorem

773
00:57:32,910 --> 00:57:35,920
prover you know i use DPLL or something like that

774
00:57:35,920 --> 00:57:41,610
so it in the integral from minus infinity to plus infinity video

775
00:57:42,880 --> 00:57:44,510
multiply by

776
00:57:44,540 --> 00:57:47,630
he was powell minus omega

777
00:57:48,950 --> 00:57:50,310
in the world

778
00:57:50,320 --> 00:57:52,060
joseph the following

779
00:57:52,380 --> 00:57:55,320
at the same point in time so he sought to more hope it will be

780
00:57:55,500 --> 00:57:57,040
to be so

781
00:57:57,060 --> 00:58:01,650
so that's

782
00:58:01,700 --> 00:58:04,890
what will happen with this

783
00:58:04,900 --> 00:58:08,120
when you when you look at the atelier of

784
00:58:08,140 --> 00:58:09,240
at all

785
00:58:09,270 --> 00:58:15,200
toilet twenty that so let's take something which which is the building which is invaluable

786
00:58:15,230 --> 00:58:21,190
so there's something which receive light you some electrical signals from it

787
00:58:21,240 --> 00:58:25,480
and the motivation of this they are

788
00:58:25,490 --> 00:58:27,120
it's just

789
00:58:27,140 --> 00:58:32,250
account for each time you receive like you will have account been there is

790
00:58:32,270 --> 00:58:33,800
as the capacitance

791
00:58:36,960 --> 00:58:40,060
what you want to get to know to get votes about

792
00:58:40,300 --> 00:58:43,740
as an output so that you can manage things that

793
00:58:43,750 --> 00:58:47,960
a very simple thing to do is to is to use the US

794
00:58:48,000 --> 00:58:52,440
you want to voltage converters neural systems so let's say you put stuff

795
00:58:52,440 --> 00:58:56,360
molecule with still to it

796
00:58:56,380 --> 00:58:57,680
and then you can

797
00:58:57,690 --> 00:58:59,810
we can wonder what is

798
00:59:00,050 --> 00:59:02,200
what is the speed at which the interior

799
00:59:02,210 --> 00:59:03,310
it is

800
00:59:03,320 --> 00:59:05,120
none of the formulas are going out

801
00:59:05,120 --> 00:59:07,550
and you will find that

802
00:59:08,180 --> 00:59:13,050
the output of that in the frequency domain is is the function of the

803
00:59:14,500 --> 00:59:20,070
i supplied by the by the capacitor is supplied by the capacitor is called the

804
00:59:20,220 --> 00:59:22,730
fall of the function

805
00:59:22,760 --> 00:59:30,470
so then we have to

806
00:59:30,490 --> 00:59:34,460
two were but which is that we take the magnitude of this this

807
00:59:35,050 --> 00:59:39,530
concept function and reproduction in the log

808
00:59:41,010 --> 00:59:45,710
and then you will see that in fundamental frequency what will be the way would

809
00:59:46,140 --> 00:59:49,500
polls for function which is just silly

810
00:59:51,330 --> 00:59:55,420
below the so-called we have defined as a product of

811
00:59:55,450 --> 00:59:57,860
but still lives in the first round

812
00:59:57,880 --> 01:00:01,260
you've got the one molecule and after that

813
01:00:01,320 --> 01:00:08,320
he starts to decrease so about not just an asymptotic representation of it which is

814
01:00:08,370 --> 01:00:10,350
just quite line

815
01:00:10,460 --> 01:00:15,530
just assume that to support your local public goods

816
01:00:15,540 --> 01:00:20,860
and you have got a three DVD films between the maximum blood on the the

817
01:00:20,860 --> 01:00:25,090
real thing which is that when you are just before

818
01:00:25,100 --> 01:00:29,790
so the first thing that is in some ways with frequency and when you are

819
01:00:29,820 --> 01:00:35,650
the the phase of the existing nodes is moving by minus forty five degrees

820
01:00:37,030 --> 01:00:41,130
if you put to sleep on this particular frequency input

821
01:00:41,750 --> 01:00:46,230
what we have chosen this frequency is is about a hundred and sixty kilometers so

822
01:00:46,250 --> 01:00:48,320
you put fifty input

823
01:00:49,860 --> 01:00:53,500
you put a hundred and eighty two that time you see that you start to

824
01:00:53,500 --> 01:00:56,880
lose some of the should because because you start to be

825
01:00:56,890 --> 01:00:58,250
on this

826
01:00:58,290 --> 01:00:59,530
in this region

827
01:00:59,550 --> 01:01:01,860
and if you go higher

828
01:01:01,880 --> 01:01:07,260
which is a five hundred kilowatt you've got almost nothing because you used to be

829
01:01:07,300 --> 01:01:08,420
a little

830
01:01:08,500 --> 01:01:11,740
law and in this

831
01:01:11,750 --> 01:01:15,160
so the best response in if you

832
01:01:17,330 --> 01:01:19,640
i replied to fill that gap

833
01:01:19,750 --> 01:01:22,550
one of the system which is what is the time response of the system you

834
01:01:22,550 --> 01:01:24,070
just have to make the

835
01:01:24,120 --> 01:01:26,070
four you from

836
01:01:26,090 --> 01:01:30,420
this this is a new we define an exponential

837
01:01:30,450 --> 01:01:34,250
since you put been in the input of the cell c and this is what

838
01:01:34,250 --> 01:01:35,350
you get

839
01:01:35,450 --> 01:01:38,080
it goes up and then it goes on this

840
01:01:38,090 --> 01:01:42,230
ossie osborn time which is one of possible

841
01:01:42,250 --> 01:01:44,730
and then if you put those

842
01:01:44,740 --> 01:01:48,900
so in fact but so you have at the input something which is way

843
01:01:48,990 --> 01:01:53,360
the instead of the following day is input it would be

844
01:01:53,400 --> 01:01:54,900
it would be

845
01:01:55,290 --> 01:02:01,250
slaughter on the same for going down so that comes from the idea but this

846
01:02:01,550 --> 01:02:03,730
is the that is about two point two

847
01:02:03,760 --> 01:02:08,120
the bank and in this case it to point to make poster

848
01:02:08,210 --> 01:02:10,290
and of course

849
01:02:10,900 --> 01:02:15,180
with this kind of things we can only go ontology dlb festivals when the PIN

850
01:02:15,180 --> 01:02:20,250
diode was given to be able to get to get involved so it means that

851
01:02:20,260 --> 01:02:22,520
we are far away from what we want

852
01:02:22,540 --> 01:02:26,000
it means that a signal still is not good

853
01:02:26,500 --> 01:02:29,250
the you don't have to

854
01:02:29,290 --> 01:02:33,860
after the second i know even if you want something working properly then if you

855
01:02:33,870 --> 01:02:38,050
stand on the on the optical links on a succession of ore one of the

856
01:02:38,070 --> 01:02:40,770
and you will see something like that

857
01:02:40,780 --> 01:02:46,610
and this is called an eye under the good a good quantities of quality of

858
01:02:47,210 --> 01:02:49,110
a competition like this

859
01:02:49,120 --> 01:02:53,580
is given by the opening of this

860
01:02:53,600 --> 01:02:57,240
so feedback

861
01:02:57,250 --> 01:03:01,100
that's the bottom some formal education because the next step would be to to amplify

862
01:03:01,100 --> 01:03:05,780
so here is it since been you've got an input and each time you

863
01:03:05,790 --> 01:03:08,120
some about

864
01:03:08,140 --> 01:03:11,050
so from two weeks you've got

865
01:03:13,840 --> 01:03:17,430
thanks to what you want to go to the petition as well and then

866
01:03:17,450 --> 01:03:18,780
if i just your

867
01:03:18,790 --> 01:03:21,420
no it's not a course title

868
01:03:21,430 --> 01:03:24,140
good you point here which is just

869
01:03:24,160 --> 01:03:26,300
the multiplication

870
01:03:26,320 --> 01:03:30,920
the public goods input with all this he felt about

871
01:03:31,880 --> 01:03:35,120
it did

872
01:03:35,130 --> 01:03:41,380
we we close it then it means that we get from the output a function

873
01:03:41,380 --> 01:03:46,000
of the output on the input and we make found so the equation because lies

874
01:03:46,000 --> 01:03:51,080
at this point six is of course the edition of the input but that that

875
01:03:51,080 --> 01:03:55,050
but also with us if you remove a fraction of the output and then if

876
01:03:55,050 --> 01:03:59,270
you compute everything you find something like that which is

877
01:03:59,290 --> 01:04:01,990
this is the feedback

878
01:04:02,000 --> 01:04:09,280
what what does the feedback is that the electronics for this new is just an

879
01:04:11,000 --> 01:04:12,410
to be done

880
01:04:12,430 --> 01:04:14,270
is the feedback loop

881
01:04:14,270 --> 01:04:19,060
of these three signals and one signal is sitting here one zero zero one is

882
01:04:19,060 --> 01:04:23,590
sitting in zero one zero one is sitting in zero zero one

883
01:04:23,630 --> 01:04:27,930
even i can calculate the mean of those three it's one thirty one thirty one

884
01:04:29,170 --> 01:04:33,810
so you calculate the energy in one thirty one thirty one third and three times

885
01:04:33,810 --> 01:04:35,980
one nine four one third

886
01:04:35,990 --> 01:04:42,860
which is exactly what this a free energy differences between orthogonal and simplex

887
01:04:42,910 --> 01:04:46,900
is one minus one over in other words that the factor

888
01:04:46,920 --> 01:04:53,120
and energy that you lose by using orthogonal codes instead of simplex code

889
01:04:53,130 --> 01:04:56,360
why do people every is orthogonal codes

890
01:04:56,380 --> 01:05:00,610
if it's just pure loss in energy in doing so

891
01:05:00,630 --> 01:05:04,250
one reason is when n gets up to be six to eight

892
01:05:04,270 --> 01:05:07,090
it doesn't amount to a whole lot

893
01:05:07,140 --> 01:05:11,870
and the other reason is

894
01:05:11,890 --> 01:05:14,950
if you look at

895
01:05:14,970 --> 01:05:19,890
if you look in modulating these things and assign weights and things like that

896
01:05:19,900 --> 01:05:22,010
you you suddenly say

897
01:05:22,480 --> 01:05:24,640
the one you're using this

898
01:05:24,850 --> 01:05:29,110
it becomes easier to keep frequency locking phase lock

899
01:05:29,130 --> 01:05:31,320
and it does when you use this

900
01:05:31,330 --> 01:05:34,210
we have to think about that argument a little bit to make sense out of

901
01:05:34,210 --> 01:05:39,590
it but that in fact is what people often use orthogonal signals

902
01:05:39,610 --> 01:05:43,690
because in fact they can recover other things from it

903
01:05:43,750 --> 01:05:46,760
well because they're are actually sending the mean also

904
01:05:46,770 --> 01:05:50,360
and the mean is the thing that lets them recover all these other neat things

905
01:05:51,260 --> 01:05:55,110
so that sort of

906
01:05:55,120 --> 01:05:59,380
sometimes rules out this and sometimes rules that

907
01:05:59,420 --> 01:06:03,510
OK orthogonal by orthogonal codes have the same energy

908
01:06:03,540 --> 01:06:05,070
well look at them

909
01:06:05,820 --> 01:06:09,930
these cues signal points each have the same energy and

910
01:06:09,950 --> 01:06:13,110
these two have the same energy is these two

911
01:06:13,120 --> 01:06:14,870
so the average energy

912
01:06:14,920 --> 01:06:18,940
is the same as the energy of each point so this and this have the

913
01:06:18,940 --> 01:06:20,410
same energy

914
01:06:20,440 --> 01:06:24,880
what happens to the probability of error

915
01:06:24,900 --> 01:06:27,520
we can evaluate it exactly

916
01:06:28,570 --> 01:06:32,680
except here's the case we're just looking at the end of two case gives you

917
01:06:32,680 --> 01:06:34,240
the right answer

918
01:06:34,300 --> 01:06:36,940
i mean here to make an error you have to go

919
01:06:36,970 --> 01:06:39,170
across that boundary there

920
01:06:39,190 --> 01:06:44,140
here to make an error after either go cross that boundary we're go across that

921
01:06:45,110 --> 01:06:48,380
their probability essentially goes up by a factor of two

922
01:06:48,410 --> 01:06:50,270
same thing happens here

923
01:06:50,290 --> 01:06:53,910
you just get twice as many ways to make errors carriages you had before

924
01:06:53,970 --> 01:06:57,940
and all of these ways to make errors are equally probable

925
01:06:58,060 --> 01:07:03,110
all the points are equally distant from each other so you essentially just double the

926
01:07:03,110 --> 01:07:08,650
number of likely ways you can make errors and their probability essentially goes up by

927
01:07:09,630 --> 01:07:11,150
because up a little

928
01:07:11,160 --> 01:07:17,070
well because the you think about goes up a little more a little less than

929
01:07:18,570 --> 01:07:23,350
it's it's almost

930
01:07:23,370 --> 01:07:29,890
OK so i want to actually find the probability of error now

931
01:07:29,940 --> 01:07:32,900
if you're sending an orthogonal

932
01:07:33,800 --> 01:07:40,160
namely we take an orthogonal codes where we pick as many codewords we want to

933
01:07:40,170 --> 01:07:44,800
and might be sixty four might be a hundred and twenty eight whatever

934
01:07:46,300 --> 01:07:47,570
we want to

935
01:07:47,580 --> 01:07:49,940
try to figure out

936
01:07:49,960 --> 01:07:51,660
how to evaluate

937
01:07:51,680 --> 01:07:57,210
the probability of error for this kind of code after you face the fact

938
01:07:57,230 --> 01:07:58,460
that in fact

939
01:07:58,470 --> 01:08:03,660
these lines are not orthogonal to each other

940
01:08:03,660 --> 01:08:06,150
OK well where you do this

941
01:08:06,150 --> 01:08:07,870
as you say OK

942
01:08:08,760 --> 01:08:10,040
even though

943
01:08:10,040 --> 01:08:13,430
even though this is a slightly messy problem

944
01:08:14,170 --> 01:08:16,060
clear from symmetry

945
01:08:16,060 --> 01:08:20,350
but you get the same error probability no matter which signal point you said

946
01:08:20,370 --> 01:08:25,460
namely every single point is exactly the same as every other signal point which you

947
01:08:25,460 --> 01:08:26,950
call the first signal

948
01:08:26,980 --> 01:08:29,760
it depends only on which you happen to call

949
01:08:29,770 --> 01:08:34,590
the first author normal direction whether it's this or this or this

950
01:08:34,590 --> 01:08:38,120
i can i can change it in any way i want to win the problem

951
01:08:38,120 --> 01:08:40,490
is still exactly the same

952
01:08:40,520 --> 01:08:45,330
OK so what i'm going to do is try to find their probability when i

953
01:08:45,330 --> 01:08:47,410
say this signal here

954
01:08:47,420 --> 01:08:49,930
well over twice and one zero zero

955
01:08:49,940 --> 01:08:55,560
actually understand the square root of a zero zero because i want to talk about

956
01:08:55,560 --> 01:09:00,350
the energy here why my torturing you with this

957
01:09:00,360 --> 01:09:04,330
well fifty years ago fifty five years ago shannon came out with this

958
01:09:04,340 --> 01:09:05,920
smartalice paper

959
01:09:05,980 --> 01:09:09,470
which said there's something called channel capacity

960
01:09:09,490 --> 01:09:14,600
and what he said channel capacity was was the minimum rate

961
01:09:14,610 --> 01:09:18,960
he was the maximum radius which you could transmit on channel

962
01:09:19,020 --> 01:09:22,440
and still get zero error probability

963
01:09:22,460 --> 01:09:26,160
sort of the simplest and most famous case of that

964
01:09:26,170 --> 01:09:27,420
is where you have

965
01:09:27,440 --> 01:09:32,900
white cassino ways to deal with your trying to transmit through this way cassie always

966
01:09:32,900 --> 01:09:36,690
and you can use as much bandwidth as you want

967
01:09:36,710 --> 01:09:40,360
namely you can spread the signals out as much as you want to you can

968
01:09:40,360 --> 01:09:43,950
sort of see from starting to look at this picture

969
01:09:43,960 --> 01:09:47,380
but you're going to be a little better off

970
01:09:47,380 --> 01:09:48,910
for example

971
01:09:48,970 --> 01:09:51,130
if you want to send one bit

972
01:09:52,060 --> 01:09:55,340
in these two dimensions here with orthogonal signals

973
01:09:55,350 --> 01:09:59,360
if you can think of what happens down here for m equals four

974
01:09:59,490 --> 01:10:03,080
you would wind up with four orthogonal signals

975
01:10:03,120 --> 01:10:06,060
and if you want that was for orthogonal signals

976
01:10:06,070 --> 01:10:07,660
you're sending two bits

977
01:10:07,690 --> 01:10:11,120
so you can use twice as much energy for each of them so you can

978
01:10:11,120 --> 01:10:13,010
scale each of these up

979
01:10:13,020 --> 01:10:13,830
to be

980
01:10:16,440 --> 01:10:18,420
two zero zero zero

981
01:10:20,190 --> 01:10:24,660
two zero zero and so forth are sending twice as much energy

982
01:10:24,670 --> 01:10:28,760
you're filling out more bandwith because you need more degrees of freedom to send the

983
01:10:30,450 --> 01:10:35,920
but who cares because we have all been what we want to get more complicated

984
01:10:35,930 --> 01:10:40,170
but the question is what happens if we go to a very large set of

985
01:10:40,170 --> 01:10:41,880
orthogonal signals

986
01:10:41,920 --> 01:10:44,580
and what we're going to find is that when we go to a very large

987
01:10:44,580 --> 01:10:46,620
set of orthogonal signals

988
01:10:46,950 --> 01:10:50,270
we can get the error probability which goes to zero

989
01:10:50,300 --> 01:10:54,160
as the number of orthogonal signals gets bigger

990
01:10:54,180 --> 01:10:59,080
it goes to zero very fast we sent more bits with each signal

991
01:10:59,090 --> 01:11:04,620
and the place where it goes to zero is exactly channel capacity

992
01:11:04,670 --> 01:11:09,760
now in your homework you're going to work out a simpler version of all this

993
01:11:09,960 --> 01:11:13,380
in the simpler version is something called the union bound

994
01:11:13,390 --> 01:11:15,900
and in the union venue just

995
01:11:15,910 --> 01:11:19,040
the probability of error when you send this

996
01:11:19,040 --> 01:11:21,220
in fact those are exactly the ones you throw away

997
01:11:21,270 --> 01:11:24,910
they are just written out in more general expression for this your if you're interested

998
01:11:24,910 --> 01:11:27,390
in the function of only some of your variables

999
01:11:27,400 --> 01:11:28,230
you can

1000
01:11:28,260 --> 01:11:31,870
rewrite this in a way i don't think OK three because i don't think will

1001
01:11:31,870 --> 01:11:35,030
be productive you end up averaging

1002
01:11:35,050 --> 01:11:37,320
something that you can analytically some over

1003
01:11:37,320 --> 01:11:39,680
on the samples of

1004
01:11:39,710 --> 01:11:44,710
most of your variables are the ones are interested in train

1005
01:11:44,730 --> 01:11:47,670
the row blackwell theorem would tell us that there is always a good thing to

1006
01:11:47,670 --> 01:11:52,890
do if it applied which doesn't but empirically it works incredibly well it's often a

1007
01:11:52,890 --> 01:11:59,430
good idea

1008
01:12:03,410 --> 01:12:05,070
so far

1009
01:12:05,090 --> 01:12:08,200
i've said you've got this fixed distribution you're interested in you might

1010
01:12:08,230 --> 01:12:12,910
debate which variables you want to keep control when constructing estimated that

1011
01:12:12,950 --> 01:12:16,920
it's fairly straightforward what we do is going to write down a joint probability

1012
01:12:16,920 --> 01:12:20,210
evaluated up to a constant and is one of the standard algorithms might might be

1013
01:12:20,210 --> 01:12:23,430
a bit messy to apply them but there are these recommendations in the papers of

1014
01:12:23,430 --> 01:12:27,240
how to check that giving sensible results

1015
01:12:28,620 --> 01:12:31,350
this is where a lot of money colour courses well and and you get the

1016
01:12:31,350 --> 01:12:33,170
idea that

1017
01:12:33,190 --> 01:12:37,280
advanced monte carlo methods are just going to be about finding good q say

1018
01:12:37,320 --> 01:12:40,730
you want to make a monte carlo estimate better you just need to make better

1019
01:12:40,730 --> 01:12:45,250
proposal for around the space better and that's sure

1020
01:12:45,270 --> 01:12:48,230
and i don't want to disparage that line of research is very important so if

1021
01:12:48,230 --> 01:12:50,240
you're interested in a particular application

1022
01:12:50,290 --> 01:12:53,860
you can have all sorts of domain knowledge about the application so you can go

1023
01:12:54,520 --> 01:12:59,390
and work out good ways of moving around your state space in your problem i

1024
01:12:59,390 --> 01:13:01,510
think is into metropolis hastings

1025
01:13:01,620 --> 01:13:05,170
in it for the rest of the lectures talk about more generic tricks that don't

1026
01:13:05,170 --> 01:13:10,680
involve specific applications and they're going to solve the

1027
01:13:10,690 --> 01:13:12,700
move away from this very standard

1028
01:13:12,720 --> 01:13:18,880
setting up one fixed distribution with one set of variables

1029
01:13:18,890 --> 01:13:22,280
so the first the first chunk of this is all going to be about what's

1030
01:13:22,280 --> 01:13:24,270
called auxiliary variable methods

1031
01:13:25,410 --> 01:13:27,600
this is kind of counterintuitive so

1032
01:13:27,650 --> 01:13:31,760
why are we doing and seems well to some of the things that we're finding

1033
01:13:31,760 --> 01:13:36,090
it difficult to some and maybe if you could sum over some of those variables

1034
01:13:36,090 --> 01:13:39,140
analytically you should do that first reduce noise

1035
01:13:39,150 --> 01:13:40,710
o thirty variable say

1036
01:13:40,730 --> 01:13:44,740
well why don't we just make a problem because sentence something over just the variables

1037
01:13:44,740 --> 01:13:50,520
x let's introducing new variables v and some over days from over joint distribution of

1038
01:13:50,540 --> 01:13:54,480
the variables are interested in and more variables

1039
01:13:55,460 --> 01:13:58,190
this here is an identity and it looks as well

1040
01:13:58,230 --> 01:14:01,930
we can sort out the these analytically because we can push the integral of to

1041
01:14:01,930 --> 01:14:04,410
be through and get rid of them to get back to his that's what we

1042
01:14:04,410 --> 01:14:07,280
should do but in fact

1043
01:14:07,330 --> 01:14:10,090
sometimes it's a good idea too

1044
01:14:10,110 --> 01:14:14,380
deliberately introduce more often not some that analytically but sample

1045
01:14:15,410 --> 01:14:20,440
your variables and some extra variables to before you start estimator

1046
01:14:22,120 --> 01:14:25,690
to me that seem like a very strange thing today until you see the algorithms

1047
01:14:25,690 --> 01:14:30,130
and sort of see what they achieve and then it certainly seems more appealing to

1048
01:14:30,130 --> 01:14:32,010
the first of these algorithms

1049
01:14:32,070 --> 01:14:37,380
it was called when someone edits an algorithm for spin system so called using models

1050
01:14:37,380 --> 01:14:43,050
of sometimes boltzmann machines or or logistic models these models have a lot of names

1051
01:14:45,190 --> 01:14:49,430
a way of updating expensive to say

1052
01:14:49,450 --> 01:14:52,830
here the data has been system has two colours red and blue and they could

1053
01:14:52,830 --> 01:14:54,640
be binary variables one zero

1054
01:14:54,640 --> 01:14:58,780
and the model so that adjacent colours like to be the same so under drawn

1055
01:14:58,780 --> 01:15:01,960
from the distribution you get regions tend to be

1056
01:15:01,980 --> 01:15:03,540
the same colour

1057
01:15:03,590 --> 01:15:04,890
so this is often

1058
01:15:04,910 --> 01:15:09,210
embedded somewhere in some some sort of model one continuity in space

1059
01:15:11,920 --> 01:15:16,180
very very heavily research and statistical physics

1060
01:15:16,190 --> 01:15:19,010
let's imagine doing gibbs sampling on

1061
01:15:19,040 --> 01:15:23,210
a system like this is a gibbs sampling would go through each of the spins

1062
01:15:23,330 --> 01:15:26,380
tendency for this one then i'm going to look at my neighbours and see how

1063
01:15:26,380 --> 01:15:28,850
many of them blue and how many of the red and then i'm going to

1064
01:15:28,850 --> 01:15:31,700
decide whether to people who already and if i more blue neighbours i'm going to

1065
01:15:31,700 --> 01:15:33,200
tend to be people

1066
01:15:35,500 --> 01:15:38,960
i get something sweet would involve going to every one of these variables in turn

1067
01:15:38,960 --> 01:15:40,550
and doing flat

1068
01:15:40,640 --> 01:15:45,740
and if the coupling strength between adjacent sites is very strong then it's very hard

1069
01:15:45,740 --> 01:15:49,420
to give something to make a big change the updating any of the variables in

1070
01:15:50,000 --> 01:15:53,630
it doesn't want to change from being played because that would be very probable so

1071
01:15:53,630 --> 01:15:57,070
it's only really able to change the the variables from the edge with some of

1072
01:15:57,070 --> 01:15:59,690
the labels the blue and some of them are so if you do we get

1073
01:15:59,690 --> 01:16:01,190
something sweet through this

1074
01:16:01,240 --> 01:16:04,090
just the boundaries are going to shift by about one

1075
01:16:04,090 --> 01:16:07,220
and this picture would just like ever so slightly different it would just

1076
01:16:07,240 --> 01:16:13,080
the band drift slowly by random walk backwards and forwards

1077
01:16:13,140 --> 01:16:16,620
this went along algorithm which are not going to drive here

1078
01:16:16,630 --> 01:16:19,930
as light of extra variables which are

1079
01:16:19,990 --> 01:16:21,430
depicted by the black

1080
01:16:21,430 --> 01:16:25,770
grid it as a binary variable linking site

1081
01:16:25,790 --> 01:16:30,610
on on the graph so for every edge in the graph as an extra variable

1082
01:16:30,640 --> 01:16:33,960
to add more variables and you actually had to start with

1083
01:16:33,990 --> 01:16:37,740
and the reason it does this is the

1084
01:16:37,790 --> 01:16:42,850
once you've got this big system of colours and the edges it's possible to derive

1085
01:16:42,910 --> 01:16:47,200
a markov chain update where you can move around the whole state space very rapidly

1086
01:16:47,840 --> 01:16:49,470
it puts in the edges

1087
01:16:49,490 --> 01:16:50,510
which links

1088
01:16:50,530 --> 01:16:54,600
together some of the knights you can imagine that there are clusters of nodes where

1089
01:16:54,600 --> 01:16:57,970
you can walk around the edges to get all of the nodes within a cluster

1090
01:16:58,160 --> 01:17:02,370
and you can't get all the nodes because there are no edges linking to

1091
01:17:02,410 --> 01:17:06,620
then it has an update where you can read clusters once it got set of

1092
01:17:06,620 --> 01:17:10,890
bonds it's able to move to a new state so this is one iteration is

1093
01:17:10,890 --> 01:17:11,980
when someone

1094
01:17:12,040 --> 01:17:15,730
and the coloring at the end it's completely different to the colouring beginning whereas give

1095
01:17:15,730 --> 01:17:19,460
something would have taken many many many sweets through the system in order to do

1096
01:17:19,460 --> 01:17:21,290
this diffusion

1097
01:17:21,370 --> 01:17:24,860
you haven't explained really how this works but this is

1098
01:17:24,910 --> 01:17:26,760
the first famous example

1099
01:17:26,780 --> 01:17:30,170
of an auxiliary variable method and you something you should know that in the back

1100
01:17:30,170 --> 01:17:33,730
of your head that sometimes people are able to introduce variables in a way that

1101
01:17:33,730 --> 01:17:36,450
will move around quickly

1102
01:17:36,460 --> 01:17:40,760
it's not that generally applicable it is in theory but in practice it's really only

1103
01:17:41,440 --> 01:17:43,270
successful and it's been systems

1104
01:17:43,280 --> 01:17:46,240
then to tell you about a simpler system which came later

1105
01:17:46,280 --> 01:17:48,750
which is very generally applicable

1106
01:17:48,760 --> 01:17:50,650
and it's only going to introduce one

1107
01:17:50,650 --> 01:17:52,170
extra variable

1108
01:17:52,180 --> 01:17:54,750
it turns out that you can derive it in exactly the same way as the

1109
01:17:54,750 --> 01:17:57,870
previous socks

1110
01:17:59,400 --> 01:18:03,580
one of the earliest slide said

1111
01:18:03,580 --> 01:18:07,840
the way you sample from a distribution is to draw samples from underneath the car

1112
01:18:07,880 --> 01:18:12,600
the drastic avenue just sample uniformly from under area pointed out that can be difficult

1113
01:18:12,610 --> 01:18:15,800
to do because it means you've got to compute integrals about

1114
01:18:15,830 --> 01:18:20,460
slice sampling says why don't we run the markov chain

1115
01:18:20,520 --> 01:18:24,240
that runs around underneath the earth the curve instead

1116
01:18:24,260 --> 01:18:27,780
so now we're not going to be something independently or like we did in rejection

1117
01:18:27,780 --> 01:18:32,010
sampling we still gonna be running around and because it's going to be a walk

1118
01:18:32,010 --> 01:18:33,840
the walks around under the curve

1119
01:18:33,850 --> 01:18:35,970
as samples will be dependent

1120
01:18:35,990 --> 01:18:40,220
this fly something to produce one extra variables we did we did have at position

1121
01:18:40,220 --> 01:18:43,520
x and i think space and now we've got to hide underneath the curve to

1122
01:18:43,520 --> 01:18:46,440
else to be around other applications this is not so

1123
01:18:48,870 --> 01:18:51,890
this is just to show you sort of visually the effect

1124
01:18:52,140 --> 01:18:56,350
so this says one regular sation this is out regular sation as we

1125
01:18:56,360 --> 01:18:58,940
vary the strength of the regulation parameter

1126
01:18:59,960 --> 01:19:03,500
of course in the end everything goes to zero the weights go to zero

1127
01:19:03,700 --> 01:19:07,310
but as you can see one regular sation certainly just go to zero

1128
01:19:07,320 --> 01:19:11,880
and stays zero okay and you can consider that the more important

1129
01:19:11,890 --> 01:19:14,840
variables are the ones that are lost the longer ok

1130
01:19:14,840 --> 01:19:18,080
whereas now to everything kind of goes to zero roughly the same

1131
01:19:18,080 --> 01:19:21,240
time and so it's harder to determine which of these things actually

1132
01:19:21,240 --> 01:19:28,110
has to be thrown ok so now what's the dacian view of this the

1133
01:19:28,110 --> 01:19:30,960
patient view is that in both of these cases we really what we're

1134
01:19:30,960 --> 01:19:33,730
dealing with is a prior distribution over hypotheses

1135
01:19:34,020 --> 01:19:38,590
ok and when the data comes in compute the posterior distribution

1136
01:19:39,230 --> 01:19:45,590
and the difference between al to and one is what kind of distribution

1137
01:19:45,600 --> 01:19:49,670
do we assume as prior ok so in the output case

1138
01:19:49,900 --> 01:19:52,530
we have circular gaussian that's the prior

1139
01:19:53,080 --> 01:19:56,810
and so when we compute the posterior we also obtain a gaussian

1140
01:19:56,820 --> 01:20:01,160
distribution ok and so for that reason it's actually very easy to

1141
01:20:01,170 --> 01:20:03,980
work with delta regular sation in many cases

1142
01:20:04,170 --> 01:20:07,720
and least all kinds of interesting algorithms gaussian processes

1143
01:20:07,730 --> 01:20:12,560
and so in the one case what we uses double-exponential prior

1144
01:20:12,870 --> 01:20:16,100
ok that corresponds to this kind of diamond shape the sort of

1145
01:20:17,080 --> 01:20:19,570
goes out much much quicker than gaussian

1146
01:20:20,940 --> 01:20:25,850
and so as a result you sort of put in a little bit different kind

1147
01:20:25,850 --> 01:20:29,270
of prior knowledge and to you're going to get a slightly different

1148
01:20:29,270 --> 01:20:33,960
hypotheses when you do your search this is something that is

1149
01:20:33,970 --> 01:20:38,940
under your control and the choices really guided by to criteria

1150
01:20:39,350 --> 01:20:44,960
one is the ease of the optimization because working with

1151
01:20:44,970 --> 01:20:49,310
for example with conjugate priors really makes things easy from

1152
01:20:49,310 --> 01:20:51,900
the point of view of solving the optimization problem

1153
01:20:51,900 --> 01:20:56,080
or sometimes choosing a sort of smoothness and the prior smoothness

1154
01:20:56,080 --> 01:20:59,420
and the error function makes a gradient based optimization easier

1155
01:20:59,420 --> 01:21:04,710
to deal with and then the other sort of important consideration is

1156
01:21:04,720 --> 01:21:07,260
what do we actually know about the problem

1157
01:21:07,770 --> 01:21:11,800
and so here it's sort of it's easy problems regression problems

1158
01:21:11,960 --> 01:21:14,120
but often on we deal with structured data

1159
01:21:14,280 --> 01:21:17,640
the prior actually reflects something that we know about the

1160
01:21:17,650 --> 01:21:21,490
hypothesis space ok for example for trying to infer gene networks

1161
01:21:21,490 --> 01:21:24,460
maybe we know that certain genes cannot be connected and so

1162
01:21:24,460 --> 01:21:26,150
we can build that into the prior

1163
01:21:29,240 --> 01:21:32,810
so this is just to give you a cartoon picture of this asian view

1164
01:21:32,820 --> 01:21:38,890
of regular sation ok and what we're going to do here is start

1165
01:21:39,050 --> 01:21:42,860
with a prior over the weight space this ok this is the picture

1166
01:21:42,870 --> 01:21:47,190
here with the red circle is a gaussian prior over the weight space

1167
01:21:47,580 --> 01:21:51,410
and it's sort of centered at zero okay

1168
01:21:52,080 --> 01:21:56,110
so in the axis of any data we can use this to draw weight vectors

1169
01:21:56,150 --> 01:21:58,950
ok and each weight vector is going to be a hypothesis

1170
01:21:59,000 --> 01:22:03,650
ok so here is red lines represent differ hypotheses that

1171
01:22:03,660 --> 01:22:06,890
we've drawn from the prior ok you consider all over the place

1172
01:22:07,190 --> 01:22:10,180
always say is we like the weights to be roughly small

1173
01:22:10,470 --> 01:22:13,940
ok but there's no rhyme or reason until you actually see

1174
01:22:14,140 --> 01:22:19,680
some data ok now to other hard to even over here on the right hand

1175
01:22:19,690 --> 01:22:23,370
side the second row we have a data point so little blue circle

1176
01:22:23,640 --> 01:22:29,400
ok and so on now what we do when we do the regular zation essentially

1177
01:22:29,410 --> 01:22:33,250
as we compute the posterior over the weight-space over the parameter

1178
01:22:33,260 --> 01:22:38,020
space and that is here says we would like to fit the data

1179
01:22:38,520 --> 01:22:42,030
ok so all the lines are encouraged to go through this point

1180
01:22:42,040 --> 01:22:45,950
to fit the point ok so compared to here where the lines were all

1181
01:22:45,950 --> 01:22:49,020
over the place here we have one there's all over the place but they

1182
01:22:49,020 --> 01:22:51,820
do fit the new point that we've just a lot

1183
01:22:52,440 --> 01:22:57,540
ok now what does that mean from the point of view

1184
01:22:57,890 --> 01:23:02,090
of the weights ok what it means is that

1185
01:23:02,420 --> 01:23:07,190
this gaussian it used to be round ok is no longer around because

1186
01:23:07,190 --> 01:23:10,700
we've taken the around prior when we combined it with the likelihood

1187
01:23:10,700 --> 01:23:15,120
of the data likelihood is here ok this sort of redlining

1188
01:23:15,520 --> 01:23:19,190
likelihood says all weights are good so long as they fit this point

1189
01:23:19,790 --> 01:23:23,850
ok so when we do that we get a gaussian that's more squished

1190
01:23:24,050 --> 01:23:28,440
ok so on one side we have to go through those the data point that

1191
01:23:28,440 --> 01:23:30,950
we have on the other decide which so still have a lot of freedom

1192
01:23:30,950 --> 01:23:34,630
so there's some lot of variance ok in the weight vector

1193
01:23:34,870 --> 01:23:38,870
but we restricted the sum ok so now the second point comes in

1194
01:23:39,010 --> 01:23:44,180
ok here's a second point and now you can see that we want actually

1195
01:23:44,190 --> 01:23:46,980
to fit both of these things ok so what happens

1196
01:23:47,480 --> 01:23:53,590
is we take this sort of this zero probability over the weight

1197
01:23:53,590 --> 01:23:56,820
vector we combining with the new likelihood from the new point

1198
01:23:56,820 --> 01:23:59,390
and then we have something that's very sort of concentrated

1199
01:23:59,390 --> 01:24:04,030
in one spot ok and now if you look at the last that result

1200
01:24:04,210 --> 01:24:08,040
they all all roughly through the two points there is some variability

1201
01:24:08,220 --> 01:24:13,160
ok because we have a distribution ok but they're all pretty

1202
01:24:13,170 --> 01:24:17,740
well aligned together and as we get more data what happens is the

1203
01:24:17,740 --> 01:24:21,060
spot that tells me what's the best the weight vector kind of squishes

1204
01:24:21,060 --> 01:24:25,930
in the variance reuses and the lines get more and more aligned together

1205
01:24:26,610 --> 01:24:32,800
okay so how does this relate to bias-variance well with put and some

1206
01:24:32,810 --> 01:24:36,560
bias here through the choice of prior ok

1207
01:24:37,070 --> 01:24:40,160
and then the data comes in ok and the data

1208
01:24:40,450 --> 01:24:45,260
is helping to decide how to sort of shrink this

1209
01:24:45,610 --> 01:24:48,860
in order to actually fit the observations that we have

1210
01:24:49,570 --> 01:24:53,900
ok and the more data we have the more we're going to shrink this

1211
01:24:53,900 --> 01:24:57,030
ok and the limit of infinite amount of data maybe we're going

1212
01:24:57,030 --> 01:25:02,290
to know fit this exactly correct so that was there's always

1213
01:25:02,300 --> 01:25:03,680
skewed by the data

1214
01:25:06,080 --> 01:25:09,520
so why is this interesting well it's interesting because it gives

1215
01:25:09,520 --> 01:25:14,190
us more then just estimates of the values of the output it actually

1216
01:25:14,200 --> 01:25:17,650
also gives us some kind of uncertainty estimates

1217
01:25:17,810 --> 01:25:21,560
in in where our hypothesis is confident on its output

1218
01:25:21,770 --> 01:25:26,950
and where it is not ok so this is a sort of another picture of the

1219
01:25:26,950 --> 01:25:30,120
same problem basically you have these circles here which are the

1220
01:25:30,120 --> 01:25:33,780
data points that are observed the true function as the green function

1221
01:25:33,790 --> 01:25:38,990
here and we have these red lines that are drawn from the sear

1222
01:25:39,000 --> 01:25:41,240
distribution over the weight vector ok

1223
01:25:41,900 --> 01:25:45,230
so what you see here is that when we have a data point the blue

1224
01:25:45,230 --> 01:25:47,890
point here around that data point we have

1225
01:25:48,040 --> 01:25:51,170
a little uncertain because we know we have to fit it

1226
01:25:51,380 --> 01:25:54,390
ok not exactly because were allowed some noise but

1227
01:25:54,680 --> 01:25:57,560
more last whereas when you are away from the data

1228
01:25:57,560 --> 01:26:00,870
there's a lot of uncertainty ok because you don't really know there

1229
01:26:00,870 --> 01:26:03,200
you can really tell what's a good hypothesis

1230
01:26:03,890 --> 01:26:07,580
and you can see in these lines all kind of go through a loop point

1231
01:26:07,590 --> 01:26:11,070
but they're all over the place as we get more points ok

1232
01:26:11,430 --> 01:26:14,610
these lines focus in order to capture these points

1233
01:26:15,090 --> 01:26:19,340
outside of where we have data we still have a lot of uncertainty

1234
01:26:19,820 --> 01:26:24,560
ok so the moral of the story is that you can always generalize well

1235
01:26:24,570 --> 01:26:28,600
around where data actually is if you're working with the

1236
01:26:28,750 --> 01:26:32,610
beijing method you can actually get analytical estimates sometimes

1237
01:26:32,620 --> 01:26:35,150
or empirical estimates of this uncertainty

1238
01:26:35,300 --> 01:26:39,340
ok and you can work with multiple hypothesis at the same time

1239
01:26:39,340 --> 01:26:45,570
right and was less zero then you misclassified it

1240
01:26:45,590 --> 01:26:49,270
so by setting some of the sea ice to be larger than one

1241
01:26:49,290 --> 01:26:55,110
on i can actually has some examples with a large and negative and therefore on

1242
01:26:55,110 --> 01:26:59,090
the line my album to misclassifies on the examples the training set

1243
01:27:00,070 --> 01:27:05,360
however i encourage you have not to do that by adding to the optimisation objective

1244
01:27:05,650 --> 01:27:09,980
this sort penalty term penalizes serving ice belong

1245
01:27:10,920 --> 01:27:15,460
and so this is an optimisation problem with the parameters on the w

1246
01:27:15,480 --> 01:27:18,710
and all the seas all the sea ice

1247
01:27:18,730 --> 01:27:22,110
and this is also a convex optimisation problem

1248
01:27:23,880 --> 01:27:25,790
it turns out that

1249
01:27:25,790 --> 01:27:31,920
similar to how we want to do all of the support vector machine

1250
01:27:31,940 --> 01:27:35,940
you can also work on the dole for this optimisation problem so i want actually

1251
01:27:35,940 --> 01:27:39,500
do it just to show you the steps right we should use you construct the

1252
01:27:39,520 --> 01:27:41,090
general isaac dungeon

1253
01:27:41,840 --> 01:27:47,790
now if you are not going to use alpha oxygen to denote the french multipliers

1254
01:27:47,790 --> 01:27:49,340
now corresponding to

1255
01:27:49,480 --> 01:27:52,830
the set of constraints and we have to use c and this new set of

1256
01:27:52,830 --> 01:27:57,840
constraints on the sea ice been zero this gives us a new so french multipliers

1257
01:27:57,840 --> 01:28:08,820
and so the the grandeur of the optimisation objective minus some from

1258
01:28:08,840 --> 01:28:19,150
for you

1259
01:28:19,170 --> 01:28:20,900
very minor

1260
01:28:25,290 --> 01:28:26,940
so that's all

1261
01:28:27,190 --> 01:28:29,130
the dungeon

1262
01:28:29,170 --> 01:28:31,590
discovered optimisation objective

1263
01:28:32,520 --> 01:28:36,500
you know or i guess what plus alpha a times each of these constraints which

1264
01:28:36,500 --> 01:28:38,440
can be zero

1265
01:28:38,520 --> 01:28:42,520
and so we can

1266
01:29:01,920 --> 01:29:08,170
so what we divide entire two again this is really you know the same as

1267
01:29:08,190 --> 01:29:14,690
on and when you divide to do all this optimisation problem we simplify

1268
01:29:14,710 --> 01:29:18,420
we find that on you get the following

1269
01:29:18,440 --> 01:29:20,790
you have to maximize of

1270
01:29:20,800 --> 01:29:26,500
that the about for which is actually the same as before

1271
01:29:33,110 --> 01:30:04,480
and so it turns out when you when you derive the dual

1272
01:30:04,520 --> 01:30:06,130
and simplify

1273
01:30:06,130 --> 01:30:09,340
it turns out that the only way to do it changes compared to the previous

1274
01:30:09,340 --> 01:30:13,730
one is that rather than the constraint the alpha creating real zero

1275
01:30:13,750 --> 01:30:17,110
we now have a constraint to the alpha of is zero c

1276
01:30:17,150 --> 01:30:21,440
so this derivation is very hard because just go home and try to do yourself

1277
01:30:21,440 --> 01:30:23,730
this is really essential to the same as

1278
01:30:23,790 --> 01:30:28,460
and when you simplify it turns out you can simplify the hours you to our

1279
01:30:28,460 --> 01:30:29,960
approach multiplies the way

1280
01:30:29,980 --> 01:30:34,170
you end up with with just these constraints in the office

1281
01:30:38,480 --> 01:30:40,250
see how hundred the time

1282
01:30:44,920 --> 01:30:50,110
just as an aside i want i won't bother i want

1283
01:30:50,130 --> 01:30:53,570
i want to write easy there it turns out that on

1284
01:30:53,610 --> 01:30:59,090
remember i wrote down the KKT conditions rain last lecture the karush kuhn tucker conditions

1285
01:30:59,110 --> 01:31:00,300
forms of

1286
01:31:00,420 --> 01:31:05,190
the necessary conditions for something to be optimal solution to constrained optimisation problem

1287
01:31:05,210 --> 01:31:10,710
so if you think the conditions on it turns out we can actually derive convergence

1288
01:31:10,710 --> 01:31:14,630
conditions so we want to solve the after this optimisation problem

1289
01:31:14,650 --> 01:31:19,320
so when we know the alpha ceph converge to the global optimum

1290
01:31:19,380 --> 01:31:22,110
it turns out we can use the following

1291
01:31:22,130 --> 01:31:24,690
it turns out that on

1292
01:31:58,520 --> 01:32:01,570
and then i don't see a lot about these

1293
01:32:01,610 --> 01:32:05,520
so there's also the KKT conditions you can derive

1294
01:32:05,570 --> 01:32:09,610
these said convergence conditions for an album that you might choose to use

1295
01:32:09,630 --> 01:32:13,840
to try to solve the optimisation problem in terms of the office

1296
01:32:16,460 --> 01:32:21,020
that's the one norm soft margin SVM and this is the

1297
01:32:21,110 --> 01:32:26,570
change the album handle non linearly separable datasets as was also the a single lies

1298
01:32:26,590 --> 01:32:32,960
there may still be linearly separable but you may choose not to separate lineages

1299
01:32:35,460 --> 01:32:36,900
he call

1300
01:32:37,110 --> 01:32:39,380
questions about this

1301
01:32:39,380 --> 01:32:42,270
seventeen important check questions

1302
01:33:17,960 --> 01:33:32,570
a few questions

1303
01:33:32,840 --> 01:33:36,690
because the pretty hand this to make sense

1304
01:33:36,690 --> 01:33:39,150
it's great

1305
01:33:41,270 --> 01:33:44,170
the last thing i want to do is on

1306
01:33:44,190 --> 01:33:50,690
talk about our love for actually solving this optimisation problem on so right we

1307
01:33:50,790 --> 01:33:55,060
o down this to optimisation problem with convergence criteria

1308
01:33:55,070 --> 01:33:59,790
so let's come up with an efficient algorithm to actually solve this optimisation problem

1309
01:34:02,610 --> 01:34:04,920
i want to do this part needs to

1310
01:34:05,020 --> 01:34:10,250
mean excuse to one another called coordination centre is useful to see you

1311
01:34:10,270 --> 01:34:13,730
so what actually want to do is is tell you about on

1312
01:34:13,900 --> 01:34:19,000
now album called coordinate ascent which is useful to know about and turned out that

1313
01:34:19,000 --> 01:34:20,040
they want apply

1314
01:34:20,060 --> 01:34:24,480
you know in the simplest form to this problem will then be modified slightly in

1315
01:34:24,480 --> 01:34:30,230
an innovative us very efficient algorithm for solving this some optimization problems

1316
01:34:30,250 --> 01:34:33,380
on that was the reason that i had to derive the dual not only so

1317
01:34:33,380 --> 01:34:38,820
they use kernels but also so can point out like like like like the out

1318
01:34:39,060 --> 01:34:40,710
so what

1319
01:34:40,710 --> 01:34:42,960
with posterior probabilities that adapt the data

1320
01:34:44,090 --> 01:34:45,330
as much as thinking about

1321
01:34:45,730 --> 01:34:50,260
at each time there are various possibilities that might happen so again with the same example

1322
01:34:51,260 --> 01:34:52,030
we can imagine

1323
01:34:52,620 --> 01:34:56,910
nowadays it's in the traditional way of modeling analyzes use something like a mixture of

1324
01:34:57,230 --> 01:35:00,710
the two component has mixture where one component allows for their lives

1325
01:35:01,210 --> 01:35:04,040
so there's two possible models but that's two possible models

1326
01:35:04,510 --> 01:35:05,500
at each time point

1327
01:35:06,510 --> 01:35:08,580
and if we look at the change points in this data

1328
01:35:09,320 --> 01:35:14,000
we might adapt is adopt a very similar strategy for modeling change points

1329
01:35:14,670 --> 01:35:15,700
and it may be selected

1330
01:35:16,130 --> 01:35:20,210
it may be that there are various components of the state vector that we make

1331
01:35:20,630 --> 01:35:22,720
expect anticipate a subject

1332
01:35:23,120 --> 01:35:24,950
i subject to abrupt changes

1333
01:35:25,590 --> 01:35:28,380
and then we'll be thinking about the same kind of mixture strategy there

1334
01:35:29,040 --> 01:35:30,970
for possibly several some parameters

1335
01:35:32,210 --> 01:35:36,030
so this is mixtures again for switching mechanisms for outliers

1336
01:35:36,460 --> 01:35:38,000
for automatic intervention

1337
01:35:38,900 --> 01:35:44,730
in a dynamical model context which again is all part of the historical panoply of methodology

1338
01:35:47,170 --> 01:35:48,120
so sequential learning

1339
01:35:49,290 --> 01:35:53,550
just to sort of complete the picture before we come to some some more examples

1340
01:35:54,710 --> 01:35:56,110
this is another textbook example

1341
01:35:56,700 --> 01:35:58,220
where we're looking forward in time

1342
01:35:58,950 --> 01:36:02,710
we've got a model that comprises it's composed of several components

1343
01:36:03,340 --> 01:36:03,980
there's a trend

1344
01:36:04,660 --> 01:36:05,740
there's a dynamic regression

1345
01:36:06,400 --> 01:36:09,420
there's a seasonal pattern its commercial sales data and

1346
01:36:10,870 --> 01:36:11,770
particular context

1347
01:36:12,340 --> 01:36:15,460
and what we're doing here is tracking the data and at this time point here we've seen

1348
01:36:15,940 --> 01:36:19,830
ten data points for forecasting had this is some notional forecast intervals

1349
01:36:20,660 --> 01:36:23,590
am because we know the data reported them on on the part is blue

1350
01:36:24,380 --> 01:36:28,070
and here we are tracking the eco-efficient ov thee predictor variable

1351
01:36:28,660 --> 01:36:33,060
and as we come through time when learning about this this parameter it's changing

1352
01:36:33,490 --> 01:36:35,330
the posterior intervals one degenerate

1353
01:36:35,920 --> 01:36:37,200
because is changing through time

1354
01:36:38,070 --> 01:36:41,030
we open to intervention so as we go ahead a few time points

1355
01:36:41,450 --> 01:36:42,970
as we see departures from the model

1356
01:36:43,370 --> 01:36:47,360
we may have a mixture structure we may be using other forms of informed intervention

1357
01:36:48,100 --> 01:36:53,630
at were allowed to step in and change posterior distributions in order to reflect an attempt to

1358
01:36:55,480 --> 01:36:56,160
forecast errors

1359
01:36:57,430 --> 01:36:59,150
at each timepoint forecasting ahead

1360
01:36:59,890 --> 01:37:05,020
asking questions about what might happen we do that by simulation across these days and a great deal

1361
01:37:05,460 --> 01:37:08,430
in simple linear systems it's it's analytic

1362
01:37:09,730 --> 01:37:10,970
and then as we go through time

1363
01:37:11,710 --> 01:37:14,360
after the end point we are interested in time series analysis

1364
01:37:15,620 --> 01:37:16,080
so will

1365
01:37:16,560 --> 01:37:18,530
distinguished at term from

1366
01:37:19,010 --> 01:37:22,940
the forward sequential updating learning analysis the adaptive learning

1367
01:37:23,600 --> 01:37:26,980
where we are more interested in looking at what is happening locally and how things

1368
01:37:26,980 --> 01:37:30,040
are changing anticipating change because the forecasting goals

1369
01:37:30,840 --> 01:37:35,930
to the looking back the retrospection thee analysis so what we've learned and how we planted

1370
01:37:36,410 --> 01:37:40,860
and how what we've learned will help us understand how we might modify the model for the future

1371
01:37:41,330 --> 01:37:45,140
so having tracked all the way through time we can then retrospectively update all these

1372
01:37:45,480 --> 01:37:48,900
relevant posterior distributions for the historical trajectory

1373
01:37:49,450 --> 01:37:50,220
the coefficient

1374
01:37:50,730 --> 01:37:52,390
that has changed over time subtly

1375
01:37:53,780 --> 01:37:57,640
and also in this case the variance be residual error which is this

1376
01:37:58,230 --> 01:38:00,610
a couple of points underwent some shops that increases that

1377
01:38:01,030 --> 01:38:01,450
as we saw

1378
01:38:02,100 --> 01:38:04,900
focus areas that warrant unexpectedly large

1379
01:38:09,340 --> 01:38:10,620
model composition has

1380
01:38:12,630 --> 01:38:14,830
a reciprocal model decomposition

1381
01:38:16,780 --> 01:38:18,590
so we build models out of components

1382
01:38:30,960 --> 01:38:31,900
and by the same token

1383
01:38:32,710 --> 01:38:34,420
when the building when we built a model

1384
01:38:35,430 --> 01:38:36,970
i'm not sure you a couple of examples

1385
01:38:37,780 --> 01:38:39,510
very often we're interested in components

1386
01:38:40,400 --> 01:38:41,600
we're interested in taking a model

1387
01:38:42,280 --> 01:38:44,460
the state space model with the multidimensional state

1388
01:38:44,930 --> 01:38:45,860
and decomposing

1389
01:38:46,430 --> 01:38:47,420
the signal in the model

1390
01:38:47,870 --> 01:38:49,490
into potentially interpretable

1391
01:38:50,140 --> 01:38:51,790
and actionable components

1392
01:38:53,100 --> 01:38:57,760
and this is a circuit second key aspect of this idea are breaking things apart

1393
01:38:58,540 --> 01:39:00,220
so let's take a particular subclass

1394
01:39:00,730 --> 01:39:02,180
of dynamic linear new models

1395
01:39:03,350 --> 01:39:04,610
the particular subclass hear

1396
01:39:05,200 --> 01:39:07,590
we've got some data that's the signal x error

1397
01:39:08,600 --> 01:39:11,420
the particular subclass hear has these matrices

1398
01:39:12,390 --> 01:39:13,000
the regression

1399
01:39:13,550 --> 01:39:17,310
matrix if you like this is the regression parameter which follows the state space model

1400
01:39:18,390 --> 01:39:24,940
and the markovian transition matrix effigy here are time-invariant they don't have a subscript e

1401
01:39:26,590 --> 01:39:28,510
we sometimes call these time series models

1402
01:39:29,200 --> 01:39:32,940
and the reason we call time series models is that all linear time series models

1403
01:39:33,180 --> 01:39:35,220
or linear gaseous in time series models

1404
01:39:36,480 --> 01:39:40,350
there is a view that all the time series know about autoregressive moving average processes

1405
01:39:41,650 --> 01:39:42,630
this special cases

1406
01:39:43,120 --> 01:39:44,360
there are other special cases

1407
01:39:44,940 --> 01:39:46,970
so in a very real sense all

1408
01:39:47,390 --> 01:39:53,430
traditional linear time series analysis sits inside the state space models and bayesian been doing

1409
01:39:53,910 --> 01:39:57,320
traditional time series analysis without knowing it for many years as a result

1410
01:39:58,460 --> 01:40:01,650
on these foundational concept a decomposition then

1411
01:40:02,190 --> 01:40:04,590
the key here is that in any such model

1412
01:40:06,530 --> 01:40:07,880
one can take the signal

1413
01:40:08,900 --> 01:40:10,560
this is just a theory and theory of

1414
01:40:11,160 --> 01:40:13,170
differential equations or difference equations

1415
01:40:13,900 --> 01:40:18,810
and break it down into a series of components such that they sum to give us the signal

1416
01:40:20,170 --> 01:40:23,750
and these components correspond to the eigen structure in this matrix j

1417
01:40:24,190 --> 01:40:25,140
that defines the model

1418
01:40:33,280 --> 01:40:34,690
sorry skip this slide again

1419
01:40:39,690 --> 01:40:42,920
can take one of three different forms of several forms

1420
01:40:43,490 --> 01:40:49,000
depending on the i can structure this matrix j if are unit igon values or replicate i can values

1421
01:40:49,000 --> 01:40:49,920
i mentioned before r

1422
01:40:51,860 --> 01:40:54,670
the feeders are not actually observed right we we

1423
01:40:55,180 --> 01:40:58,710
assume that what happens is we draw the random measure from the dirichlet process

1424
01:40:59,250 --> 01:41:01,330
then we put it in two hour make some mixture

1425
01:41:01,770 --> 01:41:04,170
and then we sample from the mixture and what we see is the data

1426
01:41:04,750 --> 01:41:08,800
the data from this mixture and i would like to get back at what this mixing distribution has look like

1427
01:41:10,860 --> 01:41:15,650
and the way we do that is with an algorithm with the gibbs sampling algorithm that i think is

1428
01:41:16,100 --> 01:41:18,650
is the easiest way to understand this organism is

1429
01:41:19,210 --> 01:41:22,530
to think of it as a kind of because that's really what it's kind of

1430
01:41:22,530 --> 01:41:25,520
a sampling version of you end up with an infinite number of components in the

1431
01:41:26,440 --> 01:41:27,550
the way it works is the following

1432
01:41:28,220 --> 01:41:28,800
we compute

1433
01:41:29,430 --> 01:41:33,720
assignment matrix of assignment probabilities was the data points in with that we've seen so

1434
01:41:33,720 --> 01:41:35,490
we've seen data points one three and

1435
01:41:36,070 --> 01:41:37,680
the rose correspond to data points

1436
01:41:38,720 --> 01:41:41,350
and the columns here correspond to the clusters

1437
01:41:41,960 --> 01:41:44,350
to our current estimate of what the number of clusters is

1438
01:41:45,560 --> 01:41:48,830
so this is not necessarily the true number of clusters is the estimate that

1439
01:41:49,560 --> 01:41:51,100
our algorithm currently things

1440
01:41:51,730 --> 01:41:53,000
is a is the number of clusters

1441
01:41:54,030 --> 01:41:55,870
and those these rules rosier

1442
01:41:56,320 --> 01:41:58,280
and then we have an additional roll over here

1443
01:41:59,100 --> 01:42:01,830
and that corresponds to the probability of creating a new class

1444
01:42:03,770 --> 01:42:04,740
so basically we take

1445
01:42:05,740 --> 01:42:09,350
these probabilities here correspond to be keane terms in this sum

1446
01:42:11,290 --> 01:42:17,900
and this probability these probabilities over here correspond to this integral and so we get this from evaluating this term

1447
01:42:18,350 --> 01:42:20,480
we get this you from evaluating this term

1448
01:42:21,240 --> 01:42:22,970
and we write them all into a matrix and then

1449
01:42:23,430 --> 01:42:27,350
because they should be assigned probabilities we have to normalize them by rules

1450
01:42:27,850 --> 01:42:29,690
and so each rule adds up to one

1451
01:42:32,620 --> 01:42:34,360
and this is exactly like the thee

1452
01:42:35,300 --> 01:42:39,630
the matrix of assignment probabilities that you compute in the e step of margaret

1453
01:42:40,150 --> 01:42:42,840
except for this additional role for this edition colony

1454
01:42:43,620 --> 01:42:45,100
india my reason we don't have it

1455
01:42:45,520 --> 01:42:48,460
you know we don't have a device for creating new clusters in a finite mixture

1456
01:42:48,460 --> 01:42:50,710
model so we don't get this probability here

1457
01:42:51,110 --> 01:42:53,410
basically if we would would just cut this out

1458
01:42:54,660 --> 01:42:55,070
take this

1459
01:42:55,710 --> 01:42:58,660
partial matrix you end we normalize each rotor one

1460
01:42:59,340 --> 01:43:01,920
then would have exactly the assignment probability matrix and you

1461
01:43:04,050 --> 01:43:06,470
okay now how do we do get something with this well

1462
01:43:07,380 --> 01:43:08,560
we use this matrix here

1463
01:43:09,780 --> 01:43:11,270
sample assignment variables

1464
01:43:13,130 --> 01:43:14,340
these variables which

1465
01:43:16,350 --> 01:43:21,260
the value of this variable is a clustering index so for each observation xt exchange

1466
01:43:21,500 --> 01:43:22,830
we have a variable phi jay

1467
01:43:23,720 --> 01:43:25,070
and the value takes is

1468
01:43:25,720 --> 01:43:26,810
the index of a cluster

1469
01:43:28,050 --> 01:43:28,910
so one through

1470
01:43:30,230 --> 01:43:33,660
are could take value zero and that means create a new cluster four this point

1471
01:43:35,690 --> 01:43:38,910
and how do we get this variable well be we take the corresponding role

1472
01:43:39,360 --> 01:43:40,910
the role corresponding to exchange

1473
01:43:41,350 --> 01:43:42,760
and we sample multinomial from

1474
01:43:43,650 --> 01:43:47,460
yeah so basically what we're saying is we assume that these terms are currently are

1475
01:43:47,470 --> 01:43:51,120
mixture weights are our current estimate of these weights we something like nominate

1476
01:43:52,490 --> 01:43:56,140
okay we do that for each point sequentially sequentially because it's a give center

1477
01:43:56,870 --> 01:43:57,140
and then

1478
01:43:58,760 --> 01:44:00,500
we get for each point we get

1479
01:44:03,460 --> 01:44:07,700
we get a cluster assignment and why is it important that we do that sequentially because

1480
01:44:08,610 --> 01:44:10,600
because if we at some point

1481
01:44:11,340 --> 01:44:11,920
if we draw

1482
01:44:12,930 --> 01:44:16,320
if we draw an assignment to this term then we have to create a new cluster

1483
01:44:16,790 --> 01:44:18,270
the number of clusters become larger

1484
01:44:19,030 --> 01:44:22,470
and then we have to we compute this matrix and the next term could be assigned to the class

1485
01:44:23,670 --> 01:44:25,280
so what happens at some point

1486
01:44:25,700 --> 01:44:27,080
as we go through this matrix

1487
01:44:27,970 --> 01:44:29,780
what happens at some point in this iteration

1488
01:44:30,230 --> 01:44:33,300
can influence further points and that's the characteristic of the gibbs sampler

1489
01:44:37,080 --> 01:44:39,080
so that's the assignment step we sample this

1490
01:44:39,520 --> 01:44:41,990
this variability multinomial e and then we have

1491
01:44:42,900 --> 01:44:43,840
another this and this

1492
01:44:44,680 --> 01:44:49,370
really corresponds to the east and yemen algorithm and then we have another step which is

1493
01:44:50,270 --> 01:44:54,930
corresponding to the am step you i very very simple parameter and what we do here is

1494
01:44:55,380 --> 01:44:58,940
we take all the data points that are currently assigned to a given cluster

1495
01:44:58,940 --> 01:45:02,460
the following content is provided under creative commons license

1496
01:45:02,500 --> 01:45:08,870
your support will help MIT opencourseware continue to offer high quality educational resources for free

1497
01:45:08,880 --> 01:45:13,720
to make a donation or to view additional materials from hundreds of MIT courses

1498
01:45:13,730 --> 01:45:21,150
visit MIT opencourseware OCW that MIT that EDU

1499
01:45:21,210 --> 01:45:22,320
i really

1500
01:45:22,370 --> 01:45:25,280
stated everything

1501
01:45:25,280 --> 01:45:28,460
about discrete coding is clearly as i could notes

1502
01:45:28,480 --> 01:45:31,850
i stated again as clearly as i could class

1503
01:45:31,910 --> 01:45:35,030
i stated again as clearly as i could

1504
01:45:35,090 --> 01:45:38,960
and making up problems that would illustrate the ideas

1505
01:45:39,020 --> 01:45:43,340
and if i talked about it again it will just be totally repetitive so

1506
01:45:43,350 --> 01:45:45,850
so at this point if you want to

1507
01:45:45,860 --> 01:45:47,960
understand things better

1508
01:45:47,990 --> 01:45:52,490
you got to come up with specific questions and i will be delighted to

1509
01:45:52,490 --> 01:45:54,930
to deal with the some

1510
01:45:55,210 --> 01:45:56,960
so we want to go on

1511
01:45:56,970 --> 01:46:00,140
there's one other thing i want to talk about

1512
01:46:00,170 --> 01:46:03,750
we're not having a new problem set out today

1513
01:46:03,770 --> 01:46:08,580
i don't think most of you will concentrate on it very well

1514
01:46:09,460 --> 01:46:13,020
but we will are

1515
01:46:13,130 --> 01:46:15,940
tell you what the problems are

1516
01:46:16,880 --> 01:46:18,500
which will be do in

1517
01:46:18,610 --> 01:46:22,350
october fourteenth

1518
01:46:22,390 --> 01:46:25,360
pass it out later

1519
01:46:25,410 --> 01:46:29,080
but it's

1520
01:46:29,100 --> 01:46:34,990
problems one through seven

1521
01:46:42,860 --> 01:46:46,030
lectures eight to ten

1522
01:46:46,050 --> 01:46:51,580
and one other one all the way at the end the problem twenty six

1523
01:46:51,630 --> 01:46:57,750
one two three four five six seven

1524
01:46:57,770 --> 01:47:01,860
and twenty six

1525
01:47:02,000 --> 01:47:08,160
so you can get started on them whenever you choose

1526
01:47:08,200 --> 01:47:10,310
in past other

1527
01:47:11,850 --> 01:47:16,160
problem set form the next time

1528
01:47:20,010 --> 01:47:23,410
last time we started to talk about

1529
01:47:23,440 --> 01:47:27,810
the difference between three months and the bike integration

1530
01:47:27,820 --> 01:47:31,350
most people tell me this is further

1531
01:47:31,380 --> 01:47:34,100
into mathematics and i should go

1532
01:47:34,120 --> 01:47:38,130
if you if you agree with them

1533
01:47:38,190 --> 01:47:42,180
after we spend a week or two one this please let me know

1534
01:47:42,200 --> 01:47:45,400
and i will torture future students with it

1535
01:47:45,410 --> 01:47:47,410
my senses

1536
01:47:48,250 --> 01:47:51,940
in the things we're going to be dealing with for most of the rest of

1537
01:47:51,940 --> 01:47:53,160
the term

1538
01:47:53,180 --> 01:47:55,730
knowing a little bit extra

1539
01:47:55,750 --> 01:47:59,850
about mathematics is going to say you awful lot of

1540
01:47:59,900 --> 01:48:04,930
time worrying about trivial little things you you should be worrying about

1541
01:48:05,090 --> 01:48:10,340
in other words the great mathematicians of the nineteenth century who developed

1542
01:48:10,500 --> 01:48:12,690
this is really the

1543
01:48:12,850 --> 01:48:15,930
the nineteenth century were part the twentieth century

1544
01:48:16,220 --> 01:48:19,750
these mathematicians were really engineers at heart

1545
01:48:19,840 --> 01:48:22,780
the twentieth century in the twenty first century

1546
01:48:22,790 --> 01:48:26,130
mathematics is very much because

1547
01:48:26,180 --> 01:48:28,130
very separated from

1548
01:48:28,150 --> 01:48:29,480
from physics

1549
01:48:29,590 --> 01:48:31,380
and applications

1550
01:48:31,400 --> 01:48:35,630
but in the nineteenth century and early twenties twentieth century

1551
01:48:37,350 --> 01:48:41,530
and physicists were were almost the same animals

1552
01:48:41,560 --> 01:48:44,430
you could scratch one and you find another one there

1553
01:48:44,470 --> 01:48:49,570
they were very much interested in dealing with real things

1554
01:48:49,590 --> 01:48:54,190
they were like the very best mathematicians everywhere

1555
01:48:54,200 --> 01:48:56,910
and the very best of engineers everywhere

1556
01:48:56,950 --> 01:48:59,780
they really wanted to make life simpler

1557
01:48:59,820 --> 01:49:02,470
instead of making life more complicated

1558
01:49:02,790 --> 01:49:07,900
one way that many people express it is that mathematicians are lazy

1559
01:49:07,910 --> 01:49:12,160
and because they're lazy they don't want to go through a lot of work and

1560
01:49:12,160 --> 01:49:15,520
therefore they feel driven to simplify things

1561
01:49:15,620 --> 01:49:17,920
there's not a lot of truth in that

1562
01:49:17,950 --> 01:49:20,070
and what we're going to learn here

1563
01:49:20,080 --> 01:49:22,440
i think will in fact simplify

1564
01:49:22,450 --> 01:49:23,730
what you know

1565
01:49:23,740 --> 01:49:28,240
about fourier series and fourier integrals a great deal

1566
01:49:28,450 --> 01:49:31,950
engineers typically don't worry about those things

1567
01:49:32,040 --> 01:49:36,900
an awful lot of engineers and unfortunately even those who write books

1568
01:49:36,940 --> 01:49:39,050
often state theorems

1569
01:49:39,060 --> 01:49:42,190
and leave out the last clause of the theorem

1570
01:49:42,210 --> 01:49:45,250
and the last clause of most of those theorems

1571
01:49:45,310 --> 01:49:47,340
the only way to make them true

1572
01:49:47,380 --> 01:49:51,010
just add claws or not as the case may be

1573
01:49:51,040 --> 01:49:52,540
at the end

1574
01:49:52,560 --> 01:49:58,310
which makes what they state absolutely meaningless because anything you can add were not as

1575
01:49:58,310 --> 01:49:59,440
the case may be

1576
01:49:59,500 --> 01:50:03,690
and it becomes true at that point in the whole question becomes one of those

1577
01:50:03,690 --> 01:50:05,980
cases under which is true

1578
01:50:06,010 --> 01:50:09,370
that's what we're going to deal with a little bit

1579
01:50:10,820 --> 01:50:13,020
we're going to say just enough

1580
01:50:13,140 --> 01:50:15,300
so you start to understand

1581
01:50:15,310 --> 01:50:18,450
these major things that cause problems

1582
01:50:18,460 --> 01:50:21,790
and i hope you'll get to the point where you don't have to worry about

1583
01:50:21,790 --> 01:50:24,190
them after that

1584
01:50:24,260 --> 01:50:28,750
OK well the first thing we started talking about the last time we were talking

1585
01:50:28,750 --> 01:50:31,870
about the difference between remind integration

1586
01:50:31,880 --> 01:50:35,390
and the big integration remind was great mathematician

1587
01:50:35,440 --> 01:50:37,320
but he came before

1588
01:50:37,330 --> 01:50:40,050
all of these mathematicians

1589
01:50:40,060 --> 01:50:41,930
we're following the bag

1590
01:50:41,940 --> 01:50:45,790
and who started to deal with all of the problems with this

1591
01:50:45,800 --> 01:50:49,540
classical integration which started to fall apart

1592
01:50:49,580 --> 01:50:57,290
throughout most of the nineteenth century now what raymond said well

1593
01:50:57,360 --> 01:50:58,460
split up

1594
01:50:58,560 --> 01:51:02,190
the axis into equal intervals

1595
01:51:02,320 --> 01:51:06,230
then approximate the function within each interval

1596
01:51:06,240 --> 01:51:08,290
add up all of those

1597
01:51:08,300 --> 01:51:11,000
all of those approximate values

1598
01:51:11,050 --> 01:51:13,380
and then let this

1599
01:51:13,440 --> 01:51:19,240
but this quantization over the time axis becomes finer and finer and finer

1600
01:51:19,310 --> 01:51:23,390
and if you're lucky you'll you'll come to limit

1601
01:51:23,400 --> 01:51:26,390
and you can sort of see when you get to limiting when you don't get

1602
01:51:26,390 --> 01:51:29,200
to limit of function is smooth enough

1603
01:51:29,250 --> 01:51:33,190
and you break it up finely enough you're going to get a very good approximation

1604
01:51:33,480 --> 01:51:37,440
and if you break it up more and more finally the approximation gets better and

1605
01:51:38,340 --> 01:51:42,610
if the function is very wild if it jumps around wildly

1606
01:51:42,630 --> 01:51:44,890
well look at examples of that later

1607
01:51:44,900 --> 01:51:46,920
then this doesn't work

1608
01:51:46,940 --> 01:51:51,110
you will see what i in fact this approach does work banks that now

1609
01:51:51,130 --> 01:51:54,670
instead of quantizing along this axis

1610
01:51:55,940 --> 01:51:58,360
on this axis

1611
01:51:58,370 --> 01:52:00,290
so he said OK let's

1612
01:52:00,300 --> 01:52:02,060
so far was zero

1613
01:52:02,110 --> 01:52:07,630
quantized quantizing epsilon two epsilon three at and so forth

1614
01:52:07,630 --> 01:52:12,830
and you know everybody when they talk about slander thinking about something small

1615
01:52:12,840 --> 01:52:17,010
and there are also thinking about making it smaller and smaller and smaller and hoping

1616
01:52:17,010 --> 01:52:19,130
that something like this happens

1617
01:52:19,130 --> 01:52:23,190
so so then what he says and q quantized this axis

1618
01:52:23,250 --> 01:52:26,540
start to look at how much of the function

1619
01:52:26,550 --> 01:52:30,270
it lies in each one of those little windows

1620
01:52:30,320 --> 01:52:36,260
my drawing that out here mu two is the amount of the function that lies

1621
01:52:36,630 --> 01:52:43,170
between two epsilon and three eps along now the function lies between two epsilon three

1622
01:52:43,170 --> 01:52:44,390
at ceylon

1623
01:52:44,430 --> 01:52:47,750
starting at this point which i label t one

1624
01:52:47,760 --> 01:52:50,980
going up to this point label t two

1625
01:52:51,000 --> 01:52:56,170
all over here it's not in this interval until we get back to t three

1626
01:52:56,170 --> 01:52:59,010
if is in gamma here well it was in gamma here

1627
01:52:59,010 --> 01:53:01,860
if it's to be here well into some form this

1628
01:53:01,960 --> 01:53:03,480
and so on

1629
01:53:03,550 --> 01:53:07,940
every rule has the property that for the whole proof has that property

1630
01:53:07,940 --> 01:53:12,510
well that property means is that every formula that occurs in the proof

1631
01:53:12,570 --> 01:53:17,110
is already a subformula of the conclusion that you're trying to prove

1632
01:53:17,110 --> 01:53:20,460
so you do not need to consider the rest of the language you can restrict

1633
01:53:20,460 --> 01:53:25,340
attention to subformulae the thing in trying to prove

1634
01:53:25,400 --> 01:53:27,260
and that's really

1635
01:53:27,260 --> 01:53:29,440
really important

1636
01:53:29,460 --> 01:53:32,110
o thing to be able to do it means you get some control

1637
01:53:32,150 --> 01:53:36,530
on what this person might look like

1638
01:53:36,580 --> 01:53:40,690
now all of that's lovely

1639
01:53:40,760 --> 01:53:43,940
but what about quantifiers here

1640
01:53:44,010 --> 01:53:44,880
the cry

1641
01:53:44,940 --> 01:53:47,690
can we fit them into the picture

1642
01:53:51,800 --> 01:53:53,420
of course

1643
01:53:53,490 --> 01:53:55,340
we can

1644
01:53:55,340 --> 01:53:57,050
but there are some

1645
01:53:57,090 --> 01:54:01,440
there are some wrinkles on

1646
01:54:02,670 --> 01:54:04,900
we get rules of course four

1647
01:54:04,940 --> 01:54:08,780
introduction of quantifiers left and right for the universal quantifier

1648
01:54:10,190 --> 01:54:13,230
we want to k so we want to show

1649
01:54:13,260 --> 01:54:19,130
for example that

1650
01:54:19,170 --> 01:54:20,840
so delta

1651
01:54:20,920 --> 01:54:24,030
that for all x a entails something

1652
01:54:24,070 --> 01:54:26,670
how do we prove that

1653
01:54:28,480 --> 01:54:31,300
we show it

1654
01:54:34,630 --> 01:54:35,440
by i

1655
01:54:35,460 --> 01:54:37,440
deriving whatever it was here

1656
01:54:37,440 --> 01:54:38,920
from some instance

1657
01:54:38,920 --> 01:54:41,420
and then generalize

1658
01:54:41,460 --> 01:54:43,420
subject to the usual

1659
01:54:43,440 --> 01:54:45,960
restriction no funny business

1660
01:54:48,410 --> 01:54:52,820
for x in a

1661
01:54:56,670 --> 01:54:57,590
and that's

1662
01:54:57,610 --> 01:55:00,760
o k

1663
01:55:00,900 --> 01:55:05,090
what about universal quantifier on the right

1664
01:55:05,110 --> 01:55:08,010
we want to show that

1665
01:55:08,030 --> 01:55:10,280
everything is a

1666
01:55:10,280 --> 01:55:13,750
you know unless one of the side conditions cells

1667
01:55:13,750 --> 01:55:15,570
how do we show that

1668
01:55:16,530 --> 01:55:21,800
we show

1669
01:55:23,110 --> 01:55:25,460
he follows

1670
01:55:25,480 --> 01:55:26,690
i would say

1671
01:55:26,710 --> 01:55:29,230
we can derive a with this site

1672
01:55:32,920 --> 01:55:33,670
he is

1673
01:55:33,690 --> 01:55:36,960
provable so therefore everything's provable

1674
01:55:36,980 --> 01:55:39,210
that's not ordinarily

1675
01:55:44,110 --> 01:55:46,550
but in certain conditions it is

1676
01:55:46,730 --> 01:55:50,530
we had exercise this morning is the last thing i did

1677
01:55:50,570 --> 01:55:54,570
under some conditions that's OK it's OK effects

1678
01:55:54,590 --> 01:55:57,230
does not occur free

1679
01:55:57,360 --> 01:56:00,170
in gamma

1680
01:56:00,210 --> 01:56:04,820
or dealt

1681
01:56:04,820 --> 01:56:11,230
OK then there's no special pleading for exit could be anything

1682
01:56:11,300 --> 01:56:15,990
what's true alexia is true while true of anything else because in the rest of

1683
01:56:15,990 --> 01:56:20,190
this thing there's no special pleading for x doesn't matter what it's called

1684
01:56:20,250 --> 01:56:23,900
bassist x is true everything

1685
01:56:24,300 --> 01:56:31,210
this is this is the common sort of reasoning you know you are familiar with

1686
01:56:31,210 --> 01:56:33,230
this from high school

1687
01:56:33,260 --> 01:56:35,320
you want to show that every triangle

1688
01:56:35,320 --> 01:56:38,780
as an acute angles service for

1689
01:56:38,820 --> 01:56:41,300
so let a b c b a triangle and then you go to do is

1690
01:56:41,300 --> 01:56:44,300
to do is to use of what you show is that ABC is because in

1691
01:56:44,300 --> 01:56:46,510
acute angles of

1692
01:56:46,530 --> 01:56:49,280
you say well there forever trying

1693
01:56:49,320 --> 01:56:51,690
so it is this inference and you don't

1694
01:56:51,750 --> 01:56:56,170
because nothing turns on what you call you called it a b c et al

1695
01:56:56,190 --> 01:56:57,260
x y z

1696
01:56:57,280 --> 01:57:01,530
you could call it anything you didn't make any special assumptions about a and b

1697
01:57:01,530 --> 01:57:05,260
and c didn't assume that line a straight line they

1698
01:57:05,340 --> 01:57:09,980
you he didn't make any any

1699
01:57:09,980 --> 01:57:12,110
site assumptions about

1700
01:57:14,320 --> 01:57:18,670
we get rules for the universal quantifier and exactly the dual roles for the existential

1701
01:57:19,880 --> 01:57:22,710
one like this

1702
01:57:22,860 --> 01:57:41,360
something follows from then it follows from the existence of something or other satisfying a

1703
01:57:43,360 --> 01:57:44,800
the variable here

1704
01:57:44,800 --> 01:57:47,440
know there's no special pleading for it

1705
01:57:47,510 --> 01:57:49,130
and a similar thing

1706
01:57:49,170 --> 01:57:50,920
on the dual thing to

1707
01:57:50,960 --> 01:57:51,840
so this

1708
01:57:51,840 --> 01:57:55,320
but on the right

1709
01:57:57,070 --> 01:58:08,400
gamma entails the reason i

1710
01:58:08,420 --> 01:58:10,760
unless delta

1711
01:58:10,780 --> 01:58:15,780
if it's if it gives you a particular instance of it

1712
01:58:19,710 --> 01:58:27,090
there is no funny business

1713
01:58:27,090 --> 01:58:30,650
so OK we can get rules the quantifiers

1714
01:58:30,690 --> 01:58:35,920
and that gives us proof system that we can work with arbitrary sequence a sequence

1715
01:58:35,920 --> 01:58:40,070
system for first-order logic

1716
01:58:40,110 --> 01:58:41,820
but now the

1717
01:58:41,860 --> 01:58:45,880
subformula property has become a little bit more complicated

1718
01:58:45,920 --> 01:58:47,800
because here

1719
01:58:47,840 --> 01:58:48,820
for instance

1720
01:58:48,840 --> 01:58:51,190
and here

1721
01:58:51,190 --> 01:58:53,760
here there is a formula

1722
01:58:53,800 --> 01:58:56,090
that doesn't actually occurred

1723
01:58:56,110 --> 01:58:57,420
below the line

1724
01:58:57,420 --> 01:59:00,280
OK because got this term t in it

1725
01:59:00,280 --> 01:59:03,550
that we just pull out of the air somewhere we don't know what what to

1726
01:59:03,730 --> 01:59:05,820
do that might be

1727
01:59:05,860 --> 01:59:10,750
so there's a weaker version of the subformula property you know up to substitution of

1728
01:59:10,750 --> 01:59:12,510
terms for variables

1729
01:59:12,570 --> 01:59:14,980
everything here because you

1730
01:59:15,030 --> 01:59:17,360
so we still sort of have some

1731
01:59:17,380 --> 01:59:20,610
kind of control of which fragments of the language you might be wanting to look

1732
01:59:21,460 --> 01:59:23,440
but what we can't control

1733
01:59:23,550 --> 01:59:27,460
is what terms might get introduced as we go

1734
01:59:29,360 --> 01:59:33,190
and it's important that we can't control that

1735
01:59:33,190 --> 01:59:36,630
because errol martin is going to come in here and persuaded the first order logic

1736
01:59:36,630 --> 01:59:38,860
is not decidable

1737
01:59:38,880 --> 01:59:41,260
and if we could keep control of this

1738
01:59:42,320 --> 01:59:45,820
we have an algorithm for deciding which we

1739
01:59:45,880 --> 01:59:50,050
can't have unless the allows councils to

1740
01:59:53,800 --> 01:59:55,670
it's not quite as

1741
01:59:55,730 --> 01:59:59,170
trivial is in the propositional case for that reason

1742
01:59:59,170 --> 02:00:01,170
there's another reason why it's not

1743
02:00:01,190 --> 02:00:04,670
as trivial as in the propositional case as well

1744
02:00:04,730 --> 02:00:08,170
and that is to do with

1745
02:00:11,090 --> 02:00:13,340
the this thing

1746
02:00:13,340 --> 02:00:17,170
is a consequence relation

1747
02:00:17,170 --> 02:00:19,090
the first night they learned to play to

1748
02:00:20,150 --> 02:00:24,110
pi one and it's so simple it just print hello world and starts to fly

1749
02:00:24,110 --> 02:00:29,370
and he's like outcome you're flying in as you i just typed important anti-gravity

1750
02:00:29,380 --> 02:00:33,560
and i also drank everything in the medicine cabinet but i think it's the right

1751
02:00:33,640 --> 02:00:37,270
so this

1752
02:00:37,330 --> 02:00:39,120
it was such a popular

1753
02:00:39,130 --> 02:00:41,010
little cartoon

1754
02:00:41,040 --> 02:00:45,990
then they jump into the interactive interlude now here that in python three which is

1755
02:00:45,990 --> 02:00:48,660
the long-awaited third generation

1756
02:00:50,090 --> 02:00:55,200
they added the integrand gravity module to the core language

1757
02:00:55,280 --> 02:01:00,530
so i'm running python three because all these third-party libraries don't run python three but

1758
02:01:00,530 --> 02:01:01,960
i've installed the sort of the

1759
02:01:01,980 --> 02:01:06,340
test here so i don't have a long history of for example of this module

1760
02:01:06,340 --> 02:01:09,690
is kind of fun that to tim peters wrote

1761
02:01:09,700 --> 02:01:12,480
so it's basically a little

1762
02:01:12,530 --> 02:01:16,030
o point let's composition of pi found beautiful is better than ugly

1763
02:01:16,040 --> 02:01:19,900
explicit is better than implicit symbols than complex in sort of gives you is and

1764
02:01:20,080 --> 02:01:21,390
you should be programming

1765
02:01:21,420 --> 02:01:25,230
so in that vein they also wrote the anti-gravity modules so

1766
02:01:25,430 --> 02:01:29,150
in order to actually check

1767
02:01:29,380 --> 02:01:38,240
OK so i'm going to go into the

1768
02:01:38,260 --> 02:01:44,340
keeping time we flash me every once also no kind of where i am

1769
02:01:51,360 --> 02:01:55,000
when the start of briefly to show some of the features of item that i

1770
02:01:55,000 --> 02:01:57,210
think you really need

1771
02:01:57,240 --> 02:02:01,310
and then i'm going to focus on on the mat on the pilot features hopefully

1772
02:02:01,310 --> 02:02:05,980
show you some things you haven't seen if you have a python programmer

1773
02:02:05,990 --> 02:02:09,890
so this is the python shows just enhanced python shell and i started with this

1774
02:02:10,950 --> 02:02:15,760
moti which basically says important matplotlib important be giving in matlab like

1775
02:02:15,780 --> 02:02:18,560
the environment social straight out of the gates

1776
02:02:18,560 --> 02:02:20,830
you can start doing things

1777
02:02:20,920 --> 02:02:27,780
make a histogram of one thousand random numbers normally distributed with a hundred bands

1778
02:02:27,810 --> 02:02:31,890
michigan graph

1779
02:02:31,960 --> 02:02:37,430
looks like a normal distribution so like matlab you can fairly easily too easy things

1780
02:02:38,720 --> 02:02:42,130
but not really to emulate matlab so i want to show you some of the

1781
02:02:42,130 --> 02:02:43,990
interesting things the python can do

1782
02:02:44,070 --> 02:02:47,430
beyond those capabilities to start out with so

1783
02:02:47,440 --> 02:02:52,200
people talk about python has been that early batteries included some input the URL URL

1784
02:02:53,440 --> 02:02:55,440
this is a module for searching data of

1785
02:02:55,500 --> 02:02:58,640
on websites and

1786
02:02:58,690 --> 02:03:02,310
again this is something you can develop the little bit harder so since in the

1787
02:03:02,310 --> 02:03:09,140
financial services industry financial example from yahoo finance does anybody famous stock that has lost

1788
02:03:09,150 --> 02:03:11,340
a lot of money in the last

1789
02:03:15,080 --> 02:03:21,880
you academics no no no takers no favor stocks to analyse

1790
02:03:21,880 --> 02:03:26,560
this one has lost me a lot of money my daughter's love these crocs and

1791
02:03:26,600 --> 02:03:29,590
that's not been good from my portfolio so

1792
02:03:29,630 --> 02:03:35,350
we have a finite set of historical prices you can download the CSV file spreadsheets

1793
02:03:35,350 --> 02:03:37,340
so i'm going to just

1794
02:03:37,350 --> 02:03:39,780
right click and copy the link location

1795
02:03:39,810 --> 02:03:44,590
so this is a link to grab the CSV file from yahoo finance and

1796
02:03:44,610 --> 02:03:47,950
and i'm going to say that link here in the shell

1797
02:03:47,970 --> 02:03:51,930
just pasted into that this variable URL

1798
02:03:51,930 --> 02:03:54,530
and i'm going to grab the of

1799
02:03:54,540 --> 02:03:57,590
the web using the URL library

1800
02:03:57,590 --> 02:03:59,740
the URL little

1801
02:03:59,810 --> 02:04:02,020
that you insist or in the trooper

1802
02:04:02,040 --> 02:04:04,840
the temporary file for us

1803
02:04:05,280 --> 02:04:08,390
and so it has the URL

1804
02:04:08,410 --> 02:04:13,680
and as stated in this temporary file called their something something this csv file system

1805
02:04:13,680 --> 02:04:15,830
going court the CSV file we have this

1806
02:04:15,840 --> 02:04:18,350
this nice CSV importer which is

1807
02:04:18,370 --> 02:04:22,950
really we work you for a lot in the financial industry sources my life

1808
02:04:26,190 --> 02:04:29,410
well let's see what the file looks like real quick

1809
02:04:29,420 --> 02:04:31,790
so it looks like a standard csv file

1810
02:04:31,810 --> 02:04:34,190
i can show is nice

1811
02:04:34,210 --> 02:04:37,960
let you type unix commands are arbitrary shell commands you can kind just i just

1812
02:04:38,110 --> 02:04:38,860
to last

1813
02:04:38,960 --> 02:04:43,280
so we basically have these headers and these different types now this is the date

1814
02:04:43,280 --> 02:04:47,780
string some price stayed in some volume data which is an integer so already we

1815
02:04:47,860 --> 02:04:53,810
we're not a whole world floats is that laughter in

1816
02:04:54,630 --> 02:04:58,580
what i've done with this are this or record

1817
02:04:58,600 --> 02:05:00,990
which is very nice to structure from

1818
02:05:01,010 --> 02:05:04,260
sqa seven hundred seventeen rows in it

1819
02:05:04,280 --> 02:05:09,360
and opened the first one

1820
02:05:09,370 --> 02:05:14,430
it has a daytime object is the first row it's eleven twelve that's yesterday

1821
02:05:14,460 --> 02:05:15,870
some price data

1822
02:05:15,870 --> 02:05:17,940
some integer data

1823
02:05:18,080 --> 02:05:23,150
so essentially what we've done here is created an array of these trucks

1824
02:05:23,150 --> 02:05:27,010
it's why i always xis the latent wise data because mapping for me go from

1825
02:05:27,010 --> 02:05:28,470
x to y

1826
02:05:28,510 --> 02:05:30,670
if i have a functional mapping from x to y

1827
02:05:30,690 --> 02:05:34,340
that's it specifies how my data is generated so i can do that for this

1828
02:05:35,130 --> 02:05:37,420
i can say what position annexes which is

1829
02:05:37,440 --> 02:05:40,530
the one dimensional space and that's time

1830
02:05:40,570 --> 02:05:44,340
and then i can is assigned cosine function to generate the not

1831
02:05:44,360 --> 02:05:47,690
this is also generated by looking at some

1832
02:05:47,740 --> 02:05:51,570
two dimensional space and saying well in one direction is the spiral and the other

1833
02:05:51,570 --> 02:05:52,720
is just linear

1834
02:05:52,720 --> 02:05:56,030
so all these things are generated by functions

1835
02:05:56,030 --> 02:05:58,050
so what happens if we

1836
02:05:58,090 --> 02:06:02,280
try to model the functions directly one thing to show

1837
02:06:02,340 --> 02:06:03,490
here's an idea

1838
02:06:03,510 --> 02:06:06,190
from seven humbling but it's sort of inspired by

1839
02:06:06,190 --> 02:06:08,590
by some of my work i believe

1840
02:06:08,690 --> 02:06:13,780
is to show that you can use this idea for model selection so

1841
02:06:13,820 --> 02:06:17,990
if i have a function regression function that models from f

1842
02:06:18,010 --> 02:06:19,280
from x to y

1843
02:06:19,300 --> 02:06:20,670
with some noise

1844
02:06:20,720 --> 02:06:23,470
and i'm going to say that this regression function is a guassian process i'm not

1845
02:06:23,470 --> 02:06:24,670
going to go through that

1846
02:06:24,720 --> 02:06:26,440
i have a regression function

1847
02:06:26,530 --> 02:06:28,990
and i look at the quality that regression function

1848
02:06:29,010 --> 02:06:33,110
one thing i might claim is it that's good smooth regression function from the latent

1849
02:06:33,110 --> 02:06:36,860
in the data space then i got a good representation of the manifold

1850
02:06:36,920 --> 02:06:38,760
now casting processes

1851
02:06:38,780 --> 02:06:40,630
allow us to school

1852
02:06:40,650 --> 02:06:42,780
regression function

1853
02:06:42,840 --> 02:06:44,740
so look at these data points here

1854
02:06:44,760 --> 02:06:49,220
this is the regression is input and this is an output is one dimensional regression

1855
02:06:49,280 --> 02:06:52,820
because we're going to go from one d to multiple dimensions or to data to

1856
02:06:52,820 --> 02:06:54,030
multi dimensions

1857
02:06:54,050 --> 02:06:59,300
now this is the parameters of the regression function will change

1858
02:07:00,800 --> 02:07:04,470
i do belong here so i'm changing and that of the regression function see here

1859
02:07:04,490 --> 02:07:08,990
is really weekly regression function here is to smooth the regression function

1860
02:07:09,010 --> 02:07:12,860
OK now here it looks like a really good regression through these points

1861
02:07:12,860 --> 02:07:15,780
these error bars either side

1862
02:07:15,860 --> 02:07:21,240
in fact not even better according to school so this is the log likelihood score

1863
02:07:21,240 --> 02:07:23,170
OK now starting to draw

1864
02:07:23,220 --> 02:07:25,470
it's only snow

1865
02:07:25,490 --> 02:07:30,720
now what's nice magasin processes is i can find what the best regression function is

1866
02:07:30,720 --> 02:07:33,720
parameterized it and give log likelihood score

1867
02:07:33,760 --> 02:07:37,340
so the question is can you use this

1868
02:07:41,490 --> 02:07:46,800
some of these visualizations some of these embedded these reconstructions

1869
02:07:46,820 --> 02:07:53,010
in this case here is the data and here is some reconstructions with isomap MVU

1870
02:07:53,010 --> 02:07:56,220
maximum variance unfolding platini collapsed and only

1871
02:07:56,320 --> 02:07:59,070
so i haven't talked about these three methods

1872
02:07:59,070 --> 02:08:02,650
here is the likelihood for these different reconstructions

1873
02:08:02,700 --> 02:08:06,010
so once high

1874
02:08:06,010 --> 02:08:10,720
so it's a guassian process regression i'm short on tyneside in real time so that

1875
02:08:10,720 --> 02:08:15,530
i would like to talk about model the guassian process regression so

1876
02:08:15,550 --> 02:08:18,380
it's the best type of regression you can possibly do

1877
02:08:18,440 --> 02:08:23,360
the easiest way to think about the next generation

1878
02:08:23,400 --> 02:08:26,050
check out processes before you do anything

1879
02:08:26,050 --> 02:08:27,440
nonlinear regression

1880
02:08:27,510 --> 02:08:29,490
so in this case is need to be nonlinear

1881
02:08:29,510 --> 02:08:33,740
but if these different approaches isomap MVU

1882
02:08:33,820 --> 02:08:34,570
i LLE

1883
02:08:34,590 --> 02:08:38,690
and by the lasso and you can see it scoring use the best now by

1884
02:08:38,860 --> 02:08:41,650
i would say looks pretty good

1885
02:08:41,700 --> 02:08:45,940
i that looks OK as well and the platini like that look OK to something

1886
02:08:45,940 --> 02:08:48,150
wrong look at low score LLE

1887
02:08:48,150 --> 02:08:53,090
so this regression scoring is telling you you've got a good and bad

1888
02:08:53,150 --> 02:08:55,820
his something with a hole in it

1889
02:08:55,960 --> 02:08:57,070
the data

1890
02:08:58,470 --> 02:09:03,260
these guys all struggle with that we hold in this case

1891
02:09:03,360 --> 02:09:08,090
only and you really recovers and that's reflected in the likelihood score

1892
02:09:08,110 --> 02:09:11,150
now of course in truth you don't have

1893
02:09:11,190 --> 02:09:14,970
the ground truth so these scores are reflecting

1894
02:09:15,030 --> 02:09:17,470
the ground truth is called you don't need the ground truth

1895
02:09:17,550 --> 02:09:20,010
to see this you have to know the ground truth

1896
02:09:20,010 --> 02:09:24,980
look by these girls attending the the quality of embedding without ground truth so gasoline

1897
02:09:24,980 --> 02:09:28,510
processes provide away

1898
02:09:28,590 --> 02:09:31,990
to look at the quality of different embedding so in this case and he must

1899
02:09:31,990 --> 02:09:36,010
have done something weird like jump across the with the connectivity structure i don't know

1900
02:09:36,010 --> 02:09:41,550
why have useful but they've also failed item at doing the best here

1901
02:09:41,670 --> 02:09:45,780
and again you isomap coming out of the top ranked so here is this what

1902
02:09:45,780 --> 02:09:49,470
this is basically saying is that you can use downstream processes

1903
02:09:49,490 --> 02:09:54,170
two school structure and give probabilistic model

