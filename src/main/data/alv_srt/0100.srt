1
00:00:00,000 --> 00:00:01,100
this is the

2
00:00:01,100 --> 00:00:05,390
energy per second that we are consuming

3
00:00:05,480 --> 00:00:07,340
what is this

4
00:00:07,340 --> 00:00:10,820
this is the energy per second the power station is delivering

5
00:00:10,830 --> 00:00:13,180
two us

6
00:00:13,210 --> 00:00:15,600
what is this

7
00:00:15,610 --> 00:00:18,830
that's lost energy

8
00:00:18,880 --> 00:00:21,920
if i screw or the producer

9
00:00:21,950 --> 00:00:23,150
in this table

10
00:00:23,150 --> 00:00:28,590
it goes into the universe is gone

11
00:00:28,600 --> 00:00:31,120
sorry economy

12
00:00:31,140 --> 00:00:34,100
the men's that we try to make this

13
00:00:34,150 --> 00:00:36,150
as small as possible

14
00:00:36,200 --> 00:00:38,290
this is the power that is available

15
00:00:38,300 --> 00:00:42,010
but you get the loss of power in terms of the minus sign here

16
00:00:42,090 --> 00:00:44,160
you get less

17
00:00:44,210 --> 00:00:46,700
in boston

18
00:00:46,740 --> 00:00:50,310
so how can you make this i square are low

19
00:00:50,350 --> 00:00:51,540
what is the

20
00:00:51,590 --> 00:00:54,170
the resistance of the wire

21
00:00:54,190 --> 00:00:56,890
that is rho which is the resistivity

22
00:00:56,920 --> 00:00:59,110
time the length of the wire

23
00:00:59,110 --> 00:01:03,110
divided by the cross section of the wire

24
00:01:03,120 --> 00:01:05,480
so you have several options

25
00:01:05,540 --> 00:01:11,270
you could make a very large and very thick copper wire that expensive

26
00:01:11,290 --> 00:01:15,500
you would also make the wires out of goals has lower resistivity

27
00:01:15,560 --> 00:01:17,070
than copper

28
00:01:17,120 --> 00:01:18,900
that's also expensive

29
00:01:18,950 --> 00:01:21,660
people are thinking of making these

30
00:01:21,700 --> 00:01:23,170
transmission wires

31
00:01:23,180 --> 00:01:28,450
of superconducting material they have to call them very low temperatures

32
00:01:28,510 --> 00:01:30,900
it's outrageously expensive

33
00:01:30,950 --> 00:01:32,400
that's the way you could get

34
00:01:32,400 --> 00:01:37,100
the resistance down

35
00:01:37,140 --> 00:01:38,350
that's not look

36
00:01:39,370 --> 00:01:41,730
the current

37
00:01:41,800 --> 00:01:46,460
what can we do is the current

38
00:01:46,520 --> 00:01:49,120
suppose we consume

39
00:01:49,170 --> 00:01:50,970
hundred megawatts

40
00:01:50,990 --> 00:01:53,380
not unreasonable number

41
00:01:53,380 --> 00:01:55,320
so we are consuming

42
00:01:55,330 --> 00:01:57,870
one hundred mag

43
00:01:57,920 --> 00:02:02,140
and just for the sake of the argument suppose at the

44
00:02:02,180 --> 00:02:05,050
the potential is wonderful

45
00:02:05,050 --> 00:02:07,470
so v

46
00:02:07,500 --> 00:02:08,960
it's hundredfold

47
00:02:09,190 --> 00:02:12,950
what now is the current

48
00:02:13,870 --> 00:02:16,520
current time potential

49
00:02:16,540 --> 00:02:17,910
it's me power

50
00:02:17,970 --> 00:02:19,550
so my current is now

51
00:02:20,690 --> 00:02:23,540
and is

52
00:02:26,660 --> 00:02:28,900
that the potential be

53
00:02:28,950 --> 00:02:31,140
in the wire

54
00:02:31,160 --> 00:02:34,180
is a hundred thousand four

55
00:02:34,270 --> 00:02:37,900
a thousand times higher now the current

56
00:02:37,940 --> 00:02:40,390
is only a thousand mps

57
00:02:40,440 --> 00:02:42,360
is the same

58
00:02:42,360 --> 00:02:48,630
our both cases in my consuming iterate a hundred million joules per second

59
00:02:50,660 --> 00:02:55,170
i square already lost on the way from the power station we

60
00:02:55,180 --> 00:02:58,450
is a million times slower in this case then in that case

61
00:02:58,490 --> 00:03:00,900
because i is a thousand times lower

62
00:03:00,950 --> 00:03:03,900
and the lost because i square

63
00:03:03,950 --> 00:03:06,190
so now you understand why

64
00:03:06,230 --> 00:03:08,700
electricity when it is transported

65
00:03:08,710 --> 00:03:10,310
from one place to another

66
00:03:10,340 --> 00:03:11,710
why does it done

67
00:03:11,770 --> 00:03:13,860
at the highest

68
00:03:13,900 --> 00:03:16,720
voltage possible

69
00:03:16,810 --> 00:03:19,090
when you get to boston

70
00:03:19,140 --> 00:03:22,630
you obviously got to do something about this enormous potential

71
00:03:22,640 --> 00:03:24,300
because if you were to deliver

72
00:03:24,310 --> 00:03:27,150
a hundred thousand potential difference there

73
00:03:28,060 --> 00:03:30,040
the population in boston

74
00:03:30,130 --> 00:03:33,710
will execute itself so now you've got to come down in voltage which you do

75
00:03:33,710 --> 00:03:36,900
with transformers we'll talk about that later in the course

76
00:03:36,950 --> 00:03:40,710
so you bring it down to a comfortable voltage which is

77
00:03:40,760 --> 00:03:43,390
in the united states about hundred and

78
00:03:43,430 --> 00:03:47,670
and falls in europe it's two twenty

79
00:03:47,720 --> 00:03:49,090
now comes the question

80
00:03:49,090 --> 00:03:50,810
how high can you make

81
00:03:50,830 --> 00:03:51,890
he of a

82
00:03:51,900 --> 00:03:53,860
the higher you could make it

83
00:03:53,860 --> 00:03:55,620
the last lost that would be

84
00:03:55,670 --> 00:03:57,380
along the way

85
00:03:57,420 --> 00:03:59,860
well you've got to stay away from the

86
00:03:59,890 --> 00:04:03,940
breakdown electric field which is the three million volt per meter

87
00:04:03,990 --> 00:04:06,620
if at the surface of these cables

88
00:04:06,620 --> 00:04:10,810
you get three million volt meter you get corona discharge that's a big loss want

89
00:04:10,810 --> 00:04:13,180
to stay away from the

90
00:04:13,240 --> 00:04:15,650
and so typical cables

91
00:04:15,690 --> 00:04:17,360
have about a radius

92
00:04:17,400 --> 00:04:20,690
are there is no are is the radius of the cable

93
00:04:20,710 --> 00:04:23,900
of about two centimeters

94
00:04:23,940 --> 00:04:26,120
given the cross sectional area

95
00:04:26,160 --> 00:04:31,550
i think about ten to the minus three meters square that's correct

96
00:04:33,360 --> 00:04:34,600
and the

97
00:04:34,610 --> 00:04:36,300
potential v

98
00:04:38,400 --> 00:04:41,680
is roughly three hundred kilowatts

99
00:04:41,840 --> 00:04:46,300
and with that configuration used a

100
00:04:46,310 --> 00:04:47,900
comfortably below the

101
00:04:47,910 --> 00:04:49,210
electric fields

102
00:04:49,390 --> 00:04:51,150
of three million volt meter

103
00:04:51,230 --> 00:04:55,190
you don't get the corona discharge

104
00:04:55,400 --> 00:04:58,340
the length of the cable al

105
00:04:58,400 --> 00:05:02,220
there were something like thousand kilometres

106
00:05:02,240 --> 00:05:04,470
not an unreasonable number

107
00:05:04,550 --> 00:05:06,660
thousand kilometres distance from

108
00:05:06,710 --> 00:05:09,460
if we get our electricity from niagara falls

109
00:05:10,900 --> 00:05:12,400
not a reasonable number

110
00:05:12,410 --> 00:05:13,940
you can calculate now

111
00:05:13,960 --> 00:05:15,730
what the

112
00:05:15,740 --> 00:05:20,760
resistance of that cable would be

113
00:05:20,810 --> 00:05:23,210
but that was resistance are

114
00:05:23,230 --> 00:05:26,960
because times l divided by age

115
00:05:27,120 --> 00:05:29,770
take up

116
00:05:29,830 --> 00:05:34,460
that has a resistivity of two times ten to the minus eight

117
00:05:34,480 --> 00:05:36,140
as i units

118
00:05:36,140 --> 00:05:39,320
we have length of ten to six meters of cable

119
00:05:39,330 --> 00:05:41,140
and we have a

120
00:05:41,180 --> 00:05:42,790
cross sectional area

121
00:05:42,790 --> 00:05:45,480
ten to the minus three

122
00:05:45,510 --> 00:05:46,550
square metres

123
00:05:46,580 --> 00:05:47,950
so that

124
00:05:47,970 --> 00:05:50,350
thousand kilometre cable

125
00:05:50,390 --> 00:05:52,950
we only have the a resistance of

126
00:05:52,970 --> 00:05:56,220
twenty arms

127
00:05:56,240 --> 00:05:59,340
and to make the numbers of the easy

128
00:05:59,390 --> 00:06:01,190
if we

129
00:06:01,250 --> 00:06:03,710
however current

130
00:06:03,720 --> 00:06:06,170
it's a of three hundred and years

131
00:06:06,190 --> 00:06:09,550
then the power

132
00:06:09,560 --> 00:06:12,340
the power station produces

133
00:06:12,400 --> 00:06:15,100
we take the three hundred kilowatts for now

134
00:06:17,040 --> 00:06:18,120
it would be

135
00:06:18,250 --> 00:06:22,150
three hundred cannonballs

136
00:06:24,670 --> 00:06:26,590
three hundred and three years

137
00:06:26,590 --> 00:06:28,250
and that is about

138
00:06:28,310 --> 00:06:33,360
ninety megawatts that's close to my hundred that i had in mind earlier

139
00:06:33,390 --> 00:06:36,620
you can calculate now with the losses

140
00:06:36,660 --> 00:06:40,960
the losses i squared or

141
00:06:41,020 --> 00:06:42,870
you know are twenty owns

142
00:06:42,870 --> 00:06:44,100
you know i

143
00:06:44,110 --> 00:06:46,080
the three hundred mps

144
00:06:46,150 --> 00:06:50,070
so you'll find out that you have about two megawatts

145
00:06:50,080 --> 00:06:51,900
that's not bad

146
00:06:51,910 --> 00:06:54,470
two out of ninety

147
00:06:54,520 --> 00:06:59,400
so we have about two percent energy loss in transportation

148
00:06:59,410 --> 00:07:04,300
you can also calculate know what the differences in potential between the power station

149
00:07:04,310 --> 00:07:05,310
in boston

150
00:07:05,320 --> 00:07:08,190
a minus b xi are

151
00:07:08,250 --> 00:07:10,580
you know i three hundred and

152
00:07:10,580 --> 00:07:16,780
OK so could conclusions about some work that we have temporal observations post popularity dropoff

153
00:07:16,780 --> 00:07:17,960
follows a power law

154
00:07:17,980 --> 00:07:23,800
topological observations is we find parallels in degree distribution cascade sizes and the stars are

155
00:07:23,800 --> 00:07:25,640
much more common than chains

156
00:07:25,700 --> 00:07:29,940
and we also generate had to cascade generating model

157
00:07:30,010 --> 00:07:36,620
based on epidemiology the best are based in epidemiology allergy that matches frequent cascades and

158
00:07:36,620 --> 00:07:42,940
size power laws

159
00:07:42,960 --> 00:07:51,170
so the next work where will point two in case studies

160
00:07:51,190 --> 00:07:58,050
it has to do with outbreak detection

161
00:07:58,070 --> 00:08:02,440
so the idea is that there are two two isomorphic problems there's first the problem

162
00:08:02,440 --> 00:08:06,820
of finding sources of contamination in water network

163
00:08:06,840 --> 00:08:12,090
and then then i similar problem is finding hot stories and blogs and the the

164
00:08:12,090 --> 00:08:13,290
goal in

165
00:08:13,300 --> 00:08:17,310
the protection and we don't the water network is they want to minimize the time

166
00:08:17,330 --> 00:08:20,210
time to detection of the population affected

167
00:08:20,240 --> 00:08:24,490
and we also want so we want to be able to maximize the probability of

168
00:08:24,490 --> 00:08:29,480
detection and also minimizes his replacement costs we have a whole lot never just plan

169
00:08:29,550 --> 00:08:31,190
censoring everybody's house

170
00:08:31,210 --> 00:08:32,490
there are only

171
00:08:32,560 --> 00:08:37,340
the decay cost money to put sensors in the water network

172
00:08:40,380 --> 00:08:43,600
and since similarly

173
00:08:43,620 --> 00:08:46,560
we want to minimize the number of blogs and we have to we have to

174
00:08:46,560 --> 00:08:49,290
look at read every day two

175
00:08:49,300 --> 00:08:53,500
to actually get a good idea what's going on in the blogosphere and get in

176
00:08:53,510 --> 00:09:01,260
any given idea for everybody else finds out about it

177
00:09:01,270 --> 00:09:04,880
so they call this algorithm cell and

178
00:09:04,900 --> 00:09:05,770
is that

179
00:09:05,770 --> 00:09:07,680
so is the host pronounced

180
00:09:09,230 --> 00:09:12,500
during on this

181
00:09:12,510 --> 00:09:14,260
OK so

182
00:09:14,310 --> 00:09:16,350
you only read the paper did not

183
00:09:16,380 --> 00:09:21,820
to the actual acronym pronounced so given graph

184
00:09:21,830 --> 00:09:27,800
comment budget budget sensors and data on how to commit contamination spread over the network

185
00:09:27,800 --> 00:09:28,700
which should be

186
00:09:28,740 --> 00:09:31,350
already made adjacency matrix and

187
00:09:31,370 --> 00:09:32,940
in the blog graph

188
00:09:33,050 --> 00:09:37,410
we want to minimize the time to detect the outbreak or the time that

189
00:09:37,460 --> 00:09:40,700
the time that an outbreak occurs in the blogosphere we want to be able to

190
00:09:40,700 --> 00:09:45,060
see what's happening before before it's old news

191
00:09:45,060 --> 00:09:49,590
so some of our algorithm main idea is that uses some on some modularity and

192
00:09:49,590 --> 00:09:57,310
lazy evaluation and the full citation is here

193
00:09:57,350 --> 00:09:59,950
and they did

194
00:09:59,980 --> 00:10:06,030
experiments on both water water water contamination networks and and blogosphere

195
00:10:06,080 --> 00:10:11,250
and they found that the using the self algorithm is better than any heuristic methods

196
00:10:11,250 --> 00:10:14,320
of blog outlinks blog and links

197
00:10:14,340 --> 00:10:18,710
the number of posts in random

198
00:10:29,570 --> 00:10:33,090
i think it has to do with the size of the cascade

199
00:10:33,120 --> 00:10:35,540
and that if

200
00:10:37,320 --> 00:10:41,610
the minimum number of people that have already linked to it before before we have

201
00:10:41,620 --> 00:10:44,040
we actually read read that long

202
00:10:44,050 --> 00:10:48,320
for that topic

203
00:10:48,340 --> 00:10:53,670
so according to the self algorithm the best ten blogs to read

204
00:10:53,700 --> 00:10:59,140
our instapundit done server and some others and

205
00:10:59,160 --> 00:10:59,910
so the

206
00:11:00,010 --> 00:11:02,140
here the here the scores

207
00:11:02,160 --> 00:11:07,090
the self algorithm identifies and here is number of posts number of in links

208
00:11:07,190 --> 00:11:11,170
blog out links and all out links which includes

209
00:11:11,180 --> 00:11:13,650
links to

210
00:11:14,270 --> 00:11:16,850
web pages that are necessarily blogs

211
00:11:16,880 --> 00:11:22,610
so some interesting things to note is that

212
00:11:22,660 --> 00:11:26,960
is it is in this argument finding that have small cost that is that the

213
00:11:26,960 --> 00:11:29,090
number of posts is is small

214
00:11:29,090 --> 00:11:32,260
so we don't have to read that many posts to get somebody to get some

215
00:11:32,260 --> 00:11:34,050
of these blogs

216
00:11:34,070 --> 00:11:35,590
and we would still be able to

217
00:11:35,600 --> 00:11:39,300
identify some hot stories before before they catch on

218
00:11:39,350 --> 00:11:41,040
everybody else

219
00:11:41,090 --> 00:11:47,140
is not

220
00:11:49,260 --> 00:11:51,360
is this

221
00:11:54,250 --> 00:11:58,320
this show was

222
00:12:04,160 --> 00:12:05,730
he was

223
00:12:05,740 --> 00:12:09,420
right thank you for bringing that up i put i knew it the best in

224
00:12:09,420 --> 00:12:12,530
quotes for reasons that are

225
00:12:39,610 --> 00:12:42,450
right so

226
00:12:48,600 --> 00:12:50,030
not everybody spends

227
00:12:50,030 --> 00:12:52,900
the whole work reading blogs right

228
00:12:53,080 --> 00:13:02,380
all right here

229
00:13:02,510 --> 00:13:08,190
if you have another thing to note is that michelle malkin has twelve thousand in

230
00:13:08,210 --> 00:13:09,360
links so

231
00:13:09,380 --> 00:13:12,940
chances are she starts a lot of arguments or some sort of so something like

232
00:13:12,940 --> 00:13:16,020
that you have a lot of people reading lots of people linking to our

233
00:13:16,020 --> 00:13:17,940
and therefore

234
00:13:19,360 --> 00:13:28,090
despite by nature of having started a lot of cascades

235
00:13:28,190 --> 00:13:31,280
despite her having started a lot of cascades you might

236
00:13:31,410 --> 00:13:32,360
i have

237
00:13:32,380 --> 00:13:36,530
got the highest score in this in this method because it's no time she picks

238
00:13:36,530 --> 00:13:42,100
things up in no time because she is the initiator of lot of

239
00:13:43,840 --> 00:13:44,880
so the next

240
00:13:44,900 --> 00:13:48,750
next case study is how can we extract communities and

241
00:13:48,860 --> 00:13:49,900
first we

242
00:13:49,910 --> 00:13:53,710
the first paper that all

243
00:13:53,730 --> 00:13:58,160
go over in brief is using PCA on structure

244
00:13:58,190 --> 00:14:03,550
so we have the cascades take on different shapes there again served by frequency

245
00:14:03,900 --> 00:14:10,830
but how can we identify how can we use cascades to identify communities

246
00:14:10,880 --> 00:14:14,840
and the way we do that is to perform PCA on cascade types who have

247
00:14:14,840 --> 00:14:16,190
a sparse matrix

248
00:14:18,350 --> 00:14:24,920
do we take all night online thousand cascade types we have some isolated post

249
00:14:26,230 --> 00:14:27,670
two no cascade

250
00:14:27,690 --> 00:14:31,350
three star etcetera and take longer count

251
00:14:31,350 --> 00:14:39,070
helpless won and project onto two principal components

252
00:14:39,080 --> 00:14:42,760
and we were able to observe that the content of blogs and cascade behavior often

253
00:14:42,760 --> 00:14:48,670
related that we have projecting on two principal components musings some hand labeling we have

254
00:14:48,670 --> 00:14:54,540
conservative blogs and we humorous blogs

255
00:14:56,550 --> 00:15:01,350
so we we generally think of our our principal components in this experiment is corresponding

256
00:15:01,350 --> 00:15:06,170
to stars and chains allow the humorous blogs have very short very short stars

257
00:15:06,190 --> 00:15:11,400
and the conservative blogs general generally have a lot of very very long trees

258
00:15:11,410 --> 00:15:14,800
of arguments

259
00:15:14,810 --> 00:15:18,460
and the full citation is here last year's ICWSM and so i'm not going to

260
00:15:18,460 --> 00:15:22,450
OK thank you i should warn you all i'll have nothing whatsoever to say about

261
00:15:22,450 --> 00:15:24,570
the the optics of light

262
00:15:24,630 --> 00:15:31,570
what i'm going to talk about today is some work by my group on unsupervised

263
00:15:31,570 --> 00:15:34,190
learning for language

264
00:15:35,730 --> 00:15:39,120
i'm not going to make any cognitive claims whatsoever in this talk but i'm going

265
00:15:39,140 --> 00:15:42,300
to on cognitive analogy briefly so please forgive me

266
00:15:42,350 --> 00:15:46,210
as humans there are two ways that we learn languages

267
00:15:46,260 --> 00:15:47,940
you've probably done both of these

268
00:15:48,030 --> 00:15:51,660
one is to learn a language in the classroom that look something like this

269
00:15:51,700 --> 00:15:52,680
you tighten

270
00:15:52,690 --> 00:15:56,520
rules you get shown examples you told how everything works and you get all the

271
00:15:56,520 --> 00:15:59,510
irregular verb forms listed for you and so on

272
00:15:59,590 --> 00:16:02,800
at some point you take a test and you have to to do some combination

273
00:16:02,800 --> 00:16:05,300
of regurgitation and generalisation

274
00:16:05,310 --> 00:16:07,910
OK it turns out for humans is it works great

275
00:16:07,970 --> 00:16:10,620
but this is one of the ways we have and this is basically what supervised

276
00:16:10,620 --> 00:16:12,760
NLP is like

277
00:16:12,810 --> 00:16:15,010
the other way we learn languages as humans

278
00:16:15,050 --> 00:16:18,330
something like this

279
00:16:18,340 --> 00:16:22,040
and in this case basically the data washes every you will this is the kid

280
00:16:22,040 --> 00:16:26,040
the data washes over you and you sort out for yourself what's going on underneath

281
00:16:26,050 --> 00:16:27,930
this is what unsupervised NLP

282
00:16:28,430 --> 00:16:32,190
should be like and could be like and i'm interested in building systems like the

283
00:16:32,190 --> 00:16:33,900
system on the right

284
00:16:34,770 --> 00:16:40,650
pick apart and figure out for themselves what kinds of structure is going on underneath

285
00:16:41,410 --> 00:16:45,040
my goal in this talk in this talk i'm going to

286
00:16:45,100 --> 00:16:48,020
kind of illustration by by several examples

287
00:16:48,040 --> 00:16:52,410
i'm going to talk about inducing various kinds of linguistic structure that are not in

288
00:16:52,410 --> 00:16:53,130
the data

289
00:16:53,180 --> 00:16:56,290
for each example is going to be slightly different but they're all going to show

290
00:16:56,300 --> 00:17:00,510
the property that kind of structure i'm interested in is some kind of complex linguistic

291
00:17:01,710 --> 00:17:05,400
in one case this will be syntactic parsing in one case become reference in last

292
00:17:05,400 --> 00:17:06,790
will be translation

293
00:17:06,800 --> 00:17:11,020
these are going to be kind of rich interacting combinatorial combinatorial structures

294
00:17:11,070 --> 00:17:14,430
and we take a lot of data and trying to figure out what's going on

295
00:17:14,430 --> 00:17:18,520
underneath that data in a way that's not annotated in the data itself

296
00:17:18,570 --> 00:17:21,690
the characteristics those are basically the characteristics of the problem

297
00:17:21,740 --> 00:17:24,770
the characteristics of the solutions that i'm going to sketch they all have something in

298
00:17:24,770 --> 00:17:30,900
common and the thread that's going title together is some combination of incremental or hierarchical

299
00:17:30,900 --> 00:17:35,300
learning and i mean that very vague way because what that means is going to

300
00:17:35,300 --> 00:17:38,920
vary from case to case but i find this to be a very important tying

301
00:17:38,920 --> 00:17:44,050
thread that you either can't or don't want to learn everything all at once

302
00:17:44,060 --> 00:17:47,420
this also the solution is i'm going to sketch are going to involve a very

303
00:17:47,420 --> 00:17:50,620
careful and often iterative choice of what to model

304
00:17:50,640 --> 00:17:53,400
and sometimes even more importantly what not to model

305
00:17:53,450 --> 00:17:56,490
so we'll see a a couple cases where you can model everything you don't want

306
00:17:56,490 --> 00:18:01,210
to model everything and one good way to make progress this works very well in

307
00:18:01,210 --> 00:18:05,300
our group is to kind of look at what your system is doing build something

308
00:18:05,300 --> 00:18:09,650
simple and then kind of only introduce the variables that you absolutely must model

309
00:18:09,750 --> 00:18:13,770
so from here on out going to talk about several concrete examples

310
00:18:13,890 --> 00:18:17,650
going first talk about the task of unsupervised grammar refinement

311
00:18:17,650 --> 00:18:19,050
this is

312
00:18:19,050 --> 00:18:23,050
related to but not exactly the same as grammar induction of talk about this this

313
00:18:24,050 --> 00:18:28,280
i will talk about coreference resolution and finally some work on translation

314
00:18:29,710 --> 00:18:35,170
so hopefully many of you were at mike's invited talk earlier

315
00:18:35,270 --> 00:18:38,020
and you talk a little bit about syntactic analysis

316
00:18:38,070 --> 00:18:40,460
syntactic parsing i'm going to talk about the same

317
00:18:40,490 --> 00:18:42,430
the same problem

318
00:18:42,520 --> 00:18:47,170
the goal here is to take sentences and remember most sensors are actually very complicated

319
00:18:47,170 --> 00:18:51,420
so i'll show simple examples but please don't take from this that the examples in

320
00:18:51,420 --> 00:18:56,430
practice are simple so here's the basically an average length sentence from newswire hurricane emily

321
00:18:56,430 --> 00:19:00,610
howled toward mexico's caribbean coast on sunday packing a hundred and thirty five mile-per-hour winds

322
00:19:00,610 --> 00:19:05,720
and torrential rain and causing panic in cancun where frightened tourists squeezed into musty shelters

323
00:19:05,740 --> 00:19:09,120
OK this is not he saw the cat right if it were he saw the

324
00:19:09,120 --> 00:19:10,390
cat this would be easy

325
00:19:10,460 --> 00:19:14,740
this is hard because for example those winds and rain and panic but it's not

326
00:19:14,740 --> 00:19:15,710
all the same

327
00:19:15,720 --> 00:19:18,360
it's not a conjunction of three things here and you need to sort out what

328
00:19:18,360 --> 00:19:21,620
the various pieces of the sentence are and how they combined together and that's going

329
00:19:21,620 --> 00:19:24,770
to be the job of the syntactic parser why do we do this we do

330
00:19:24,770 --> 00:19:31,340
this we do this because it's kind of an important intermediary step between

331
00:19:31,360 --> 00:19:35,780
surface language and semantics and is also these kinds of tree structures are very useful

332
00:19:35,780 --> 00:19:38,840
for a lot of problems like machine translation

333
00:19:38,840 --> 00:19:43,150
OK so back to simple examples but please if there's nothing else remember from this

334
00:19:43,150 --> 00:19:47,750
about syntactic processing please remember that the reality is very complicated and it's not always

335
00:19:47,750 --> 00:19:49,390
he was right period

336
00:19:49,400 --> 00:19:51,850
that's going to be the training example here

337
00:19:53,250 --> 00:19:54,450
in particular

338
00:19:54,460 --> 00:19:58,290
if you want to get a grammar and you would like to be passing these

339
00:19:58,290 --> 00:20:01,170
structures we you might look at them and say those look awful lot like context

340
00:20:01,170 --> 00:20:02,450
free grammars to me

341
00:20:02,460 --> 00:20:05,400
and then you think well where my going to get a context free grammar well

342
00:20:05,400 --> 00:20:09,040
i try to brainstorm one in terms of the work very well i could have

343
00:20:09,070 --> 00:20:14,200
a bunch of annotated sentences like this where somebody has been kind enough to painstakingly

344
00:20:14,200 --> 00:20:17,420
mark out with the syntactic structures are like this to say well there's a noun

345
00:20:17,420 --> 00:20:21,290
phrase and it contains the proposition and contains the pronoun and so on

346
00:20:21,310 --> 00:20:24,560
OK if we had a bunch of these and we do these are called treebanks

347
00:20:24,560 --> 00:20:27,310
we could read off what the rules are from the grammar so if i had

348
00:20:27,310 --> 00:20:30,600
this tree i would read have well there is one instance of a going to

349
00:20:30,600 --> 00:20:34,610
sentence there is one instance of a sentence going to announce phrase verb phrase followed

350
00:20:34,610 --> 00:20:36,520
by a period and so on

351
00:20:36,580 --> 00:20:40,460
i'd like to emphasise that if you just take for example we have forty thousand

352
00:20:40,460 --> 00:20:44,940
past english sentences and i look get just the noun phrases alone there are thirty

353
00:20:44,940 --> 00:20:49,150
or forty kinds of noun phrase rules there are ten thousand and you find new

354
00:20:49,150 --> 00:20:51,830
ones as you look at new sentences and this is because of all kinds of

355
00:20:51,830 --> 00:20:56,570
phenomena interacting punctuation and all kinds of modification and all kinds of complexities that don't

356
00:20:56,570 --> 00:21:00,350
show up in the simple sentences and their entourage and combinatorial

357
00:21:00,400 --> 00:21:03,960
OK so but you could take this these forty thousand sentences you could read off

358
00:21:03,960 --> 00:21:06,880
in the centre of mass frame

359
00:21:06,900 --> 00:21:11,340
before the collision because after the collision there's nothing there zero

360
00:21:11,340 --> 00:21:14,610
in case of a completely inelastic collision

361
00:21:14,610 --> 00:21:16,650
there's no kinetic energy left

362
00:21:16,650 --> 00:21:18,110
in the centre of mass frame

363
00:21:18,150 --> 00:21:20,610
so we go to the the centre of mass frame

364
00:21:20,650 --> 00:21:23,960
so we first have to calculate what you want

365
00:21:24,000 --> 00:21:25,820
well u one

366
00:21:25,840 --> 00:21:27,150
equals we one

367
00:21:28,170 --> 00:21:30,300
the centre of of mass

368
00:21:30,320 --> 00:21:33,050
and we know the centre of mass is

369
00:21:33,110 --> 00:21:34,210
it's right there

370
00:21:37,540 --> 00:21:39,000
and if you

371
00:21:39,050 --> 00:21:40,650
or subtraction

372
00:21:40,670 --> 00:21:42,710
it is by no means difficult

373
00:21:42,730 --> 00:21:44,780
you will find and two

374
00:21:44,800 --> 00:21:46,540
five by m one

375
00:21:46,550 --> 00:21:48,000
plus and two

376
00:21:49,750 --> 00:21:51,670
the one

377
00:21:51,730 --> 00:21:53,460
you check that i hope

378
00:21:53,480 --> 00:21:55,170
and now we go to

379
00:21:55,230 --> 00:21:57,460
calculate you

380
00:21:57,460 --> 00:21:59,230
we want to know what the

381
00:22:00,710 --> 00:22:02,540
the centre of mass one

382
00:22:02,590 --> 00:22:04,340
that of course is v two

383
00:22:04,380 --> 00:22:06,090
minus the centre of mass

384
00:22:06,130 --> 00:22:07,420
this was zero

385
00:22:07,480 --> 00:22:09,550
it is and what

386
00:22:09,590 --> 00:22:12,210
divided by n one

387
00:22:12,250 --> 00:22:13,730
plus and two

388
00:22:13,750 --> 00:22:16,860
thank you want to the difference is only one upstairs

389
00:22:16,880 --> 00:22:18,440
and the

390
00:22:20,050 --> 00:22:22,210
now we are going to

391
00:22:22,230 --> 00:22:24,230
calculate the kinetic energy

392
00:22:24,280 --> 00:22:27,420
in the centre of mass frame

393
00:22:28,750 --> 00:22:29,860
that equals

394
00:22:29,880 --> 00:22:31,960
one half

395
00:22:31,980 --> 00:22:33,770
of n one

396
00:22:36,000 --> 00:22:39,190
you want create this one half

397
00:22:41,040 --> 00:22:43,540
thank you to create that's all we have

398
00:22:43,590 --> 00:22:49,090
before the collision occurs

399
00:22:49,150 --> 00:22:52,230
by the way this is not my this is a plus sign

400
00:22:52,250 --> 00:22:54,320
and this is minds

401
00:22:54,360 --> 00:22:55,840
this one comes this way

402
00:22:55,860 --> 00:22:57,670
and this one was in that way

403
00:22:57,730 --> 00:23:00,630
now i can i can calculate that for you you know you want to you

404
00:23:00,630 --> 00:23:01,590
know you too

405
00:23:01,630 --> 00:23:06,440
suppose of hindsight makes no difference because they cancel any out what is going to

406
00:23:07,340 --> 00:23:08,460
one have

407
00:23:10,590 --> 00:23:12,040
divided by and one

408
00:23:12,050 --> 00:23:13,860
plus and two

409
00:23:14,860 --> 00:23:17,110
the long creek

410
00:23:17,130 --> 00:23:20,630
and this is exactly the same way that we have there

411
00:23:20,670 --> 00:23:22,300
so what you see here

412
00:23:22,300 --> 00:23:25,690
if you allow me for having to get some steps algebra

413
00:23:25,710 --> 00:23:28,770
you have to do a little massaging to get from here to here

414
00:23:28,770 --> 00:23:32,750
you see here this is the kinetic energy before the collision and all that kinetic

415
00:23:34,860 --> 00:23:36,270
removed went

416
00:23:36,270 --> 00:23:37,670
two heat

417
00:23:37,690 --> 00:23:39,840
is the maximum you can never lose

418
00:23:39,860 --> 00:23:41,150
and this is what we call

419
00:23:41,170 --> 00:23:44,280
internal kinetic energy of the system

420
00:23:44,300 --> 00:23:51,380
so going to the centre of mass is then you can always immediately

421
00:23:51,440 --> 00:23:56,360
calculate what the maximum heat is that you can expect from a collision

422
00:23:56,460 --> 00:23:58,710
we can take a very special case

423
00:23:58,770 --> 00:24:01,340
and we can take two

424
00:24:01,400 --> 00:24:04,170
going to infinity is like having a piece of body

425
00:24:04,280 --> 00:24:05,900
i slam on the wall

426
00:24:05,920 --> 00:24:07,590
get stuck

427
00:24:07,610 --> 00:24:09,590
and what is the maximum

428
00:24:09,590 --> 00:24:10,860
it that you can

429
00:24:10,880 --> 00:24:13,520
produce that's all the energy there is

430
00:24:13,540 --> 00:24:14,420
if m two

431
00:24:14,440 --> 00:24:17,670
become infinitely high then

432
00:24:17,710 --> 00:24:19,440
and one can be ignored

433
00:24:19,460 --> 00:24:24,630
two councils and two you get one half feet

434
00:24:24,630 --> 00:24:25,340
and one

435
00:24:25,380 --> 00:24:27,320
he one created

436
00:24:27,360 --> 00:24:28,770
that's obvious

437
00:24:28,780 --> 00:24:32,880
that's completely trivial i have a piece of body slammed against the wall it has

438
00:24:32,880 --> 00:24:36,750
a certain amount of kinetic energy where you see the your reference frame or in

439
00:24:36,750 --> 00:24:40,570
the reference frame of the center of mass it's immediately obvious that all the kinetic

440
00:24:40,570 --> 00:24:41,840
energy is lost

441
00:24:41,860 --> 00:24:46,070
and that's exactly what you see comes out of these equations where do you go

442
00:24:46,070 --> 00:24:48,710
to the centre of mass already do it from

443
00:24:48,710 --> 00:24:51,110
twenty six one hundred

444
00:24:51,190 --> 00:24:53,130
and i would like to return to the

445
00:24:53,150 --> 00:24:54,610
air traffic and do

446
00:24:54,610 --> 00:24:57,420
several completely inelastic collisions

447
00:24:57,440 --> 00:25:00,380
with you again

448
00:25:00,380 --> 00:25:02,420
we have to assume that momentum

449
00:25:02,440 --> 00:25:04,070
is conserved

450
00:25:04,170 --> 00:25:07,360
never is completely that we can come close

451
00:25:07,420 --> 00:25:10,570
and we will

452
00:25:10,570 --> 00:25:12,540
i have two cars

453
00:25:15,750 --> 00:25:21,020
so we're going to not completely elastic

454
00:25:25,860 --> 00:25:27,820
you know that

455
00:25:27,860 --> 00:25:29,800
so they hate each other

456
00:25:29,860 --> 00:25:31,750
and they get stuck

457
00:25:31,800 --> 00:25:32,980
i get a certain

458
00:25:33,000 --> 00:25:36,460
velocity which i put into the first car

459
00:25:38,570 --> 00:25:39,820
it's the second car

460
00:25:39,840 --> 00:25:41,710
and it gets stuck

461
00:25:41,710 --> 00:25:43,540
and i see there

462
00:25:43,550 --> 00:25:45,880
what might be primus

463
00:25:45,940 --> 00:25:48,190
see there on the blackboard

464
00:25:48,230 --> 00:25:50,300
but if the two are the same

465
00:25:50,360 --> 00:25:51,900
after one year

466
00:25:51,940 --> 00:25:54,420
one and one here that's the ratio

467
00:25:54,440 --> 00:25:56,210
so the outcome is that the prime

468
00:25:56,230 --> 00:25:57,380
must be one half

469
00:25:58,750 --> 00:26:00,710
so this must be

470
00:26:01,940 --> 00:26:03,270
p one

471
00:26:03,270 --> 00:26:07,340
now i have mass which is half the other one

472
00:26:07,380 --> 00:26:09,000
i into the other

473
00:26:09,020 --> 00:26:10,670
they get stuck together

474
00:26:10,670 --> 00:26:11,960
and now i get

475
00:26:13,020 --> 00:26:14,800
divided by one plus two

476
00:26:14,800 --> 00:26:15,650
i get

477
00:26:15,690 --> 00:26:20,630
one third notice in both cases i get a plus sign

478
00:26:20,650 --> 00:26:22,210
that's of course obvious

479
00:26:22,210 --> 00:26:23,900
if i blow into something

480
00:26:23,920 --> 00:26:27,610
and they stick together to continue in the same direction

481
00:26:27,630 --> 00:26:30,960
so now i will do the timing in exactly the same way that i did

482
00:26:32,040 --> 00:26:33,670
except that now

483
00:26:35,070 --> 00:26:37,780
i have completely inelastic collision

484
00:26:37,840 --> 00:26:40,920
i don't have the time t one

485
00:26:40,920 --> 00:26:42,090
i will have time

486
00:26:42,110 --> 00:26:43,770
the prime

487
00:26:43,780 --> 00:26:46,540
the cars have a slightly different

488
00:26:48,050 --> 00:26:50,710
two thirty seven

489
00:26:50,730 --> 00:26:56,040
plus and minus one gram

490
00:26:56,050 --> 00:26:57,300
and i have one

491
00:26:57,300 --> 00:26:58,920
that is four hundred

492
00:26:59,020 --> 00:27:00,770
and seventy four

493
00:27:00,820 --> 00:27:03,320
plus and minus one gram not too different from this

494
00:27:05,250 --> 00:27:06,820
i have

495
00:27:06,860 --> 00:27:08,730
two of these guys

496
00:27:08,750 --> 00:27:10,520
and i have one of these

497
00:27:10,550 --> 00:27:12,630
and first i'm going to slam

498
00:27:12,670 --> 00:27:14,070
these two on two

499
00:27:14,070 --> 00:27:15,210
each other

500
00:27:15,230 --> 00:27:16,570
and so when they

501
00:27:17,570 --> 00:27:19,550
i expect the speech

502
00:27:19,690 --> 00:27:21,380
to be half

503
00:27:21,420 --> 00:27:23,150
so i get a certain amount

504
00:27:23,190 --> 00:27:24,210
p one

505
00:27:24,210 --> 00:27:25,230
thirty one

506
00:27:25,250 --> 00:27:26,630
the prime will be

507
00:27:26,670 --> 00:27:28,340
twice as long because

508
00:27:28,340 --> 00:27:30,690
this goes down by a factor of two

509
00:27:30,770 --> 00:27:32,800
so when i multiply

510
00:27:32,840 --> 00:27:33,770
this number

511
00:27:33,780 --> 00:27:35,230
by one half

512
00:27:35,250 --> 00:27:38,300
i would like to get number back

513
00:27:38,380 --> 00:27:39,840
he i get

514
00:27:39,980 --> 00:27:42,000
time for this car

515
00:27:42,000 --> 00:27:43,750
the first one to come in

516
00:27:43,750 --> 00:27:45,480
this means time

517
00:27:45,480 --> 00:27:47,770
when they continue to get up to speed is

518
00:27:47,820 --> 00:27:49,320
three times slower

519
00:27:49,380 --> 00:27:51,840
so this time will be three times higher

520
00:27:51,840 --> 00:27:56,050
with far fewer number measurement that you thought where possible we can actually

521
00:27:56,050 --> 00:27:58,960
i guess the quantum state of the system because you can use the low rank

522
00:27:58,960 --> 00:28:00,590
structure of the system to do it

523
00:28:00,650 --> 00:28:05,650
OK so what the girls did was to use the tools of matrix completion to

524
00:28:05,650 --> 00:28:08,210
actually show that you could solve this problem

525
00:28:08,230 --> 00:28:09,730
and he did much more

526
00:28:09,750 --> 00:28:15,360
OK so these are problems like this in quantum mechanics and people actually using these

527
00:28:15,360 --> 00:28:18,800
techniques to actually interfere quantum states from

528
00:28:18,820 --> 00:28:22,230
quantum state tomography measurements

529
00:28:22,230 --> 00:28:23,500
OK so

530
00:28:23,550 --> 00:28:27,420
i'm done with matrix completion in the second part of the talk i will talk

531
00:28:27,460 --> 00:28:30,920
a little bit about robust PCA but this part is of course much shorter because

532
00:28:30,920 --> 00:28:33,190
we introduce most of what we need now

533
00:28:33,210 --> 00:28:39,110
OK so first of all it and if you add noise to

534
00:28:39,130 --> 00:28:43,710
matrix you could say yes it's interesting to recover matrix which is exactly going from

535
00:28:43,710 --> 00:28:48,670
those less data but in practice and i'm not exactly low rank plus i have

536
00:28:48,670 --> 00:28:53,130
no data alexa like the ratings are contaminated with no is it would be a

537
00:28:53,130 --> 00:28:56,480
pity if the whole theory we could not accommodate

538
00:28:56,480 --> 00:29:01,210
inexact measurements and so what you do when you have an exact measurement you do

539
00:29:01,210 --> 00:29:03,610
what you always do we don't enforce

540
00:29:03,690 --> 00:29:09,920
equality constraints of course now you DIY's inexact so you can only enforce an approximate

541
00:29:09,920 --> 00:29:13,520
fit to the data among all

542
00:29:13,550 --> 00:29:18,800
low rank among all major is compatible with data you can pick the one with

543
00:29:18,840 --> 00:29:24,020
minimum nuclear norm and when you do this things work that is you get an

544
00:29:24,020 --> 00:29:28,380
error which is proportional to the noise level in another way of saying this is

545
00:29:28,380 --> 00:29:32,770
that one matrix completion from those less data of curves then if you have a

546
00:29:32,770 --> 00:29:36,650
small amount of nicely will be small as well

547
00:29:36,670 --> 00:29:41,190
it's a big field of research again by no mean we are the only one

548
00:29:41,210 --> 00:29:47,400
to work on this many statisticians including martin wainwright who will be here i think

549
00:29:47,420 --> 00:29:49,670
next week or tomorrow forgotten

550
00:29:49,670 --> 00:29:51,550
who has done

551
00:29:51,550 --> 00:29:54,110
a lot of important work in this area

552
00:29:54,130 --> 00:30:01,550
right so we can actually we require recovery matrices steeply from from noisy data

553
00:30:01,610 --> 00:30:06,110
OK so now i would like to come back to this matrix problem to motivate

554
00:30:06,130 --> 00:30:10,340
the second part of the talk and the reason i started to work on this

555
00:30:10,340 --> 00:30:14,140
is because the first time i gave a talk about matrix completion there was a

556
00:30:14,140 --> 00:30:17,650
gentleman in the audience who raised his hand and say you know many something you

557
00:30:17,650 --> 00:30:22,230
forget and the thing you forget the that there's lots of bogus ratings

558
00:30:22,250 --> 00:30:23,770
in this data matrix

559
00:30:23,780 --> 00:30:28,800
he said i don't know why but there are people who enter ratings have nothing

560
00:30:28,800 --> 00:30:32,130
to do with their own preferences

561
00:30:32,130 --> 00:30:36,730
and so is is only dangerous that use these ratings to actually predict all the

562
00:30:36,750 --> 00:30:39,460
ratings when these ratings are completely bogus

563
00:30:39,480 --> 00:30:44,170
and i said yes it is it is very dangerous i didn't notice so what

564
00:30:44,170 --> 00:30:45,250
can i do

565
00:30:45,280 --> 00:30:50,690
so now we have a slightly more complicated problem where we have actually part of

566
00:30:50,690 --> 00:30:56,210
an observation about the low rank matrix in the netflix matrix but we also have

567
00:30:56,210 --> 00:31:00,750
a sparse error term on top of it which corresponds to entries that are completely

568
00:31:02,670 --> 00:31:04,340
OK so i need to deal with this

569
00:31:04,360 --> 00:31:09,090
so the problem is now i don't want to make noisy matrix completion robust because

570
00:31:09,090 --> 00:31:12,250
of the small additive noise like i want to make it will buses have very

571
00:31:12,250 --> 00:31:14,860
bad stuff that could happen

572
00:31:14,880 --> 00:31:20,420
now this motivates us to introduce a problem that i like a lot which is

573
00:31:20,420 --> 00:31:25,020
the blind deconvolution problem which i'm going to state again as simply as i can

574
00:31:25,020 --> 00:31:27,300
in this problem

575
00:31:27,320 --> 00:31:31,520
we've got a data matrix which is available to me it's available to the statistician

576
00:31:31,550 --> 00:31:36,340
always machine learner and it's some of the low rank matrix

577
00:31:36,360 --> 00:31:38,320
and the sparse matrix

578
00:31:38,340 --> 00:31:41,770
right so what that says is that what you see is is some of the

579
00:31:41,770 --> 00:31:45,800
lowest matrix and sparse matrix so what do i mean by this means that this

580
00:31:45,800 --> 00:31:50,860
is a low rank matrix that you cannot observe because some entries have been corrected

581
00:31:50,880 --> 00:31:54,150
can the corrections is exactly carried by zeta

582
00:31:54,190 --> 00:31:57,710
the only thing you do is you observe the some of these two matrices

583
00:31:57,730 --> 00:32:03,730
and what i'm playing would be nice to recover l and e accurately

584
00:32:03,750 --> 00:32:09,860
again seems impossible and he would be great to do it because if i could

585
00:32:09,860 --> 00:32:14,690
do it i could i could detect the raining and apply matrix completion to such

586
00:32:14,730 --> 00:32:17,300
good ratings and complete my matrix accurately

587
00:32:17,340 --> 00:32:23,130
OK so it would be great but what would be great it would be great

588
00:32:23,130 --> 00:32:28,520
essentially because had we do dimensionality reduction so typically

589
00:32:28,570 --> 00:32:32,440
what we all learn is principal component analysis and i'm sure everybody knows what it

590
00:32:32,440 --> 00:32:36,460
is what you have is you've got a data matrix and which is the low

591
00:32:36,460 --> 00:32:40,090
rank matrix plus

592
00:32:40,280 --> 00:32:44,770
perturbation and so what we think of this is if the points

593
00:32:44,840 --> 00:32:48,190
we have a low rank matrix and we look at the columns

594
00:32:48,190 --> 00:32:49,730
of this matrix

595
00:32:50,630 --> 00:32:54,650
and what we think about when we the principal component we look at the column

596
00:32:54,650 --> 00:32:58,150
vectors and we say well you know in fact this column vectors live in very

597
00:32:58,150 --> 00:33:00,610
high dimensional but actually clustered

598
00:33:00,630 --> 00:33:05,440
along the low dimensional space

599
00:33:05,460 --> 00:33:09,210
and the goal is to recover low dimensional space and the way you do this

600
00:33:09,210 --> 00:33:13,570
is by principal component analysis as i'm sure you all know so what you do

601
00:33:13,570 --> 00:33:17,340
is you say well how my good to look at the recovers the low dimensional

602
00:33:18,550 --> 00:33:22,730
in this high dimensional data set well i'm going to try to find a matrix

603
00:33:22,730 --> 00:33:25,780
which has more rank which is as close as possible to the data matrix you

604
00:33:25,780 --> 00:33:30,320
gave me and it looks like a horrible communism communist

605
00:33:30,340 --> 00:33:35,170
combinatorial problem because we have a rank constraint which is absolutely and friendly but this

606
00:33:35,170 --> 00:33:38,570
is the only hard combinatorial problem i know how to solve because i know the

607
00:33:39,440 --> 00:33:42,050
and the solution is just calculate the SVD

608
00:33:42,070 --> 00:33:44,710
don't small singular values and you're done

609
00:33:44,730 --> 00:33:49,020
OK and that's what we all learn to every undergraduate

610
00:33:49,070 --> 00:33:53,320
taking a course in statistics at stanford

611
00:33:53,320 --> 00:33:56,970
contained into the characteristic vector just say

612
00:33:57,010 --> 00:34:01,290
just looking into the characteristic vector just look whether it is zero or not is

613
00:34:01,300 --> 00:34:03,040
close to zero

614
00:34:03,090 --> 00:34:05,860
but in so doing we are actually

615
00:34:05,900 --> 00:34:08,310
we're not using a lot of information

616
00:34:08,320 --> 00:34:13,440
because the components in the characteristic vector this is an empirical finding

617
00:34:13,560 --> 00:34:17,840
the second thing a lot of information about the structure of the class

618
00:34:17,850 --> 00:34:21,300
actually we we so empirically that

619
00:34:21,890 --> 00:34:27,720
the components of the characteristic vector provide us with information about the centrality of the

620
00:34:27,720 --> 00:34:29,750
objects within the class

621
00:34:29,760 --> 00:34:32,290
so the more central in one sense

622
00:34:32,300 --> 00:34:36,330
the object is within this class to higher would be it's components

623
00:34:36,340 --> 00:34:40,470
this is pretty interesting because it allows us to

624
00:34:40,470 --> 00:34:43,670
there is empirical observation not

625
00:34:43,710 --> 00:34:48,560
but we empirically founded several occasions several different locations

626
00:34:48,650 --> 00:34:50,170
this means that

627
00:34:50,190 --> 00:34:52,100
we have an empirical

628
00:34:52,130 --> 00:34:54,150
and heuristic way

629
00:34:54,170 --> 00:34:57,790
for extracting the prototype from the cluster

630
00:34:57,840 --> 00:35:02,240
in a purely pairwise certain framework is not

631
00:35:02,290 --> 00:35:05,810
if you working within the central clustering framework

632
00:35:07,340 --> 00:35:10,950
object is represented in terms of feature vector

633
00:35:10,990 --> 00:35:12,990
we we get the cluster

634
00:35:13,040 --> 00:35:18,080
so we get on a cloud on the one-dimensional space

635
00:35:18,090 --> 00:35:22,720
then we can compute the same prototype by for example computing the centroid

636
00:35:22,770 --> 00:35:27,090
the centre of mass whatever the club that would be your product

637
00:35:27,100 --> 00:35:30,350
well if you walk in the only pairwise set

638
00:35:30,360 --> 00:35:34,140
you don't have the notion of the centre of the notion of control

639
00:35:34,150 --> 00:35:35,970
for example if you're objects

640
00:35:35,980 --> 00:35:38,230
are characterized in terms of graph

641
00:35:38,300 --> 00:35:40,940
what is the centroid and graph

642
00:35:40,980 --> 00:35:45,790
so actually in the graph you really to have been some attempts at defining this

643
00:35:45,790 --> 00:35:50,520
notion of try but you know it's war heuristic what we do

644
00:35:50,540 --> 00:35:53,640
if we have at our disposal feature vector

645
00:35:55,010 --> 00:35:56,650
if this is true

646
00:35:56,700 --> 00:36:02,020
so this means that when i find clustering dominance set if i want to extract

647
00:36:02,020 --> 00:36:07,450
the representative prototypes for four percent i just i can just look at the

648
00:36:07,490 --> 00:36:14,770
so the characteristic vector and defined the prototype is the component corresponded to the highest

649
00:36:15,770 --> 00:36:17,630
this empirically says that

650
00:36:18,900 --> 00:36:23,340
object is more central for the world than the others

651
00:36:23,990 --> 00:36:26,860
so to convince ourselves to of course we have to

652
00:36:27,010 --> 00:36:29,080
you with the actual real

653
00:36:30,900 --> 00:36:34,240
we did some experiments either synthetic or not

654
00:36:34,250 --> 00:36:39,870
we generated two thousand nine season goes to gaussian processes and of course

655
00:36:39,930 --> 00:36:45,300
if we start replicator dynamics from different starting points you may end up with different

656
00:36:46,240 --> 00:36:49,890
so what we did was to run the replicator dynamics from

657
00:36:49,900 --> 00:36:52,140
randomly chosen kind

658
00:36:52,200 --> 00:36:55,480
and for example it converges to this class

659
00:36:55,540 --> 00:36:56,990
this is the dominant

660
00:36:57,000 --> 00:37:01,830
then we run the algorithm starting from a very distant point and then we get

661
00:37:01,830 --> 00:37:04,310
we end up with the other class

662
00:37:04,330 --> 00:37:05,250
so if we

663
00:37:05,310 --> 00:37:10,840
around the replicator dynamics from randomly generated by this example end that we got this

664
00:37:10,840 --> 00:37:12,360
one or this one

665
00:37:13,700 --> 00:37:18,120
let me show you the

666
00:37:20,770 --> 00:37:21,980
these doesn't

667
00:37:23,680 --> 00:37:25,790
OK this is

668
00:37:25,900 --> 00:37:28,580
the plot of the characteristic vector

669
00:37:28,600 --> 00:37:30,670
of this dominance here

670
00:37:30,680 --> 00:37:35,200
and this is the plot of the characteristic vector this so we just blocked

671
00:37:36,110 --> 00:37:38,920
values in the convergence factors

672
00:37:38,970 --> 00:37:40,140
so you can see

673
00:37:40,150 --> 00:37:42,420
this blue one corresponds to this

674
00:37:42,430 --> 00:37:46,540
set of course we've all zeros here which correspond to this one

675
00:37:46,630 --> 00:37:50,650
but the nice thing is that you don't have that distribution of

676
00:37:50,670 --> 00:37:52,970
the more central is the point

677
00:37:53,040 --> 00:37:58,310
the higher is the value so we have a sort of bell shape structure

678
00:37:58,320 --> 00:38:02,490
so in one sense if i if i want to extract the representative for this

679
00:38:02,490 --> 00:38:07,020
class i just take the the highest value here that would be represented in one

680
00:38:08,210 --> 00:38:10,180
the most central one

681
00:38:10,340 --> 00:38:13,400
know how this is the same happens here

682
00:38:13,450 --> 00:38:19,310
this is zero this is the class zero then you have this bell shape function

683
00:38:19,320 --> 00:38:24,220
so this is important practical applications for example suppose that we have a large database

684
00:38:24,220 --> 00:38:25,560
of images

685
00:38:25,580 --> 00:38:32,590
and i know you horses dogs whatever we want to cluster the images into categories

686
00:38:32,670 --> 00:38:35,510
category down category or whatever

687
00:38:35,520 --> 00:38:37,520
then we have now a query image

688
00:38:37,540 --> 00:38:39,980
we want to know which category seem

689
00:38:40,060 --> 00:38:42,340
there's a horse dog whatever

690
00:38:42,360 --> 00:38:46,550
we might want to compare this image with all the images in the dataset is

691
00:38:46,560 --> 00:38:47,910
quite time consuming

692
00:38:48,060 --> 00:38:49,210
you i want to

693
00:38:49,300 --> 00:38:52,650
compared to query image with the product instead

694
00:38:53,750 --> 00:38:58,000
if we have a problem for large data datasets will allow us to

695
00:38:58,010 --> 00:38:59,540
to save a lot of time

696
00:38:59,550 --> 00:39:05,770
for example you can these images the from

697
00:39:05,820 --> 00:39:10,760
well there are several ways to do that for example you can

698
00:39:12,070 --> 00:39:13,100
for example

699
00:39:13,140 --> 00:39:15,840
think about shape actually

700
00:39:15,910 --> 00:39:19,430
this is a bit misleading because we never said that we are not talking about

701
00:39:20,740 --> 00:39:24,760
the images and then we have similarity between images

702
00:39:24,800 --> 00:39:26,980
so we don't transform the image

703
00:39:27,040 --> 00:39:29,610
and to point is the key point

704
00:39:29,670 --> 00:39:32,560
as we are working on completing very one set

705
00:39:32,590 --> 00:39:36,920
what we need to do is a measure of similarity between

706
00:39:36,980 --> 00:39:40,280
so there are several ways which we can compare

707
00:39:40,330 --> 00:39:43,330
images once you have that can learn from

708
00:39:48,260 --> 00:39:51,820
thing which is worth noting is that

709
00:39:51,830 --> 00:39:55,500
remember the starting point and stress in the spine which is quite important

710
00:39:55,520 --> 00:39:59,070
we started from the beginning

711
00:39:59,140 --> 00:40:03,530
from a very different question we ask ourselves what is the cluster then we derive

712
00:40:03,550 --> 00:40:05,420
the notion

713
00:40:05,480 --> 00:40:12,460
while the other approaches asking different different question how can i divide organised

714
00:40:12,480 --> 00:40:19,080
partition is a partition the talking to partition the data set into the best possible

715
00:40:19,090 --> 00:40:22,980
OK if we do this way i mean if you the partition one

716
00:40:23,030 --> 00:40:25,150
we have no way of solving

717
00:40:25,170 --> 00:40:28,920
very important problem in computer vision which is the

718
00:40:28,930 --> 00:40:31,070
background foreground color

719
00:40:31,110 --> 00:40:36,690
how can extract the foreground objects from the background and it's the particular case of

720
00:40:36,690 --> 00:40:38,060
image segmentation

721
00:40:38,060 --> 00:40:41,210
way which is different from one i four

722
00:40:41,230 --> 00:40:45,760
we're not dividing the image into different segments just extracting

723
00:40:45,830 --> 00:40:47,600
not just from that

724
00:40:47,630 --> 00:40:50,100
so now there's is evidence

725
00:40:50,150 --> 00:40:52,080
both psychological

726
00:40:52,080 --> 00:40:54,100
is logically that

727
00:40:54,140 --> 00:40:55,320
they were

728
00:40:55,360 --> 00:40:57,330
visual systems quite good

729
00:40:58,040 --> 00:41:01,960
in extracting foreground from background

730
00:41:02,010 --> 00:41:04,400
you for example in this

731
00:41:04,440 --> 00:41:06,250
receptors scenes

732
00:41:06,330 --> 00:41:08,020
you have some not

733
00:41:08,030 --> 00:41:10,920
noisy lines was we can

734
00:41:10,980 --> 00:41:14,260
extract very easily history circles here

735
00:41:14,270 --> 00:41:15,340
the shape

736
00:41:15,350 --> 00:41:17,280
and even if we are not here

737
00:41:17,320 --> 00:41:19,170
history circus

738
00:41:19,190 --> 00:41:21,990
but that is about problems

739
00:41:23,390 --> 00:41:25,210
if we take this example

740
00:41:25,210 --> 00:41:30,130
and i should say the reason i'm putting this

741
00:41:30,180 --> 00:41:31,990
image processing in

742
00:41:32,950 --> 00:41:38,120
it is because the what we've sort of traditionally called image processing has

743
00:41:38,140 --> 00:41:44,510
really evolved and merged in many ways with lots of other activities

744
00:41:44,550 --> 00:41:49,230
so going back we've had a lot of work traditionally in the signal processing community

745
00:41:49,230 --> 00:41:50,420
on this topic

746
00:41:50,460 --> 00:41:53,480
and more recently in computational photography

747
00:41:53,480 --> 00:41:59,040
where fact computational photography more lasses image processing which is published and siggraph

748
00:41:59,050 --> 00:42:00,510
right so

749
00:42:00,550 --> 00:42:03,620
that's another way to think about it so one of the key concepts has recently

750
00:42:03,620 --> 00:42:06,800
become very popular is this idea of non-local means some of you may have heard

751
00:42:06,800 --> 00:42:08,260
about it

752
00:42:08,270 --> 00:42:11,260
not going on the computer vision we've had

753
00:42:11,270 --> 00:42:17,740
algorithms have been developed in the in the vision community for anisotropic diffusion world

754
00:42:17,760 --> 00:42:21,800
probably familiar to some extent or another with these things of the bilateral filter which

755
00:42:21,800 --> 00:42:25,330
is now found applications in lots of places including graphics

756
00:42:25,370 --> 00:42:30,540
i'm going to graphics there's been the idea of moving least squares

757
00:42:30,590 --> 00:42:35,410
four interpolating three d scan data into smooth surfaces

758
00:42:35,460 --> 00:42:38,740
people have been using the same graphics for a long long time and it turns

759
00:42:38,740 --> 00:42:42,630
out we've been using an image processing people business and using similar idea which calling

760
00:42:42,630 --> 00:42:44,130
it different things

761
00:42:44,150 --> 00:42:50,520
and then you have ideas from machine learning and statistics including boosting spectral clustering and

762
00:42:50,520 --> 00:42:53,520
various other concepts that have found their way into

763
00:42:53,600 --> 00:42:56,050
well we traditionally called image processing

764
00:42:56,070 --> 00:42:58,270
and also in applied math

765
00:42:58,880 --> 00:43:03,870
fairly recently there's been a lot of interest in fact the siam journal on imaging

766
00:43:03,870 --> 00:43:09,270
sciences new journal that publishes image processing things with the a heavy mathematical so really

767
00:43:09,600 --> 00:43:13,590
there's been a convergence of ideas across a lot of different fields and hopefully

768
00:43:13,600 --> 00:43:18,240
my presentation will give you a little rough idea of how these ideas all relate

769
00:43:18,270 --> 00:43:19,790
to each other

770
00:43:19,830 --> 00:43:24,460
OK so let me at least from my spirit perspective give you an overview of

771
00:43:24,460 --> 00:43:29,100
the computational problems in imaging that we deal with so typically you have a real

772
00:43:29,100 --> 00:43:33,270
scene which gets blurred and down sampled and noisy

773
00:43:33,300 --> 00:43:37,280
and the measurements may look like this and then what we might call inverse problems

774
00:43:37,280 --> 00:43:39,780
or reconstruction problems could be

775
00:43:39,800 --> 00:43:44,340
just denoising could be upscaling or interpolation could be learning

776
00:43:44,360 --> 00:43:50,060
for the purposes of this talk the concentrating on the denoising problems but a lot

777
00:43:50,060 --> 00:43:52,700
of what i say will have applications in fact direct use

778
00:43:53,210 --> 00:43:56,080
in a bunch of these other areas as well

779
00:43:56,090 --> 00:44:00,120
so the framework is general enough to sort of subsume algorithms in all of these

780
00:44:00,120 --> 00:44:02,590
different applications

781
00:44:03,400 --> 00:44:07,870
so let me get down to the specifics of the common

782
00:44:07,870 --> 00:44:16,460
framework for all of these concepts is basically nonparametric ways of estimating functions

783
00:44:16,470 --> 00:44:21,030
o or point estimation procedures as as they are known in statistics

784
00:44:21,680 --> 00:44:25,960
the problem is in data fitting problem so let's imagine like i said that the

785
00:44:25,960 --> 00:44:28,550
problem of denoising we are given an image

786
00:44:28,560 --> 00:44:33,870
or video and you like to reduce the amount of noise present in the data

787
00:44:33,950 --> 00:44:37,120
so let's say why i here are the pixels

788
00:44:37,140 --> 00:44:38,750
better noisy

789
00:44:39,620 --> 00:44:41,580
e of x y

790
00:44:41,590 --> 00:44:47,310
our pixel values at some position x i and e here is the air so

791
00:44:47,430 --> 00:44:51,680
the data to model is like this so as i said these are the samples

792
00:44:51,700 --> 00:44:55,160
this is what we call is what we call the regression function the function we

793
00:44:55,160 --> 00:44:56,620
should try to fit

794
00:44:56,640 --> 00:45:01,470
and the noise we just assumed to be zero mean i i d noise what

795
00:45:01,470 --> 00:45:06,990
we make no other distributional assumptions so i'm not assuming the noises gaussians neural plasticity

796
00:45:06,990 --> 00:45:09,770
and or anything just that

797
00:45:09,810 --> 00:45:12,640
so if you look at this picture the idea is that you have a bunch

798
00:45:12,640 --> 00:45:14,590
of data points

799
00:45:14,590 --> 00:45:17,870
and what you like to do is to estimate the value of the function at

800
00:45:17,900 --> 00:45:19,500
particular points

801
00:45:19,550 --> 00:45:24,740
given this data so now this data i denoted here y one y two through

802
00:45:24,740 --> 00:45:28,450
y and this data could be the entire image

803
00:45:28,500 --> 00:45:33,490
or it could be just some patch of the image right so essentially what you're

804
00:45:33,490 --> 00:45:34,710
interested in is

805
00:45:34,750 --> 00:45:37,560
estimating the value of one pixel

806
00:45:37,610 --> 00:45:40,950
given either the whole image or some patches which

807
00:45:40,960 --> 00:45:45,270
and the particular form of the index is going to remain unspecified for the time

808
00:45:45,270 --> 00:45:47,700
being and this is actually part of the strength of this is that we are

809
00:45:47,700 --> 00:45:51,070
not using a model based approach in typical

810
00:45:51,090 --> 00:45:55,710
traditional approaches and in image processing what we've done is we've sort of assume some

811
00:45:55,710 --> 00:45:58,270
and I said, Rick I need some help

812
00:45:58,330 --> 00:46:00,270
and he put me in touch with with the

813
00:46:00,390 --> 00:46:08,450
top researchers, who were developing melanoma therapeutics and and these guys had some very, very much more

814
00:46:08,760 --> 00:46:13,920
interesting offerings, but none of them could tell me which one was best for me.

815
00:46:13,950 --> 00:46:17,190
and that was the challenge, that's where the guesswork comes in.

816
00:46:17,360 --> 00:46:21,910
what I wanted like any good AI researcher would want is that at each point in

817
00:46:21,910 --> 00:46:23,360
my cancer journey

818
00:46:23,410 --> 00:46:27,290
to be able to have all of the world's knowledge and resources together with the decision

819
00:46:27,290 --> 00:46:30,010
tools to be able to make the optimal decision.

820
00:46:30,020 --> 00:46:31,790
and then because I am

821
00:46:31,820 --> 00:46:36,760
public minded I would like to capture the outcomes good or bad and at each

822
00:46:36,760 --> 00:46:40,440
stage so that they were there to improve the decisions for the next patient coming

823
00:46:41,880 --> 00:46:44,400
in the end, though, I had to make a gut call

824
00:46:44,410 --> 00:46:46,750
and I put my, bet my life

825
00:46:46,760 --> 00:46:50,060
on a clinical trial that failed.

826
00:46:50,080 --> 00:46:55,710
and what's what's interesting about this is the whole notion of clinical trials, which the

827
00:46:55,710 --> 00:47:01,710
entire drug industry relies on to develop drugs, it's very expensive, very time consuming it's where a lot of

828
00:47:01,770 --> 00:47:03,210
those fifteen years go.

829
00:47:03,270 --> 00:47:08,060
but the problem with clinical trials and a disease like cancer, which as I said is very

830
00:47:08,060 --> 00:47:14,330
heterogeneous, is that when you recruit a hundred or a thousand patients for a trial you're really dealing

831
00:47:14,330 --> 00:47:16,730
with a fruit cocktail of diseases.

832
00:47:16,750 --> 00:47:20,540
and so when a drug, you know, works on forty percent of

833
00:47:20,640 --> 00:47:24,010
patients and another drug works on ten percent of the patients

834
00:47:24,020 --> 00:47:26,000
is the forty percent drug better for you?

835
00:47:26,020 --> 00:47:30,400
well, not necessarily, it depends whether you're more similar to that ten percent of the

836
00:47:30,400 --> 00:47:32,760
patients or the forty percent.

837
00:47:32,770 --> 00:47:39,590
another serious problem is that many, many trials fell because they only helped one or

838
00:47:39,590 --> 00:47:41,090
two people.

839
00:47:41,100 --> 00:47:45,250
so, come on, what disease did we cure,

840
00:47:45,460 --> 00:47:47,770
these people have that we've cured?

841
00:47:47,790 --> 00:47:51,310
and these learnings are typically lost

842
00:47:51,350 --> 00:47:53,540
so this is another opportunity

843
00:47:53,560 --> 00:47:59,630
to ask ourselves, you know maybe we've already cured twenty percent, fifty percent of cancer, who

844
00:47:59,630 --> 00:48:02,210
knows? we just don't know it yet

845
00:48:02,220 --> 00:48:05,220
and I wanna ask a question which i'm going to repeat several times today

846
00:48:05,220 --> 00:48:07,090
do you think a little AI can help?

847
00:48:07,110 --> 00:48:09,510
and this kind of problem in connecting

848
00:48:10,690 --> 00:48:13,480
excuse me, results to the people who need them?

849
00:48:13,510 --> 00:48:14,650
that's that's

850
00:48:14,710 --> 00:48:19,040
where I think we can make a contribution. when i talked about we have much more

851
00:48:19,040 --> 00:48:24,780
knowledge about cancer. today we understand concer at a cancer at a fairly deep level

852
00:48:24,790 --> 00:48:31,090
in in terms of interactomes of genes with other genes with proteins, proteins with

853
00:48:31,100 --> 00:48:33,480
other proteins, we have signaling diagrams

854
00:48:33,510 --> 00:48:38,650
such as this one. much of what we know is probably wrong or incomplete, but

855
00:48:38,660 --> 00:48:43,750
we know things and here's a couple of cancer pathways that are responsible for a

856
00:48:43,750 --> 00:48:49,840
large number of melanomas and on on the left the yellow pathway is a MAP kinase pathway

857
00:48:49,840 --> 00:48:54,770
it's the one that makes the gas pedals stick on, so the cells proliferate and on the

858
00:48:56,270 --> 00:48:58,000
my left, my

859
00:48:58,040 --> 00:48:59,820
right, right here, is

860
00:49:00,460 --> 00:49:05,510
the PI three pathway in blue, which is a brake pedal, it's supposed to make

861
00:49:05,510 --> 00:49:10,770
the cells either arrest the cell cycle or destroy themselves

862
00:49:10,780 --> 00:49:16,380
if they're going out of control. the BRAF pathway gene at the top

863
00:49:16,380 --> 00:49:20,890
of the MAP kinase pathway is implicated in about sixty or sixty five percent of

864
00:49:20,890 --> 00:49:25,880
melanomas and just recently there's been a drug that's been developed for it. so we can in

865
00:49:25,950 --> 00:49:30,360
a very targeted way find the mutation and hit it and it will arrest those cancers, they'll

866
00:49:30,370 --> 00:49:31,710
melt away.

867
00:49:31,710 --> 00:49:38,000
the problem is that cancer is smart and it will find ways compensatory mechanisms and

868
00:49:38,360 --> 00:49:39,460
and then

869
00:49:39,870 --> 00:49:43,690
you know some of the surviving clones PI three K might might mutate so take

870
00:49:43,700 --> 00:49:49,540
the breaks off and we've gotta hit that too, unfortunately, there's just now a drug that's coming into

871
00:49:49,640 --> 00:49:51,980
trials for being able to do that.

872
00:49:52,000 --> 00:49:56,390
so the ability to be able to analyse cancer at this causal level

873
00:49:56,410 --> 00:49:57,660
and be able to

874
00:49:57,800 --> 00:50:02,550
place bets by taking drugs and then capture the learnings and use that not just to treat

875
00:50:02,570 --> 00:50:07,040
the patient, but to be able to improve our understanding of whether this causal model

876
00:50:07,040 --> 00:50:13,500
is right is the big breakthrough and leads us to the possibility of trying to treat

877
00:50:14,710 --> 00:50:17,860
to win the war on cancer in a different way, which is one patient

878
00:50:17,860 --> 00:50:20,820
at a time.

879
00:50:20,860 --> 00:50:25,100
so the the basic idea is to do what I wanted to do for me to

880
00:50:25,100 --> 00:50:29,010
be able to tell you for each patient, take all of the knowledge and resources in

881
00:50:29,010 --> 00:50:29,830
the world

882
00:50:29,870 --> 00:50:31,110
and try to

883
00:50:31,120 --> 00:50:36,410
understand what it is that's the best possible treatment for that patient

884
00:50:36,420 --> 00:50:38,040
at that moment in time?

885
00:50:38,160 --> 00:50:43,090
then apply the patient, apply the treatment and see how the patient does and capture

886
00:50:43,090 --> 00:50:47,490
the learnings, not just over that patient, but over all patients in order to drive the

887
00:50:47,490 --> 00:50:53,660
field forward in general. so that's the paradigm and here is basically how it works.

888
00:50:53,710 --> 00:50:58,160
I think about this as taking the fifteen years and the billion dollars and shrinking that down

889
00:50:58,160 --> 00:50:59,330
to the time

890
00:50:59,380 --> 00:51:05,710
and and the dollars the an individual patient might have. so if we're gonna take fifteen years

891
00:51:05,710 --> 00:51:08,220
and shrink it down to a month or two

892
00:51:08,240 --> 00:51:14,620
we're gonna have to make not incremental improvements but major, you know, slices, slashes

893
00:51:14,710 --> 00:51:18,350
to to this timeline. and the first is to get rid of clinical trials, because

894
00:51:18,350 --> 00:51:24,590
they really don't make sense in heterogeneous diseases like cancer, and replace them with deep

895
00:51:24,600 --> 00:51:29,760
genomic analysis of individual patient's tumors.

896
00:51:29,790 --> 00:51:33,390
and then the second thing we're gonna have to do is

897
00:51:33,410 --> 00:51:37,320
to we don't have time to develop new drugs, so we're gonna have to work

898
00:51:37,320 --> 00:51:39,070
with the drugs that we that we

899
00:51:39,080 --> 00:51:40,820
have on hand, but that's

900
00:51:41,340 --> 00:51:45,570
actually a much bigger space than it appears, there are five thousand drugs have been approved

901
00:51:45,570 --> 00:51:51,590
by the FDA and similar organisations around the world, not to mention the investigational agents

902
00:51:51,590 --> 00:51:57,060
that are currently on trial. and many of these drugs we only are, we don't

903
00:51:57,060 --> 00:52:01,340
even have the vaguest idea what the mechanism of action are

904
00:52:01,340 --> 00:52:06,130
for those drugs, even aspirin, right, we're continuing to find new uses for it. so they

905
00:52:06,130 --> 00:52:10,900
the probabilities are pretty good that if we apply computational and systems biology, we can

906
00:52:10,900 --> 00:52:12,580
attempt to find a match

907
00:52:12,590 --> 00:52:18,360
for that patient against some drugs in our in our

908
00:52:18,670 --> 00:52:24,110
either used as intended or used off-label or used in cocktails.

909
00:52:24,140 --> 00:52:28,280
and then when we find something that works, we can test it on other patients and see

910
00:52:28,280 --> 00:52:34,110
how many others it works for and ultimately find subclasses of patients who respond to

911
00:52:34,110 --> 00:52:38,280
particular therapy. so we have the blue patients and the green patients, they respond to

912
00:52:38,280 --> 00:52:40,280
their respective colored pills

913
00:52:40,300 --> 00:52:42,940
and that's what we're trying to do.

914
00:52:43,170 --> 00:52:47,970
someday we'll have it all mapped-out. we'll have what are called treatment guidelines, which are

915
00:52:47,970 --> 00:52:53,300
basically a decision tree where a patient comes in, gets their tumor tested, and depending on the particular

916
00:52:53,300 --> 00:52:54,790
mutations that they have

917
00:52:54,810 --> 00:52:59,030
can get treated with the right drugs. unfortunately there's a large part of

918
00:52:59,040 --> 00:52:59,910
this space

919
00:53:00,820 --> 00:53:06,890
is currently unpopulated, unexplore and that's the opportunity that we have and the challenge that

920
00:53:06,890 --> 00:53:11,490
we have is this to fill out that space one patient at the time.

921
00:53:11,520 --> 00:53:15,920
so I'm going to next talk about how we're doing that and there are

922
00:53:15,920 --> 00:53:17,680
other people who are doing this.

923
00:53:17,690 --> 00:53:23,440
I'm gonna first talk about collabrx one, which is a personalized oncology research service

924
00:53:23,440 --> 00:53:27,480
that I started with my colleagues, mainly Rafael Lehrer

925
00:53:27,940 --> 00:53:30,070
at collabrx.

926
00:53:30,080 --> 00:53:35,840
and then I'm going to talk about cancer commons, which is a rapid learning community,

927
00:53:35,860 --> 00:53:39,690
the goal of which is to be able to capture the learnings from every cancer patient

928
00:53:39,690 --> 00:53:43,430
in order to be able to rapidly grow the field.

929
00:53:43,440 --> 00:53:49,050
so collabrx one starts with a patient and their doctor. patient goes in and gets

930
00:53:49,050 --> 00:53:53,720
a biopsy or surgery and we have some tumor specimens. we want to apply deep

931
00:53:53,730 --> 00:54:00,810
genomic analysis and I'm talking about genome wide snipped copy number expression analysis even

932
00:54:01,850 --> 00:54:08,650
which is becoming, the the prices are imploding, it used to cost a million dollars, five hundred

933
00:54:08,650 --> 00:54:14,360
thousand dollars just five years ago, fifty thousand dollars today,

934
00:54:14,390 --> 00:54:17,520
five thousand dollars next year, I mean the world isn't even

935
00:54:17,550 --> 00:54:21,950
recognizing what's happening on the imploding price crib, we've gotta be ready for that. get

936
00:54:21,950 --> 00:54:25,650
the deep analysis of the tumor, do the analysis of the level of the wiring

937
00:54:25,650 --> 00:54:27,910
diagram that I talked about earlier,

938
00:54:27,920 --> 00:54:34,060
make a recommendation for that patient, feed it back in the report to the doctor.

939
00:54:34,070 --> 00:54:37,840
and then it either works or it doesn't work. if it doesn't work we have

940
00:54:37,840 --> 00:54:39,230
a new data point

941
00:54:39,260 --> 00:54:42,440
and it's not data point in a mass

942
00:54:42,440 --> 00:54:46,980
it's not even one data point, it's thousands of data points from this genome wide analysis.

943
00:54:47,040 --> 00:54:51,660
go back and try to understand now more about that that space that wasn't filled

944
00:54:52,290 --> 00:54:58,720
and of course doing this over multiple patients gives us a chance to really analyse and understand and do leaning.

945
00:54:58,740 --> 00:55:05,030
so the interesting questions of course occur with the outliers, the one or two patients who

946
00:55:05,100 --> 00:55:09,310
responded when no one else did. again what disease did these patients have that

947
00:55:09,310 --> 00:55:10,890
we've already cured?

948
00:55:10,900 --> 00:55:16,310
or there are patients who didn't respond, to go back to the scientists on whose recommendations

949
00:55:16,780 --> 00:55:20,440
we based the treatment plan in the first place and ask what did you not

950
00:55:20,440 --> 00:55:26,430
understand about the disease or about the mechanism of action of the drugs in order to

951
00:55:26,430 --> 00:55:29,440
have led to this result and what can you now learn from the new data

952
00:55:29,440 --> 00:55:30,370
that we have?

953
00:55:30,390 --> 00:55:35,060
the result of all this learning we're trying to capture in a reference model,

954
00:55:35,100 --> 00:55:38,860
which is basically a way to populate out this

955
00:55:38,920 --> 00:55:44,310
tree that I showed you. the reference model here's a picture of a reference model for

956
00:55:44,310 --> 00:55:49,610
melanoma as it stands today. there are set of some types of melanoma, each of

957
00:55:49,610 --> 00:55:56,440
which corresponds to a mutation or a combination of mutations tying back to the pathways. and each of these

958
00:55:56,780 --> 00:56:01,810
subtypes are actionable in the sense that there exists a clear

959
00:56:01,850 --> 00:56:06,310
a clear approved laboratory that can be used to determine if a patient

960
00:56:06,310 --> 00:56:11,740
is in or out of that subtype and at least one rational therapy that could

961
00:56:11,740 --> 00:56:14,990
be applied. these don't have to be proved in clinical trials by no means, but

962
00:56:14,990 --> 00:56:21,440
there has to be a rational argument that said if the patient has this biology, this drug ought to work.

963
00:56:21,450 --> 00:56:26,810
and then the goal is to do some some carefully planned experiments, namely patients

964
00:56:26,810 --> 00:56:30,490
come in, we find out what subtype they're in, we give them one of the approved

965
00:56:30,510 --> 00:56:36,680
therapies, we capture the learnings. some patients are going to respond, some won't and that's going

966
00:56:36,680 --> 00:56:39,350
to inevitably lead to splitting those subtypes.

967
00:56:40,140 --> 00:56:44,330
this list that you see here is the seed list that was created by talking to a

968
00:56:44,330 --> 00:56:50,320
lot of experts and then doing a meta-analysis of the research literature and the data

969
00:56:50,320 --> 00:56:53,940
and there are ten of them. in fact the bottom one has already dropped off

970
00:56:53,940 --> 00:56:55,680
there's only nine, but

971
00:56:55,800 --> 00:56:59,520
I'm pretty sure in a year or two we're gonna have twenty or thirty or

972
00:56:59,520 --> 00:57:03,210
forty, I don't know how many. and the interesting thing is that some of those

973
00:57:03,210 --> 00:57:06,510
subtypes are going to look a lot like subtypes in other cancers that are

974
00:57:06,510 --> 00:57:07,600
not melanoma

975
00:57:07,660 --> 00:57:10,410
and that's where we're going to be able to get the cross learning to

976
00:57:12,090 --> 00:57:18,660
so I'm now gonna talk about cancer commons, which is a grand experiment to

977
00:57:18,660 --> 00:57:22,260
be able to take the learnings from collabrx one and everyone else who is working in

978
00:57:22,260 --> 00:57:24,020
personalized oncology,

979
00:57:24,090 --> 00:57:28,690
which is a field that is really beginning to take off, it's at the very very

980
00:57:28,690 --> 00:57:31,180
kind of leading edge of the innovation curve.

981
00:57:31,190 --> 00:57:37,310
and basically the goal is in this rapid learning community to give each patient the

982
00:57:37,310 --> 00:57:42,310
best possible treatment for them at this moment in time, capture the learnings, analyse

983
00:57:42,940 --> 00:57:48,760
try to do incorporate those learning back into the into the causal models and try

984
00:57:48,760 --> 00:57:50,830
to drive the field forward in this way.

985
00:57:50,890 --> 00:57:56,370
so cancer commons is not only that community, but a platform for real-time translational research in

986
00:57:56,370 --> 00:58:02,810
cancer on which this community can collaborate with each other and the community includes importantly not

987
00:58:02,810 --> 00:58:08,680
just doctors and researchers but patients and all of the ecosystem of service providers

988
00:58:08,680 --> 00:58:12,920
that are needed in order to be able to get them to do this

989
00:58:12,920 --> 00:58:14,970
guerrilla war, this aggressive style

990
00:58:15,020 --> 00:58:18,080
of N of one research.

991
00:58:18,110 --> 00:58:24,540
the cancer commons platform is lives in the cloud it's very scalable and we've used it for building

992
00:58:24,540 --> 00:58:29,610
communities that many different scales all of which are connected, so we've used it first

993
00:58:30,030 --> 00:58:34,680
to implement collabrx one, the service I just talked about, and the important thing there is

994
00:58:34,680 --> 00:58:37,870
that this is a completely virtual service in the sense that I don't have any

995
00:58:37,870 --> 00:58:40,430
laboratories, the closest thing I have as a lab

996
00:58:40,450 --> 00:58:41,630
is my kitchen.

997
00:58:41,650 --> 00:58:48,490
but we do we outsource everything to different service providers in that ecosystem this bespeaks

998
00:58:48,490 --> 00:58:53,460
of my e-commerce background. we've also used it to together a community of

999
00:58:53,460 --> 00:58:59,920
researchers, who are involved in helping to develop the melanoma reference model and we're now putting

1000
00:59:00,310 --> 00:59:04,620
collabrx one together with this reference model so that we can use the reference model

1001
00:59:04,620 --> 00:59:08,770
to guide the initial treatment of patients, and then when patients don't respond to take

1002
00:59:08,770 --> 00:59:13,560
their specimens and put it through the very same research process that was used to capture the model

1003
00:59:13,560 --> 00:59:15,160
in the first place.

1004
00:59:15,170 --> 00:59:21,270
we are now hoping to move forward and use this same platform to connect many

1005
00:59:21,270 --> 00:59:25,680
groups like the group was involved in creating the initial reference model in order to

1006
00:59:25,680 --> 00:59:27,720
be able to explore

1007
00:59:28,480 --> 00:59:36,540
different combinations of melanoma therapies, different combinations of investigational agents which are on trial

1008
00:59:36,920 --> 00:59:42,730
which is truly revolutionary because drug companies traditionally have not allowed their drugs in trial

1009
00:59:42,730 --> 00:59:44,510
to be tested in combination.

1010
00:59:44,530 --> 00:59:49,340
but like AIDS it's often the case that these therapies won't work as monotherapies,

1011
00:59:49,340 --> 00:59:53,070
they have to be used in combination so we're trying to put together a

1012
00:59:53,120 --> 00:59:57,810
community built on the cancer commons platform so that when a patient comes in, they

1013
00:59:57,810 --> 01:00:00,880
will get routed to the trial that's most likely to help them.

1014
01:00:00,880 --> 01:00:04,380
well according to my name tag i'm andrew blake

1015
01:00:04,400 --> 01:00:08,120
and so this must be the vision lecture

1016
01:00:08,540 --> 01:00:15,070
i know you've been working incredibly hard i have to admire your stamina to be

1017
01:00:15,070 --> 01:00:18,770
able to keep going four thousand and still come back for more seems to me

1018
01:00:19,200 --> 01:00:23,400
incredible and you know the evidence of how hard you work is that of the

1019
01:00:23,400 --> 01:00:25,950
air in this room seem to have been breeds already

1020
01:00:25,950 --> 01:00:30,860
but i have there's enough left for to just you know one more burst of

1021
01:00:30,880 --> 01:00:32,840
of concentration

1022
01:00:32,860 --> 01:00:34,850
so the organisers gave me

1023
01:00:34,860 --> 01:00:37,600
you know very broad brief computer vision

1024
01:00:38,440 --> 01:00:41,480
you can't talk about the whole of computer vision but you know i'm going to

1025
01:00:41,480 --> 01:00:46,660
do a little bit of scene-setting with respect to computer vision first and then sort

1026
01:00:46,660 --> 01:00:53,680
of halfway through today funnels and fairly sharply to discuss one particular problem innovation and

1027
01:00:53,680 --> 01:00:56,470
how to model it probabilistically

1028
01:00:57,330 --> 01:00:59,650
how to do inference

1029
01:00:59,680 --> 01:01:00,860
with that model

1030
01:01:02,480 --> 01:01:04,830
because you had a lot of different views now

1031
01:01:04,910 --> 01:01:10,600
of different ways of setting up probabilistic models and doing inference and the vision community

1032
01:01:10,600 --> 01:01:16,240
have their own i guess you probably say somewhat perverse way of doing things and

1033
01:01:16,700 --> 01:01:20,630
what have singular would be will be more polite

1034
01:01:20,660 --> 01:01:28,020
and so that's what i'm going to tell you that

1035
01:01:28,070 --> 01:01:30,740
so what is

1036
01:01:30,760 --> 01:01:32,160
vision what we mean by

1037
01:01:32,180 --> 01:01:35,020
you this is the problem we want to solve with vision but i thought i'd

1038
01:01:35,020 --> 01:01:36,930
just show you a few

1039
01:01:36,940 --> 01:01:40,930
things in vision that the work because there are many more things that we

1040
01:01:41,050 --> 01:01:45,500
i like to have working for example i'd like to have this video working the

1041
01:01:45,500 --> 01:01:49,130
sony career knowledge but i can't get down to work so i think i have

1042
01:01:49,130 --> 01:01:53,600
to act that one out but you know this is this is a very in

1043
01:01:53,600 --> 01:01:59,610
fact contrary to appearances i am it's only career robot just rather big size and

1044
01:01:59,630 --> 01:02:03,380
that's why i'm so very kind of mobile and you know my joints are very

1045
01:02:03,610 --> 01:02:05,690
human-like and

1046
01:02:05,710 --> 01:02:10,220
i can dance and catchable when it's thrown to me and all this kind of

1047
01:02:10,220 --> 01:02:14,620
thing so that's what that's that's that's you know be good to make the japanese

1048
01:02:14,620 --> 01:02:21,310
particularly because the honda as team and various other japanese robots the japanese actually hold

1049
01:02:21,310 --> 01:02:24,070
the show every now and again a sort of national show a bit like you

1050
01:02:24,070 --> 01:02:29,130
know we would have added big games something that you have a robot show where

1051
01:02:29,130 --> 01:02:34,810
all the japanese public come and sort of interact with robots and the robots to

1052
01:02:34,810 --> 01:02:39,810
soak tested for robot that has managed to run around four hundred and eighty days

1053
01:02:40,120 --> 01:02:44,780
without you know being rude to anyone or falling over is allowed in the kind

1054
01:02:44,780 --> 01:02:48,730
of very public area of the show and the robot it's not quite so services

1055
01:02:48,730 --> 01:02:51,410
that gets sort put in the pan you can look at the people but not

1056
01:02:51,410 --> 01:02:54,620
actually get too close to and so on and this is the sort of this

1057
01:02:54,670 --> 01:02:56,840
national sport so the

1058
01:02:56,850 --> 01:03:00,850
the japanese clearly one of these very much

1059
01:03:00,850 --> 01:03:02,030
i don't know if i want

1060
01:03:02,060 --> 01:03:06,790
sort of human looking thing serving drinks in my house you know that sort of

1061
01:03:06,850 --> 01:03:10,760
old-fashioned view of robots but you know i think may be more reasonable view is

1062
01:03:10,760 --> 01:03:14,340
that you would like a little box on the floor that you know when you're

1063
01:03:14,340 --> 01:03:17,950
older and you fall over or common sort of take a picture of you and

1064
01:03:18,040 --> 01:03:21,910
hold my telephone to your mouth so you can call the call centre and be

1065
01:03:21,910 --> 01:03:25,530
rescued i mean you know people taking that kind of thing pretty seriously and

1066
01:03:25,540 --> 01:03:27,410
fair enough i think

1067
01:03:27,420 --> 01:03:32,690
what else might we like to do with vision well there's the

1068
01:03:32,700 --> 01:03:35,820
these kinds of things this is the

1069
01:03:37,130 --> 01:03:39,920
the stanford robocar

1070
01:03:39,970 --> 01:03:43,530
this is the sort of next generation of the one that won the

1071
01:03:43,540 --> 01:03:48,540
dark challenge in two thousand four cross crossing the harvey desert you sort of read

1072
01:03:48,540 --> 01:03:51,630
about this and following this that you know there's is that the challenge was to

1073
01:03:51,630 --> 01:03:54,310
go on a hundred mile course and

1074
01:03:54,320 --> 01:03:59,100
you GPS to tell you where to go but lots of unexpected obstacles and sort

1075
01:03:59,100 --> 01:04:02,380
of sheer drops and all this kind of stuff and

1076
01:04:02,390 --> 01:04:03,640
anyway this version

1077
01:04:04,410 --> 01:04:09,230
and that one have vision to to help detect obstacles different kinds of vision and

1078
01:04:09,230 --> 01:04:14,390
this one has vision to help it sort of renegotiate its environment even when they

1079
01:04:14,390 --> 01:04:18,530
are moving of the moving parts of the environment so actually

1080
01:04:18,560 --> 01:04:21,430
in the is that they could do this kind of thing so if you if

1081
01:04:21,430 --> 01:04:25,930
you're coming up to another robot and you want to take it the course organiser

1082
01:04:25,930 --> 01:04:30,070
came out the great big red button stop the whole race and you know that

1083
01:04:30,070 --> 01:04:33,340
guy had to stay well this guy was in past and you know that was

1084
01:04:33,340 --> 01:04:36,550
that was as much as they could do couldn't deal with moving obstacles it wasn't

1085
01:04:36,550 --> 01:04:39,550
in the room in the rules now that's the kind of

1086
01:04:39,560 --> 01:04:45,130
a great sort of demonstration i guess of you know the robustness of visual inference

1087
01:04:45,130 --> 01:04:48,380
can be embedded in engineering system

1088
01:04:48,400 --> 01:04:52,110
you might not one of those yourself because you have to be a little bit

1089
01:04:52,110 --> 01:04:57,400
nervous of of driving a car that was driving itself not not even asking your

1090
01:04:57,400 --> 01:05:02,550
opinion about about where it should go so maybe something more acceptable to kind of

1091
01:05:02,550 --> 01:05:05,840
spin-off that you might expect from that kind of research would be

1092
01:05:07,450 --> 01:05:14,360
this your cruise control from daimler-benz you know daimler daimler benz are really in the

1093
01:05:14,360 --> 01:05:19,820
forefront of your combining vision and radar and all kinds of sensors to make a

1094
01:05:19,820 --> 01:05:22,690
car whose default behaviour is

1095
01:05:22,730 --> 01:05:29,170
more natural so the most primitive kind of cruise control the steering is the is

1096
01:05:29,170 --> 01:05:33,050
the castor action of the wheels in your car you all cars have this the

1097
01:05:33,050 --> 01:05:38,680
wheels tend to snap to the to the forward position that's because they're mounted slightly

1098
01:05:38,690 --> 01:05:45,640
obliquely and so they can mechanically some mechanical intelligence building but imagine if that sort

1099
01:05:45,640 --> 01:05:52,260
of behavior is controlled electronically and in response to sensory data and so on

1100
01:05:55,170 --> 01:05:59,810
what this one is this is the mercedes-benz

1101
01:05:59,820 --> 01:06:03,300
so you be quite nice

1102
01:06:04,130 --> 01:06:06,990
to have a car did that i guess

1103
01:06:07,110 --> 01:06:08,610
the disturbing

1104
01:06:08,610 --> 01:06:13,770
so so not not not not everything's peachy can kind see here

1105
01:06:13,790 --> 01:06:18,360
that is cynicism sort of inclusion for the guy got to could drink which we

1106
01:06:18,370 --> 01:06:21,230
what i see me trying to stop the students

1107
01:06:23,370 --> 01:06:27,470
system falls apart consider this looks nothing like it

1108
01:06:27,490 --> 01:06:32,900
to there again

1109
01:06:33,150 --> 01:06:36,230
and so on

1110
01:06:36,290 --> 01:06:39,650
to to get around that we go back to our friend the robust error function

1111
01:06:39,730 --> 01:06:44,520
parameter function to actually help a lot with that it slows things down but essentially

1112
01:06:44,520 --> 01:06:48,770
what we do we should register function again this instead of making this least squares

1113
01:06:48,860 --> 01:06:52,780
you kind of put a cap on how much appearance variation each pixel can have

1114
01:06:52,780 --> 01:06:57,110
so if something comes in and so it's the across something it's extremely outside the

1115
01:06:57,250 --> 01:07:00,700
realm of what the the pixel variation should be

1116
01:07:00,740 --> 01:07:04,680
we don't high that as much weight as we would with the least squares

1117
01:07:06,850 --> 01:07:09,400
robust error function is we shown before

1118
01:07:09,480 --> 01:07:15,980
and this was some improvements

1119
01:07:15,990 --> 01:07:19,010
required for

1120
01:07:19,020 --> 01:07:21,590
you can kind of see here the traditional i

1121
01:07:21,640 --> 01:07:23,270
falls apart

1122
01:07:23,380 --> 01:07:31,040
things but with the robust i am everything's basically could

1123
01:07:31,610 --> 01:07:36,320
we've got some interesting useful applications software

1124
01:07:36,360 --> 01:07:43,520
someone is standing before about a mass replacement things so we've actually got so it's

1125
01:07:43,520 --> 01:07:45,570
not something like this

1126
01:07:45,580 --> 01:07:52,730
essentially what we did with i with that kind of set of

1127
01:07:52,750 --> 01:07:55,780
basically using someone's head in terms of rotation

1128
01:07:56,120 --> 01:07:58,810
in terms of being able to define a point and so

1129
01:07:58,850 --> 01:08:02,250
we had some and sit in front of the camera the kind of this is

1130
01:08:02,250 --> 01:08:04,900
moving head around and you can actually

1131
01:08:04,960 --> 01:08:08,860
moved to different points and when he blinked you can actually click on the point

1132
01:08:08,950 --> 01:08:11,480
things so

1133
01:08:11,490 --> 01:08:14,620
it's kind of cool cool them i i don't know if it's going to be

1134
01:08:14,620 --> 01:08:17,350
replacing mouses anytime soon but

1135
01:08:18,300 --> 01:08:19,510
things then

1136
01:08:19,520 --> 01:08:25,990
we also did some stuff from gaze tracking

1137
01:08:26,000 --> 01:08:29,960
if you in car and stuff and just like you want to make sure what

1138
01:08:30,770 --> 01:08:34,040
does the driver have is always in the right looking at something else

1139
01:08:34,090 --> 01:08:37,570
actually did an experiment to see that we can we use the i in in

1140
01:08:37,570 --> 01:08:41,160
terms of so we added an extra couple mesh points and they are incorrect to

1141
01:08:41,160 --> 01:08:43,900
work out what he is looking at so this is actually what he is looking

1142
01:08:43,900 --> 01:08:47,280
at a face and i interact so it's kind of noisy but you can actually

1143
01:08:47,350 --> 01:08:52,070
the there's a reasonably good job following the target across and things and hopefully system

1144
01:08:52,070 --> 01:08:57,740
applications in impacts drug-safety we're we're we're working with the car company in japan on

1145
01:08:57,740 --> 01:09:02,350
applying this sort of stuff

1146
01:09:05,510 --> 01:09:07,880
another thing that this is really useful for

1147
01:09:07,940 --> 01:09:11,890
is normalisation

1148
01:09:11,900 --> 01:09:14,140
so essentially like kind of

1149
01:09:14,150 --> 01:09:16,370
face moving all over the place here

1150
01:09:16,390 --> 01:09:19,540
and so if you want to come to do could face recognition like we're showing

1151
01:09:19,540 --> 01:09:24,260
for the dam active principles of right and normalising ties

1152
01:09:24,270 --> 01:09:29,100
as long as most pixels are still there so that's that's actually the real time

1153
01:09:29,100 --> 01:09:34,000
variation you can see that as soon as you again this is why i think

1154
01:09:34,040 --> 01:09:38,390
i think one of the first kind are i was reluctant things because well registrations

1155
01:09:38,390 --> 01:09:41,580
interesting things but i think there's a lot of other interesting things in computer vision

1156
01:09:41,580 --> 01:09:43,830
can be doing but

1157
01:09:43,840 --> 01:09:47,060
like you look at something like this and if you do the registration really well

1158
01:09:47,090 --> 01:09:50,740
all the other things he could be appointed really advance machine learning techniques to kind

1159
01:09:50,740 --> 01:09:54,140
of deal with all this pose variation things that if you just register the face

1160
01:09:54,140 --> 01:09:59,640
probably no falls apart i why what you need to anything things so not saying

1161
01:09:59,640 --> 01:10:03,530
not to trivialize the other portions of the machine learning because it's definitely places where

1162
01:10:03,540 --> 01:10:08,890
this falls down but registration is extremely important for computer vision and hopefully

1163
01:10:08,920 --> 01:10:11,030
are they emphasise that

1164
01:10:11,120 --> 01:10:13,830
a bit during this stage during this talk

1165
01:10:15,570 --> 01:10:23,290
and then we also did some stuff on animation generation like this is someone doing

1166
01:10:23,290 --> 01:10:24,740
something and at the time

1167
01:10:24,780 --> 01:10:27,080
the what a cartoon

1168
01:10:27,090 --> 01:10:29,660
things in

1169
01:10:30,590 --> 01:10:33,760
it doesn't it doesn't pretty good job

1170
01:10:34,100 --> 01:10:38,660
actually this guy here in in math he's actually currently over whether

1171
01:10:38,680 --> 01:10:43,380
if you guys know what but it's the company that peter jackson started lord of

1172
01:10:43,380 --> 01:10:46,890
the rings and so he's been working a lot of so

1173
01:10:46,910 --> 01:10:50,530
if you look now lord of the rings sort of a very expensive to try

1174
01:10:50,530 --> 01:10:53,880
and do like column and all the other actors and things like that and so

1175
01:10:53,880 --> 01:10:58,810
what i love maybe maybe companies are very interested in is automating this basically can

1176
01:10:58,890 --> 01:11:00,580
thrown holy factors

1177
01:11:00,590 --> 01:11:03,770
in my essentially so for the next kind of animation

1178
01:11:03,790 --> 01:11:05,060
big animation movie

1179
01:11:05,070 --> 01:11:08,880
and perhaps is automatically animated avatar without

1180
01:11:08,900 --> 01:11:12,460
so someone walking in this acting walking out because there's a lot of the time

1181
01:11:12,710 --> 01:11:16,490
also it's extremely difficult because you have to actually get all these points in the

1182
01:11:16,700 --> 01:11:18,860
end you have to be accurate every time you do it

1183
01:11:18,900 --> 01:11:19,850
and things

1184
01:11:19,850 --> 01:11:21,360
that's another possible

1185
01:11:22,070 --> 01:11:24,240
yes he is applied to i think

1186
01:11:24,240 --> 01:11:27,880
another type is when a person has been defrauded

1187
01:11:27,980 --> 01:11:31,720
and the bank which transactions did you actually do

1188
01:11:31,730 --> 01:11:36,050
and they decide well to compensate themselves has to make to clear here the own

1189
01:11:36,060 --> 01:11:39,500
transactions is being fought as well so they can get the

1190
01:11:39,510 --> 01:11:41,010
the money back

1191
01:11:41,040 --> 01:11:50,320
so the chances of fraud detection they then there's an issue that before the change

1192
01:11:50,790 --> 01:11:55,740
reacted to to detect all these to detect and prevent

1193
01:11:55,760 --> 01:11:58,390
there's a technical issues of the

1194
01:11:58,410 --> 01:12:03,530
these are simple time series we have full credit card fraud these data streams and

1195
01:12:03,530 --> 01:12:06,870
by that i mean that we have data coming in order not read the article

1196
01:12:06,890 --> 01:12:10,820
intervals are coming at a time someone from the transaction we may not have all

1197
01:12:11,490 --> 01:12:17,430
the data concerning or an individual account when we have to make that determination

1198
01:12:19,360 --> 01:12:25,430
in kind two ways we need to make decisions quickly as possible

1199
01:12:25,480 --> 01:12:29,950
so so these are online systems but there is the a possibility is missing in

1200
01:12:29,950 --> 01:12:34,290
this talk we can do some of the processing in the back to of learning

1201
01:12:34,780 --> 01:12:37,180
but the the other

1202
01:12:37,310 --> 01:12:41,240
potential ways of getting around problem of time in this as an anecdote i heard

1203
01:12:42,240 --> 01:12:49,540
when automatic petrol pumps or gas comes in america first started sleeping according to the

1204
01:12:49,540 --> 01:12:53,050
machine and start delivering deal

1205
01:12:53,060 --> 01:12:57,130
there was a time delay between the maxi but according to them to make the

1206
01:12:57,130 --> 01:13:01,510
simulation where they should be allowed to continue this transaction so they decided in this

1207
01:13:01,510 --> 01:13:03,990
simple case was just two automatic allow

1208
01:13:04,000 --> 01:13:07,280
fifty cents or dollars worth of fuel into your car

1209
01:13:07,290 --> 01:13:09,370
and that was to estimate the time to

1210
01:13:09,380 --> 01:13:14,390
to make the decision about whether to allow the the transactions continue any further so

1211
01:13:14,390 --> 01:13:15,760
as an example of where

1212
01:13:15,780 --> 01:13:22,850
custom to inconvenience was trumps the actual losses to fraud

1213
01:13:22,860 --> 01:13:28,450
thomas again in form four in the four quickly as possible a

1214
01:13:28,500 --> 01:13:31,120
can starting to defrauded

1215
01:13:31,140 --> 01:13:34,300
as i said it's not the same amount of time that passed but it's the

1216
01:13:34,300 --> 01:13:40,720
number of transactions really that foreign transaction to be missed before that is being identified

1217
01:13:40,740 --> 01:13:43,600
finally there's the issue of imbalanced classes again

1218
01:13:45,870 --> 01:13:47,800
australia in the last year

1219
01:13:48,120 --> 01:13:53,000
the prior announced that they had about sixty five percent of the total number of

1220
01:13:53,070 --> 01:13:54,890
plastic card transactions

1221
01:13:54,910 --> 01:13:56,940
were rules

1222
01:13:57,130 --> 01:14:06,940
and the issues behind them that being discussed this morning and previously

1223
01:14:07,680 --> 01:14:12,670
with the caveat that david said about this disgusting little bit brought this this is

1224
01:14:12,680 --> 01:14:19,150
roughly two brought the four to ten bits four detection using statistical methods to supervised

1225
01:14:19,300 --> 01:14:21,230
anomaly detection

1226
01:14:21,240 --> 01:14:26,880
supervised methods take historical instances of fraud and b search patterns again

1227
01:14:26,890 --> 01:14:31,630
and again broadly speaking there

1228
01:14:31,640 --> 01:14:37,030
so these patterns that finds a lot more likely before the list less likely to

1229
01:14:37,060 --> 01:14:42,420
have falsely flag which is the transactions of for using this approach that support and

1230
01:14:42,420 --> 01:14:47,170
this page is that course which has been taking

1231
01:14:47,220 --> 01:14:49,910
on the other hand these techniques

1232
01:14:49,930 --> 01:14:54,700
can't find new talk before so this is what normally takes comes in and so

1233
01:14:54,720 --> 01:14:59,570
don't stop instances build profile of usual behaviour of the customer

1234
01:14:59,620 --> 01:15:02,130
any significant deviations from this profile

1235
01:15:02,160 --> 01:15:05,350
it is considered as potentially fortunes flag is that

1236
01:15:05,390 --> 01:15:09,760
again there's plenty of situations where you might do something which is

1237
01:15:09,800 --> 01:15:12,180
unusually for what passes

1238
01:15:12,180 --> 01:15:14,810
this shows that there is more potential for

1239
01:15:14,820 --> 01:15:18,400
falsely flag transactions as forward

1240
01:15:18,420 --> 01:15:22,410
but they were ultimately has potential to that to changing patterns of fraud and this

1241
01:15:22,410 --> 01:15:24,650
is the approach that my quality of true

1242
01:15:24,690 --> 01:15:27,830
it is time

1243
01:15:27,850 --> 01:15:31,570
so big last confidence in the anomaly detection sort of area

1244
01:15:31,580 --> 01:15:34,040
well we don't do this

1245
01:15:34,050 --> 01:15:40,260
we don't build a profile on individual account holder we just look at other accountancy

1246
01:15:40,260 --> 01:15:41,650
who similar

1247
01:15:42,250 --> 01:15:46,810
and the idea that being that if historically a certain number of people

1248
01:15:46,880 --> 01:15:51,300
have been behaving similarly we assume that invites him for a while in the future

1249
01:15:51,540 --> 01:15:56,760
so we do monitor an account with respect to speak of similar accounts and if

1250
01:15:56,780 --> 01:15:59,330
you if he can deviate significantly from

1251
01:15:59,340 --> 01:16:07,710
those people members then we assume that enormous behavior we might flagged as potentially for

1252
01:16:07,720 --> 01:16:10,560
so simple toy example of this

1253
01:16:10,570 --> 01:16:14,530
we this much we had data for one particular account which is just the amount

1254
01:16:14,550 --> 01:16:18,530
this person spent two weeks on their credit card for

1255
01:16:18,630 --> 01:16:20,520
in the in weeks we

1256
01:16:20,530 --> 01:16:22,300
no n minus one

1257
01:16:22,310 --> 01:16:25,530
data may be is for free

1258
01:16:25,570 --> 01:16:30,070
and just to ten point everton we're looking at a particular account wanted become call

1259
01:16:30,070 --> 01:16:32,770
that the target account

1260
01:16:32,780 --> 01:16:37,560
now people should know whether on the week ending the map they spent is anomalous

1261
01:16:37,560 --> 01:16:40,130
somewhere on the texan technique would take over

1262
01:16:40,150 --> 01:16:44,510
the whole previous history there will be some some kind of maybe updating method with

1263
01:16:44,510 --> 01:16:48,300
some subset of the data which is initiated at the bottom there

1264
01:16:48,310 --> 01:16:55,040
and just build a simple program to see whether the value y is an outlier

1265
01:16:55,050 --> 01:16:58,630
but this has the potential problems example would be

1266
01:16:59,940 --> 01:17:03,650
the week was actually the period of the christmas in the UK

1267
01:17:03,690 --> 01:17:07,710
and the and we had not not profile to for an entire year's with the

1268
01:17:07,710 --> 01:17:12,420
data so this likely personally spending an awful lot more

1269
01:17:12,430 --> 01:17:15,970
so one way to get out of that is assuming that the

1270
01:17:16,000 --> 01:17:18,250
the bank has more than one customer

1271
01:17:18,300 --> 01:17:21,380
i guess i should like to joke about northern rock at this point

1272
01:17:21,950 --> 01:17:26,330
if you have any other customers

1273
01:17:26,340 --> 01:17:28,900
well we can and

1274
01:17:29,400 --> 01:17:33,840
basically each column there then represents one week

1275
01:17:33,860 --> 01:17:38,400
the final we we can we can just basing standardised data on that basis we

1276
01:17:38,400 --> 01:17:44,360
can just do some population standardisation just to remove these kind of effects from

1277
01:17:44,430 --> 01:17:48,140
from the data

1278
01:17:48,160 --> 01:17:52,400
so pick just take this one step further what we what we do then take

1279
01:17:52,910 --> 01:17:55,830
the time period from one to n minus one

1280
01:17:55,890 --> 01:18:01,780
and with disorderly accounts with respect to the similarity to update the target counts this

1281
01:18:01,790 --> 01:18:03,060
function pi

1282
01:18:03,090 --> 01:18:06,930
pi one then what would be the index of the account is most similar to

1283
01:18:06,950 --> 01:18:12,250
the target account has appeared from time what we want to we can minus one

1284
01:18:13,680 --> 01:18:18,020
so simply what we do is just take the k closest which is shaded in

1285
01:18:18,020 --> 01:18:20,800
gray in the final column that

1286
01:18:20,800 --> 01:18:25,780
on the final weekend and we just to see what value y is is enormous

1287
01:18:25,800 --> 01:18:29,720
compared to an outlier compared to those values x y one and two x y

1288
01:18:29,720 --> 01:18:34,180
k in that column

1289
01:18:37,000 --> 01:18:42,480
crucial issue about finding outlets appeared is that the the allies himself this necessarily lost

1290
01:18:42,480 --> 01:18:48,720
the population is an example of someone who did on full protection

1291
01:18:48,770 --> 01:18:54,840
what we have here is a day along the sort the only

1292
01:18:54,850 --> 01:18:56,130
x axis

1293
01:19:03,040 --> 01:19:04,570
on the y axis

1294
01:19:04,570 --> 01:19:08,100
and i can calculate now what the angles are

1295
01:19:08,110 --> 01:19:11,250
where on the screen here on the wall

1296
01:19:11,250 --> 01:19:12,900
the maximum of four

1297
01:19:12,920 --> 01:19:14,340
those maximum

1298
01:19:14,360 --> 01:19:15,990
and by the way there will be

1299
01:19:16,100 --> 01:19:21,200
based on the nineteen ninety of the zeroth symmetry

1300
01:19:21,210 --> 01:19:27,390
and these men are so small that you will even see only see the maximum

1301
01:19:27,400 --> 01:19:31,040
so let's calculate what angles we will then see the first

1302
01:19:31,090 --> 01:19:33,480
by the way these things have names we call it

1303
01:19:33,480 --> 01:19:37,030
zero order

1304
01:19:37,060 --> 01:19:39,290
and we call it the first order

1305
01:19:39,340 --> 01:19:40,490
and we call

1306
01:19:40,680 --> 01:19:41,800
this one

1307
01:19:41,820 --> 01:19:43,800
the first order

1308
01:19:43,840 --> 01:19:45,680
and we call the second order

1309
01:19:45,680 --> 01:19:49,050
and this is also called first order but it is on the other side so

1310
01:19:49,060 --> 01:19:50,740
we call these orders

1311
01:19:50,750 --> 01:19:53,740
on the specter

1312
01:19:53,780 --> 01:19:57,130
so you can now calculate the sign of data of and

1313
01:19:57,130 --> 01:20:06,590
it is and times land divided by d

1314
01:20:06,600 --> 01:20:07,880
and so when

1315
01:20:07,880 --> 01:20:09,990
when n is zero

1316
01:20:10,040 --> 01:20:11,060
that year

1317
01:20:11,080 --> 01:20:14,350
you zeros you get of course maximum

1318
01:20:14,430 --> 01:20:17,800
what is zero order

1319
01:20:19,950 --> 01:20:22,780
we call zero

1320
01:20:22,800 --> 01:20:25,350
now you first order

1321
01:20:25,400 --> 01:20:27,980
is when the sign of data one

1322
01:20:29,100 --> 01:20:32,480
is land that divided by g and i can take my lab which in my

1323
01:20:32,480 --> 01:20:34,790
case of my later i have to tell you

1324
01:20:34,830 --> 01:20:35,840
i land

1325
01:20:35,850 --> 01:20:39,240
five hundred and thirty two nanometer

1326
01:20:39,240 --> 01:20:43,600
it green

1327
01:20:43,650 --> 01:20:47,190
so i can calculate what they don't one is and i find sixteen point three

1328
01:20:49,810 --> 01:20:52,640
and then i can go to the second order

1329
01:20:52,650 --> 01:20:56,070
they are two for that color is different for different colors

1330
01:20:56,130 --> 01:20:58,310
and i find thirty four degrees

1331
01:20:58,330 --> 01:21:01,790
and i can go to the third order

1332
01:21:01,830 --> 01:21:03,590
i find theta three

1333
01:21:03,690 --> 01:21:06,080
is then fifty seven degrees

1334
01:21:06,630 --> 01:21:08,990
and there is no fourth order

1335
01:21:09,030 --> 01:21:12,990
that would make the final state larger than one that only

1336
01:21:13,010 --> 01:21:18,450
zero order and then there are first second and third order

1337
01:21:18,530 --> 01:21:20,260
the grain that you have

1338
01:21:20,310 --> 01:21:22,080
i always carried out with me

1339
01:21:22,130 --> 01:21:23,970
no matter where i go it's easy to to

1340
01:21:24,010 --> 01:21:26,510
fourteen your calendar

1341
01:21:26,550 --> 01:21:31,510
and so i will show you know by simply shining through this

1342
01:21:31,520 --> 01:21:32,490
thirty is great

1343
01:21:32,490 --> 01:21:34,040
also either on the wall

1344
01:21:34,070 --> 01:21:35,920
zero order would fall

1345
01:21:35,930 --> 01:21:39,760
right smack in the middle so to speak first order

1346
01:21:39,780 --> 01:21:43,580
sixteen degrees away so if you know my distance to the wall you can calculate

1347
01:21:43,580 --> 01:21:46,150
how far it is the only need enough

1348
01:21:46,180 --> 01:21:47,570
so you'll see these

1349
01:21:48,790 --> 01:21:51,240
this is extremely narrow

1350
01:21:51,290 --> 01:21:53,140
i value for n

1351
01:21:54,330 --> 01:21:56,470
well speaks for itself i have two

1352
01:21:56,470 --> 01:22:00,040
i rotate migrating to make sure that the groups are in this direction

1353
01:22:00,100 --> 01:22:03,340
the groups are in this direction the spreading is out in this direction

1354
01:22:03,360 --> 01:22:04,800
so you see

1355
01:22:04,820 --> 01:22:06,420
so one that you see

1356
01:22:06,450 --> 01:22:10,800
right now on the right side of the screen there is my zero order

1357
01:22:10,910 --> 01:22:14,860
you can tell that it is zero order because they have only one color

1358
01:22:14,910 --> 01:22:16,470
and then you see

1359
01:22:16,520 --> 01:22:19,990
one with the little you see the first order now on the right side of

1360
01:22:19,990 --> 01:22:20,990
the screen

1361
01:22:20,990 --> 01:22:26,920
and that angle should be very accurately about but i calculated the sixteen degrees

1362
01:22:26,950 --> 01:22:29,250
and then you see on the blackboard here

1363
01:22:29,270 --> 01:22:30,500
so it right here

1364
01:22:30,510 --> 01:22:31,980
you see the second order

1365
01:22:32,020 --> 01:22:35,220
and then you see in the order if you have good eyes and the fourth

1366
01:22:35,220 --> 01:22:38,010
order doesn't exist

1367
01:22:38,030 --> 01:22:40,750
and imagine that between those maximum

1368
01:22:40,800 --> 01:22:45,580
if i really u sixteen on lines there will be fifteen hundred and ninety nine

1369
01:22:45,580 --> 01:22:53,530
point with exact zeros and then all the silly minimax semantic you don't even see

1370
01:22:53,630 --> 01:22:55,300
that is the power

1371
01:22:55,350 --> 01:22:57,650
of grading when you use

1372
01:22:57,680 --> 01:23:00,910
many lines

1373
01:23:00,990 --> 01:23:05,080
if i use sixteen hundred lines i use eight hundred two

1374
01:23:05,140 --> 01:23:09,340
times more lines than when i have a double slit interferometer

1375
01:23:09,530 --> 01:23:13,760
double slit interference will be extremely different from this

1376
01:23:13,780 --> 01:23:17,010
that's why those locations i so narrow

1377
01:23:17,060 --> 01:23:19,950
because it would double slit interference you will get

1378
01:23:19,990 --> 01:23:21,840
cosine square function

1379
01:23:21,900 --> 01:23:23,670
and so you would see

1380
01:23:23,710 --> 01:23:27,240
the maximum will be a hundred times brawler

1381
01:23:27,250 --> 01:23:28,700
and this

1382
01:23:28,750 --> 01:23:30,780
that's the power of using

1383
01:23:32,440 --> 01:23:37,320
a hundred lines

1384
01:23:37,380 --> 01:23:41,170
now i want you to get your gratings out

1385
01:23:41,220 --> 01:23:42,800
and i wanted to look

1386
01:23:42,860 --> 01:23:44,710
simply at

1387
01:23:49,750 --> 01:23:53,030
and the reason why i want you to do that is that

1388
01:23:53,090 --> 01:23:54,450
i wanted to be

1389
01:23:57,110 --> 01:23:59,260
because you may not see

1390
01:23:59,330 --> 01:24:01,770
what you erroneously expected

1391
01:24:01,820 --> 01:24:04,040
that is zero order

1392
01:24:05,100 --> 01:24:07,570
incredibly narrow

1393
01:24:07,580 --> 01:24:08,960
it is not

1394
01:24:09,000 --> 01:24:12,080
its brain it's the lamp itself

1395
01:24:12,090 --> 01:24:14,970
of course if you light source

1396
01:24:14,980 --> 01:24:18,090
in angular size is way larger

1397
01:24:18,110 --> 01:24:23,400
then this angular size you cannot expect to see the light source gets more

1398
01:24:23,420 --> 01:24:24,510
in other words

1399
01:24:24,550 --> 01:24:28,030
clearly the limiting factor of seeing

1400
01:24:28,070 --> 01:24:29,210
zero order

1401
01:24:29,230 --> 01:24:30,280
very narrow

1402
01:24:30,280 --> 01:24:32,980
is only if the light source itself

1403
01:24:32,980 --> 01:24:35,860
it is small enough in size

1404
01:24:35,940 --> 01:24:38,860
so when you look at this light now

1405
01:24:38,860 --> 01:24:41,280
you will see all colors zero order

1406
01:24:41,300 --> 01:24:44,110
that's one thing that's important you just see the land

1407
01:24:44,130 --> 01:24:49,980
that year zero the maximum that's the red blue green yellow violet that's all of

1408
01:24:51,900 --> 01:24:53,230
and then you will see

1409
01:24:53,300 --> 01:24:55,480
on your site

1410
01:24:55,530 --> 01:24:57,130
will appear

1411
01:24:57,130 --> 01:24:58,150
first the blue

1412
01:24:58,150 --> 01:25:03,220
the square in this exponential you integrate what you actually get is a quadratic form

1413
01:25:03,220 --> 01:25:05,180
that looks like this

1414
01:25:05,190 --> 01:25:08,520
so this tells you that the normalisation constant in this case

1415
01:25:08,530 --> 01:25:10,720
is just a quadratic forms

1416
01:25:10,740 --> 01:25:12,280
in data

1417
01:25:14,370 --> 01:25:18,170
this is a nice little relation i encourage you to do is an exercise if

1418
01:25:18,170 --> 01:25:22,130
you take the dual of the quadratic form that involves q inverse

1419
01:25:22,150 --> 01:25:25,500
the dual will be a quadratic form that involves q

1420
01:25:25,540 --> 01:25:29,780
so q goes to q inverse when you do duels that's that's in isolation to

1421
01:25:31,260 --> 01:25:35,620
so if we put the pieces back together what we're going to get is that

1422
01:25:35,650 --> 01:25:36,680
in order to

1423
01:25:36,720 --> 01:25:42,290
do inference we should actually compute solve this quadratic programme here

1424
01:25:42,300 --> 01:25:46,350
and if you solve that quite quadratic programme you'll see that you

1425
01:25:46,400 --> 01:25:51,220
get the unique solution of new high schools who inverse times data

1426
01:25:51,240 --> 01:25:53,530
and it's worth

1427
01:25:53,620 --> 01:25:57,600
remembering now remember how motivated things to begin with i said since they you want

1428
01:25:57,600 --> 01:25:59,130
to compute a

1429
01:25:59,140 --> 01:26:05,400
the matrix inverse problem that's basically what gets you an inference is it to compute

1430
01:26:05,400 --> 01:26:08,110
the mean you have to do matrix inverse

1431
01:26:08,950 --> 01:26:13,350
what i said is one variational representation would involve the quadratic form

1432
01:26:13,400 --> 01:26:17,490
if you solve this you get this solution this is the the gas you mean

1433
01:26:17,490 --> 01:26:18,650
you can check

1434
01:26:18,660 --> 01:26:22,300
so what this is saying is that in this particular case we we took our

1435
01:26:22,300 --> 01:26:23,820
general principle

1436
01:26:23,880 --> 01:26:28,280
and if we specialise in go through the the step of calculations

1437
01:26:28,280 --> 01:26:33,610
that in fact used to cover what already is familiar very rate variational representation to

1438
01:26:34,980 --> 01:26:38,870
so what was easy here a couple things that are easy

1439
01:26:38,910 --> 01:26:43,050
it was nice that we had an explicit form for the dual function

1440
01:26:43,260 --> 01:26:48,670
and what was also easy these mean parameters these are expectations from user expectations of

1441
01:26:48,670 --> 01:26:50,200
these guys

1442
01:26:50,510 --> 01:26:54,670
these were just unconstrained here gauss you mean can move wherever it wants right there's

1443
01:26:54,680 --> 01:26:57,430
no constraints on against me

1444
01:26:57,440 --> 01:27:03,450
so that's why this problem and have been very easy just an unconstrained quadratic programme

1445
01:27:03,470 --> 01:27:07,740
OK so any questions about the example

1446
01:27:15,530 --> 01:27:19,170
if you want to get experience with sort of duality calculations just working to this

1447
01:27:19,170 --> 01:27:23,300
example is is a good one it's it's pretty simple not too much algebra and

1448
01:27:23,300 --> 01:27:27,110
it's something that pops up over and over again tools of quadratic forms are an

1449
01:27:27,110 --> 01:27:29,190
important special case

1450
01:27:29,640 --> 01:27:35,450
it would be more ambitious and we also learn the covariance

1451
01:27:35,480 --> 01:27:36,940
number now i had

1452
01:27:36,970 --> 01:27:41,540
sufficient statistics where the matrix of sufficient statistics like this

1453
01:27:41,890 --> 01:27:44,850
i had a matrix of parameters like this

1454
01:27:46,470 --> 01:27:50,580
this examples of trickier to compute but it also leads to another kind of convex

1455
01:27:50,580 --> 01:27:52,690
programming that's interesting

1456
01:27:52,750 --> 01:27:57,290
sort of one step up from quadratic programs

1457
01:27:57,380 --> 01:28:02,300
so what i'm doing here is like compute the log normalisation constant i have to

1458
01:28:02,300 --> 01:28:05,680
integrate this thing but i i have matrix this

1459
01:28:06,670 --> 01:28:12,580
it represents the inner product of the matrix with this matrix of sufficient statistics

1460
01:28:12,600 --> 01:28:16,080
so if you actually do this integral

1461
01:28:16,110 --> 01:28:21,400
what you find is the dual function involves a log determinant of the matrix of

1462
01:28:21,400 --> 01:28:23,250
mean parameters

1463
01:28:23,320 --> 01:28:27,620
and for those of you have experience with kelsey entropy

1464
01:28:27,630 --> 01:28:32,480
you recognise this is exactly the the negative of calcium entropy

1465
01:28:32,490 --> 01:28:36,280
one interpretation is that

1466
01:28:36,290 --> 01:28:39,610
determinant tells you something about the volume

1467
01:28:40,300 --> 01:28:45,500
enclosed by something else seems like a big ellipsoid is what sam showed us and

1468
01:28:45,500 --> 01:28:48,870
the determinant tells you something about the volume of an ellipsoid

1469
01:28:48,880 --> 01:28:52,610
the volume volumes bigger than the uncertainty is somehow bigger

1470
01:28:52,620 --> 01:28:58,150
so that's why it's intuitive entropy should be something that be like log determinant it's

1471
01:28:58,150 --> 01:29:01,930
logo of the volume of the uncertainty ellipsoid

1472
01:29:01,940 --> 01:29:03,470
that's the interpretation

1473
01:29:03,480 --> 01:29:09,250
in this case if you sort of stick the pieces back together

1474
01:29:09,290 --> 01:29:12,310
putting the entropy function in our dual calculation

1475
01:29:12,360 --> 01:29:17,270
and remembering that we had the semi definite constraints on the matrix member i i

1476
01:29:17,270 --> 01:29:22,160
had that constraint an earlier slide what you actually get in this case is a

1477
01:29:22,160 --> 01:29:24,510
log determinant problem to solve

1478
01:29:24,520 --> 01:29:28,180
and if you solve this you'll find that you compute exactly again the gas you

1479
01:29:29,060 --> 01:29:33,050
but you also compute the covariance matrix

1480
01:29:33,070 --> 01:29:36,770
so what you want to take away from this is that we would have started

1481
01:29:36,770 --> 01:29:37,620
with this

1482
01:29:37,630 --> 01:29:40,510
general variational principle right here

1483
01:29:40,510 --> 01:29:42,890
right this quite abstract thing

1484
01:29:42,890 --> 01:29:43,630
and we can

1485
01:29:44,270 --> 01:29:48,330
do this kind of inequality so remember with this inequality come from

1486
01:29:49,150 --> 01:29:55,270
i told you before that an optimal separating how are separating hyperplane in canonical form has this property that

1487
01:29:56,120 --> 01:29:57,260
the positive points

1488
01:29:57,640 --> 01:30:01,400
for the positive points this quantity here is at least plus one or larger

1489
01:30:02,190 --> 01:30:05,410
for the negative prices quantities is minus one or small

1490
01:30:06,400 --> 01:30:07,680
now if we multiply

1491
01:30:09,480 --> 01:30:10,280
o thing here with

1492
01:30:10,880 --> 01:30:11,380
the label

1493
01:30:11,990 --> 01:30:14,220
we can write both inequalities as one

1494
01:30:15,750 --> 01:30:18,010
and the inequalities will exactly

1495
01:30:18,440 --> 01:30:19,050
look like this

1496
01:30:21,630 --> 01:30:23,110
so shattering means now

1497
01:30:23,580 --> 01:30:25,300
that no matter how we put the labels

1498
01:30:27,550 --> 01:30:29,550
we can always satisfied

1499
01:30:31,130 --> 01:30:36,780
uh inequality by the choice of some w that satisfies the constraint that length

1500
01:30:37,460 --> 01:30:40,570
so shattering by canonical hyperplanes with this constraint means

1501
01:30:41,310 --> 01:30:42,430
no matter what the labels are

1502
01:30:42,920 --> 01:30:44,920
we can always satisfy all these inequalities

1503
01:30:46,300 --> 01:30:47,370
with such hyperplane

1504
01:30:48,490 --> 01:30:51,290
so let's assume that this is the case and then see what follows from it

1505
01:30:52,640 --> 01:30:53,270
it turns out that

1506
01:30:53,710 --> 01:30:55,510
something follows about the business dimension

1507
01:30:56,640 --> 01:30:58,920
and the proof will first and show

1508
01:31:00,350 --> 01:31:02,140
the more points we can shatter

1509
01:31:02,960 --> 01:31:03,320
like this

1510
01:31:04,180 --> 01:31:06,040
the larger a certain quantity must be

1511
01:31:06,880 --> 01:31:07,740
is this quantity here

1512
01:31:09,790 --> 01:31:14,720
in the next step we will show that this quantity is actually upper bounded in terms of

1513
01:31:15,670 --> 01:31:20,010
the radius of the smallest sphere and if we put these things together we forget about the missing dimension

1514
01:31:20,740 --> 01:31:23,390
so we all we first get a bound how many

1515
01:31:23,900 --> 01:31:25,470
points we can at most shat

1516
01:31:26,270 --> 01:31:27,470
which gives us the damage

1517
01:31:31,040 --> 01:31:31,780
so let's take this

1518
01:31:32,280 --> 01:31:32,610
o thing here

1519
01:31:33,330 --> 01:31:35,290
and the summit over all are

1520
01:31:40,810 --> 01:31:42,400
so we sum over all points

1521
01:31:43,300 --> 01:31:45,570
the which gives us and are

1522
01:31:46,100 --> 01:31:48,320
on the right hand side we have the one before

1523
01:31:48,960 --> 01:31:51,410
and here we just have the sum of these things

1524
01:31:53,310 --> 01:31:55,820
going back and forth we have more than why inside

1525
01:31:56,960 --> 01:31:59,370
and here we have someone why i x i

1526
01:32:01,630 --> 01:32:02,960
so this an inequality here

1527
01:32:04,780 --> 01:32:05,920
on the other hand we can

1528
01:32:06,420 --> 01:32:07,080
up a bond

1529
01:32:07,820 --> 01:32:12,110
this quantity using the cushy schwartz inequality is this dot product is upper bounded by

1530
01:32:12,110 --> 01:32:13,550
the length of the first arguement

1531
01:32:14,430 --> 01:32:16,750
times the length of the second arguement

1532
01:32:18,140 --> 01:32:21,870
now the length of the first arguement by assumption is upper bounded by lambda

1533
01:32:24,420 --> 01:32:27,180
now we know that this thing here is upper bounded by this

1534
01:32:27,770 --> 01:32:31,340
the same time we know that the same thing is lower bounded by are therefore

1535
01:32:32,480 --> 01:32:34,160
are is lower bounded by this

1536
01:32:34,720 --> 01:32:36,820
more are divided by long ties

1537
01:32:37,280 --> 01:32:38,240
lower bounded by this

1538
01:32:40,270 --> 01:32:43,800
you tell me if i'm going too fast or too slow just ask whenever you

1539
01:32:44,170 --> 01:32:44,580
get stuck

1540
01:32:45,560 --> 01:32:46,960
so that's all first results

1541
01:32:47,470 --> 01:32:48,650
the very elementary

1542
01:32:49,150 --> 01:32:49,900
the second one is

1543
01:32:51,360 --> 01:32:53,380
also simple maybe a little bit less elementary

1544
01:32:54,240 --> 01:32:54,680
and now

1545
01:32:55,690 --> 01:32:58,380
by assumption no matter what the labels on the can do that

1546
01:32:59,240 --> 01:33:01,510
so we are free what to do with the labels

1547
01:33:02,060 --> 01:33:03,290
this inequality is always true

1548
01:33:05,530 --> 01:33:07,410
let's consider a particular type of labels

1549
01:33:08,090 --> 01:33:11,430
labels that take the independent random variables

1550
01:33:11,850 --> 01:33:13,970
take taking values plus minus one

1551
01:33:14,800 --> 01:33:18,580
these are also called item variables this kind of random variables

1552
01:33:20,580 --> 01:33:20,880
let's see

1553
01:33:22,070 --> 01:33:22,740
this quantity

1554
01:33:23,560 --> 01:33:25,770
which is the quantity that we had on the previous slide

1555
01:33:27,040 --> 01:33:29,730
let's see what the expectation of this thing here

1556
01:33:30,600 --> 01:33:31,790
it's like a square root

1557
01:33:33,650 --> 01:33:34,080
okay so

1558
01:33:35,480 --> 01:33:37,050
we first there rewrite

1559
01:33:37,550 --> 01:33:40,620
this thing here is the dot product of the thing with itself

1560
01:33:41,840 --> 01:33:43,980
and then we can take expectation out

1561
01:33:45,270 --> 01:33:48,170
well we can we keep it out sorry we we take the sum of

1562
01:33:48,960 --> 01:33:51,630
we can do that because expectation is a linear operation

1563
01:33:53,100 --> 01:33:56,320
and we take one of two some solid and then we take a look at

1564
01:33:56,870 --> 01:33:58,840
how does this thing here behave inside

1565
01:33:59,870 --> 01:34:04,000
and well how does behave they actually two part of a

1566
01:34:04,000 --> 01:34:04,480
one of them

1567
01:34:04,930 --> 01:34:05,470
in this sum

1568
01:34:06,170 --> 01:34:09,260
and one of them is the parts where the labels are

1569
01:34:10,540 --> 01:34:12,110
in this is different from here

1570
01:34:12,880 --> 01:34:15,670
and one of them is the part where the indices are the same

1571
01:34:16,720 --> 01:34:18,570
and let's deal with them separately

1572
01:34:19,520 --> 01:34:20,420
so the first one

1573
01:34:20,860 --> 01:34:22,300
where the indices are different

1574
01:34:23,110 --> 01:34:26,240
and we can put together with the expectation in this term

1575
01:34:27,510 --> 01:34:29,060
the second one where this is

1576
01:34:29,560 --> 01:34:31,780
the same we put together like this

1577
01:34:33,820 --> 01:34:34,870
now it turns out that the

1578
01:34:35,330 --> 01:34:36,010
the first term

1579
01:34:37,570 --> 01:34:44,680
for symmetry reasons because these wires are independent of variables so they will sort all cancel each other out

1580
01:34:45,680 --> 01:34:47,900
whereas the second term is something that's left

1581
01:34:48,660 --> 01:34:50,580
and the second term looks like this

1582
01:34:52,140 --> 01:34:55,810
and so this is our second term here this is just the norm of this vector

1583
01:34:59,750 --> 01:35:04,090
of course the why is don't change the length of the don't change this non so indian we just

1584
01:35:04,580 --> 01:35:06,920
left with this exciting squares

1585
01:35:07,350 --> 01:35:10,730
and we don't in expectation anymore because everything that's random is gone

1586
01:35:20,210 --> 01:35:26,070
since now we use our second assumption which is all the points are contained in a ball of radius are

1587
01:35:27,140 --> 01:35:28,390
so we know that this is true

1588
01:35:31,580 --> 01:35:32,020
looking back

1589
01:35:32,820 --> 01:35:33,390
so we had

1590
01:35:33,900 --> 01:35:36,580
we know that this quantity here is upper bounded by this

1591
01:35:37,110 --> 01:35:39,490
we know each one of those is in a ball of radius are

1592
01:35:40,140 --> 01:35:42,620
therefore this quantity is also upper bounded by

1593
01:35:47,490 --> 01:35:48,140
well and we have

1594
01:35:48,690 --> 01:35:49,980
lowercase are times

1595
01:35:50,720 --> 01:35:51,600
this all squared

1596
01:35:52,730 --> 01:35:55,180
we end up with this thing here on the right-hand side

1597
01:35:59,010 --> 01:36:00,760
which we have derived

1598
01:36:01,390 --> 01:36:02,790
using a

1599
01:36:02,800 --> 01:36:07,020
which are which holds true for the expectation over the random choice of the labels

1600
01:36:08,240 --> 01:36:10,090
this is an statement on in expectation

1601
01:36:10,690 --> 01:36:12,000
now we want something that's

1602
01:36:12,430 --> 01:36:13,830
we want this expectation to go

1603
01:36:14,990 --> 01:36:18,630
we wanted to combine this with this other say that we had about this quantity

1604
01:36:19,870 --> 01:36:24,370
now if something is true for an expectation then there must be at least one

1605
01:36:24,370 --> 01:36:28,210
set of labels here for which is also true i mean if for all the

1606
01:36:28,210 --> 01:36:28,780
labels this

1607
01:36:30,240 --> 01:36:32,530
if an inequality is true expectation then

1608
01:36:33,150 --> 01:36:36,580
there must be at least one instantiation where it's also true because if you were

1609
01:36:36,660 --> 01:36:40,280
violated all instantiations and we take expectation over it

1610
01:36:40,790 --> 01:36:42,970
and then in expectation would also be violated

1611
01:36:43,470 --> 01:36:47,820
so let's take that one probably many but let's pick the ones that label

1612
01:36:48,400 --> 01:36:50,080
it's one of those sets labels for which

1613
01:36:50,750 --> 01:36:52,190
this inequality holds true

1614
01:36:53,040 --> 01:36:54,850
so far we haven't fixed the labels

1615
01:36:55,300 --> 01:36:55,990
now we fix them

1616
01:36:57,300 --> 01:36:59,780
that once said this inequality is true

1617
01:37:01,460 --> 01:37:02,810
and this is what we're going to combine

1618
01:37:03,580 --> 01:37:06,380
so now in the first part of the proof we have shown

1619
01:37:07,110 --> 01:37:07,790
this is true

1620
01:37:09,640 --> 01:37:13,330
in the second part we have shown that before specific set of labels is true

1621
01:37:13,360 --> 01:37:15,550
so the first thing was true positive of labels

1622
01:37:16,060 --> 01:37:20,060
this was true for a specific set of labels we're gonna use that everywhere else here

1623
01:37:21,190 --> 01:37:23,010
and now we combine these two things

1624
01:37:23,010 --> 01:37:27,080
forming clusters of similar points and the start merging them

1625
01:37:28,630 --> 01:37:30,560
points that are more similar to the clusters

1626
01:37:31,750 --> 01:37:32,290
and so on

1627
01:37:32,760 --> 01:37:33,550
so you see that

1628
01:37:33,870 --> 01:37:35,250
two clusters in the data

1629
01:37:36,250 --> 01:37:38,250
the internally within each cluster

1630
01:37:38,860 --> 01:37:39,830
small clusters

1631
01:37:40,120 --> 01:37:42,280
marched at some level similarity

1632
01:37:44,510 --> 01:37:44,700
and so

1633
01:37:49,630 --> 01:37:51,110
as mentioned that the

1634
01:37:51,690 --> 01:37:52,220
interest in

1635
01:37:52,860 --> 01:37:54,870
data clustering and this is supported by

1636
01:37:55,310 --> 01:37:55,620
two way

1637
01:37:56,980 --> 01:37:57,590
so far

1638
01:37:58,060 --> 01:37:59,750
history and the color

1639
01:38:00,680 --> 01:38:01,670
six thousand hits

1640
01:38:02,080 --> 01:38:02,460
also the

1641
01:38:03,830 --> 01:38:04,480
and on the

1642
01:38:05,340 --> 01:38:06,020
science like

1643
01:38:08,450 --> 01:38:09,780
we live like this

1644
01:38:13,410 --> 01:38:15,640
so why is clustering so important for

1645
01:38:18,410 --> 01:38:22,930
but is not feasible to label large collections of objects in our life

1646
01:38:23,030 --> 01:38:23,540
the simple

1647
01:38:28,190 --> 01:38:29,210
prior knowledge the

1648
01:38:31,540 --> 01:38:31,950
in the data

1649
01:38:33,260 --> 01:38:33,820
the clusters

1650
01:38:34,170 --> 01:38:36,080
what what time people of

1651
01:38:36,630 --> 01:38:40,680
posting news items on the left side this only problem

1652
01:38:43,600 --> 01:38:50,070
and then cluster the organisation provides an efficient browsing as recommendation

1653
01:38:53,560 --> 01:38:54,060
so every

1654
01:38:54,530 --> 01:38:56,820
internet company search for me and so on

1655
01:38:57,590 --> 01:38:58,730
there's a lot of effort

1656
01:39:00,880 --> 01:39:01,450
it just

1657
01:39:02,010 --> 01:39:02,900
but wisely

1658
01:39:03,570 --> 01:39:04,710
so some examples

1659
01:39:06,530 --> 01:39:13,600
clustering users the the on the face of three hundred thousand status of this permanent on tens of thousands of

1660
01:39:15,380 --> 01:39:17,450
and what is the price the clusters

1661
01:39:17,920 --> 01:39:23,400
based on the topic of the status message so it can display that not only is interested in nineteen

1662
01:39:25,700 --> 01:39:26,770
it was also

1663
01:39:27,300 --> 01:39:28,470
during the postings on

1664
01:39:28,950 --> 01:39:29,460
when i nineteen

1665
01:39:32,270 --> 01:39:34,160
it is clear from the articles on

1666
01:39:35,010 --> 01:39:35,670
on the of the

1667
01:39:41,300 --> 01:39:43,370
a new cluster is created in the

1668
01:39:45,600 --> 01:39:47,870
and the list of articles that have

1669
01:39:52,220 --> 01:39:53,280
in the i

1670
01:39:53,980 --> 01:39:54,370
on the

1671
01:39:54,560 --> 01:39:55,900
the individual

1672
01:39:56,200 --> 01:39:58,520
the images associated with those articles

1673
01:39:58,940 --> 01:39:59,510
the also

1674
01:40:02,250 --> 01:40:03,930
and example illustrated

1675
01:40:07,120 --> 01:40:08,380
the ability to the

1676
01:40:09,210 --> 01:40:13,980
video on youtube design start displaying other individuals

1677
01:40:14,800 --> 01:40:16,320
which similar to the video

1678
01:40:18,040 --> 01:40:19,300
he was popularity

1679
01:40:20,280 --> 01:40:21,620
many properties

1680
01:40:26,850 --> 01:40:30,610
here's an example of clustering more efficient image retrieval

1681
01:40:31,230 --> 01:40:33,050
if i want to get this image is

1682
01:40:39,260 --> 01:40:40,890
twenty seven images

1683
01:40:42,500 --> 01:40:43,950
in the two of them seem to be

1684
01:40:45,120 --> 01:40:46,100
related one

1685
01:40:47,060 --> 01:40:47,720
food items

1686
01:40:49,560 --> 01:40:50,300
but if i

1687
01:40:50,470 --> 01:40:53,290
the clustering of these images and then

1688
01:40:54,180 --> 01:40:55,060
in the same way

1689
01:40:55,590 --> 01:40:56,620
the more relevant

1690
01:40:57,010 --> 01:40:57,650
images which

1691
01:40:59,380 --> 01:40:59,980
it is

1692
01:41:00,750 --> 01:41:03,320
so the two accuracy food category

1693
01:41:04,280 --> 01:41:07,400
without the average precision is forty seven percent

1694
01:41:10,850 --> 01:41:13,760
so basically show the organising data

1695
01:41:14,120 --> 01:41:15,330
using this one

1696
01:41:17,050 --> 01:41:18,680
this is extremely important

1697
01:41:21,920 --> 01:41:23,100
processing it is

1698
01:41:26,140 --> 01:41:27,690
what about the clustering algorithm

1699
01:41:28,520 --> 01:41:30,830
i should say that hundred thousand dollars

1700
01:41:31,660 --> 01:41:32,960
so that when you now

1701
01:41:33,360 --> 01:41:33,730
the other

1702
01:41:34,840 --> 01:41:35,510
he published

1703
01:41:36,050 --> 01:41:37,810
but if you look carefully designed

1704
01:41:38,910 --> 01:41:39,540
different from

1705
01:41:40,120 --> 01:41:41,510
the act said she

1706
01:41:42,350 --> 01:41:43,120
be published

1707
01:41:46,000 --> 01:41:48,080
many of these algorithms are admissible

1708
01:41:50,070 --> 01:41:51,150
what the united

1709
01:41:51,820 --> 01:41:52,600
this is a theory

1710
01:41:55,190 --> 01:41:58,900
it will is admissible decision rule is admissible if is no other

1711
01:41:59,650 --> 01:42:02,370
decision rule which always outperforms it

1712
01:42:04,430 --> 01:42:05,120
so that means

1713
01:42:05,800 --> 01:42:06,360
the simplest

1714
01:42:07,040 --> 01:42:08,610
these results children it

1715
01:42:09,570 --> 01:42:12,180
the basis of it is better

1716
01:42:13,140 --> 01:42:15,350
spectral clustering or any of the other

1717
01:42:20,160 --> 01:42:22,540
was discovered in the early nineteen fifty

1718
01:42:23,810 --> 01:42:24,860
like the independent

1719
01:42:25,330 --> 01:42:25,870
the light

1720
01:42:29,010 --> 01:42:30,260
about a mixture model

1721
01:42:31,290 --> 01:42:31,690
comes in the

1722
01:42:32,300 --> 01:42:32,880
you want

1723
01:42:32,970 --> 01:42:33,880
the approaches

1724
01:42:35,360 --> 01:42:35,910
the first

1725
01:42:37,000 --> 01:42:41,050
and in some sense you can see he's a special instance of sun

1726
01:42:43,130 --> 01:42:48,390
he's spectral clustering and these two as well what currently is the

1727
01:42:48,390 --> 01:42:52,140
recall there a very important it is

1728
01:42:52,150 --> 01:42:58,610
the area of DIR system in the middle and and they are very useful in

1729
01:42:59,100 --> 01:43:05,380
different real-world application one of these is the topographical map people interpretation it is this

1730
01:43:05,690 --> 01:43:14,250
specific application the some case study using data mining think that we have the dust

1731
01:43:14,250 --> 01:43:20,990
to the centre topographic map in this case we can show objects belonging to different

1732
01:43:21,220 --> 01:43:26,040
and what happens is that when we look at the moment we wanted to enter

1733
01:43:26,170 --> 01:43:32,670
the scene was that we see in the my so these very few what's so

1734
01:43:32,680 --> 01:43:38,500
the meaning of my interpretation when i look at these i immediately recognised in my

1735
01:43:38,500 --> 01:43:44,510
mind that a area that is the blue color is typically press nineteen even like

1736
01:43:44,510 --> 01:43:49,170
in this case so this is what happened when we wanted to put in my

1737
01:43:49,410 --> 01:43:56,820
obviously is very easy to interpret to some symbols time a encoded was meaning is

1738
01:43:56,820 --> 01:44:02,670
encoded by the muppet creator so there is typically allowed to say OK the blue

1739
01:44:02,670 --> 01:44:08,530
line are for something very very but the problem is that of interpreting them up

1740
01:44:08,590 --> 01:44:14,210
in order to this school the discovered some objects that are not explicitly represented in

1741
01:44:14,210 --> 01:44:20,600
the for example looking at this map and recognise that if if there is some

1742
01:44:20,600 --> 01:44:24,510
industrial area or not or not to

1743
01:44:24,520 --> 01:44:31,630
so here is where the mining can be useful and can be useful to know

1744
01:44:31,640 --> 01:44:37,170
when it is integrated with DSS system this means that typical difference in IQ of

1745
01:44:37,170 --> 01:44:44,820
women age meaning visualising exploring querying geographical data is in some time integrated with the

1746
01:44:44,820 --> 01:44:50,400
typical meta mining cup capabilities and is is that the many companies can be was

1747
01:44:50,740 --> 01:44:56,210
was used for for example for mining the some operational definition of the geographical objects

1748
01:44:56,640 --> 01:45:03,640
that are not directly six percent of the metadata and the use these operational definitions

1749
01:45:03,640 --> 01:45:06,490
to to quickly descended although

1750
01:45:06,500 --> 01:45:08,430
on the

1751
01:45:08,510 --> 01:45:16,250
there you just to the finishing of the system and in this case the system

1752
01:45:16,250 --> 01:45:22,560
as some training from united also to learn not operational definition objects and also the

1753
01:45:22,570 --> 01:45:27,980
script from right in order to discover discovered some descriptive of the architecture of the

1754
01:45:27,980 --> 01:45:33,160
system to looking is that i have found that architecture for especially the mining ceased

1755
01:45:33,160 --> 01:45:39,280
and we have the user guide it is designed to just two for an expert

1756
01:45:39,320 --> 01:45:44,300
in an expert system and we have a special databases where we still notable for

1757
01:45:44,300 --> 01:45:47,360
special data and and special

1758
01:45:47,420 --> 01:45:54,510
and the user management tool a tool need to use user profiles so they were

1759
01:45:54,530 --> 01:46:01,040
different users with different reading a book or two to the user that is the

1760
01:46:01,040 --> 01:46:07,300
amount to the user can also all using GIS functionality of the user that is

1761
01:46:07,410 --> 01:46:10,740
that the mind that is able to perform data mining task

1762
01:46:11,020 --> 01:46:14,790
my management for the many acquiring four

1763
01:46:14,800 --> 01:46:20,860
and the population of acquiring the data writing data a simple query just to visualise

1764
01:46:21,380 --> 01:46:28,600
objects belonging to specifically is all the leading and these is very important for us

1765
01:46:28,600 --> 01:46:36,400
assessing quality into since the dot in this case the user can also relate to

1766
01:46:36,510 --> 01:46:43,550
each request to as system especially the mining query language this provided to or interpreted

1767
01:46:43,590 --> 01:46:50,390
squarely by checking is called that is going to from the syntactic and semantic to

1768
01:46:50,610 --> 01:46:56,570
you and and the film which is means that interact with the special databases data

1769
01:46:56,570 --> 01:47:00,930
mining to see that the reason that we did special mining about that sort of

1770
01:47:00,930 --> 01:47:03,690
thing the results

1771
01:47:03,970 --> 01:47:08,070
the more alone there were that we have adopted to represent them up with this

1772
01:47:08,090 --> 01:47:12,910
is diseased case is that the solution to political model is the establishment is divided

1773
01:47:12,910 --> 01:47:17,350
into a grid of side length and is to simplify

1774
01:47:17,360 --> 01:47:23,640
the localisation process look to localize its object within a specific assembler and whatever it

1775
01:47:23,640 --> 01:47:29,550
is you object organised in two i give physical and logical physical one went describe

1776
01:47:29,550 --> 01:47:36,380
it geometry and logic women tend to describe the semantic of the geographical object representation

1777
01:47:36,380 --> 01:47:42,720
for the physical i want to learn the legal wife ontological anarchy differently so that

1778
01:47:42,730 --> 01:47:50,230
some experts that have some colleague of the department one is to combine have defined

1779
01:47:50,230 --> 01:47:56,260
it as interesting for a specific task that wrote philosophy and so on and they

1780
01:47:56,320 --> 01:48:01,480
can be also found that specialize in these is that you can

1781
01:48:01,490 --> 01:48:06,040
this is a representation of the ways of translation in object relations bayes and we

1782
01:48:06,040 --> 01:48:10,260
can look at the map is a collection of science and what is interesting is

1783
01:48:10,260 --> 01:48:15,250
that the logical object to describe the logical i key in this case we are

1784
01:48:15,250 --> 01:48:20,170
presented is there you keep by exploiting the only from more fearsome it is defined

1785
01:48:20,190 --> 01:48:25,390
in object relations that the more than so we have when in i of type

1786
01:48:25,400 --> 01:48:31,250
in this case and the physical information is represented as the geometry that type filter

1787
01:48:31,250 --> 01:48:38,600
to store and the index and so efficiently query all physical data

1788
01:48:38,620 --> 01:48:39,780
the problem

1789
01:48:39,790 --> 01:48:43,710
the first thing that we have a working this on his architect and we have

1790
01:48:43,710 --> 01:48:46,680
to deal with the different technologies

1791
01:48:46,790 --> 01:48:52,490
one side of the mining on one side is the object relation DBMS the activity

1792
01:48:52,500 --> 01:48:56,970
of the enzyme one we intend to store and for the court

1793
01:48:57,150 --> 01:49:02,550
query but the discovery and in the classical GPS system and the problem is that

1794
01:49:02,550 --> 01:49:06,580
we have to integrate all of this technology in a way that is transparent to

1795
01:49:06,590 --> 01:49:12,150
from the and users some time is not very expert user

1796
01:49:12,230 --> 01:49:17,490
it can be the typical user of as system the solution to the design in

1797
01:49:17,500 --> 01:49:19,200
this case is two

1798
01:49:19,210 --> 01:49:27,230
the mining query languages to interface to women manage aged interfacing between you and the

1799
01:49:27,230 --> 01:49:34,630
entire system and i different technologies that are behind these systems

1800
01:49:34,680 --> 01:49:43,770
so in this case the user can formulate a mining query equity and in this

1801
01:49:43,770 --> 01:49:51,050
way it's it's completely it was not in charge of the problem of the representation

1802
01:49:51,050 --> 01:49:55,470
of the topological physical representation because it's inside the

1803
01:49:55,510 --> 01:50:04,660
system and all the proceedings should be chosen for transforming the data and the understanding

1804
01:50:04,660 --> 01:50:05,520
of this data

1805
01:50:05,580 --> 01:50:12,110
mining on the training of this interesting part and presenting this part use the old

1806
01:50:12,110 --> 01:50:17,720
is defined as the inside the typical of the mining process that are completely summarizes

1807
01:50:17,730 --> 01:50:20,220
in the query and are not

1808
01:50:20,460 --> 01:50:29,270
individual use that so in this case the user can discover some part my from

1809
01:50:30,030 --> 01:50:38,060
query and i exactly the same way that a user can discover they can extract

1810
01:50:38,070 --> 01:50:43,420
the data by performing quite clearly so if the is queries to query a simple

1811
01:50:43,420 --> 01:50:47,710
but the point is if you sort of attach penalties to these constraints

1812
01:50:47,760 --> 01:50:52,750
these penalties are are real numbers again adjusted these are actually the lacrosse multipliers the

1813
01:50:54,090 --> 01:50:58,070
what you can show that the solution there is a solution to this problem presuming

1814
01:50:58,070 --> 01:50:59,760
these constraints are OK

1815
01:50:59,800 --> 01:51:03,370
it's always going to have this kind of exponential form

1816
01:51:03,430 --> 01:51:05,820
so it's going to be important

1817
01:51:05,820 --> 01:51:11,060
that you get things in this kind of exponential form

1818
01:51:11,080 --> 01:51:15,390
so this is the sort of leads into exponential families

1819
01:51:16,900 --> 01:51:20,980
as i said these are some of classical thing in statistics

1820
01:51:21,000 --> 01:51:26,050
and the way they are specified is we can have these collections of local functions

1821
01:51:26,050 --> 01:51:27,620
are prose

1822
01:51:27,640 --> 01:51:30,480
and those are typically called sufficient statistics

1823
01:51:30,530 --> 01:51:35,190
there sufficient because they they are not to specify the family

1824
01:51:35,220 --> 01:51:39,110
and then you have the vector of weights are parameters stated you have one of

1825
01:51:39,110 --> 01:51:43,880
them for every one of your sufficient statistics so alpha is an index here

1826
01:51:44,710 --> 01:51:48,110
and then you simply look at a parameterized family of densities

1827
01:51:48,110 --> 01:51:50,220
that takes this kind of forms

1828
01:51:50,250 --> 01:51:55,420
you've got to some a weighted sum of your potential functions

1829
01:51:55,440 --> 01:51:59,980
and this guy all i'm doing here is rewriting the normalisation constant this is just

1830
01:51:59,980 --> 01:52:01,390
log easy

1831
01:52:01,430 --> 01:52:05,360
but it's so is traditional to write this in a slightly different form as a

1832
01:52:05,790 --> 01:52:12,440
where if they is log of of this normalisation quantity here

1833
01:52:12,460 --> 01:52:16,750
so if go back to example here

1834
01:52:16,750 --> 01:52:21,210
right if we look at these two particular functions p one and FT two being

1835
01:52:21,210 --> 01:52:22,890
x and x

1836
01:52:22,950 --> 01:52:27,840
for the previous page told me is that if i solve the maximum entropy problem

1837
01:52:28,680 --> 01:52:32,360
what i would get is together

1838
01:52:32,430 --> 01:52:36,610
family of distributions that looks like this

1839
01:52:36,620 --> 01:52:44,230
be proportional to the exponential of data one times x stated two times x squared

1840
01:52:44,370 --> 01:52:50,660
and x here would i'm assuming will belong to the real line

1841
01:52:50,670 --> 01:52:57,540
right so what kind of random variables is that thing there

1842
01:53:11,020 --> 01:53:16,790
probably seen things like this before

1843
01:53:16,800 --> 01:53:24,970
what if data one more zero inflated to remind minus one if you plotted that

1844
01:53:24,980 --> 01:53:31,200
thing what would it look like

1845
01:53:31,210 --> 01:53:34,290
right into normal or gas that's all it is

1846
01:53:34,370 --> 01:53:39,280
this is perhaps a slightly different form that you might not be used to what's

1847
01:53:39,280 --> 01:53:42,170
going on here is that the state is

1848
01:53:42,190 --> 01:53:46,130
can be expressed in terms of the moments the music

1849
01:53:46,180 --> 01:53:51,540
you could be used to seen in terms of of this

1850
01:53:51,580 --> 01:53:57,920
in terms of its variance data one here would be minus mu one of the

1851
01:53:58,950 --> 01:54:01,860
and they two would be

1852
01:54:01,860 --> 01:54:08,840
and this a plus minus one the variance squared

1853
01:54:09,020 --> 01:54:14,760
so if you substitute that in in rearranged find the more usual form for the

1854
01:54:15,910 --> 01:54:20,810
right so all that says is that

1855
01:54:20,830 --> 01:54:24,340
this is the well known fact that the gaussian distribution is the one that has

1856
01:54:24,340 --> 01:54:29,420
maximum entropy subject to constraints in the first two moments right what we did here

1857
01:54:29,420 --> 01:54:33,360
was constrain the first moment and the second moment

1858
01:54:33,370 --> 01:54:36,520
and then one a particular case of what we said is that at the end

1859
01:54:36,520 --> 01:54:38,020
of the day you get again year

1860
01:54:38,500 --> 01:54:43,620
and you can work to that the details as a little exercise

1861
01:54:43,720 --> 01:54:45,140
right so

1862
01:54:45,660 --> 01:54:48,610
here are some examples these things

1863
01:54:48,620 --> 01:54:52,380
probably many of you have seen before this is the one we just gaussians

1864
01:54:52,390 --> 01:54:55,330
the random variables on the real line

1865
01:54:55,440 --> 01:54:58,470
we'll worry too much about this measure here

1866
01:54:58,500 --> 01:55:02,750
this device say that distinguishes between whether continuous

1867
01:55:02,760 --> 01:55:06,450
in which case we'll say it along with respect to live that measure

1868
01:55:06,470 --> 01:55:12,140
or for discrete that's with respect to something that puts mass on individual points but

1869
01:55:12,800 --> 01:55:16,810
the weight loss of p of x it would look like this for gaussians

1870
01:55:16,870 --> 01:55:19,530
for bernoulli

1871
01:55:19,530 --> 01:55:23,090
that's the variable that just like a coin flipping variable we spoke about the last

1872
01:55:23,090 --> 01:55:26,860
time it would look like this

1873
01:55:27,030 --> 01:55:30,980
and you can also look at exponentials plus on spiritual is

1874
01:55:31,050 --> 01:55:35,730
so there are lots of things that basically there also called loglinear because they up

1875
01:55:35,730 --> 01:55:40,660
to this normalisation constant there's sort of a linear in the parameters here when you

1876
01:55:40,660 --> 01:55:41,850
take log

1877
01:55:41,870 --> 01:55:47,750
so that's that's what exponential families look like

1878
01:55:52,070 --> 01:55:57,640
right so the point now is that we can really think about many not all

1879
01:55:57,640 --> 01:55:58,110
but many

1880
01:55:58,580 --> 01:56:03,300
graphical models is just the exponential families

1881
01:56:03,360 --> 01:56:05,250
right these cases are all

1882
01:56:05,260 --> 01:56:08,170
these are all scale these are just a single variable

1883
01:56:08,200 --> 01:56:14,360
member graphical models were most interested in the multivariate case we have many random variables

1884
01:56:14,410 --> 01:56:18,110
so we have the whole graph and we have one variable each node

1885
01:56:18,140 --> 01:56:20,670
but you can really think about

1886
01:56:20,680 --> 01:56:24,830
just taking something from an exponential family of the nodes my people newly might be

1887
01:56:26,340 --> 01:56:28,810
and then by combining these things together

1888
01:56:29,150 --> 01:56:32,340
we can get a larger exponential family

1889
01:56:32,360 --> 01:56:38,190
and what sort of key here is that the sufficient statistics these things these fees

1890
01:56:38,190 --> 01:56:40,570
that we were talking about before

1891
01:56:40,940 --> 01:56:44,170
what we need for it to be a graphical model is the fees have to

1892
01:56:44,170 --> 01:56:45,870
be local

1893
01:56:45,920 --> 01:56:49,060
and they have to be local in the sense that the graph tells them to

1894
01:56:49,060 --> 01:56:50,720
be local

1895
01:56:50,860 --> 01:56:54,330
right so it means that you should only have functions on cliques for instance on

1896
01:56:54,330 --> 01:56:58,670
one two three i shouldn't have any function that a function of both two and

1897
01:56:58,670 --> 01:56:59,890
what i mean he

1898
01:56:59,900 --> 01:57:04,400
is there is often a physical quantity that you are measuring

1899
01:57:04,420 --> 01:57:08,540
it is living on some object in space

1900
01:57:08,600 --> 01:57:12,500
one dimension two dimensions whatever the has a certain amount of symmetry

1901
01:57:12,520 --> 01:57:17,160
all right and the periodicity of the non is a consequence of the symmetry of

1902
01:57:17,160 --> 01:57:23,050
the object so it's often the county meaning the second so here you have

1903
01:57:23,090 --> 01:57:30,510
say someone has some physical quantity is not always but often enough physical quantity

1904
01:57:38,790 --> 01:57:40,680
a region

1905
01:57:40,860 --> 01:57:43,740
we have symmetry

1906
01:57:43,790 --> 01:57:48,580
the region itself repeats region itself the repeating pattern

1907
01:57:48,580 --> 01:57:52,640
all right so the periodicity of the finer the periodicity of the physical quantity that

1908
01:57:52,640 --> 01:57:56,680
you're ring is a consequence of the fact that is distributed on

1909
01:57:56,920 --> 01:57:59,730
on its over some region it's itself

1910
01:57:59,760 --> 01:58:02,820
has some symmetry to periodicity

1911
01:58:02,860 --> 01:58:10,990
arises from the symmetry periodicity here of the order of the of the physical quantity

1912
01:58:11,030 --> 01:58:16,000
measuring arises because the periodicity of the of the symmetry of the object were treated

1913
01:58:16,000 --> 01:58:19,690
with example from the symmetry

1914
01:58:19,720 --> 01:58:23,900
the net effect giving the example of example the really started the subject and will

1915
01:58:23,900 --> 01:58:26,580
study this

1916
01:58:28,330 --> 01:58:30,650
is the distribution of heat

1917
01:58:30,710 --> 01:58:37,720
on a circular ring

1918
01:58:37,780 --> 01:58:38,820
e g

1919
01:58:38,820 --> 01:58:45,120
the distribution of heat

1920
01:58:45,140 --> 01:58:53,300
on a circular parts of the object the physical quantity that you're interested in is

1921
01:58:53,300 --> 01:58:54,930
the temperature

1922
01:58:54,960 --> 01:58:59,700
the temperature in a certain region and the region is

1923
01:59:01,990 --> 01:59:02,720
all right

1924
01:59:02,740 --> 01:59:04,960
the ring has circular symmetry

1925
01:59:05,040 --> 01:59:10,010
around OK so you're measuring the temperature

1926
01:59:11,940 --> 01:59:13,520
on the ranking

1927
01:59:13,570 --> 01:59:17,150
and that periodic because if you go once around at the same place of the

1928
01:59:17,150 --> 01:59:21,580
temperature is periodic as function as a as a function of the spatial variable that

1929
01:59:21,580 --> 01:59:26,850
describes where you are on the right time is not involved here position is involved

1930
01:59:26,890 --> 01:59:30,230
riodic in space not periodic in time

1931
01:59:33,220 --> 01:59:41,570
in the spatial variables gives you the position in the periodicity arises because the object

1932
01:59:41,570 --> 01:59:44,430
itself is symmetric because the object repeats

1933
01:59:44,440 --> 01:59:50,570
that's why this sort of example is why one often sees in this this actually

1934
01:59:50,570 --> 01:59:55,080
turns out to be very far reaching and quite deep that fourier analysis is often

1935
01:59:55,080 --> 02:00:00,030
associated with questions of symmetry

1936
02:00:00,040 --> 02:00:02,450
in this sort of most mathematical form

1937
02:00:02,710 --> 02:00:07,530
you often find free series developed in and in this context and free transport developed

1938
02:00:07,530 --> 02:00:12,940
in the context of symmetry so you often see for analysis from to say

1939
02:00:12,960 --> 02:00:18,600
three analysis

1940
02:00:18,710 --> 02:00:22,970
analysis is often

1941
02:00:23,100 --> 02:00:29,170
associated with problems are just all with with with analysis of questions that have to

1942
02:00:29,170 --> 02:00:31,360
do with the absence of like symmetry

1943
02:00:31,380 --> 02:00:34,530
so most often associated with problems

1944
02:00:34,550 --> 02:00:39,710
with symmetry just leave it very general this is the very first part of the

1945
02:00:39,710 --> 02:00:43,120
problem the distribution of heat honoring we solve the problem that was the problem the

1946
02:00:43,130 --> 02:00:47,420
fourier himself considered are either introduced some of the methods into the and also that

1947
02:00:47,420 --> 02:00:50,890
that what everything's alright so again it's not periodicity in time

1948
02:00:50,950 --> 02:00:53,730
his periodicity in space

1949
02:00:53,820 --> 02:00:56,940
and for those of you who had or may have courses in this that the

1950
02:00:56,940 --> 02:01:02,450
mathematical framework for this very general we're looking for a fourier analysis is group theory

1951
02:01:02,490 --> 02:01:06,720
because the theory of groups in mathematics is the way of mathema ties in the

1952
02:01:06,720 --> 02:01:11,260
idea of symmetry and then one extend the ideas for all policies into

1953
02:01:11,270 --> 02:01:14,540
to take into account of groups that is to say to take into account the

1954
02:01:14,540 --> 02:01:17,680
the world you feel better than you've ever felt before

1955
02:01:17,700 --> 02:01:21,910
that's the only way that your body rewarding you with dopamine up here is one

1956
02:01:21,910 --> 02:01:25,950
of my father's stories he's in fact has a different theory

1957
02:01:25,970 --> 02:01:31,850
he says vigorous exercise you know running or or cycles a bike is like banging

1958
02:01:31,850 --> 02:01:37,760
your head on the wall is said it feels best when you stop and and

1959
02:01:37,760 --> 02:01:40,700
the feel that you get at the end of the vigorous run is your body

1960
02:01:40,700 --> 02:01:46,350
saying thank you for stopping that african torture that feels much better

1961
02:01:46,950 --> 02:01:48,810
the researchers disagree

1962
02:01:48,930 --> 02:01:53,410
the reality is it's it's dopamine now you give someone you catch them and you

1963
02:01:53,410 --> 02:01:56,910
give them genuine recognition genuine recognition

1964
02:01:56,930 --> 02:02:00,890
i like the job you did on that proposal and the reason i say that

1965
02:02:01,830 --> 02:02:05,740
dot dot dot is not enough to say you're looking great today

1966
02:02:05,760 --> 02:02:08,140
no value or you did a good job

1967
02:02:08,140 --> 02:02:09,100
no value

1968
02:02:09,100 --> 02:02:10,280
we all

1969
02:02:10,990 --> 02:02:13,280
when somebody's flattering us

1970
02:02:13,330 --> 02:02:16,410
you meet somebody say have you lost weight

1971
02:02:16,410 --> 02:02:19,990
and inside your head you say get african life

1972
02:02:20,990 --> 02:02:23,890
get out of it i mean they're not gonna get me with something is is

1973
02:02:23,970 --> 02:02:27,100
that have you lost weight and the reason i ask that is that the last

1974
02:02:27,100 --> 02:02:30,220
m i so you were bulging over the top of the straw another falling off

1975
02:02:30,220 --> 02:02:33,160
you know you're on to something

1976
02:02:33,180 --> 02:02:37,720
when you give somebody genuine recognition when you catch them doing something right you give

1977
02:02:37,720 --> 02:02:42,700
them genuine feedback on it you trigger that release

1978
02:02:42,720 --> 02:02:44,970
of dopamine

1979
02:02:46,620 --> 02:02:50,680
triggers the amygdala says hey whatever you did just now

1980
02:02:50,700 --> 02:02:52,910
do it again i like that

1981
02:02:52,970 --> 02:02:54,850
so it reinforces

1982
02:02:54,910 --> 02:02:56,910
that behavior here's is the reality

1983
02:02:57,030 --> 02:03:01,780
none of us get told often enough that we are good at the things we

1984
02:03:01,780 --> 02:03:04,950
do and when somebody tells us that

1985
02:03:04,990 --> 02:03:10,040
not only does the cause that squirt dopamine it makes us them then again

1986
02:03:10,060 --> 02:03:14,410
i want to deal with you again this is the charismatic equation they feel or

1987
02:03:14,410 --> 02:03:18,310
they do better as result and it can be as simple as giving them some

1988
02:03:18,310 --> 02:03:22,350
positive feedback and if you can think of anything else to say

1989
02:03:22,350 --> 02:03:25,100
well complement to repair and

1990
02:03:25,100 --> 02:03:29,470
even complimenting somebody's appearance but you notice you need to be a little bit specific

1991
02:03:29,620 --> 02:03:36,030
that has a positive impact upon when you sense people are in they that place

1992
02:03:36,030 --> 02:03:40,490
where they in their darkest where you just know that there's something going on just

1993
02:03:40,490 --> 02:03:44,490
the smallest complements small bitter recognition can make all the difference in the world do

1994
02:03:44,490 --> 02:03:46,640
yourself a favor

1995
02:03:46,640 --> 02:03:50,040
i checked that movie out it's also for the ladies at the lovely little love

1996
02:03:50,040 --> 02:03:53,800
story it actually i i must be getting soft on my own page but i

1997
02:03:53,800 --> 02:03:57,330
i felt my eyes well up a little bit at the end of great fun

1998
02:03:57,350 --> 02:03:58,350
great fun

1999
02:03:59,600 --> 02:04:03,100
this second recognition is not only chemical

2000
02:04:03,120 --> 02:04:04,990
but actually psychological

2001
02:04:05,010 --> 02:04:07,280
there is nothing the marketing people

2002
02:04:08,300 --> 02:04:11,870
as the norm of reciprocity

2003
02:04:11,890 --> 02:04:16,870
what that is simply means what the psychologist mean when they say that is when

2004
02:04:16,870 --> 02:04:19,350
you do a favor for somebody

2005
02:04:19,490 --> 02:04:21,740
whether they wanted to favor or not

2006
02:04:21,760 --> 02:04:27,010
what they value the favor not whether this solicited what they like you are not

2007
02:04:27,010 --> 02:04:30,990
even if they don't like you there's a part of them says i want

2008
02:04:31,010 --> 02:04:37,580
i told him return favor i owe him reciprocal action i owe him a favor

2009
02:04:37,580 --> 02:04:43,700
back you give somebody some genuine recognition that little squirt dopamine say that's pretty good

2010
02:04:43,720 --> 02:04:46,260
they measure i o one

2011
02:04:46,280 --> 02:04:50,680
that is the ninety nine percent of people respond to the normal reciprocity go into

2012
02:04:51,310 --> 02:04:53,310
the biggest department store in

2013
02:04:53,350 --> 02:04:55,430
go into

2014
02:04:55,470 --> 02:04:59,040
actually go into the cosmetics department

2015
02:04:59,060 --> 02:05:03,370
and body eventually you heard of la prairie

2016
02:05:03,390 --> 02:05:10,760
and if the man had library anybody know if you haven't don't

2017
02:05:11,640 --> 02:05:17,240
learn anything about it it's very expensive cream for ladies comes in small tubes and

