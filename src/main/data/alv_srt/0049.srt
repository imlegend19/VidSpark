1
00:00:00,000 --> 00:00:01,120
very high he likely

2
00:00:01,660 --> 00:00:05,530
back means i'm saying it's hard w time will be low and saying i'm saying

3
00:00:05,530 --> 00:00:09,130
yes if you can think that's a great i like that line right so your

4
00:00:09,210 --> 00:00:13,450
prior in some sense only a distribution over these models some reflects

5
00:00:14,060 --> 00:00:18,530
the regularizer and you can actually make these very very precise and several of us

6
00:00:18,530 --> 00:00:20,980
have done this analysis and in terms of trying

7
00:00:21,680 --> 00:00:26,080
the results and then eventually guarantees and future performance by doing this kind of analysis

8
00:00:27,310 --> 00:00:30,660
so so this is the important message and u may not be

9
00:00:31,280 --> 00:00:35,470
using labs so you may be using is vehemently we'll be using something else make

10
00:00:35,470 --> 00:00:40,690
sure your model is regularized don't only look at your training set performance right

11
00:00:41,090 --> 00:00:46,020
and in the morning breakaway people often do cross validation and that's a reasonable approach

12
00:00:46,020 --> 00:00:48,200
to do this or bootstrap and things so that's sort

13
00:00:48,570 --> 00:00:52,850
those are all trying to capture this thing i'm not going to trust my performance

14
00:00:52,990 --> 00:00:57,200
on the training set entirely right so because i may be doing hundred percent accuracy

15
00:00:57,200 --> 00:00:57,760
on the phoenix

16
00:00:58,400 --> 00:01:02,070
so this is a very important sort of thing to remember as you're building models

17
00:01:02,070 --> 00:01:05,840
which are eventually going to be deployed and they're going be running on real data

18
00:01:09,410 --> 00:01:11,840
i have not yet touched on nonlinear models

19
00:01:12,430 --> 00:01:16,390
that that nonlinear models in machine learning is that one the popular

20
00:01:16,520 --> 00:01:20,950
peaks is is what is called the kernel trick it's not a trick it's a very nice mathematical

21
00:01:23,590 --> 00:01:28,440
essentially what it does is that it if if this is your classification problem right

22
00:01:28,440 --> 00:01:30,750
so so this is one of the other class

23
00:01:31,270 --> 00:01:35,670
there is just no way you can come up with a straight line which separates the two point two classes

24
00:01:38,090 --> 00:01:42,900
if you actually map to a higher dimensions which depends on x one square next to square

25
00:01:43,750 --> 00:01:48,220
right some some some second order polynomial six months when x quite x one x

26
00:01:49,000 --> 00:01:53,030
then in the higher dimensional space they become very separate

27
00:01:53,920 --> 00:01:58,090
right so they become they become perfectly separated so that's essentially the essence of the

28
00:01:58,090 --> 00:01:59,610
kernel trick is that even tho

29
00:02:00,410 --> 00:02:05,020
this is going to be nonlinear you're going to need a non-linear separating the low dimensional space

30
00:02:05,650 --> 00:02:09,000
if you actually take it to hire him and we are not talking dimension reduction

31
00:02:09,480 --> 00:02:11,770
using if you take it to a high dimensional space

32
00:02:12,320 --> 00:02:14,060
a linear separator would work

33
00:02:15,450 --> 00:02:17,900
so so these are what we call the corners which you know

34
00:02:18,660 --> 00:02:21,020
they are product in high dimensional spaces the coronal

35
00:02:21,590 --> 00:02:22,590
and there is a very nice

36
00:02:23,060 --> 00:02:27,160
sort of unity behind this is in the prison garnered but spaces so if you

37
00:02:27,160 --> 00:02:30,950
if you like mathematics you're going to like the construction that goes on the theory

38
00:02:30,960 --> 00:02:34,080
that goes behind these constructions in practice they walk

39
00:02:34,980 --> 00:02:39,130
one of the big success areas but these have been employed in computational biology

40
00:02:39,970 --> 00:02:45,010
there are extensively used and people have developed all kinds of corners far for comparing

41
00:02:45,010 --> 00:02:47,430
similarities between two extremely complicated things

42
00:02:49,780 --> 00:02:54,480
in this is something you already know data itself is a vector in two dimensions

43
00:02:54,900 --> 00:02:57,740
right when i'm talking about something like computational biology

44
00:02:58,750 --> 00:03:02,270
you know you're talking about combining to proteins or something of that sort i'm not

45
00:03:02,470 --> 00:03:06,430
which but they they are not finite dimensional vectors right they have

46
00:03:06,850 --> 00:03:08,340
something very complicated

47
00:03:08,940 --> 00:03:13,090
maybe it's a tree maybe maybe it's a graph may has a three-tier structure to

48
00:03:13,090 --> 00:03:16,830
it and as long as you can define a common among two things

49
00:03:17,220 --> 00:03:19,030
this this trick can be pulled up

50
00:03:19,640 --> 00:03:24,380
you're you're pushing it a hilbert space marine products can be computed and new should

51
00:03:24,380 --> 00:03:25,280
be able to pull this off

52
00:03:26,780 --> 00:03:31,260
this literature is very stable there been books written on this you know you can

53
00:03:31,260 --> 00:03:34,620
just go anywhere and look for books on kernel methods in machine learning

54
00:03:35,450 --> 00:03:38,630
kernelized classification regression dimension reduction has been used

55
00:03:39,650 --> 00:03:45,120
how learn the kernel for a given domain might current learning and commissioned called extensively

56
00:03:45,120 --> 00:03:48,930
developed and it's still actively started but this is a very much urea

57
00:03:50,070 --> 00:03:52,580
this is one way to capture nonlinearity and this is

58
00:03:53,290 --> 00:04:00,080
there are other problems some of district doesn't work and we started looking at some climate data where the nonlinearities

59
00:04:00,580 --> 00:04:05,360
and we're still not sure where they will be able to pull off this idea in the context

60
00:04:06,750 --> 00:04:08,430
so so the last several mainstream

61
00:04:09,210 --> 00:04:11,950
team i'm going to talk about the notion of ensembles

62
00:04:12,700 --> 00:04:14,020
so what ensembles do

63
00:04:14,540 --> 00:04:15,040
is bad

64
00:04:15,480 --> 00:04:17,380
they actually do market

65
00:04:18,670 --> 00:04:19,590
from the raw data

66
00:04:20,030 --> 00:04:22,870
maybe by slightly modifying the auditory strike

67
00:04:23,360 --> 00:04:28,160
it may be picking some features may be waiting that are doing some little modification

68
00:04:28,160 --> 00:04:29,470
on the data that you have

69
00:04:29,960 --> 00:04:33,640
and then you are cooking up a whole bunch of classifiers like that's and in

70
00:04:33,640 --> 00:04:37,660
the end you come up with a way to listen to all of those classifiers

71
00:04:37,660 --> 00:04:39,760
have their opinion on the new data point

72
00:04:40,230 --> 00:04:45,430
you're listening to all of them with a suitable weighted combination of consensus function right hand

73
00:04:46,030 --> 00:04:47,760
and this is actually one of the

74
00:04:48,670 --> 00:04:50,650
so so what people have shown

75
00:04:51,210 --> 00:04:52,510
in terms of the theory

76
00:04:53,070 --> 00:04:53,640
it is about

77
00:04:54,250 --> 00:04:59,460
we talked about regularization right if you are building like five hundred such classifiers maybe

78
00:04:59,680 --> 00:05:01,900
cooking on one extremely complicated classifier

79
00:05:02,310 --> 00:05:06,670
so if you've been one decision tree instead if you are building temples decision trees

80
00:05:06,990 --> 00:05:11,770
online alleviated versions of the same dataset you may think of the fact that are

81
00:05:11,780 --> 00:05:14,380
w regression don't really go through the rules

82
00:05:15,960 --> 00:05:20,760
the theory that was developed in this to quite some time to understand why doing so well empirically

83
00:05:21,260 --> 00:05:23,290
is that this combination actually uses

84
00:05:23,940 --> 00:05:24,440
the loss

85
00:05:25,140 --> 00:05:28,170
but it does not increase the complexity in a very precise sense

86
00:05:28,950 --> 00:05:29,290
right so

87
00:05:29,770 --> 00:05:32,010
so it is okay if you'd been what

88
00:05:32,150 --> 00:05:35,170
jim decision trees on minority weighted versions of the data

89
00:05:35,770 --> 00:05:40,130
and then i have all of them vote and do a weighted combination success stories

90
00:05:42,900 --> 00:05:47,850
to some extent and boosting and random forests which which have been extremely widely used

91
00:05:48,400 --> 00:05:51,810
and i i think you know we we may actually has some speakers maybe tomorrow

92
00:05:51,890 --> 00:05:55,240
day after talking about some of these ensemble methods are used in google

93
00:05:55,760 --> 00:05:59,690
and they can be scaled up to that level in order to do this and

94
00:05:59,690 --> 00:06:01,790
they walk phenomenally well right so

95
00:06:02,370 --> 00:06:06,870
and so initially for a small dataset they may look completely crazy man this is your

96
00:06:07,360 --> 00:06:10,470
randomly building some decision trees and combine their outputs

97
00:06:10,960 --> 00:06:15,590
if you're really working with large scale data these things start walking phenomenally well and

98
00:06:15,630 --> 00:06:18,220
we have some level of theoretical understanding of why that happens

99
00:06:18,830 --> 00:06:24,510
again you know these are things you will probably find in standard statistical software like we got another pieces

100
00:06:26,390 --> 00:06:31,290
and then they they have different ways of combining these got creating as well as

101
00:06:31,290 --> 00:06:35,850
coal mining these classifiers but these are methods that we like they are stable we

102
00:06:35,850 --> 00:06:37,830
understand when they walk in the don't work

103
00:06:39,740 --> 00:06:41,660
okay so i'm going to wrap up this section

104
00:06:43,030 --> 00:06:48,240
by getting into an area that's that's several of us have gotten into and we are realizing

105
00:06:48,820 --> 00:06:50,470
that we don't have methods

106
00:06:51,070 --> 00:06:51,450
so far

107
00:06:52,390 --> 00:06:54,810
high dimensional problems what we mean by that

108
00:06:54,810 --> 00:06:56,950
yeah this district is it minus

109
00:06:58,140 --> 00:06:58,870
x some land

110
00:07:02,340 --> 00:07:02,840
abide by their

111
00:07:04,890 --> 00:07:05,890
z is in the

112
00:07:07,150 --> 00:07:07,830
my sex

113
00:07:11,360 --> 00:07:11,910
on lambda

114
00:07:13,480 --> 00:07:14,430
and you can do the integral

115
00:07:16,700 --> 00:07:17,370
it's not hard

116
00:07:18,170 --> 00:07:18,870
that is lambda

117
00:07:19,650 --> 00:07:21,450
in nineteen on land

118
00:07:22,260 --> 00:07:23,450
minus minus you

119
00:07:23,790 --> 00:07:24,160
and lambda

120
00:07:25,980 --> 00:07:29,150
i really want to emphasise how easy and straightforward and simple this is

121
00:07:31,080 --> 00:07:34,870
it's just a few lines of computer code because all we need now put into your computer

122
00:07:35,300 --> 00:07:37,470
is this statement here which defines

123
00:07:38,210 --> 00:07:38,890
x given lambda

124
00:07:40,180 --> 00:07:40,880
it depends on z

125
00:07:41,580 --> 00:07:44,220
and then you put in this one which defines how that depends on lambda

126
00:07:45,190 --> 00:07:47,310
and now you've done it just makes that says

127
00:07:48,000 --> 00:07:49,440
you can infer lambda

128
00:07:53,200 --> 00:07:54,560
by just multiplying together

129
00:07:58,110 --> 00:07:59,620
all these things here so you have

130
00:08:00,990 --> 00:08:02,930
hole-in-one z s and of them

131
00:08:03,390 --> 00:08:05,400
hand loaded minus signs

132
00:08:10,380 --> 00:08:11,910
z is a function of lambda

133
00:08:12,620 --> 00:08:13,220
the country

134
00:08:13,810 --> 00:08:15,390
and this here is a function of lambda

135
00:08:16,150 --> 00:08:19,980
it depends on x so it is lambda dependence his land in here

136
00:08:20,390 --> 00:08:21,490
and just multiplying together

137
00:08:22,160 --> 00:08:26,630
and that's there is your all likelihood as we are always

138
00:08:27,930 --> 00:08:28,510
the lambda

139
00:08:29,840 --> 00:08:32,710
you either by whatever about lambda beforehand

140
00:08:37,930 --> 00:08:38,670
andy normalize

141
00:08:43,710 --> 00:08:47,760
and this is we uninteresting normalizing constant you don't care about if we want to do is

142
00:08:48,280 --> 00:08:49,910
compare alternative theories about lambda

143
00:08:50,870 --> 00:08:53,800
so let's look at the meeting this the data-dependent bit

144
00:08:54,310 --> 00:08:57,800
and this is where it you first time you see this it

145
00:08:58,260 --> 00:09:00,120
i find it just thrilling to

146
00:09:00,610 --> 00:09:02,830
observe what just automatically happens

147
00:09:03,230 --> 00:09:04,380
if you take this function

148
00:09:05,430 --> 00:09:07,220
um stick with the lines

149
00:09:08,710 --> 00:09:10,350
here's the code units right down

150
00:09:11,200 --> 00:09:11,980
written it in

151
00:09:12,430 --> 00:09:15,410
the report which is a language that you can use to define functions

152
00:09:15,970 --> 00:09:16,490
and portland

153
00:09:16,980 --> 00:09:19,950
well defined function peopple ex ante el which is

154
00:09:20,830 --> 00:09:25,190
this function here x and lambda lambda as turned out on both

155
00:09:25,760 --> 00:09:26,760
so the top line defined

156
00:09:27,390 --> 00:09:28,550
the second line defined z

157
00:09:30,150 --> 00:09:33,070
i've defined z slightly different output one over all into the pier

158
00:09:34,340 --> 00:09:35,610
but it's it's equivalent

159
00:09:36,460 --> 00:09:37,860
we define eight we define be

160
00:09:38,620 --> 00:09:38,980
and then

161
00:09:39,750 --> 00:09:44,050
we can plot these as a function of x which is the normal way of plotting these things so you

162
00:09:44,560 --> 00:09:45,350
six lambda

163
00:09:47,920 --> 00:09:48,480
any plot

164
00:09:50,170 --> 00:09:51,190
yeah that's given lambda

165
00:09:51,840 --> 00:09:55,490
as a function of x and it looks like and that's all very normal

166
00:09:57,680 --> 00:10:00,300
so that's what it looks like a three different values of lambda

167
00:10:01,280 --> 00:10:03,290
and if you have a very small value of lambda is

168
00:10:04,100 --> 00:10:05,180
or constraint on left

169
00:10:05,860 --> 00:10:07,000
big value lambda

170
00:10:09,660 --> 00:10:11,130
you more uniform distribution

171
00:10:12,240 --> 00:10:15,660
and everyone these probability densities over x integrates to one

172
00:10:16,130 --> 00:10:18,630
and it's all very normal and this is one

173
00:10:19,550 --> 00:10:20,620
and the area under

174
00:10:21,370 --> 00:10:22,070
the green curve

175
00:10:26,250 --> 00:10:26,740
also one

176
00:10:29,880 --> 00:10:32,590
that's all very standard and very simple and just exponential

177
00:10:33,220 --> 00:10:36,360
and then based theorem says okay now you want to know lambda given x will

178
00:10:36,420 --> 00:10:38,830
just what the same function but look at it

179
00:10:39,320 --> 00:10:40,180
as a function of lambda

180
00:10:40,680 --> 00:10:43,980
so take these innocent exponential functions and look the other way around

181
00:10:45,340 --> 00:10:47,710
andy this is what they look like so this is

182
00:10:48,160 --> 00:10:49,050
the probability of x

183
00:10:54,540 --> 00:10:55,310
sorry is not

184
00:10:55,890 --> 00:10:56,790
this is a

185
00:10:58,870 --> 00:11:03,250
andy i obviously do this we need to fix x to a particular value so

186
00:11:03,560 --> 00:11:07,640
i'm showing these three different outcomes for three different possible axis x could be for

187
00:11:07,640 --> 00:11:08,240
example three

188
00:11:08,900 --> 00:11:09,950
could be five three twelve

189
00:11:10,810 --> 00:11:14,950
and this is what effective lambda looks like for three the light blue curve

190
00:11:15,470 --> 00:11:16,720
five it's the blue one

191
00:11:17,230 --> 00:11:18,720
and for twelve it's the purple one

192
00:11:19,910 --> 00:11:22,430
and the amazing thing is just one of those functions

193
00:11:23,460 --> 00:11:23,960
take one

194
00:11:24,650 --> 00:11:28,940
if we fix x three and then plot events given lambda as a function of lambda

195
00:11:29,910 --> 00:11:31,780
i'm showing lambda on the horizontal axis here

196
00:11:32,240 --> 00:11:33,450
and i put on a log scale

197
00:11:35,070 --> 00:11:35,740
it's got a peak

198
00:11:36,620 --> 00:11:37,930
so a single data point

199
00:11:38,470 --> 00:11:40,850
is not giving you this nice peaky function and saying

200
00:11:44,030 --> 00:11:46,830
just one data point that happened to arrive at

201
00:11:47,460 --> 00:11:48,390
x equals three

202
00:11:50,170 --> 00:11:51,960
one data point now tells you

203
00:11:55,030 --> 00:11:58,000
tells use and opinions you will have a different values of lambda

204
00:12:01,040 --> 00:12:03,650
extremely small values of lambda close to zero

205
00:12:04,920 --> 00:12:06,350
are essentially ruled out by

206
00:12:09,040 --> 00:12:10,990
values are laminated to also

207
00:12:11,750 --> 00:12:13,950
are perfectly happy they did a good job of predicting

208
00:12:14,630 --> 00:12:15,640
and larger values

209
00:12:16,090 --> 00:12:20,230
of slightly rule them but not very much just slightly penalized compared to

210
00:12:21,760 --> 00:12:22,850
is to also

211
00:12:28,240 --> 00:12:29,460
so just one data point

212
00:12:29,990 --> 00:12:31,650
i can tell you something about what language should be

213
00:12:32,070 --> 00:12:33,960
and as you piling more and more data points

214
00:12:34,490 --> 00:12:36,080
you multiply these functions by

215
00:12:37,150 --> 00:12:40,250
the special case the data points out beyond the midpoint

216
00:12:41,400 --> 00:12:42,760
gives you the purple line here

217
00:12:43,290 --> 00:12:46,460
which is saying okay we can definitely rule out tiny values of lambda there's no

218
00:12:46,460 --> 00:12:49,110
way land that could be point one o point three or something like that

219
00:12:49,660 --> 00:12:52,610
you get a point at a twelve that's crazy so that's ruled out

220
00:12:53,130 --> 00:12:54,710
but all values of lambda

221
00:12:55,220 --> 00:12:58,880
from ten twenty thirty hundred they are all equally probable

222
00:13:00,210 --> 00:13:02,490
rather they predicted this data point equally well

223
00:13:03,510 --> 00:13:06,650
because they all predate essentially uniform distributions so they're all

224
00:13:07,050 --> 00:13:09,930
equally unsurprised by seeing a value at twelve

225
00:13:12,380 --> 00:13:14,450
okay so we multiply these together

226
00:13:15,220 --> 00:13:17,070
and when you multiply those functions together

227
00:13:17,620 --> 00:13:21,090
i have multiplied the functions imagining that there are just three data points at three

228
00:13:21,090 --> 00:13:34,250
OK morning

229
00:13:36,770 --> 00:13:39,300
the loss of you guys think

230
00:13:39,320 --> 00:13:41,950
they're coming

231
00:13:47,630 --> 00:13:50,700
so far you refer

232
00:13:50,700 --> 00:13:54,790
a bunch of matter solving a bunch of problems which are sort of

233
00:13:54,810 --> 00:13:57,530
now in the area of reinforcement learning

234
00:13:57,570 --> 00:14:03,260
and you start to wonder maybe or hopefully i mean how all these things together

235
00:14:03,260 --> 00:14:05,790
i mean this is just one of these

236
00:14:05,820 --> 00:14:08,390
thousands and more and more methods

237
00:14:08,390 --> 00:14:09,320
applied to

238
00:14:09,330 --> 00:14:11,420
problems where you

239
00:14:11,430 --> 00:14:16,570
you have to learn from data or something or some underlying principle underlying principles

240
00:14:17,790 --> 00:14:20,170
a few

241
00:14:20,390 --> 00:14:25,400
rules or

242
00:14:26,890 --> 00:14:29,640
we sort of hold everything together

243
00:14:29,650 --> 00:14:31,790
and in the next three hours

244
00:14:32,970 --> 00:14:34,390
i will present the

245
00:14:35,450 --> 00:14:40,110
statistical and computational foundations of machine learning

246
00:14:40,120 --> 00:14:43,180
unfortunately i mean three houses

247
00:14:43,230 --> 00:14:44,420
not too much

248
00:14:45,280 --> 00:14:49,540
i cannot really flesh out all the connections to all the other talks you've heard

249
00:14:49,560 --> 00:14:51,120
but i mean you will

250
00:14:51,140 --> 00:14:54,470
see here and there where the connections are

251
00:14:54,510 --> 00:14:57,700
especially the job talk in reinforcement learning

252
00:14:57,760 --> 00:15:01,610
we defer last week

253
00:15:01,620 --> 00:15:07,470
OK so one slide sort of one slide summary of my three hours today

254
00:15:08,510 --> 00:15:11,370
at the end of the target the same slide and you know then you can

255
00:15:11,370 --> 00:15:13,760
put it sort of in context

256
00:15:14,540 --> 00:15:16,450
the that of the economy

257
00:15:21,250 --> 00:15:23,430
like maybe we can

258
00:15:23,450 --> 00:15:26,820
can you see that OK or

259
00:15:30,490 --> 00:15:32,900
i would consider

260
00:15:32,910 --> 00:15:34,940
sequential data

261
00:15:34,960 --> 00:15:39,240
so that means essentially non IID data is the sequence of state data for instance

262
00:15:39,240 --> 00:15:39,970
i mean

263
00:15:40,030 --> 00:15:44,340
because metadata or stock market data or or other time series x one x two

264
00:15:44,340 --> 00:15:45,780
x three optics and

265
00:15:45,840 --> 00:15:48,370
and you want to predict xn plus one

266
00:15:48,590 --> 00:15:53,930
most of the talk later come to the agent framework and reinforcement learning and

267
00:15:53,930 --> 00:15:57,930
of course this general setup

268
00:15:57,930 --> 00:16:00,620
includes idea of a special case

269
00:16:02,240 --> 00:16:05,050
and actually even predicting

270
00:16:05,090 --> 00:16:09,850
i think i mentioned already you could ask me why should we care about prediction

271
00:16:09,870 --> 00:16:15,150
the ultimate goal is to do something with these predictions and to maximise your profits

272
00:16:15,190 --> 00:16:17,710
if you're capitalist or

273
00:16:17,830 --> 00:16:22,210
model to capture maximize utility function or

274
00:16:22,220 --> 00:16:25,100
minimize loss if you

275
00:16:25,310 --> 00:16:28,850
takenaka picture but that's just a minus sign

276
00:16:28,870 --> 00:16:30,020
OK so

277
00:16:32,220 --> 00:16:34,550
nearly all approaches what you do it

278
00:16:34,590 --> 00:16:38,280
you start with the class of models or hypothesis

279
00:16:38,300 --> 00:16:40,970
regulator could come from

280
00:16:41,870 --> 00:16:45,150
then you can try to find

281
00:16:45,190 --> 00:16:48,710
ideally the true hypothesis of the true hypothesis about

282
00:16:48,960 --> 00:16:52,060
it's hard to tell what the true the ground truth is

283
00:16:53,310 --> 00:16:56,680
let's talk about good hypothesis whatever good means in detail

284
00:16:56,680 --> 00:17:00,150
and one classical estimators the maximum likelihood estimator so

285
00:17:00,190 --> 00:17:02,180
you have

286
00:17:02,180 --> 00:17:04,410
the local point of

287
00:17:06,340 --> 00:17:09,190
take this likelihood function

288
00:17:10,750 --> 00:17:14,660
this is sort of defining what the hypothesis mean what this age i mean for

289
00:17:14,660 --> 00:17:20,520
this hypothesis and the policies that you a probability distribution over

290
00:17:20,530 --> 00:17:22,220
o we we're sample space

291
00:17:22,240 --> 00:17:28,190
so in the maximum likelihood estimator just take the hypothesis which has a maximum likelihood

292
00:17:29,150 --> 00:17:30,780
the argument is

293
00:17:32,630 --> 00:17:36,060
it's like very low

294
00:17:36,080 --> 00:17:39,910
then it's very exceptional that all this data but this all this data so it

295
00:17:39,910 --> 00:17:41,630
should not be too exceptional

296
00:17:41,670 --> 00:17:44,420
and if the likelihood is high

297
00:17:44,470 --> 00:17:47,030
you can expect the state and they happen so

298
00:17:47,040 --> 00:17:51,850
it sort of seems to come from the hypothesis and over the arguments and in

299
00:17:51,850 --> 00:17:55,260
any case i would not consider the maximum likelihood

300
00:17:55,280 --> 00:17:56,410
and further

301
00:17:56,420 --> 00:18:00,910
because it has some problems if the class is too large

302
00:18:00,940 --> 00:18:03,280
it leads to overfitting

303
00:18:03,400 --> 00:18:07,380
so in the bayesian approach what you do is you consider the first year which

304
00:18:07,380 --> 00:18:10,290
is a much cleaner interpretations of what you want

305
00:18:11,530 --> 00:18:13,300
one the probability

306
00:18:14,070 --> 00:18:18,090
the degree of belief in a hypothesis after seeing the data

307
00:18:18,110 --> 00:18:21,070
and luckily you can compute it from the likelihood

308
00:18:21,110 --> 00:18:22,470
times prior

309
00:18:22,490 --> 00:18:25,950
so what we additionally and now is the prior

310
00:18:25,960 --> 00:18:30,670
the question is whether this prior come from and there are several principles but this

311
00:18:30,690 --> 00:18:34,510
actually only one principal with general enough to cover really i mean

312
00:18:34,590 --> 00:18:36,660
all of machine learning

313
00:18:38,150 --> 00:18:42,010
and it is actually occam's razor they combined with the core principle

314
00:18:42,090 --> 00:18:45,420
so what you should do it we should have a high prior for simple models

315
00:18:45,420 --> 00:18:47,490
and i will quantify that

316
00:18:47,530 --> 00:18:52,010
so you quantify simplicity of complexity with kolmogorov complexity

317
00:18:52,030 --> 00:18:55,030
or solomonoff induction scheme

318
00:18:55,080 --> 00:18:57,980
and then you have everything you need

319
00:18:59,360 --> 00:19:02,590
the orient to wikileaks

320
00:19:02,600 --> 00:19:04,940
in very general generic

321
00:19:04,950 --> 00:19:06,400
i tell you

322
00:19:06,410 --> 00:19:08,450
that all these works

323
00:19:08,490 --> 00:19:12,490
if you true hypothesis when your data is sampled from is in the model class

324
00:19:12,510 --> 00:19:14,900
very rough statement about

325
00:19:15,010 --> 00:19:17,420
now it's sort of like that

326
00:19:18,770 --> 00:19:21,690
so this is cover the prediction case and

327
00:19:21,800 --> 00:19:23,340
i developed

328
00:19:23,350 --> 00:19:28,150
actually element of that but i mean showing here a universal way of doing prediction

329
00:19:28,150 --> 00:19:30,400
in any kind of domain

330
00:19:30,450 --> 00:19:32,260
and the last step is to

331
00:19:33,610 --> 00:19:36,540
to generalise the to the reinforcement learning setup

332
00:19:36,570 --> 00:19:38,970
big decisions influenced by environment

333
00:19:38,990 --> 00:19:40,880
so the generally accepted

334
00:19:40,920 --> 00:19:42,480
OK so that was

335
00:19:44,390 --> 00:19:51,970
for instance if m is the class of or polynomial

336
00:19:51,980 --> 00:19:54,030
all degrees

337
00:19:54,040 --> 00:19:56,610
and then you have a few data points

338
00:19:56,700 --> 00:20:00,690
and then the best fitting polynomials of the maximum likelihood polynomial i mean if you

339
00:20:01,530 --> 00:20:03,440
gaussian noise and then

340
00:20:03,480 --> 00:20:05,710
maximum likelihood is just minimising

341
00:20:05,790 --> 00:20:07,860
this screen

342
00:20:07,910 --> 00:20:11,330
and then you get a polynomial perfectly fits the data

343
00:20:11,400 --> 00:20:13,100
which isn't

344
00:20:13,110 --> 00:20:15,340
is what

345
00:20:15,400 --> 00:20:21,200
i contains everything site contains sort of all degree one polynomials of degree two polynomial

346
00:20:21,200 --> 00:20:25,090
according to of three fully answers the union

347
00:20:25,100 --> 00:20:28,340
for instance

348
00:20:33,830 --> 00:20:36,920
so i think again this definition already

349
00:20:36,920 --> 00:20:41,620
learn some stuff tried it out learn some more stuff tried it out and it built up

350
00:20:42,650 --> 00:20:46,700
there is an enormous literature and education that says that's exactly how we always learn everything

351
00:20:48,180 --> 00:20:52,080
and so that's the way this courses focus what we'll do is just for example

352
00:20:52,080 --> 00:20:54,940
for today will learn a little bit about software engineering

353
00:20:56,450 --> 00:20:59,900
then we'll do to lab sessions where you actually try to use the things that we talk about

354
00:21:02,160 --> 00:21:04,470
then we'll come back to like have some more theory

355
00:21:05,020 --> 00:21:06,880
about how you would do programming

356
00:21:07,770 --> 00:21:09,470
and then you go back to do more stuff

357
00:21:10,420 --> 00:21:11,930
and the hope is that by

358
00:21:12,400 --> 00:21:15,040
by this tangible context

359
00:21:15,700 --> 00:21:18,660
you have a deeper appreciation and the ideas that we're trying to convey

360
00:21:23,410 --> 00:21:26,210
let me tell you a little bit about the four modules that we've chosen

361
00:21:26,650 --> 00:21:28,780
the course is gonna be organised on four modules

362
00:21:30,560 --> 00:21:35,670
each module will take about one-fourth of the course first thing look at software engineering

363
00:21:37,040 --> 00:21:40,500
as i said we don't have time to focus on or even survei

364
00:21:41,100 --> 00:21:44,530
the big idea all the big ideas in software engineering is far too big

365
00:21:44,960 --> 00:21:47,910
so really focused narrowly on water two things

366
00:21:49,770 --> 00:21:50,770
we'd like you to know

367
00:21:51,420 --> 00:21:53,430
about abstraction and modularity

368
00:21:53,850 --> 00:21:55,800
because that's such an important idea

369
00:21:57,370 --> 00:21:59,450
in the construction of big systems

370
00:22:00,730 --> 00:22:04,430
so that's gonna be our focus will begin in today's lecture

371
00:22:05,560 --> 00:22:06,910
will begin talking about

372
00:22:07,460 --> 00:22:09,430
i modularity and abstraction

373
00:22:09,850 --> 00:22:10,860
at a small scale

374
00:22:12,800 --> 00:22:14,210
how's it affect the things you type

375
00:22:15,060 --> 00:22:16,480
has instructions to a computer

376
00:22:17,930 --> 00:22:21,290
but by next week we're going be talking about whole bigger scale

377
00:22:22,390 --> 00:22:24,040
by next week we're gonna talk about

378
00:22:24,520 --> 00:22:26,650
constructing software modules

379
00:22:28,130 --> 00:22:32,230
at a much higher level in particular will talk about something that will cost state machine

380
00:22:33,780 --> 00:22:35,050
a state machine is of saying

381
00:22:36,510 --> 00:22:37,840
networks in steps

382
00:22:39,680 --> 00:22:40,900
on every step

383
00:22:42,430 --> 00:22:44,080
the state machine get a new input

384
00:22:45,630 --> 00:22:50,320
then based on any input and it's memory of what's come before r the state

385
00:22:50,320 --> 00:22:53,030
machine decides to do something that generates an output

386
00:22:53,470 --> 00:22:54,550
and then the process repeats

387
00:22:57,340 --> 00:23:01,130
we will see that that's kind an abstraction state machines

388
00:23:02,170 --> 00:23:05,410
has there's a way to think about state machines there is compositional

389
00:23:05,960 --> 00:23:06,430
but you can

390
00:23:06,950 --> 00:23:08,040
i think of it as

391
00:23:09,740 --> 00:23:10,830
just as you can think of

392
00:23:11,340 --> 00:23:14,910
low level hierarchies within a language also a lot more about that today

393
00:23:16,320 --> 00:23:19,750
so the idea would be that once you've composed a state machine

394
00:23:20,950 --> 00:23:25,900
you'll be able to join to state machines and have behavior look just like one state machine

395
00:23:26,500 --> 00:23:28,680
that's a way to get a more complicated behavior

396
00:23:29,360 --> 00:23:34,210
by constructing to simply behaviors that's what we want we wanna learn tools that lets

397
00:23:34,210 --> 00:23:40,050
us compose complex behaviors out of simple behaviors and the tangible model will be the

398
00:23:42,650 --> 00:23:47,000
we will see how to write a program that controls a robot

399
00:23:49,080 --> 00:23:49,840
a state machine

400
00:23:51,300 --> 00:23:53,790
that's certainly not the only way you can control the robot

401
00:23:54,280 --> 00:23:57,250
and it's probably not the way you would first think of it if u

402
00:23:57,680 --> 00:24:02,640
it took one course in programming and somebody said u go program the robot to do something

403
00:24:04,160 --> 00:24:07,490
what we will see is that it's a very powerful way to think about it

404
00:24:07,660 --> 00:24:10,600
for exactly this reason have modularity

405
00:24:12,510 --> 00:24:16,630
the bigger point that we will make in thinking about this first module is the idea

406
00:24:17,930 --> 00:24:24,140
how do you make systems modular r how do you use abstraction simplified the design task and in particular

407
00:24:24,550 --> 00:24:27,150
we will focus on something that will call peak at

408
00:24:29,350 --> 00:24:33,770
when you think about a system we will always think about in terms of what are the primitives

409
00:24:34,950 --> 00:24:35,970
how do you combine them

410
00:24:37,300 --> 00:24:38,520
how do you abstract

411
00:24:38,940 --> 00:24:40,220
other bigger behavior

412
00:24:42,310 --> 00:24:43,670
from those smaller behaviors

413
00:24:44,400 --> 00:24:47,710
and what are the patterns that are important to capture

414
00:24:48,990 --> 00:24:54,990
so the bigger point is this idea of peak at which we will then revisit it every subsequent model

415
00:24:58,020 --> 00:25:00,010
okay second modules on signals and systems

416
00:25:01,410 --> 00:25:02,990
that's also an enormous area

417
00:25:04,600 --> 00:25:08,270
so we only have time to do one thing the thing that we will do is

418
00:25:08,800 --> 00:25:10,930
we will think about discrete time

419
00:25:13,780 --> 00:25:16,050
how do you make a system that's cognizant

420
00:25:16,730 --> 00:25:17,750
and what is done

421
00:25:18,660 --> 00:25:22,240
so that's in the future can do things with awareness

422
00:25:23,140 --> 00:25:23,990
i've how it got there

423
00:25:25,610 --> 00:25:27,700
a good example is robotics hearing

424
00:25:28,990 --> 00:25:30,160
so the idea is going to be

425
00:25:30,610 --> 00:25:33,350
okay i think about what you do when you're driving a car

426
00:25:35,140 --> 00:25:38,560
and think about how you would tell a robot to do exact same thing

427
00:25:40,830 --> 00:25:42,800
here's a naive driving algorithm

428
00:25:43,370 --> 00:25:45,950
i don't recommend it but is widely used in boston apparently

429
00:25:48,250 --> 00:25:50,790
i find myself to the right where i would like to be

430
00:25:51,180 --> 00:25:51,870
so what should i do

431
00:25:53,500 --> 00:25:54,210
turn left

432
00:25:55,530 --> 00:25:57,860
i'm still to the right where like to be what should i do

433
00:25:59,220 --> 00:25:59,920
turn left

434
00:26:01,010 --> 00:26:03,420
o i'm exactly right should be

435
00:26:04,440 --> 00:26:05,010
what should i do

436
00:26:07,110 --> 00:26:08,100
go straight ahead

437
00:26:09,660 --> 00:26:10,270
that's a bad idea

438
00:26:11,670 --> 00:26:17,810
and what see is that perfectly innocent looking algorithms can have whole renders performance

439
00:26:19,210 --> 00:26:20,020
what will do

440
00:26:20,860 --> 00:26:25,020
is try to make abstraction about will try to make a model

441
00:26:25,670 --> 00:26:30,670
will try to capture batman map so that we don't need to build the to see the bad behavior

442
00:26:32,050 --> 00:26:32,880
will make a model

443
00:26:34,130 --> 00:26:37,540
we use the model to predict the bad algorithms stinks

444
00:26:38,980 --> 00:26:41,070
but more importantly we use the model

445
00:26:41,550 --> 00:26:43,540
the figure out algorithm that work better

446
00:26:44,730 --> 00:26:47,560
in fact will even be able to come up with balance

447
00:26:48,200 --> 00:26:49,090
on how well

448
00:26:49,570 --> 00:26:51,580
such a controller could possibly work

449
00:26:52,930 --> 00:26:55,260
so the focus in this model is going to be

450
00:26:55,710 --> 00:26:58,820
how do you make a model to predict behavior how do you analyse the model

451
00:27:00,440 --> 00:27:01,970
so that you can design a better system

452
00:27:02,870 --> 00:27:05,490
and then how do you use the modeling analysis to

453
00:27:06,470 --> 00:27:09,010
stood to make it i am well behave system

454
00:27:11,870 --> 00:27:13,350
the third modules on circuits

455
00:27:14,720 --> 00:27:15,970
again circus is huge

456
00:27:16,850 --> 00:27:20,090
we don't have time to talk about all circuits will do very simple things

457
00:27:20,640 --> 00:27:21,990
we will focus our attention

458
00:27:22,530 --> 00:27:25,100
on how you would add sensory capability

459
00:27:26,490 --> 00:27:28,490
to an already complicated system

460
00:27:29,850 --> 00:27:34,230
the idea is going to be start with the robot is is brighter start with our robots

461
00:27:36,270 --> 00:27:37,900
and design eight head

462
00:27:39,390 --> 00:27:40,040
for the robot

463
00:27:41,030 --> 00:27:42,440
the robot comes from the factory

464
00:27:42,850 --> 00:27:46,720
with sonar sensors that are sensitive these things that eight of them

465
00:27:47,180 --> 00:27:48,470
they tell you how far away

466
00:27:48,960 --> 00:27:51,670
something that reflects the e ultrasonic wave

467
00:27:54,620 --> 00:27:57,120
actually come from the factory robots can sense light

468
00:27:58,970 --> 00:28:00,760
what you do is add light sensors

469
00:28:01,980 --> 00:28:02,330
the goal

470
00:28:02,900 --> 00:28:04,470
is to make a system

471
00:28:05,070 --> 00:28:08,070
the modify the robot so that the robot tracks light

472
00:28:08,940 --> 00:28:09,830
the very simple ball

473
00:28:10,810 --> 00:28:11,950
and we we do then

474
00:28:12,550 --> 00:28:17,850
is to augment the robot with a simple sensor here showed a little more magnified here

475
00:28:19,110 --> 00:28:20,150
the idea is

476
00:28:20,700 --> 00:28:22,140
this is logo motor

477
00:28:22,140 --> 00:28:24,240
it is rather different

478
00:28:24,240 --> 00:28:26,470
in both cases

479
00:28:27,740 --> 00:28:29,180
so practically

480
00:28:30,450 --> 00:28:33,800
in the medical domain used to

481
00:28:33,800 --> 00:28:39,160
you can values and false discovery rates to give to that

482
00:28:39,160 --> 00:28:45,220
but practically trust models because the vision

483
00:28:50,240 --> 00:28:55,110
however they give you a slightly different information

484
00:28:55,120 --> 00:29:01,340
because the force is correct and he values his some idea about the fraction of

485
00:29:01,370 --> 00:29:04,030
bad guys you have in your

486
00:29:04,300 --> 00:29:06,140
selected features

487
00:29:06,160 --> 00:29:13,820
whereas the classification results you more sense about how predictive your feature set is and

488
00:29:13,820 --> 00:29:14,640
you may

489
00:29:14,660 --> 00:29:16,800
include a large

490
00:29:16,820 --> 00:29:18,050
fraction of garbage

491
00:29:18,070 --> 00:29:23,170
features in your feature set and yet get very good predictive values for example if

492
00:29:23,170 --> 00:29:27,590
if you use support vector machines on the packages

493
00:29:27,590 --> 00:29:28,800
you may get

494
00:29:28,820 --> 00:29:33,300
as good results with respect to the features set or even better results even though

495
00:29:33,300 --> 00:29:35,760
you have tons of garbage features

496
00:29:35,780 --> 00:29:41,660
in the challenges that i've organised have introduced purposely some garbage which is so i

497
00:29:41,660 --> 00:29:44,490
knew for a fact that they were going to choose

498
00:29:44,530 --> 00:29:47,240
and people have obtained you know

499
00:29:47,240 --> 00:29:52,760
top ranking performances without performing any feature selection

500
00:29:53,870 --> 00:29:55,760
you basically need both

501
00:29:55,760 --> 00:29:59,160
aspects depending on what the emphasis is

502
00:29:59,180 --> 00:30:02,200
all right if you care about

503
00:30:02,220 --> 00:30:06,950
the features that you selected for example in medical applications you care about really understanding

504
00:30:06,950 --> 00:30:09,030
which features are predicted

505
00:30:09,050 --> 00:30:13,050
knowing that to get good prediction performance and preservation maybe not enough

506
00:30:13,070 --> 00:30:14,590
in which case

507
00:30:14,620 --> 00:30:21,220
you need to resort to computing for discovery

508
00:30:21,240 --> 00:30:22,700
i have

509
00:30:22,720 --> 00:30:23,910
the question

510
00:30:46,590 --> 00:30:48,820
that's correct

511
00:30:48,870 --> 00:30:56,260
scherzo a forward selection backward elimination methods do not yield the same type of features

512
00:30:56,260 --> 00:30:57,320
that set at all

513
00:30:57,470 --> 00:31:04,680
backward elimination you may end up with a set of complementary features and as you

514
00:31:05,550 --> 00:31:10,120
removing features then you have to get softer degradation at some point at some point

515
00:31:10,200 --> 00:31:14,320
will be only one feature such that you know all the remaining features on not

516
00:31:14,340 --> 00:31:16,700
predictive together anymore

517
00:31:16,720 --> 00:31:22,450
you don't see that in forward selection because the first feature selectors predictive but then

518
00:31:22,450 --> 00:31:27,120
you run at risk of selecting first feature that is predicted by itself

519
00:31:27,120 --> 00:31:31,800
but then you miss eventually power features the together would have been more

520
00:31:31,820 --> 00:31:35,970
predictive than than that single feature the select first

521
00:31:36,010 --> 00:31:42,820
enforce selection you tend to get very compact sets of subsets of features

522
00:31:42,840 --> 00:31:45,950
and which may be an advantage or disadvantage

523
00:31:45,970 --> 00:31:51,510
because someone redundancy helps to give give give better performance on the test set

524
00:31:52,300 --> 00:31:56,640
there is no you know one method that should be recommended

525
00:31:56,660 --> 00:31:58,930
depends on what you want to achieve

526
00:31:58,930 --> 00:32:06,350
sometimes it's important that you will be able to trade number of features or accuracy

527
00:32:06,370 --> 00:32:10,570
this is an advantage in that case to do forward selection because you can smoothly

528
00:32:10,570 --> 00:32:16,910
move you're number of features without having this catastrophic degradation and sometimes you really want

529
00:32:16,910 --> 00:32:18,370
to have an optimal

530
00:32:18,450 --> 00:32:22,620
each subset of complementary features in which case back ruminations

531
00:32:22,640 --> 00:32:25,320
maybe an advantage

532
00:32:25,320 --> 00:32:35,340
so had joined but two other questions one of them was with this

533
00:32:39,410 --> 00:32:43,050
you know

534
00:32:43,050 --> 00:32:44,230
the laplacian i can maps

535
00:32:45,120 --> 00:32:46,820
which is the spectral algorithm

536
00:32:47,380 --> 00:32:51,360
and by special meaning can compute the i can decomposition at some point something

537
00:32:52,750 --> 00:32:54,890
introduced by mister belkin partha niyogi

538
00:32:55,910 --> 00:33:01,740
where you define the same neighborhood in the data space and then define the adjacency matrix where

539
00:33:02,200 --> 00:33:03,590
he data development

540
00:33:03,620 --> 00:33:06,790
it's only nonzero if i and j are neighbours

541
00:33:08,540 --> 00:33:10,610
now the idea is that good

542
00:33:11,200 --> 00:33:14,140
one dimensional embedding is one where the latent points

543
00:33:15,000 --> 00:33:16,620
minimize this cost function

544
00:33:17,120 --> 00:33:21,530
so this cost function is dependent on the latent points and basically if two points are neighbors

545
00:33:22,540 --> 00:33:23,180
this applies

546
00:33:24,340 --> 00:33:27,350
otherwise it doesn't so two points need to be close

547
00:33:28,160 --> 00:33:28,830
if the neighbours

548
00:33:31,470 --> 00:33:36,000
annotation been at distance in latent space so we'll just right now delta i j

549
00:33:37,410 --> 00:33:40,430
so there is a slight problem with a cost function house is minimized

550
00:33:42,000 --> 00:33:43,960
by putting all the data points on top of each other

551
00:33:44,690 --> 00:33:46,120
it doesn't matter whether adjacent or not

552
00:33:48,700 --> 00:33:52,460
what we do is we also have a constraint that they should be inflated and

553
00:33:52,460 --> 00:33:55,660
that will come in in a moment but we can write this thing in matrix

554
00:33:55,660 --> 00:33:59,630
form has this trace so i as i say i like writing things in matrix

555
00:34:00,300 --> 00:34:04,900
o any land and i'm gonna do this trick again a bit somebody quick now

556
00:34:04,900 --> 00:34:07,210
and maybe slower later if this time

557
00:34:08,060 --> 00:34:12,600
that means that we can say that adjacency matrix multiplied by these distances

558
00:34:14,180 --> 00:34:16,260
if we introduce something called the laplacian

559
00:34:17,480 --> 00:34:21,240
we have a degree matrix and this degree matrix is the diagonal matrix

560
00:34:21,770 --> 00:34:26,060
where the diagonal elements are the sum of the adjacency for the elements basically the

561
00:34:26,060 --> 00:34:30,170
number of neighbors if these adjacencies a constant is the number of neighbors for each

562
00:34:30,170 --> 00:34:30,800
data point

563
00:34:31,450 --> 00:34:36,240
sometimes you can set these adjacencies constant but it's then just the sum of those weights

564
00:34:37,200 --> 00:34:39,000
the laplacian is written in this form so

565
00:34:39,420 --> 00:34:44,150
it's a defined thing for a graph that is the degree matrix minus the adjacency matrix

566
00:34:45,520 --> 00:34:47,670
using the is this is the trick

567
00:34:48,230 --> 00:34:51,740
which as i say i think i may have more details on later that allows

568
00:34:51,740 --> 00:34:55,230
us to go from this squared distance is the same trick actually

569
00:34:56,100 --> 00:34:59,880
as we've been using without centering matrix this is the squared distance matrix

570
00:35:00,260 --> 00:35:01,860
and this is an inner product matrix

571
00:35:02,320 --> 00:35:04,170
the reason you can pop between these two

572
00:35:04,860 --> 00:35:05,790
is because

573
00:35:08,330 --> 00:35:08,820
try and show

574
00:35:14,700 --> 00:35:16,010
this matrix delta

575
00:35:17,470 --> 00:35:21,340
is a distance matrix so it has zeros along the values on the diagonal yeah

576
00:35:22,680 --> 00:35:23,570
all along the diagonal

577
00:35:24,440 --> 00:35:26,260
taking the trace two matrices

578
00:35:27,310 --> 00:35:29,450
the multiplied together is equivalent to

579
00:35:29,880 --> 00:35:32,140
the matlab dot times old multiplication

580
00:35:32,650 --> 00:35:36,860
followed by some some that's the way that don't ever multiply two matrices together and

581
00:35:36,860 --> 00:35:39,070
then take the trace that's extremely slow

582
00:35:39,520 --> 00:35:41,580
element-by-element multiply them and then some

583
00:35:42,070 --> 00:35:45,850
both directions that's much much faster once cubic once quadratic

584
00:35:49,100 --> 00:35:54,610
there is the distance matrix delta and that's being dot times by any which is this adjacency matrix

585
00:35:56,660 --> 00:35:58,930
so it is a sparse so you've got this delta

586
00:35:59,360 --> 00:36:04,950
times this past form which has solidified and let's call them once but it could be anything it's symmetric

587
00:36:06,980 --> 00:36:09,710
everything's symmetric neighbour and zeros everywhere else

588
00:36:11,170 --> 00:36:12,020
so are not terribly

589
00:36:17,840 --> 00:36:22,050
okay so because the zeros down the diagonal it doesn't matter what the diagonal elevators

590
00:36:22,950 --> 00:36:25,050
you can set angle they anyway you like

591
00:36:27,290 --> 00:36:29,580
so what this is saying is set

592
00:36:32,290 --> 00:36:35,570
with de-mining saying where d is a diagonal matrix

593
00:36:36,210 --> 00:36:36,830
call at al

594
00:36:40,940 --> 00:36:41,740
so if you know

595
00:36:43,340 --> 00:36:47,060
it also has the characteristic because j is the sum of the rose

596
00:36:47,500 --> 00:36:50,870
el times one is equal to zero it's got this null space

597
00:36:52,780 --> 00:36:56,330
now that means that we multiply held by a distance matrix

598
00:36:57,370 --> 00:36:58,080
what happens

599
00:37:00,490 --> 00:37:01,880
this overrepresentation hair

600
00:37:13,060 --> 00:37:14,580
we get these terms canceling

601
00:37:15,520 --> 00:37:19,930
and you get just this x x transpose appearing so in rough sketch

602
00:37:20,670 --> 00:37:21,850
that's a nice trick

603
00:37:22,320 --> 00:37:24,870
it's also used in spring systems which will mention

604
00:37:25,410 --> 00:37:26,040
a bit later

605
00:37:26,750 --> 00:37:28,980
so turned what was the next uh

606
00:37:29,570 --> 00:37:34,000
function in terms of a distance matrix into a function in terms of inner product matrix

607
00:37:34,760 --> 00:37:35,460
and this thing

608
00:37:37,060 --> 00:37:37,630
turns out to be

609
00:37:38,310 --> 00:37:40,330
quite straightforward to minimize

610
00:37:42,270 --> 00:37:44,050
the actually has this affect

611
00:37:44,680 --> 00:37:47,530
making insensitive translations in this form

612
00:37:47,780 --> 00:37:52,080
if you have this matrix here having a null space in the i can value one

613
00:37:53,110 --> 00:37:56,690
then means that's it doesn't matter what x is

614
00:37:58,200 --> 00:37:58,650
i mean is

615
00:37:59,560 --> 00:38:01,310
it's it's insensitive to translations

616
00:38:02,690 --> 00:38:06,380
that's minimized by placing all points on top of one another so what we do

617
00:38:06,380 --> 00:38:09,460
is we add a constraint and this is the constraint they choose which is

618
00:38:10,230 --> 00:38:14,960
that the inner product between each data point and the degree matrix is the

619
00:38:15,470 --> 00:38:19,370
so that each state feature and the degree matrix should be one you could choose

620
00:38:19,520 --> 00:38:24,970
a constraint where that's is just x kernel i transpose x can i is equal

621
00:38:24,970 --> 00:38:27,550
to one without the dean they happened to this constraint

622
00:38:28,290 --> 00:38:32,000
it is slightly annoying because it leads to a generalized eigen value problem rather than

623
00:38:32,000 --> 00:38:35,390
a standard eigen value problem that we've seen if they not chosen a constrained they

624
00:38:35,390 --> 00:38:39,620
just chosen the covariance the the embedding should be one which is the normal constraints

625
00:38:39,810 --> 00:38:40,620
you would get

626
00:38:41,040 --> 00:38:42,170
and i can value problem here

627
00:38:42,830 --> 00:38:44,590
but i think you know you're splitting hairs sister

628
00:38:46,010 --> 00:38:50,980
i think that's not really making much difference in means is not quite minimizing error functions we talked about before

629
00:38:51,750 --> 00:38:54,190
or when we see it in this more general framework it won't be

630
00:38:55,380 --> 00:38:59,260
so the smallest igon value this laplacian is zero because it's got

631
00:38:59,660 --> 00:39:00,170
a zero

632
00:39:00,620 --> 00:39:01,950
constant i can vector which is

633
00:39:02,390 --> 00:39:03,330
zero igon value

634
00:39:03,950 --> 00:39:08,180
and that's typically thrown away and then you retain the next q i can values for embedding

635
00:39:09,190 --> 00:39:14,080
and you're minimizing as i say this subjective here under the constraint that the variance of axes

636
00:39:14,550 --> 00:39:15,200
in fact

637
00:39:15,950 --> 00:39:17,570
deep the covariance effects

638
00:39:20,980 --> 00:39:23,220
now they tend be parameterized

639
00:39:23,880 --> 00:39:28,950
even by setting it took on a constant i convert constant values which they call the simple-minded approach

640
00:39:29,870 --> 00:39:32,240
all according to distance between two data points

641
00:39:33,690 --> 00:39:34,910
so this is the adjacency

642
00:39:35,930 --> 00:39:41,870
element hear that they will sometimes set and it's again it's not related why i mean this function appears everywhere

643
00:39:42,630 --> 00:39:47,620
there's no specific reason why they should be related in this case the obvious for

644
00:39:47,630 --> 00:39:49,080
being talked about talking about before

645
00:39:49,620 --> 00:39:55,740
they motivated by analogy between discrete graph laplacian and class beltrami operator

646
00:39:56,230 --> 00:39:58,830
which continuous diffusion equations

647
00:39:59,780 --> 00:40:03,150
which they're saying the graph compression is discrete approximation to

648
00:40:04,220 --> 00:40:06,760
i haven't played a lot with this so i don't know how sensitive

649
00:40:07,260 --> 00:40:10,400
these algorithms are to the setting up this length scale here

650
00:40:11,100 --> 00:40:13,560
and how you set these adjacency weights but it's something

651
00:40:14,240 --> 00:40:15,790
you know people do when they do this

652
00:40:16,650 --> 00:40:17,960
so you can do those two

653
00:40:18,380 --> 00:40:22,100
the smallest igon values and then you see embeddings like this this one's picked up

654
00:40:22,110 --> 00:40:26,410
the fact that the start of the run is different but then hasn't quite got

655
00:40:26,410 --> 00:40:31,050
i how do put the coin that was the entire amount of think tanks so

656
00:40:31,070 --> 00:40:34,050
skip lists are randomized structure

657
00:40:34,150 --> 00:40:37,040
and in another adjective here

658
00:40:37,060 --> 00:40:40,880
and just

659
00:40:40,950 --> 00:40:42,850
and this also and simple

660
00:40:42,960 --> 00:40:48,310
really quite so we have a simple efficient dynamic random search structure

661
00:40:48,330 --> 00:40:50,050
all those things together

662
00:40:50,060 --> 00:40:53,870
and so it's sort of like trees and that the bound is only

663
00:40:53,920 --> 00:40:58,450
a randomized them but today we're going to see a much stronger bound expectation about

664
00:40:58,450 --> 00:41:04,440
some particular skip lists all running order log and expected time

665
00:41:04,490 --> 00:41:11,950
so the running time for each operation will be able to log in expectation

666
00:41:12,600 --> 00:41:17,850
we're going to prove a much stronger results that there were large and with high

667
00:41:27,980 --> 00:41:30,090
so this is a very strong

668
00:41:32,450 --> 00:41:33,630
and that means

669
00:41:33,640 --> 00:41:36,420
that the running time of each operation

670
00:41:36,440 --> 00:41:41,260
the running time of every operations or log and almost always

671
00:41:41,310 --> 00:41:43,350
in a certain sense

672
00:41:43,360 --> 00:41:44,930
one i foreshadowing

673
00:41:49,680 --> 00:41:52,390
so something like

674
00:41:53,450 --> 00:41:59,130
the probability that order log n is at least one minus one over some polynomial

675
00:41:59,130 --> 00:42:02,930
in n and you get to set the polynomial however large you like

676
00:42:02,970 --> 00:42:04,120
so what this

677
00:42:04,130 --> 00:42:07,860
basically means is that almost all the time you take taken skip list you do

678
00:42:07,860 --> 00:42:12,650
a polynomial number operations on because presumably running a polynomial time algorithms using this data

679
00:42:12,650 --> 00:42:16,210
structure polynomial members of inserts deletes searches

680
00:42:16,220 --> 00:42:20,000
every single one of them will take or log n time

681
00:42:20,040 --> 00:42:21,580
almost guarantee

682
00:42:21,680 --> 00:42:25,940
so this is a really strong down on the tail of the distribution the mean

683
00:42:25,940 --> 00:42:30,240
is or login that's not so exciting and the fact that almost all the weight

684
00:42:30,240 --> 00:42:34,980
of this probability distributions right around like in this tiny little abseil ones

685
00:42:35,000 --> 00:42:37,290
very very tiny probabilities you can be

686
00:42:37,330 --> 00:42:40,560
bigger than again

687
00:42:40,610 --> 00:42:43,050
so that's

688
00:42:43,060 --> 00:42:44,260
we're going

689
00:42:44,270 --> 00:42:46,040
this is the a data structure by

690
00:42:46,100 --> 00:42:51,810
in nineteen nine so this is the most recent

691
00:42:51,940 --> 00:42:55,200
as actually a trip somewhere recent they're like

692
00:42:55,220 --> 00:42:57,570
ninety three or so

693
00:42:57,580 --> 00:43:00,900
but fairly recent data structure

694
00:43:00,920 --> 00:43:03,790
just insert delete search

695
00:43:03,840 --> 00:43:05,570
and it's very simple

696
00:43:05,760 --> 00:43:08,990
you can derive it if you don't know anything about the structure

697
00:43:10,350 --> 00:43:11,820
almost nothing

698
00:43:11,840 --> 00:43:17,360
now analyse analyzing the performance is lower than that of course takes are sophistication but

699
00:43:17,930 --> 00:43:19,940
the structure itself is very simple

700
00:43:19,960 --> 00:43:22,330
we're going to start from scratch

701
00:43:22,340 --> 00:43:29,690
suppose you don't know

702
00:43:29,710 --> 00:43:33,920
what a red black tree is don't know what the tree is

703
00:43:33,930 --> 00:43:36,820
because you don't even know what the tree is

704
00:43:36,830 --> 00:43:39,310
what is the simplest data structure

705
00:43:39,320 --> 00:43:42,550
restoring a bunch of items

706
00:43:42,560 --> 00:43:45,190
restoring dynamics

707
00:43:45,250 --> 00:43:46,090
o west

708
00:43:47,030 --> 00:43:48,250
link list

709
00:43:48,260 --> 00:43:52,000
now i'm going to suppose this sorted linked lists are going to be a little

710
00:43:52,000 --> 00:43:53,170
bit and

711
00:43:56,560 --> 00:44:00,750
so if you have a linked list of items is

712
00:44:00,760 --> 00:44:03,300
maybe on a doubly linked just for kicks

713
00:44:05,740 --> 00:44:11,030
how long does it take to search in assorted blinklist

714
00:44:11,040 --> 00:44:18,650
again is one answer

715
00:44:18,670 --> 00:44:24,100
and the other answer which illustrates and is the right answer so even though it's

716
00:44:24,100 --> 00:44:28,200
sorted we can do binary search because we don't have random access to linguist

717
00:44:28,210 --> 00:44:30,880
so i suppose and i'm only giving a pointer to the head

718
00:44:30,900 --> 00:44:35,730
otherwise i'm assuming it's an array and the sorted array construction log sort of like

719
00:44:35,730 --> 00:44:38,430
west-eastern kind scan through the time

720
00:44:40,270 --> 00:44:46,580
and worst case search not so good but

721
00:44:46,680 --> 00:44:52,010
we just try to improve it a little bit we will discover skip lists automatically

722
00:44:53,660 --> 00:44:56,430
this is our starting point sorted linked lists

723
00:44:56,440 --> 00:44:59,950
date and time and i'm not going to think too much about insertions deletions for

724
00:44:59,950 --> 00:45:01,110
the moment was just

725
00:45:01,160 --> 00:45:02,850
get search better

726
00:45:02,960 --> 00:45:05,590
and we were brought to this

727
00:45:05,600 --> 00:45:08,300
updates so we're randomisation come in search

728
00:45:08,320 --> 00:45:12,710
pretty easy idea so how can we make a linguist

729
00:45:12,730 --> 00:45:15,080
suppose all we know about our linkclus

730
00:45:15,090 --> 00:45:17,660
what can i do to make it faster

731
00:45:17,710 --> 00:45:21,300
this is where you need a little bit of innovation

732
00:45:21,350 --> 00:45:24,790
so creativity

733
00:45:24,840 --> 00:45:27,910
four links that's a good idea

734
00:45:27,920 --> 00:45:31,640
so i could try maybe i don't know and pointers to

735
00:45:33,210 --> 00:45:37,990
a couple steps ahead if i had a log and pointers i could do all

736
00:45:37,990 --> 00:45:41,950
powers of two and that's a pretty good search structures people is that

737
00:45:43,710 --> 00:45:46,180
like some peer to peer networks is that idea

738
00:45:46,200 --> 00:45:53,250
but that's a little too fancy for me

739
00:45:53,300 --> 00:45:56,770
you could try to build the tree on this linear

740
00:45:56,840 --> 00:45:59,520
structure that's essentially where we're going

741
00:45:59,540 --> 00:46:00,830
you could try to

742
00:46:00,880 --> 00:46:04,890
put pointers to maybe like the middle of the list from the root so you

743
00:46:04,900 --> 00:46:07,000
search between either here

744
00:46:07,160 --> 00:46:10,780
you point to the median so you can compare against the median with the sugar

745
00:46:10,790 --> 00:46:13,290
in the first half the second half that's

746
00:46:13,330 --> 00:46:18,900
definitely on the right track also a bit too sophisticated

747
00:46:18,920 --> 00:46:21,700
another list yes

748
00:46:22,400 --> 00:46:28,920
good we're is to listen to that sort of the next simplest thing you could

749
00:46:29,860 --> 00:46:36,820
and as you suggested we could maybe have pointers between them so maybe we have

750
00:46:37,460 --> 00:46:42,040
elements down here are some of the elements up here we want to have a

751
00:46:42,050 --> 00:46:49,180
point between the lists and gets a little bit crazy and how exactly

752
00:46:49,230 --> 00:46:52,380
we might do that but somehow this feels good so this is one like with

753
00:46:52,650 --> 00:46:57,290
one this is another linked list l two and to give you some inspiration

754
00:47:04,050 --> 00:47:05,920
i want to give you

755
00:47:05,970 --> 00:47:08,190
so this play and

756
00:47:08,240 --> 00:47:12,230
the game is what is the secret

757
00:47:12,240 --> 00:47:18,550
so causes fourteen if you know the answer

758
00:47:30,710 --> 00:47:32,500
we one you

759
00:47:40,040 --> 00:48:02,140
have a small class so i hope someone knows the answer

760
00:48:02,160 --> 00:48:03,630
two years earlier

761
00:48:03,640 --> 00:48:06,730
just come

762
00:48:06,730 --> 00:48:12,940
and the joint distribution of all of the hidden variables on all the observed variables

763
00:48:13,060 --> 00:48:15,980
can be written as the probability of the first

764
00:48:17,650 --> 00:48:22,290
valuable times probability first observed variable given the first variable

765
00:48:23,520 --> 00:48:25,350
the probability of

766
00:48:26,330 --> 00:48:28,440
invaluable given its

767
00:48:30,100 --> 00:48:31,600
they are able

768
00:48:31,650 --> 00:48:35,650
and the probability of each observed variable given the corresponding

769
00:48:35,670 --> 00:48:36,920
in the

770
00:48:36,980 --> 00:48:38,790
so this graph

771
00:48:38,790 --> 00:48:44,850
corresponds to this factorizations which is true for hmms and state space models

772
00:48:44,860 --> 00:48:49,150
the only difference between HMM is in state space models is that nature and the

773
00:48:49,150 --> 00:48:54,500
states are assumed to be discrete whereas in linear gaps in state space models the

774
00:48:54,500 --> 00:48:58,830
states are assumed to be real valued calcium vectors

775
00:49:00,400 --> 00:49:01,290
and now

776
00:49:01,290 --> 00:49:03,650
the interesting thing is that

777
00:49:04,600 --> 00:49:06,350
the belief propagation

778
00:49:06,360 --> 00:49:12,170
and factor graph propagation algorithms that just briefly describe are actually

779
00:49:12,210 --> 00:49:15,360
generalizations of

780
00:49:16,650 --> 00:49:21,850
the forward backward algorithm which is an algorithm for doing inference in hidden markov models

781
00:49:21,880 --> 00:49:26,880
and the common smoothing algorithm which is an algorithm for doing inference in state space

782
00:49:32,400 --> 00:49:35,420
when i was a grad student i took full

783
00:49:35,480 --> 00:49:36,500
the term

784
00:49:36,520 --> 00:49:40,310
the term long course uncommon filtering

785
00:49:40,330 --> 00:49:44,250
we spent the entire term on common filtering and i was really bored at the

786
00:49:44,250 --> 00:49:48,790
end of it now i know why i was so bored that was because

787
00:49:48,830 --> 00:49:51,670
you could learn about belief propagation

788
00:49:51,670 --> 00:49:54,670
and then common filtering is the special case

789
00:49:54,670 --> 00:49:58,540
they didn't so know it at the time called filtering is special case you can

790
00:49:58,540 --> 00:49:59,670
describe it in

791
00:49:59,790 --> 00:50:03,460
a couple of slides as being a special case and then you can say oh

792
00:50:03,480 --> 00:50:08,170
oh by the way belief propagation can be applied to hidden markov models and all

793
00:50:08,170 --> 00:50:10,130
these other graphs as well

794
00:50:13,020 --> 00:50:17,750
that's just an aside now what do you do with monthly connected

795
00:50:17,900 --> 00:50:19,630
the graphs

796
00:50:19,650 --> 00:50:23,750
so we talked about singly connected graphs but

797
00:50:23,790 --> 00:50:29,580
the real world isn't always singly connected and so we have a few options

798
00:50:29,580 --> 00:50:34,790
so there's an exact algorithm called the junction tree algorithm which i will describe but

799
00:50:34,790 --> 00:50:39,980
i have in the appendix here i have actually the description of the algorithm

800
00:50:40,100 --> 00:50:44,980
in subsequent slides the junction tree algorithm takes

801
00:50:45,000 --> 00:50:50,080
your multiple connected graph logs a bunch of variables together

802
00:50:50,130 --> 00:50:53,850
until it can form a singly connected graph

803
00:50:53,880 --> 00:51:00,130
on the super variables these several variables are variables that you take by grouping together

804
00:51:00,130 --> 00:51:03,920
a bunch of variables in your original graph

805
00:51:03,960 --> 00:51:07,460
and then it runs basically belief propagation on the supergraph

806
00:51:08,630 --> 00:51:10,940
which is called the junction tree

807
00:51:10,980 --> 00:51:14,880
now the problem with this algorithm is that it's an exact algorithm it will give

808
00:51:16,060 --> 00:51:19,580
exactly the probability want but

809
00:51:19,600 --> 00:51:23,350
if you have multiple connected graph you have no guarantee that you're

810
00:51:23,400 --> 00:51:29,190
supernodes that you form are going to have a small number of states so for

811
00:51:30,020 --> 00:51:32,670
if you have a bunch of binary variables

812
00:51:32,710 --> 00:51:34,480
a new lian

813
00:51:34,500 --> 00:51:40,810
twenty binary variables together if you force along twenty binary variables together then that supernode

814
00:51:40,810 --> 00:51:43,790
can take on a million different states

815
00:51:43,790 --> 00:51:48,650
and you have to actually enumerate all of those states to run this algorithm

816
00:51:48,690 --> 00:51:53,960
so the junction tree algorithm is exact but it can be exponentially slow

817
00:51:54,000 --> 00:51:57,580
and it turns out there's no way of doing

818
00:51:58,060 --> 00:52:00,730
inference in a general graph

819
00:52:00,770 --> 00:52:02,900
where you're guaranteed always two

820
00:52:02,920 --> 00:52:07,230
p polynomial time in the number of nodes because it's not possible

821
00:52:07,270 --> 00:52:14,080
a general graph can involve a full conditional probability full probability table

822
00:52:14,120 --> 00:52:18,750
involving exponentially many states so it's not possible to do so

823
00:52:18,770 --> 00:52:21,650
that's what you get for the junction tree algorithm

824
00:52:21,650 --> 00:52:27,420
another thing you can do is called cutset conditioning

825
00:52:29,000 --> 00:52:34,130
essentially you take your multiply connected graph and you take some variables and you say

826
00:52:34,630 --> 00:52:36,000
what if

827
00:52:36,060 --> 00:52:38,750
i said this variable to this value

828
00:52:38,790 --> 00:52:40,100
and then you run

829
00:52:40,170 --> 00:52:43,230
we propagation on the rest of the graph and then you say what if i

830
00:52:43,230 --> 00:52:46,500
said it to this other value et cetera so you

831
00:52:46,580 --> 00:52:53,330
sometimes called reasoning by assumptions because you assume different possible states for viability having observed

832
00:52:53,380 --> 00:52:55,920
and then u

833
00:52:55,960 --> 00:52:56,960
then you

834
00:52:56,980 --> 00:53:02,310
combined together the probabilities that you got from these different settings

835
00:53:02,360 --> 00:53:07,080
now you can only do this if you find a set of variables that renders

836
00:53:07,080 --> 00:53:08,860
the rest of the graph

837
00:53:08,900 --> 00:53:10,460
to be singly connected

838
00:53:10,480 --> 00:53:11,860
so they need to

839
00:53:11,880 --> 00:53:15,060
take graph and surgically

840
00:53:15,080 --> 00:53:19,830
remove a bunch of notes until you get the rest to be singly connected

841
00:53:19,880 --> 00:53:22,960
so this doesn't always work

842
00:53:23,130 --> 00:53:24,940
very efficiently either

843
00:53:25,000 --> 00:53:27,000
and this is exact as well

844
00:53:27,270 --> 00:53:32,440
and the third thing you can do is you can take your mostly connected graph

845
00:53:32,620 --> 00:53:38,040
and you can simply run belief propagation on that'll fully connected graph they can take

846
00:53:38,040 --> 00:53:39,790
let's lecture

847
00:53:41,460 --> 00:53:44,000
first of all

848
00:53:44,090 --> 00:53:48,720
i am i put a little bit of a reference list

849
00:53:48,740 --> 00:53:50,420
on a web page

850
00:53:50,440 --> 00:53:51,950
you know i'm not alone

851
00:53:52,000 --> 00:53:53,370
just basic reading

852
00:53:53,410 --> 00:53:54,590
for you

853
00:53:54,830 --> 00:53:57,550
to to be at this point the war

854
00:53:57,560 --> 00:53:59,720
understandably more of the

855
00:53:59,740 --> 00:54:01,340
so you can

856
00:54:01,350 --> 00:54:02,850
from my web page

857
00:54:02,870 --> 00:54:04,060
you can go to the

858
00:54:04,080 --> 00:54:07,690
and and the

859
00:54:07,710 --> 00:54:09,550
two thousand seven link

860
00:54:09,560 --> 00:54:13,890
and then you find it's actually link to another wikipedia article

861
00:54:13,990 --> 00:54:18,070
my mission is to find mister right

862
00:54:19,530 --> 00:54:22,360
and to me

863
00:54:22,370 --> 00:54:26,220
the message you need some if you're curious about them

864
00:54:26,270 --> 00:54:27,560
or not

865
00:54:27,580 --> 00:54:29,970
if you want more of that

866
00:54:31,070 --> 00:54:33,660
then a little

867
00:54:33,670 --> 00:54:36,270
apologies there was a little bit in the

868
00:54:36,280 --> 00:54:37,440
number denoted

869
00:54:38,280 --> 00:54:40,310
you always very careful

870
00:54:40,400 --> 00:54:47,560
somebody noticed that it was a little mistake in the reduction from nonseparable powerful analysis

871
00:54:47,580 --> 00:54:51,300
so you just go back and fix

872
00:54:51,320 --> 00:54:52,320
the way you

873
00:54:52,890 --> 00:54:54,840
wait you're primary

874
00:54:54,860 --> 00:54:56,780
is constructed

875
00:54:56,790 --> 00:55:00,150
and this is constructed by an operator and the whole thing is

876
00:55:00,280 --> 00:55:02,460
but because fixed but

877
00:55:02,480 --> 00:55:07,020
course makes the difference analysis where you have to add that the labels

878
00:55:07,040 --> 00:55:08,720
multiply the label

879
00:55:25,870 --> 00:55:28,960
being held

880
00:55:29,070 --> 00:55:33,890
the labels needed because

881
00:55:33,910 --> 00:55:38,100
we need to cancel the other labels the counts the same level the camps when

882
00:55:38,100 --> 00:55:40,390
your computer martin for you right

883
00:55:40,510 --> 00:55:43,360
if you go there and see that there is a little

884
00:55:43,390 --> 00:55:47,260
we state that in the competition the market because level of

885
00:55:47,270 --> 00:55:48,620
but then when

886
00:55:48,630 --> 00:55:50,620
going compute the normal view

887
00:55:50,640 --> 00:55:52,740
the square is normal

888
00:55:52,810 --> 00:55:56,490
the label has been go away

889
00:55:57,870 --> 00:55:59,980
what about that big

890
00:56:00,070 --> 00:56:03,600
now i would like to to

891
00:56:03,620 --> 00:56:09,960
finish up the argument i thought that the last part of the class

892
00:56:10,750 --> 00:56:12,410
in the

893
00:56:12,450 --> 00:56:19,610
conclude that this analysis is the quality of the ones

894
00:56:19,630 --> 00:56:22,200
so let me recap quickly

895
00:56:22,220 --> 00:56:26,150
so we had this argument in one

896
00:56:26,170 --> 00:56:28,610
which was

897
00:56:28,670 --> 00:56:31,450
we find the the weight of the part the

898
00:56:31,490 --> 00:56:33,330
i mean

899
00:56:33,350 --> 00:56:35,300
the value of

900
00:56:35,320 --> 00:56:39,780
the weight

901
00:56:39,830 --> 00:56:42,880
we had lost the

902
00:56:44,360 --> 00:56:48,290
where i come from that that's why

903
00:56:51,300 --> 00:56:52,390
x the

904
00:56:52,410 --> 00:56:55,180
beginning one minus

905
00:56:55,700 --> 00:56:59,220
and xt because

906
00:57:01,770 --> 00:57:05,740
and these are fixed gone because there

907
00:57:05,760 --> 00:57:08,410
that is kept constant broadly

908
00:57:08,520 --> 00:57:11,240
this is the policy is but i

909
00:57:11,250 --> 00:57:13,960
so we saw that solving these

910
00:57:14,010 --> 00:57:16,060
you end up with an update

911
00:57:16,110 --> 00:57:20,600
with the usual form of the up to WP but WP my one like it

912
00:57:20,600 --> 00:57:21,700
that he

913
00:57:21,720 --> 00:57:24,530
why the next meal with simple

914
00:57:24,700 --> 00:57:26,450
and the

915
00:57:26,500 --> 00:57:32,330
in this case it takes the form because of the box constraints arising

916
00:57:32,350 --> 00:57:34,430
from the use of the

917
00:57:35,330 --> 00:57:38,450
like variables

918
00:57:38,470 --> 00:57:39,620
let's say

919
00:57:39,630 --> 00:57:41,060
and then l

920
00:57:41,070 --> 00:57:42,930
the of that mind

921
00:57:42,930 --> 00:57:46,730
well i

922
00:57:46,750 --> 00:57:51,640
call it what you will see this is one is one

923
00:57:51,640 --> 00:57:54,960
she changes on its one

924
00:57:54,970 --> 00:57:59,370
so this many many of these

925
00:58:02,580 --> 00:58:13,640
right to decide what to do this so the french for this one where

926
00:58:14,030 --> 00:58:20,450
the prince one of for those of know that

927
00:58:20,470 --> 00:58:23,480
and lines around

928
00:58:23,550 --> 00:58:26,610
so it's very easy for

929
00:58:27,690 --> 00:58:31,900
three move in the sun

930
00:58:31,900 --> 00:58:36,720
it has its own rules as well as this very

931
00:58:36,930 --> 00:58:38,570
it's force

932
00:58:38,590 --> 00:58:41,620
the school chain

933
00:58:41,620 --> 00:58:49,120
so what do we have to reason

934
00:58:49,230 --> 00:58:52,430
i think we can get

935
00:58:52,440 --> 00:58:56,530
these or what

936
00:58:56,550 --> 00:58:59,610
all cast

937
00:58:59,610 --> 00:59:07,490
we have shown by a number of things cells

938
00:59:07,510 --> 00:59:12,660
when user group is itself is not world

939
00:59:13,960 --> 00:59:17,620
intersection is one thousand

940
00:59:18,000 --> 00:59:22,820
one of these cells the too

941
00:59:22,840 --> 00:59:31,230
and the number is the number it is zero

942
00:59:31,240 --> 00:59:34,030
so this is the largest

943
00:59:34,050 --> 00:59:36,030
so so this this

944
00:59:38,050 --> 00:59:40,280
we use it through

945
00:59:42,380 --> 00:59:45,260
so that can was the one

946
00:59:45,280 --> 00:59:49,860
in the music was

947
00:59:53,360 --> 00:59:55,240
as these things

948
00:59:55,240 --> 00:59:58,470
this is one of the first set

949
00:59:58,490 --> 01:00:00,610
e two

950
01:00:00,670 --> 01:00:03,610
the intersection of two surfaces

951
01:00:03,630 --> 01:00:06,610
in some places so this

952
01:00:06,610 --> 01:00:08,610
this is

953
01:00:08,650 --> 01:00:14,150
this section of the four hundred but this is too

954
01:00:14,340 --> 01:00:18,280
so in

955
01:00:18,300 --> 01:00:21,150
so each is

956
01:00:21,170 --> 01:00:25,070
the reason this no

957
01:00:25,090 --> 01:00:29,960
there are no in the nineteen sixty three

958
01:00:30,960 --> 01:00:32,800
until they

959
01:00:32,820 --> 01:00:34,800
these but we you

960
01:00:34,800 --> 01:00:37,550
the number of sections

961
01:00:38,530 --> 01:00:43,590
one hundred section is something like

962
01:00:43,740 --> 01:00:46,070
some of these

963
01:00:48,710 --> 01:00:52,740
so the

964
01:00:52,760 --> 01:00:56,460
it's so

965
01:00:58,940 --> 01:01:02,280
right at the degree of

966
01:01:02,300 --> 01:01:05,990
we can there that was a number

967
01:01:05,990 --> 01:01:10,380
so solutions such that the

968
01:01:15,610 --> 01:01:19,070
for i two d

969
01:01:29,650 --> 01:01:43,570
that's probably because you have to use as well as the results of what we

970
01:01:44,420 --> 01:01:54,340
three revert also in

971
01:01:54,340 --> 01:01:59,740
the potatoes are a little bit salted and also stopped biting and the sold will

972
01:01:59,740 --> 01:02:03,290
sink a N I can imagine we don't know what they are and will be

973
01:02:03,520 --> 01:02:10,910
maybe he was restaurants in was that by the way already unfortunately already for 2

974
01:02:10,910 --> 01:02:13,360
years but the habit states

975
01:02:13,880 --> 01:02:17,730
and here we have committed on and cross

976
01:02:18,360 --> 01:02:24,730
which of creating cap which are already using supporters in order to extract all kinds

977
01:02:24,730 --> 01:02:30,190
of weather warnings and cut fearless but they can get to and this is the

978
01:02:30,260 --> 01:02:34,360
and the hour aspect of this behavior that the loan not all of this behavior

979
01:02:34,360 --> 01:02:39,580
is like some hospital alone and that it when they don't have the right to

980
01:02:39,580 --> 01:02:46,580
work if they are given for example wires straight wireless by the experiment and with

981
01:02:46,580 --> 01:02:51,410
the strike where they can't get into a big band that they make them into

982
01:02:51,470 --> 01:02:59,560
very clever and here we have some very ahead inheritance in humans and a little

983
01:02:59,560 --> 01:03:05,360
bit of variation on the theme and the ability to get over changes the song

984
01:03:05,410 --> 01:03:08,530
and we don't know what the fate of the current will be probably not very

985
01:03:08,530 --> 01:03:11,300
good but some bargains to survive

986
01:03:12,530 --> 01:03:20,030
and that already said bore culture that we know so well this isn't the way

987
01:03:20,030 --> 01:03:22,390
we can think about things

988
01:03:22,410 --> 01:03:28,700
and this is the world in which we move this hour symbolic landscape something that

989
01:03:28,700 --> 01:03:34,700
is so fundamental to us which is into which is saying which is part of

990
01:03:34,700 --> 01:03:37,180
everything in fact we do and are

991
01:03:37,260 --> 01:03:43,110
so is carrying find what did you do

992
01:03:43,160 --> 01:03:47,680
why do we need this perspective on the pollution and inheritance what why is important

993
01:03:47,680 --> 01:03:54,140
for us book is on all kinds of ways of transmitted the probation fined salt

994
01:03:54,140 --> 01:03:58,730
so let's think so we have to give an account of why a quiet wife

995
01:03:58,730 --> 01:04:03,280
of interest so far as to call it an important was a study of hereditary

996
01:04:03,550 --> 01:04:09,470
if we're saying hereditary condition and population with no longer have to wait we can't

997
01:04:09,470 --> 01:04:13,930
just assume that differences in jeans that make the difference may be at the that

998
01:04:13,930 --> 01:04:17,970
the genetic differences that make the difference that will want to do something about it

999
01:04:18,050 --> 01:04:22,110
would better knows something about otherwise

1000
01:04:22,160 --> 01:04:25,140
we should be misled so

1001
01:04:25,200 --> 01:04:29,740
but it's also important because you see we we have to realize that we have

1002
01:04:29,740 --> 01:04:35,700
clonal plants a lot of invasive plants are either all day because they start from

1003
01:04:35,700 --> 01:04:40,890
very very few individuals no more or less the same genetically identical genetically so when

1004
01:04:40,890 --> 01:04:44,320
we think of pig the 1 we have real should because it's up your lines

1005
01:04:44,320 --> 01:04:48,840
that genetically identical but this is not the case if at the inheritance appear as

1006
01:04:49,070 --> 01:04:54,660
if the environment continues at the genetic variation than the candidate selection into lines and

1007
01:04:54,660 --> 01:05:03,390
would better now that this is the agricultural ecological last week as well as the

1008
01:05:03,390 --> 01:05:07,640
hereditary and it makes a difference to the way that we think about the promotion

1009
01:05:08,010 --> 01:05:10,970
and I hope I have some kind of

1010
01:05:11,240 --> 01:05:16,320
and it makes a difference to the way we think about medicine problem

1011
01:05:18,720 --> 01:05:22,600
the minister

1012
01:05:22,720 --> 01:05:28,780
sell half what can we say so I said 1st 1st already with the example

1013
01:05:28,780 --> 01:05:34,360
that they gave you the exam with be prior we saw that there can be

1014
01:05:34,360 --> 01:05:38,280
selection and evolution on that genetic access

1015
01:05:38,880 --> 01:05:44,140
this this is he's were identical genetically there were different only whether or not they

1016
01:05:44,140 --> 01:05:51,990
have a prime and the frequency of crime containing and no prime containing so changed

1017
01:05:52,050 --> 01:05:58,220
as a result of selection so this is 1 thing that many other aspects are

1018
01:05:58,260 --> 01:06:00,930
really concentrate only on those of foreign like

1019
01:06:02,240 --> 01:06:08,920
so here we have Arabidopsis and we will see are different manipulation happen among 96

1020
01:06:08,920 --> 01:06:15,510
of Arabidopsis occupied what we think of the occupants here and it's like right in

1021
01:06:15,530 --> 01:06:21,380
this this year of the genes that we're looking and what we see is that

1022
01:06:21,380 --> 01:06:26,910
when we're looking at the population of Arabidopsis plans we see a lot of oration

1023
01:06:26,910 --> 01:06:32,220
indifferent ecological conditions now we don't know in this particular study whether this at the

1024
01:06:32,220 --> 01:06:38,910
genetic variations are inherited but we know from other studies that about 50 cent between

1025
01:06:38,930 --> 01:06:43,880
about 50 % immoral as head the plants at the genetic variations are inherited for

1026
01:06:43,890 --> 01:06:47,510
something like a generations

1027
01:06:47,530 --> 01:06:49,260
so if this is the case

1028
01:06:49,260 --> 01:06:52,070
group theory and field theory and so forth

1029
01:06:52,780 --> 01:06:55,930
it's an interesting there are all sorts of other great properties

1030
01:06:55,990 --> 01:06:57,830
of this kind of

1031
01:06:57,850 --> 01:06:59,370
this kind of man

1032
01:06:59,410 --> 01:07:02,780
the main thing and how this turns out not to be true if m is

1033
01:07:02,780 --> 01:07:04,950
not prime

1034
01:07:05,010 --> 01:07:10,220
so can somebody think of let's imagine we're doing something model ten

1035
01:07:10,320 --> 01:07:14,660
somebody think number that doesn't have an inverse mod ten

1036
01:07:16,570 --> 01:07:18,010
another one is

1037
01:07:19,820 --> 01:07:23,010
OK turns out the divisors

1038
01:07:23,030 --> 01:07:24,030
OK in

1039
01:07:24,050 --> 01:07:27,140
in fact actually

1040
01:07:27,200 --> 01:07:28,410
more generally

1041
01:07:28,410 --> 01:07:32,640
something that is not relatively prime

1042
01:07:32,660 --> 01:07:38,140
OK meaning that is no common factors that UCD is not one between that number

1043
01:07:38,140 --> 01:07:40,300
and the modulus

1044
01:07:40,300 --> 01:07:42,780
OK those numbers do not have

1045
01:07:42,820 --> 01:07:44,450
in inverse

1046
01:07:45,820 --> 01:07:49,890
but it's prime every number is relatively prime

1047
01:07:50,950 --> 01:07:54,990
to the modules and that's the property that we're taking advantage

1048
01:07:55,010 --> 01:07:57,390
so this is our fact

1049
01:07:58,390 --> 01:08:00,970
and so in this case what i'm after

1050
01:08:01,780 --> 01:08:03,640
i want to divide

1051
01:08:03,640 --> 01:08:06,820
phi x zero minus y zero

1052
01:08:06,870 --> 01:08:09,280
that's what i want to do it this point

1053
01:08:09,370 --> 01:08:11,280
OK but i can't do that

1054
01:08:11,280 --> 01:08:12,680
if x is zero

1055
01:08:12,680 --> 01:08:16,120
first of all if the amazon prime

1056
01:08:16,180 --> 01:08:20,510
OK i can't necessarily do that i might be able to but can necessarily

1057
01:08:20,570 --> 01:08:25,180
but i m is prime i can definitely divide by ax reminds wiser can find

1058
01:08:25,180 --> 01:08:26,300
that inverse

1059
01:08:26,300 --> 01:08:29,200
and the other thing i have to do is make sure zero minds wiser was

1060
01:08:32,680 --> 01:08:36,450
they would be zero if these two are equal but our supposition was their equal

1061
01:08:36,470 --> 01:08:37,590
and once again

1062
01:08:37,620 --> 01:08:42,370
just bring it back to the without loss of generality if it were some other

1063
01:08:42,490 --> 01:08:45,550
a position that we were off

1064
01:08:45,590 --> 01:08:49,510
i really doing exactly the same thing with that position

1065
01:08:51,720 --> 01:08:58,970
OK so now we're going to be able to divide so we continue with our

1066
01:08:58,990 --> 01:09:04,870
continue with our proof

1067
01:09:07,490 --> 01:09:08,950
since x is zero

1068
01:09:08,950 --> 01:09:11,160
it is not equal to y zero

1069
01:09:11,160 --> 01:09:13,850
there exists an inverse

1070
01:09:13,890 --> 01:09:17,390
director minus wiser

1071
01:09:18,930 --> 01:09:22,320
and that implies just continue on from over there

1072
01:09:22,350 --> 01:09:24,620
it is zero

1073
01:09:24,680 --> 01:09:27,470
is congruent there for two minus

1074
01:09:27,490 --> 01:09:30,430
some of i want to or

1075
01:09:30,470 --> 01:09:32,620
of AI

1076
01:09:32,640 --> 01:09:33,530
x y

1077
01:09:33,530 --> 01:09:36,410
minus y i

1078
01:09:37,760 --> 01:09:40,120
x zero minus y zero

1079
01:09:40,140 --> 01:09:44,990
so what

1080
01:09:45,030 --> 01:09:46,240
so let's just

1081
01:09:46,240 --> 01:09:50,300
go back to the beginning of our proof and see what we've derived

1082
01:09:50,350 --> 01:09:53,220
we're saying we have two distinct keys

1083
01:09:54,220 --> 01:09:55,300
we picked

1084
01:09:55,300 --> 01:09:57,030
all of these a i

1085
01:09:59,430 --> 01:10:05,550
OK and we're saying that these two distinct he's hash to the same place

1086
01:10:05,590 --> 01:10:07,830
if there has to the same place

1087
01:10:07,910 --> 01:10:10,620
it says that

1088
01:10:10,640 --> 01:10:13,910
o zero

1089
01:10:15,200 --> 01:10:17,850
i had to have a particular value

1090
01:10:17,870 --> 01:10:23,510
as a function of the other AI

1091
01:10:23,550 --> 01:10:26,180
because in others one side picked

1092
01:10:26,180 --> 01:10:29,590
each of these a i from one to o are them in that order for

1093
01:10:31,450 --> 01:10:34,300
that i don't have a choice for how i pick a zero to make it

1094
01:10:35,180 --> 01:10:40,320
exactly one value allows it to collide namely the value of a zero given by

1095
01:10:41,700 --> 01:10:44,220
by fifty different value of a zero

1096
01:10:44,240 --> 01:10:47,870
they wouldn't collide

1097
01:10:47,970 --> 01:10:51,410
OK let me write that down thus

1098
01:10:51,450 --> 01:10:53,760
why you think about it

1099
01:11:41,870 --> 01:11:50,640
so for each choice

1100
01:11:50,640 --> 01:11:54,430
of these i was exactly one

1101
01:11:54,450 --> 01:12:01,280
of the n possible choices of a zero that cause a collision

1102
01:12:01,370 --> 01:12:09,430
and for all the other choices i might make of a zero there's no collision

1103
01:12:09,890 --> 01:12:14,320
so it's essentially i don't have if they're going to collide

1104
01:12:14,330 --> 01:12:19,990
i've reduced essentially the number of degrees of freedom of my randomness

1105
01:12:21,490 --> 01:12:27,140
by a factor of m so far count up the number of a AJ's

1106
01:12:31,030 --> 01:12:32,950
x and y

1107
01:12:34,930 --> 01:12:39,820
that's equal to

1108
01:12:39,850 --> 01:12:45,660
others m choices just using the product rule again m choices for a one

1109
01:12:45,700 --> 01:12:49,660
times m choices for a two

1110
01:12:49,700 --> 01:12:54,070
after m choices for a are

1111
01:12:54,120 --> 01:12:56,430
and they're only one choice

1112
01:12:56,470 --> 01:12:58,660
for a zero

1113
01:12:58,780 --> 01:13:02,320
this is

1114
01:13:02,370 --> 01:13:03,530
choices for

1115
01:13:03,550 --> 01:13:05,300
eighty one

1116
01:13:07,030 --> 01:13:09,050
they are

1117
01:13:09,100 --> 01:13:13,030
only one choice for a zero if they're going to collide not going to collide

1118
01:13:13,030 --> 01:13:17,470
it got more choices frazier but by one collide there's only one value i can

1119
01:13:19,660 --> 01:13:22,390
this value

1120
01:13:22,450 --> 01:13:26,470
OK that's the only value for which i can i will pick

1121
01:13:26,470 --> 01:13:27,690
in this sense

1122
01:13:27,710 --> 01:13:30,300
all training points we look at the whole training points

1123
01:13:30,310 --> 01:13:31,630
based on that

1124
01:13:31,650 --> 01:13:35,430
which was the basis functions function the basic things the they had had the lowest

1125
01:13:35,440 --> 01:13:36,680
error is

1126
01:13:36,700 --> 01:13:41,290
so this is the function that depends on all the x i y i

1127
01:13:42,540 --> 01:13:44,470
so this function

1128
01:13:44,640 --> 01:13:47,020
the function depending on x i y i

1129
01:13:47,120 --> 01:13:51,270
therefore even if x i y i independent

1130
01:13:51,320 --> 01:13:54,700
six one is the public school and so on

1131
01:13:55,380 --> 01:13:57,290
this function f depends on both

1132
01:13:57,300 --> 01:13:58,420
one x two

1133
01:13:58,430 --> 01:14:03,240
the discs i i swear will no longer be

1134
01:14:03,250 --> 01:14:05,080
so mathematically speaking

1135
01:14:05,230 --> 01:14:09,280
we are no longer situation where we have independently financed

1136
01:14:09,290 --> 01:14:13,200
so in such a situation where this function has seen all the training points we

1137
01:14:14,770 --> 01:14:19,110
its own people

1138
01:14:19,160 --> 01:14:20,950
so what do we do

1139
01:14:20,970 --> 01:14:22,560
so that's where the whole

1140
01:14:22,570 --> 01:14:27,120
complexity comes from one

1141
01:14:27,170 --> 01:14:30,530
so let's start with two observations

1142
01:14:30,540 --> 01:14:35,350
so let's say that before the minimizer of our we denoted by of

1143
01:14:35,360 --> 01:14:36,610
the of optimal the function

1144
01:14:36,760 --> 01:14:37,750
we knew

1145
01:14:37,830 --> 01:14:39,220
test error

1146
01:14:39,240 --> 01:14:43,370
the function of the distribution

1147
01:14:43,390 --> 01:14:47,860
you know minimizer is characterized by

1148
01:14:47,910 --> 01:14:50,390
in particular we know that

1149
01:14:50,410 --> 01:14:53,010
these two inequalities hold true

1150
01:14:53,020 --> 01:14:56,000
because of minimize this function are

1151
01:14:56,020 --> 01:14:57,440
so he has

1152
01:14:57,540 --> 01:14:59,770
is smaller than any other country

1153
01:14:59,970 --> 01:15:02,460
in particular

1154
01:15:02,470 --> 01:15:07,560
likewise if my this function are all the functional

1155
01:15:07,920 --> 01:15:09,890
the function of functions

1156
01:15:09,910 --> 01:15:14,510
and minimize this quantity therefore

1157
01:15:14,520 --> 01:15:17,980
this inequality holds

1158
01:15:18,610 --> 01:15:24,110
characteristic n is smaller than the characteristic any other function in particular

1159
01:15:24,160 --> 01:15:27,010
these two inequalities hold true

1160
01:15:27,050 --> 01:15:31,050
and in know a way what we would like for consistency

1161
01:15:31,060 --> 01:15:37,610
consistency is before statistical them and roughly speaking is that in the limit of infinity

1162
01:15:37,630 --> 01:15:39,530
many observations

1163
01:15:39,540 --> 01:15:45,490
we get the right results or limited in the observation patients period is

1164
01:15:47,650 --> 01:15:51,920
all of to by all principal crystallisation

1165
01:15:51,960 --> 01:15:54,830
will converge to is expected

1166
01:15:54,880 --> 01:15:56,930
the expected values

1167
01:15:56,940 --> 01:16:00,500
so for consistency we would like this kind of uniform convergence

1168
01:16:00,540 --> 01:16:06,650
and we would like the left side of the inequalities to converge to zero

1169
01:16:06,690 --> 01:16:08,160
so in a way

1170
01:16:08,180 --> 01:16:13,320
it was he has been many observations this if we change function chosen from the

1171
01:16:13,320 --> 01:16:15,340
training points

1172
01:16:15,390 --> 01:16:19,200
chosen by minimizing the training error will change and we would like to change that

1173
01:16:20,360 --> 01:16:21,490
it's risk

1174
01:16:21,510 --> 01:16:25,730
we get arbitrarily close to the optimal risk

1175
01:16:25,790 --> 01:16:28,900
we know that this is always allowed to do there

1176
01:16:28,980 --> 01:16:32,860
we would actually like to converge

1177
01:16:32,880 --> 01:16:37,510
so these quantities we would actually like both the zero

1178
01:16:37,610 --> 01:16:39,250
both negative

1179
01:16:39,270 --> 01:16:42,040
one way to show that they both conditions

1180
01:16:42,060 --> 01:16:46,090
show that some of them is zero

1181
01:16:46,570 --> 01:16:49,190
compute the sum of these two things

1182
01:16:49,340 --> 01:16:53,660
here we've got some

1183
01:16:55,000 --> 01:16:57,720
first of all i just rearranged in terms of

1184
01:16:57,970 --> 01:17:00,700
this were one over here

1185
01:17:00,710 --> 01:17:03,540
the i can of on this thing here

1186
01:17:03,600 --> 01:17:09,730
he was so we here would end up in the way of an f of

1187
01:17:11,030 --> 01:17:14,930
we have this thing over here as it is this thing here

1188
01:17:14,940 --> 01:17:16,350
we will bond

1189
01:17:16,360 --> 01:17:18,550
why the supremum

1190
01:17:18,640 --> 01:17:22,530
maximum with one speaking over the whole country class

1191
01:17:22,550 --> 01:17:24,660
of this quantity

1192
01:17:24,710 --> 01:17:26,370
OK so the supremum

1193
01:17:26,830 --> 01:17:32,810
born on whatever you it by substituting one specific function here some

1194
01:17:34,310 --> 01:17:36,440
so this is an upper bound

1195
01:17:38,400 --> 01:17:40,710
let's take a a look at the whole thing we would like this whole thing

1196
01:17:40,710 --> 01:17:42,130
to go to zero

1197
01:17:42,140 --> 01:17:44,320
first of all the second half here

1198
01:17:44,470 --> 01:17:49,010
f of is fixed function is we don't know this function

1199
01:17:49,060 --> 01:17:50,800
the function that minimizes the

1200
01:17:50,880 --> 01:17:52,380
the true risk

1201
01:17:52,400 --> 01:17:55,860
but independent of the training examples that we see

1202
01:17:55,870 --> 01:18:00,360
so from the point of view of language inequality we find

1203
01:18:00,400 --> 01:18:02,230
six function so we can just

1204
01:18:02,240 --> 01:18:07,260
by the time of the quality of the law of large numbers and this tells

1205
01:18:08,190 --> 01:18:09,640
in probability

1206
01:18:09,650 --> 01:18:16,760
this was converges towards this actually soliciting was this one will find this thing go

1207
01:18:16,760 --> 01:18:18,560
to zero

1208
01:18:18,610 --> 01:18:23,810
already question whether the whole some would go to zero sufficient condition for this to

1209
01:18:23,880 --> 01:18:27,660
work for the that this thing supposed to zero

1210
01:18:28,620 --> 01:18:29,950
the supremum over

1211
01:18:36,030 --> 01:18:39,120
i rephrased this year against the first half

1212
01:18:39,250 --> 01:18:43,420
district inside the zero for this is the case

1213
01:18:43,480 --> 01:18:47,380
then we would have these two inequalities

1214
01:18:47,440 --> 01:18:51,770
in this case statistically speaking one would say that the principle of empirical risk minimization

1215
01:18:51,780 --> 01:18:53,700
is consistent

1216
01:18:55,350 --> 01:18:57,250
so the question is now when does this thing

1217
01:19:01,240 --> 01:19:04,970
this is where uniform convergence can informally so the to

1218
01:19:06,280 --> 01:19:14,200
the necessary sufficient for a certain type of nontrivial consistency not much

1219
01:19:14,200 --> 01:19:15,250
he careful

1220
01:19:15,270 --> 01:19:17,470
on nontrivial consistency

1221
01:19:17,470 --> 01:19:21,540
it's actually sort of interesting issues you do theory

1222
01:19:21,570 --> 01:19:25,130
this is the definition of this is the

1223
01:19:25,180 --> 01:19:31,200
if we define crisis in this way they get the necessary and sufficient

1224
01:19:31,410 --> 01:19:34,240
but they tend to exhibit very keen on

1225
01:19:34,250 --> 01:19:38,720
but not everybody is defined consistency this way but let's leave that aside

1226
01:19:38,740 --> 01:19:40,110
so from

1227
01:19:40,130 --> 01:19:42,130
necessary sufficient causes disease

1228
01:19:42,140 --> 01:19:44,890
one side of uniform convergence

1229
01:19:45,730 --> 01:19:47,760
here is actually

1230
01:19:47,820 --> 01:19:49,280
so this is exactly

1231
01:19:49,290 --> 01:19:50,000
o thing

1232
01:19:50,110 --> 01:19:52,420
i had here

1233
01:19:54,000 --> 01:19:56,250
it's about turning proved that

1234
01:19:56,410 --> 01:20:00,330
this thing here is necessary and sufficient for consistency so

1235
01:20:00,340 --> 01:20:03,250
roughly speaking what i argued on the previous slides was

1236
01:20:03,360 --> 01:20:06,260
but if we have this convergence

1237
01:20:06,260 --> 01:20:09,550
that's you want to listen to

1238
01:20:11,520 --> 01:20:14,280
optimized from the picture

1239
01:20:14,520 --> 01:20:17,600
one of the problems

1240
01:20:17,760 --> 01:20:20,890
above all suspicion

1241
01:20:21,850 --> 01:20:28,310
i don't know about you actually

1242
01:20:30,640 --> 01:20:35,890
from all

1243
01:20:37,420 --> 01:20:40,800
there is only one peak is one of the think

1244
01:20:50,140 --> 01:20:52,160
so i

1245
01:20:53,440 --> 01:20:54,820
so what native

1246
01:20:54,930 --> 01:20:58,850
but going than that to trust the model to try to become law

1247
01:20:59,100 --> 01:21:00,530
but the

1248
01:21:00,770 --> 01:21:04,770
politician equal to the

1249
01:21:04,890 --> 01:21:07,940
joke or a checking

1250
01:21:07,940 --> 01:21:12,940
an introduction so just so index indexes what a problem with dense

1251
01:21:13,600 --> 01:21:15,760
well as much of

1252
01:21:15,770 --> 01:21:19,220
the vision but also some of the interesting

1253
01:21:20,810 --> 01:21:27,560
the parameters of the problem in water to this is what i want

1254
01:21:27,590 --> 01:21:31,320
by the way is

1255
01:21:37,600 --> 01:21:43,720
only parameters of the model which is what

1256
01:21:43,720 --> 01:21:46,590
OK put them on the input which

1257
01:21:46,640 --> 01:21:52,240
the the problem of the water sensor networks

1258
01:22:00,980 --> 01:22:07,570
one of the most

1259
01:22:22,230 --> 01:22:26,850
that is connected to the middle between zero

1260
01:22:27,140 --> 01:22:31,270
the first is the programs to promote

1261
01:22:31,270 --> 01:22:34,270
but then the model is that

1262
01:22:34,270 --> 01:22:36,560
because the problem

1263
01:22:36,840 --> 01:22:39,640
it was like that it's the largest opinion

1264
01:22:39,730 --> 01:22:41,810
so perhaps

1265
01:22:41,890 --> 01:22:45,930
one of the most of the novel out edge

1266
01:22:47,260 --> 01:22:51,030
my own research

1267
01:22:53,350 --> 01:22:56,310
the political blog discipline was trying parker

1268
01:22:57,010 --> 01:23:00,320
small signal and some action group

1269
01:23:00,340 --> 01:23:05,050
what is the is although we should just become

1270
01:23:05,070 --> 01:23:12,230
but then or on support text from this component in the course of the

1271
01:23:13,930 --> 01:23:15,630
only one

1272
01:23:15,680 --> 01:23:20,090
but what i would call

1273
01:23:25,360 --> 01:23:30,350
she is not

1274
01:23:30,420 --> 01:23:39,380
below the surface

1275
01:23:39,420 --> 01:23:42,440
what is it

1276
01:23:51,840 --> 01:23:57,560
i suppose it refers to to

1277
01:23:58,030 --> 01:24:00,970
and i have a little bit more

1278
01:24:01,050 --> 01:24:12,640
you want to know what are the ones

1279
01:24:12,680 --> 01:24:18,440
this is the largest

1280
01:24:18,490 --> 01:24:25,470
one of the larger

1281
01:24:25,480 --> 01:24:27,440
or the whale

1282
01:24:27,570 --> 01:24:30,470
so does

1283
01:24:30,890 --> 01:24:35,570
so sort

1284
01:24:35,880 --> 01:24:42,850
it doesn't what you think

1285
01:24:42,860 --> 01:24:46,360
what we want

1286
01:24:46,380 --> 01:24:53,980
also some of the largest manual solution because the sounds

1287
01:24:54,060 --> 01:24:57,260
in the seventies policy with them

1288
01:24:57,280 --> 01:24:58,390
four months

1289
01:24:58,420 --> 01:25:03,020
it's a small child the political will

1290
01:25:03,070 --> 01:25:10,640
it has been decided to the solve quantum mechanics properties

1291
01:25:12,150 --> 01:25:17,270
so problem that because we don't have possible

1292
01:25:17,280 --> 01:25:19,380
the past according to

1293
01:25:22,630 --> 01:25:28,840
what we're want

1294
01:25:28,840 --> 01:25:30,240
in this higher

1295
01:25:31,800 --> 01:25:34,840
can give you problems with so many

1296
01:25:34,840 --> 01:25:38,280
which i mentioned the so

1297
01:25:38,300 --> 01:25:42,610
i would like to give two computational problems

1298
01:25:43,740 --> 01:25:45,220
you might have

1299
01:25:48,490 --> 01:25:55,880
very efficiently compute the similarity between two objects in such high time

1300
01:26:03,320 --> 01:26:04,720
and in fact

1301
01:26:05,780 --> 01:26:08,610
we call this could

1302
01:26:08,630 --> 01:26:10,800
so good

1303
01:26:10,820 --> 01:26:13,300
it's actually

1304
01:26:13,340 --> 01:26:15,510
similarity computation

1305
01:26:15,570 --> 01:26:17,320
between the object

1306
01:26:18,650 --> 01:26:21,510
some features

1307
01:26:21,510 --> 01:26:23,760
so in talk

1308
01:26:23,800 --> 01:26:30,430
have rather between two objects we first the object to some high

1309
01:26:32,280 --> 01:26:35,720
and in that time she is we think

1310
01:26:38,260 --> 01:26:47,860
but it's good that can be

1311
01:26:50,990 --> 01:26:54,530
you can just use any similarity function

1312
01:26:54,970 --> 01:26:57,630
to compute the in problem

1313
01:26:57,650 --> 01:27:04,110
you could have to be subject to

1314
01:27:04,150 --> 01:27:05,240
it has to be

1315
01:27:06,610 --> 01:27:11,420
so that could be seen

1316
01:27:11,450 --> 01:27:15,240
could be fixed by it must be positive

1317
01:27:17,610 --> 01:27:19,150
the which

1318
01:27:19,990 --> 01:27:24,510
if you go to the web is called the gram matrix or kernel matrix

1319
01:27:24,550 --> 01:27:25,990
which is actually

1320
01:27:25,990 --> 01:27:28,090
the matrix that

1321
01:27:28,110 --> 01:27:29,920
given the similarity

1322
01:27:29,930 --> 01:27:32,220
between each

1323
01:27:32,260 --> 01:27:35,070
the training examples

1324
01:27:35,090 --> 01:27:40,990
that is the gram matrix is positive things that know not so you can make

1325
01:27:46,650 --> 01:27:49,200
what do we

1326
01:27:49,220 --> 01:27:53,130
in the

1327
01:27:53,150 --> 01:27:54,180
in the three

1328
01:27:54,240 --> 01:27:58,130
we can play a role here by

1329
01:27:59,930 --> 01:28:01,430
we have

1330
01:28:01,430 --> 01:28:03,630
to use and also

1331
01:28:03,630 --> 01:28:07,610
in the

1332
01:28:07,630 --> 01:28:12,130
the decision when you

1333
01:28:12,180 --> 01:28:14,200
we can play

1334
01:28:15,150 --> 01:28:18,260
well we can play in the world

1335
01:28:25,130 --> 01:28:26,920
now just

1336
01:28:26,930 --> 01:28:28,780
a few

1337
01:28:28,800 --> 01:28:33,050
support vector machine and then i come back to that could function

1338
01:28:33,070 --> 01:28:37,300
that we can use information extraction

1339
01:28:37,550 --> 01:28:40,700
support vector machines i

1340
01:28:42,650 --> 01:28:46,740
to be a little too many noisy features

1341
01:28:46,820 --> 01:28:50,860
so you do do not need appealing feature selection

1342
01:28:50,880 --> 01:28:53,400
but of course you might know

1343
01:28:53,400 --> 01:28:54,680
in the trade

1344
01:28:54,700 --> 01:28:58,110
when you have to optimize it should be one

1345
01:28:58,180 --> 01:29:00,380
presentation so

1346
01:29:00,400 --> 01:29:01,950
it could be used

1347
01:29:01,950 --> 01:29:06,450
to have feature selection

1348
01:29:06,570 --> 01:29:14,430
after the

1349
01:29:14,430 --> 01:29:16,400
i have i will

1350
01:29:17,860 --> 01:29:30,820
so the discriminant function the i'm not

1351
01:29:30,820 --> 01:29:33,530
only used in the

1352
01:29:33,550 --> 01:29:36,430
the support vector machine

1353
01:29:36,450 --> 01:29:40,920
they are also used in your neighborhood

1354
01:29:40,930 --> 01:29:43,550
well well well

1355
01:29:45,240 --> 01:29:46,280
your data

1356
01:29:46,300 --> 01:29:47,510
we in

1357
01:29:47,530 --> 01:29:53,860
these only memory based learning when you compare an object with all with the vector

1358
01:29:54,420 --> 01:30:00,570
with all the other objects you

1359
01:30:03,920 --> 01:30:08,360
i call people about group

1360
01:30:08,380 --> 01:30:10,300
that can be used

1361
01:30:10,320 --> 01:30:12,680
in information extraction

1362
01:30:12,680 --> 01:30:16,990
so people coconuts are

1363
01:30:17,030 --> 01:30:22,360
well for text classification are here as i said

1364
01:30:22,380 --> 01:30:25,090
ninety six categories

1365
01:30:26,340 --> 01:30:27,680
categories like

1366
01:30:27,700 --> 01:30:30,180
sports politics

1367
01:30:30,200 --> 01:30:35,280
you might also be useful in long island new york times

1368
01:30:35,300 --> 01:30:38,860
you could also exploit the structure

1369
01:30:38,860 --> 01:30:41,610
natural language and neutral

1370
01:30:42,820 --> 01:30:45,570
to my mind told some i will be more

1371
01:30:45,590 --> 01:30:47,090
about structure

1372
01:30:47,360 --> 01:30:49,300
in natural language

1373
01:30:49,300 --> 01:30:52,650
and the structure learning the structure

1374
01:30:52,680 --> 01:31:00,340
he also well because could not be used as similarity functions operate

1375
01:31:00,430 --> 01:31:04,930
actually i already proved by

1376
01:31:04,970 --> 01:31:11,360
so if we look at some kernel and granka convolution kernel

1377
01:31:11,380 --> 01:31:14,050
and also some trees which

1378
01:31:14,070 --> 01:31:18,740
an example of course you could not so

1379
01:31:18,760 --> 01:31:25,010
which can be used to structured data such as sequences

1380
01:31:30,260 --> 01:31:31,840
you might

1381
01:31:33,380 --> 01:31:36,950
similar to the art

1382
01:31:39,260 --> 01:31:44,180
you can think of sequence of compact

1383
01:31:46,570 --> 01:31:49,610
you might want to measure the similarity

1384
01:31:49,630 --> 01:31:52,930
as the number of common ground

1385
01:31:52,950 --> 01:31:58,650
where and is defined as a lot of energy can from the

1386
01:31:58,650 --> 01:32:03,960
and it contains a an integer parameter n which starts at one so one two

1387
01:32:03,960 --> 01:32:08,370
three and this is called the number of degrees of freedom in some sort of

1388
01:32:08,370 --> 01:32:11,070
analogy with the mechanical system i suppose

1389
01:32:11,080 --> 01:32:15,780
and it contains zero power law term z to the and over two minus one

1390
01:32:15,810 --> 01:32:20,430
an exponential term e to the minus the number two and that that complicated term

1391
01:32:20,430 --> 01:32:22,140
there just serves to

1392
01:32:22,190 --> 01:32:25,800
normalize the PDF to unit area

1393
01:32:25,820 --> 01:32:30,480
OK so the expectation value of the were distributed variables is equal to the number

1394
01:32:30,480 --> 01:32:31,950
of degrees of freedom

1395
01:32:31,980 --> 01:32:33,070
and the variance

1396
01:32:33,100 --> 01:32:35,710
is equal to two times the number of degrees of freedom

1397
01:32:35,750 --> 01:32:39,300
so why is it useful i just made this to this definition and quoted these

1398
01:32:39,300 --> 01:32:43,760
these properties but why is that useful it's useful because if you consider a gas

1399
01:32:43,800 --> 01:32:49,550
in distributed random variable or rather some set of independent calcium distributed variables x sub

1400
01:32:50,430 --> 01:32:52,400
with mean values URI

1401
01:32:52,430 --> 01:32:54,960
and variances sigma i square

1402
01:32:54,970 --> 01:32:57,040
now let me take the following combination

1403
01:32:57,070 --> 01:33:00,000
x i minus mu i squared

1404
01:33:00,040 --> 01:33:04,290
divided by sigma i squared and summed over all of my variables

1405
01:33:04,290 --> 01:33:07,310
so that's the kind of combination it actually comes up all the time because what

1406
01:33:07,310 --> 01:33:10,940
i'm doing here is i'm comparing XI to its mean value

1407
01:33:10,960 --> 01:33:13,560
dividing that by some measure of its with

1408
01:33:13,580 --> 01:33:17,240
squaring and summing it up so that's that's that's the type of thing that comes

1409
01:33:17,240 --> 01:33:18,460
up in connection

1410
01:33:18,540 --> 01:33:21,620
with the method of least squares for example

1411
01:33:21,680 --> 01:33:24,490
OK fine so i do that and then ask the question

1412
01:33:24,540 --> 01:33:26,510
how is that guy distributed

1413
01:33:26,550 --> 01:33:28,960
if i were to make a histogram of z

1414
01:33:28,960 --> 01:33:33,660
what would it look like the answer is it would follow consequent distribution

1415
01:33:33,720 --> 01:33:38,940
so so that's that's somehow the origin of the importance of the consequent distribution and

1416
01:33:38,940 --> 01:33:43,560
we'll revisit that when we talk about the method of least squares

1417
01:33:43,580 --> 01:33:47,150
kochi distribution i skip the landau distribution skip

1418
01:33:47,160 --> 01:33:49,100
OK so now i have

1419
01:33:49,180 --> 01:33:50,460
we started

1420
01:33:50,540 --> 01:33:53,520
five minutes late so go another twelve minutes

1421
01:33:53,520 --> 01:33:55,690
right this network

1422
01:33:55,900 --> 01:33:59,130
well so far we get we can always continue to the next

1423
01:33:59,190 --> 01:34:02,260
so let me talk now about the monte carlo method

1424
01:34:02,630 --> 01:34:06,390
i know a lot of you have probably been working with with monte carlo data

1425
01:34:06,390 --> 01:34:10,340
simulated data for the LHC experiments but i want to take a step back now

1426
01:34:10,340 --> 01:34:14,700
and just talk about the basic mathematics of this procedure

1427
01:34:14,700 --> 01:34:18,260
so at an abstract level with the monte carlo method is

1428
01:34:18,310 --> 01:34:21,060
it is a numerical technique for calculating

1429
01:34:21,100 --> 01:34:24,560
probabilities are things that can be related to probabilities

1430
01:34:24,600 --> 01:34:27,310
by using sequences of random numbers

1431
01:34:27,330 --> 01:34:31,310
and you can break this procedure down into us a certain number of steps the

1432
01:34:31,310 --> 01:34:36,100
usual steps would be first to generate a sequence or one r two

1433
01:34:36,120 --> 01:34:37,740
up to some are m

1434
01:34:37,760 --> 01:34:40,680
values that are uniformly distributed

1435
01:34:40,720 --> 01:34:44,740
between zero and one so remember we just discussed this uniform pdf

1436
01:34:44,780 --> 01:34:48,220
so a uniform distribution between zero and one

1437
01:34:49,850 --> 01:34:52,490
in the second step would be to use that sequence

1438
01:34:52,510 --> 01:34:53,390
to produce

1439
01:34:53,410 --> 01:34:55,180
a second sequence of numbers

1440
01:34:55,200 --> 01:34:56,450
x one x two

1441
01:34:56,470 --> 01:34:58,620
and so forth up to x and

1442
01:34:58,640 --> 01:35:00,780
the length of the second sequence may

1443
01:35:00,810 --> 01:35:04,410
or may not be the same as the length of the image initial sequence

1444
01:35:04,430 --> 01:35:08,120
but from the second sequence of numbers i well

1445
01:35:08,160 --> 01:35:09,370
it should follow

1446
01:35:09,370 --> 01:35:13,030
some other PDF f of x that i'm interested in

1447
01:35:13,100 --> 01:35:17,280
so i have some other PDF f of x that i'm interested in exploring its

1448
01:35:17,280 --> 01:35:21,830
properties and so i use that first sequence of random numbers to produce

1449
01:35:21,850 --> 01:35:25,580
the second sequence of random numbers such that it follows

1450
01:35:25,620 --> 01:35:27,010
f of x

1451
01:35:27,030 --> 01:35:31,810
and once i have that second sequence of random numbers i can use it the

1452
01:35:32,080 --> 01:35:36,370
the x values to estimate some property of f of f of x so for

1453
01:35:37,560 --> 01:35:42,040
if i simply look at the sequence of x values and count the fraction of

1454
01:35:42,040 --> 01:35:45,870
them the line between any two limits say between a and b

1455
01:35:45,950 --> 01:35:50,990
what is that that's the probability to find x between a and b so therefore

1456
01:35:50,990 --> 01:35:53,620
that's the integral from a to b

1457
01:35:53,640 --> 01:35:55,410
of the five xtx

1458
01:35:55,450 --> 01:36:00,350
so by doing by simply counting the fraction of x values that that lie between

1459
01:36:00,350 --> 01:36:05,370
given limits have effectively done an integral and monte carlo calculation is at least always

1460
01:36:05,370 --> 01:36:08,850
formally equivalent to doing an integral

1461
01:36:08,910 --> 01:36:11,540
we don't have to actually think of it that way and very often we can

1462
01:36:11,540 --> 01:36:16,100
think of these x values as simply simulated data and then we can treat them

1463
01:36:16,100 --> 01:36:19,450
as if they were real data and we can use them to then test our

1464
01:36:20,400 --> 01:36:25,450
statistical procedures into design the detector and to see what type of outcome you would

1465
01:36:25,450 --> 01:36:26,680
you would obtain

1466
01:36:26,740 --> 01:36:33,430
assuming a certain hypotheses for the underlying probability densities

1467
01:36:33,450 --> 01:36:36,620
so let me now look at each of those three steps in a little bit

1468
01:36:36,620 --> 01:36:38,220
more detail

1469
01:36:38,370 --> 01:36:43,060
so first we need to do is to generate a sequence of random variables values

1470
01:36:43,060 --> 01:36:45,810
uniformly distributed between zero and one

1471
01:36:47,220 --> 01:36:52,180
in this sort of early days of monte carlo which somehow originated with the atomic

1472
01:36:52,180 --> 01:36:55,560
bomb project during the war

1473
01:36:55,560 --> 01:36:57,870
to figure out what is the the case

1474
01:36:57,880 --> 01:37:00,930
this is this essentially something that you have to

1475
01:37:00,940 --> 01:37:02,300
to enforce it is

1476
01:37:02,300 --> 01:37:05,140
these have has to sum to one

1477
01:37:05,160 --> 01:37:09,310
ah yes their probabilities

1478
01:37:09,320 --> 01:37:13,300
well they are defined as being positive things always

1479
01:37:14,850 --> 01:37:18,790
in the case of potentials they will be positive in the case of

1480
01:37:19,600 --> 01:37:23,740
are you know that's no you're right

1481
01:37:24,230 --> 01:37:26,040
i think i'm tired

1482
01:37:26,090 --> 01:37:33,010
yes so basically these things through all the positive by definition

1483
01:37:33,040 --> 01:37:34,990
now that the

1484
01:37:36,810 --> 01:37:41,130
we're not enforcing these anywhere because we are working with the function here

1485
01:37:41,150 --> 01:37:44,470
so we need the explicit this this constraints

1486
01:37:45,900 --> 01:37:47,560
these things are not

1487
01:37:47,560 --> 01:37:52,870
sum to one

1488
01:37:52,890 --> 01:37:59,470
you learn something negative values because

1489
01:38:00,720 --> 01:38:02,320
i will be

1490
01:38:03,010 --> 01:38:10,360
the negative logarithm of the number between zero and one that's only positive

1491
01:38:15,100 --> 01:38:35,140
oh yes yes yes

1492
01:38:44,410 --> 01:38:46,710
notice that these

1493
01:38:46,750 --> 01:38:49,750
quantities that we have here

1494
01:38:50,650 --> 01:38:54,310
because i haven't specified is being a linear function here

1495
01:38:54,310 --> 01:38:57,040
well specified as being in in the function

1496
01:38:57,050 --> 01:39:00,520
so i'll have as will actually be a linear function of

1497
01:39:02,040 --> 01:39:05,490
and peter of some function of x and theta

1498
01:39:05,540 --> 01:39:09,800
so essentially

1499
01:39:16,580 --> 01:39:19,960
there is some of his life

1500
01:39:19,960 --> 01:39:22,910
because which should be

1501
01:39:25,190 --> 01:39:30,190
anyway into a you check is like maybe there is some sort of something

1502
01:39:30,240 --> 01:39:34,020
but anyway let's put this otherwise all make progress

1503
01:39:36,720 --> 01:39:39,340
but the key message here is that we have to

1504
01:39:39,960 --> 01:39:44,550
times the size of s equations and even each one of them is independent of

1505
01:39:44,550 --> 01:39:45,460
the other

1506
01:39:45,570 --> 01:39:50,170
why because for every pair of equations is called

1507
01:39:50,230 --> 01:39:52,330
every click

1508
01:39:52,360 --> 01:39:56,440
we solve this equation independently of every click because these theatres

1509
01:39:57,410 --> 01:40:02,570
on these as prime the only involve items in that click the order

1510
01:40:02,570 --> 01:40:06,420
the other equation of pairs of equation only will by using the other clique

1511
01:40:06,430 --> 01:40:08,310
and so on

1512
01:40:10,870 --> 01:40:13,560
so basically we can have

1513
01:40:13,630 --> 01:40:17,010
very nice result in bayesian networks which is

1514
01:40:17,040 --> 01:40:18,380
we solve

1515
01:40:18,430 --> 01:40:21,470
the maximum likelihood problem

1516
01:40:21,540 --> 01:40:26,910
separately it completely decouples over cliques so if i have very big big network it

1517
01:40:26,910 --> 01:40:28,110
doesn't matter

1518
01:40:28,170 --> 01:40:32,220
because you only have to solve the max likelihood problem independently

1519
01:40:32,260 --> 01:40:35,070
it every click

1520
01:40:35,080 --> 01:40:38,940
there's we thing about learning bayesian networks

1521
01:40:38,990 --> 01:40:39,870
it's move

1522
01:40:39,930 --> 01:40:41,860
we wikipedia

1523
01:40:41,870 --> 01:40:46,870
in my car and feel how much like estimation well the same general idea is

1524
01:40:46,870 --> 01:40:51,390
just in this case these normalisation he doesn't go to zero

1525
01:40:52,270 --> 01:40:53,480
in general

1526
01:40:53,560 --> 01:40:55,350
the function of

1527
01:40:55,410 --> 01:40:59,830
on the other hand you don't have to enforce the constraints of that's what you

1528
01:40:59,870 --> 01:41:04,240
the problem is that these scenes is the function that couples all the teeth as

1529
01:41:04,250 --> 01:41:05,840
in the party

1530
01:41:05,890 --> 01:41:09,470
these are exactly the equations the couple

1531
01:41:09,550 --> 01:41:13,980
you don't have these the coupling that you had previously because he had the task

1532
01:41:14,820 --> 01:41:16,350
you have to test prior

1533
01:41:16,430 --> 01:41:19,470
everything is separated from every as prime

1534
01:41:19,510 --> 01:41:21,390
whereas here you have to

1535
01:41:21,500 --> 01:41:27,190
and this is actually the concatenation of all the prime factors so for everybody every

1536
01:41:27,190 --> 01:41:29,570
question here you have

1537
01:41:32,970 --> 01:41:34,470
every equation

1538
01:41:34,510 --> 01:41:39,580
so in order to solve that would be complex in general can be any complex

1539
01:41:40,990 --> 01:41:44,480
yes more complicated

1540
01:41:44,530 --> 01:41:48,750
OK now we are going to make it all these more simple

1541
01:41:48,800 --> 01:41:54,370
we are going to assume that function f of x and theta is actually a

1542
01:41:54,370 --> 01:41:55,690
linear function

1543
01:41:55,710 --> 01:41:57,630
on peter

1544
01:41:57,730 --> 01:42:01,400
that makes things really what simple

1545
01:42:01,410 --> 01:42:02,850
however it will be

1546
01:42:02,870 --> 01:42:09,100
this only text into believing that on some feature of x multicore sufficient statistics of

1547
01:42:10,180 --> 01:42:11,930
and that's the expression that we have

1548
01:42:12,080 --> 01:42:14,980
they called exponential families

1549
01:42:15,010 --> 01:42:17,930
if we assume that the probability distribution has this form

1550
01:42:17,990 --> 01:42:21,800
essentially we have with the con exponential family

1551
01:42:21,850 --> 01:42:26,270
p of facts is what we call sufficient statistics theta is the natural parameter and

1552
01:42:26,270 --> 01:42:28,770
g of people in this particular case

1553
01:42:28,810 --> 01:42:31,780
so what they call the log partition function

1554
01:42:31,780 --> 01:42:35,130
so this is not going to give you the idea and then i'm going to

1555
01:42:35,320 --> 01:42:38,030
i'm going to agree that the formula but

1556
01:42:38,090 --> 01:42:42,440
just forget about the following just remember the idea here have a graph

1557
01:42:42,490 --> 01:42:44,920
now imagine that should count

1558
01:42:44,960 --> 01:42:49,980
i don't know the number of path of length three that you have here

1559
01:42:49,990 --> 01:42:53,840
that if you can't count these number and you do this is the trick of

1560
01:42:53,840 --> 01:42:58,880
removing all the nodes that have that don't have the same color imagine that just

1561
01:42:58,880 --> 01:43:01,440
having that in mind you have removed

1562
01:43:01,440 --> 01:43:06,130
OK you have chosen different colors here color that are very similar to hear that

1563
01:43:06,130 --> 01:43:08,720
is blue blue dark blue one

1564
01:43:08,740 --> 01:43:10,530
the light blue it's cetera

1565
01:43:10,550 --> 01:43:14,530
you have those colors here so i mean that here you can you will have

1566
01:43:14,530 --> 01:43:19,960
if these is darwin diseased with these nodes made of those two knows is going

1567
01:43:19,980 --> 01:43:23,460
to to to stay and if it's

1568
01:43:24,030 --> 01:43:27,570
light blue and dark blue the combination of the two want to make a new

1569
01:43:28,860 --> 01:43:31,240
so imagine that

1570
01:43:31,300 --> 01:43:37,650
i know it's difficult but then you have a different number of parts here and

1571
01:43:37,650 --> 01:43:41,460
computing the number of paths of length three in this graph

1572
01:43:41,480 --> 01:43:43,840
it is precisely equivalent to

1573
01:43:43,860 --> 01:43:48,920
computing the number of paths of length three in this graph in this graph that

1574
01:43:48,920 --> 01:43:51,110
exactly how the same label

1575
01:43:51,110 --> 01:43:51,960
we're happy

1576
01:43:52,050 --> 01:43:58,920
OK i see i see one the theoretical part again

1577
01:43:58,940 --> 01:44:02,300
i understand that

1578
01:44:04,670 --> 01:44:06,920
so you have this graph in this graph

1579
01:44:06,940 --> 01:44:10,050
you the the product of those two refs

1580
01:44:10,130 --> 01:44:12,070
and then you compute

1581
01:44:12,110 --> 01:44:18,070
the number of of of path of length three that you have been described

1582
01:44:18,090 --> 01:44:22,070
and this corresponds to the number of length of a path of length three that

1583
01:44:22,070 --> 01:44:24,090
you have here

1584
01:44:24,150 --> 01:44:29,670
and here that have exactly the same labels along the path

1585
01:44:30,570 --> 01:44:34,530
trust me

1586
01:44:34,550 --> 01:44:37,940
if don't care OK

1587
01:44:38,030 --> 01:44:41,510
so here is the kernel i was happy to show you that because it's very

1588
01:44:41,510 --> 01:44:45,820
elegant but probably should i remember the first time i saw the lecture and these

1589
01:44:45,840 --> 01:44:47,420
it took one hour

1590
01:44:47,510 --> 01:44:48,630
so maybe in

1591
01:44:48,650 --> 01:44:52,150
in seven minutes it's a bit short

1592
01:44:52,170 --> 01:44:56,650
but we should look at this paper it's is very interesting you have a lot

1593
01:44:56,650 --> 01:45:01,460
of different kernels for graphs that can be

1594
01:45:01,460 --> 01:45:03,530
computed just using the

1595
01:45:03,530 --> 01:45:06,440
the i just matrices in the

1596
01:45:06,460 --> 01:45:08,610
label matrices

1597
01:45:08,710 --> 01:45:11,530
and the final type of graph

1598
01:45:11,530 --> 01:45:16,690
i wanted to graphical that i wanted to talk to you about it was when

1599
01:45:16,690 --> 01:45:20,880
when you have a space that is defined by a graph

1600
01:45:20,940 --> 01:45:25,780
this is something that is very common that you can see when first if you

1601
01:45:25,780 --> 01:45:26,340
want to

1602
01:45:26,670 --> 01:45:30,920
two to work with the web pages you want to do classification then web pages

1603
01:45:30,920 --> 01:45:34,720
are are connected by by links

1604
01:45:34,740 --> 01:45:40,760
and you can imagine that there is a web page that those are the connection

1605
01:45:40,760 --> 01:45:45,280
and you imagine that when you have outgoing link from x two to x one

1606
01:45:45,280 --> 01:45:47,490
you also have the same thing

1607
01:45:47,510 --> 01:45:49,220
the other way around

1608
01:45:49,280 --> 01:45:54,650
it's just simplifies things and that's the question that you may ask is

1609
01:45:54,670 --> 01:46:00,070
it is how you can define a kernel between those those those points just using

1610
01:46:00,070 --> 01:46:03,630
the structure of the graph that connects all the data

1611
01:46:04,010 --> 01:46:07,280
and there is a very simple thing i'm not going to have to spend much

1612
01:46:07,280 --> 01:46:12,280
time these which is called the diffusion kernel it's really nice again

1613
01:46:12,320 --> 01:46:18,010
it just say that you take the adjacency matrix a

1614
01:46:18,110 --> 01:46:19,550
again you see that

1615
01:46:19,570 --> 01:46:24,570
x one is connected to x two here you have one between the first rule

1616
01:46:24,570 --> 01:46:25,920
and the second kind

1617
01:46:26,380 --> 01:46:29,240
x one k which is connected to x five

1618
01:46:29,260 --> 01:46:33,130
so the first line in the five con you have one

1619
01:46:33,130 --> 01:46:35,760
if the what's i

1620
01:46:35,820 --> 01:46:36,570
in addition

1621
01:46:36,590 --> 01:46:41,280
they're on the conditionals regressions in the you know that each node has come very

1622
01:46:41,280 --> 01:46:46,920
simple idea technology and but simple often good

1623
01:46:46,920 --> 01:46:50,780
this this graphical models you know it's really hard to write and marginal likelihood to

1624
01:46:50,780 --> 01:46:54,960
go uphill and it just runs really grounds it might be a good idea and

1625
01:46:54,960 --> 01:46:58,590
really large graphical model to do something extremely simple try to prove something about it

1626
01:46:58,610 --> 01:47:00,990
so you know you're not doing something ridiculous

1627
01:47:01,010 --> 01:47:04,380
and then do various problem

1628
01:47:05,840 --> 01:47:09,150
good good developing literature

1629
01:47:09,150 --> 01:47:11,530
had this is know has been

1630
01:47:11,670 --> 01:47:13,550
this literature on graphical model

1631
01:47:13,550 --> 01:47:19,650
model selection but there's lots kind of toy example would precious few real

1632
01:47:19,670 --> 01:47:24,900
the real life applications of it and and successful examples of algorithms that scale to

1633
01:47:24,900 --> 01:47:28,570
real life scale problems of this kind of a different way and see if we

1634
01:47:28,570 --> 01:47:32,920
can get closer to reality

1635
01:47:34,710 --> 01:47:37,880
the lives of

1636
01:47:37,880 --> 01:47:40,320
the things

1637
01:47:47,340 --> 01:47:50,590
i mean this papers all theories so b we personally have been looking like it

1638
01:47:50,590 --> 01:47:56,190
the way you know whatever questions a huge field in which there

1639
01:47:56,210 --> 01:48:01,460
there are lots of tons of applications of it and

1640
01:48:01,510 --> 01:48:02,900
it is

1641
01:48:02,960 --> 01:48:07,530
clearly of benefit to do multivariate regression compared regression and i don't have lists of

1642
01:48:07,530 --> 01:48:11,550
applications in my in my head but there was a big literature on it for

1643
01:48:11,550 --> 01:48:12,740
good reason

1644
01:48:12,740 --> 01:48:18,240
so we discriminate theoretical contribution here i i mean i i k i mean i

1645
01:48:18,240 --> 01:48:21,540
can i can speculate there tons of i know biology examples where i can think

1646
01:48:21,540 --> 01:48:25,940
you can clearly think it's appropriate but you know we're going to follow up with

1647
01:48:26,050 --> 01:48:31,360
more problems stuff and we did this in a nested datasets and we show that

1648
01:48:31,360 --> 01:48:34,780
it was better you know and so on and that the mean that much to

1649
01:48:37,570 --> 01:48:40,340
i guess it was kind of interest in this dataset which we have very few

1650
01:48:40,340 --> 01:48:44,240
number of examples of each of the digits and

1651
01:48:44,280 --> 01:48:48,630
it was better than doing independent ridge regression to find the right features for each

1652
01:48:48,630 --> 01:48:51,960
of the classification better than

1653
01:48:52,010 --> 01:48:53,550
doing separately

1654
01:48:53,570 --> 01:48:57,360
but i'll leave it here creative thinker problematic it's not hard to think of joint

1655
01:48:57,360 --> 01:49:01,840
features like for feature selection and you want to borrow strength across multiple features like

1656
01:49:01,940 --> 01:49:04,170
i can't believe that's widely applicable

1657
01:49:06,260 --> 01:49:09,630
OK let's take a really short break is permitted to stand up and stretch if

1658
01:49:09,630 --> 01:49:12,710
you don't mind because i would like to finish a little chunk of the last

1659
01:49:12,710 --> 01:49:14,550
part of my talk

1660
01:49:22,110 --> 01:49:29,460
OK so i'll bigger indulgences the last talk in the last part of is about

1661
01:49:29,460 --> 01:49:34,460
to happen all that so you can try to skip over quickly here but i

1662
01:49:34,460 --> 01:49:36,800
think it will be of interest to many of you this issue with independence and

1663
01:49:36,800 --> 01:49:42,670
conditional independence thing especially as filthy and summary of the audience as the very deep

1664
01:49:42,670 --> 01:49:46,010
topic in which people are believed to be a great deal in this kind of

1665
01:49:46,010 --> 01:49:51,280
a has no conditional independence story that i think is kind of interesting so relate

1666
01:49:51,280 --> 01:49:53,440
to what you've learned

1667
01:49:54,030 --> 01:49:56,960
the during the past two weeks another talks

1668
01:49:57,070 --> 01:50:00,900
the reason i bring this up at this point is that there are again it's

1669
01:50:00,990 --> 01:50:04,460
going to be an algorithm that does something and we want to give an analysis

1670
01:50:04,460 --> 01:50:08,170
of it and this is by the hardest example is the real kind kernel methods

1671
01:50:08,170 --> 01:50:13,700
hard problem machinery is not the second order cone problem or the SDP which is

1672
01:50:13,700 --> 01:50:17,920
somehow simple analyses hard so we really need to a real serious tool here so

1673
01:50:17,920 --> 01:50:20,670
we use empirical process theory get

1674
01:50:20,920 --> 01:50:24,510
good results in this case just consistency but i can give you a hint of

1675
01:50:24,510 --> 01:50:29,480
how that kind of harder argument goes and why we want to face because independence

1676
01:50:29,480 --> 01:50:31,550
is a very important concept basically

1677
01:50:31,570 --> 01:50:36,880
OK this is a joint work with kenji fukumizu inferences block

1678
01:50:36,880 --> 01:50:42,480
OK so it's kind of skipped kind quickly a little bit so

1679
01:50:42,490 --> 01:50:46,820
this is the topic that i is mainly exist in the state literature mainly in

1680
01:50:46,820 --> 01:50:50,690
the annals of statistics and its kind surprising machine learning also worked on some people

1681
01:50:50,690 --> 01:50:55,300
may have you see more and more of this kind of thing but it's basically

1682
01:50:55,300 --> 01:50:59,220
regression where you have XY pairs the coverage is high dimensional

1683
01:50:59,240 --> 01:51:02,280
and you want to find a projection of the homes recovery it onto a subspace

1684
01:51:02,280 --> 01:51:04,240
as just like you were doing PCA

1685
01:51:04,240 --> 01:51:06,860
this one has to take into account the response

1686
01:51:06,880 --> 01:51:09,090
the PCA doesn't take into account the response

1687
01:51:09,110 --> 01:51:15,220
OK so i discriminant analysis is an example of this kind classically for classification

1688
01:51:15,280 --> 01:51:19,150
we try to find some space allows you to do good separation

1689
01:51:19,210 --> 01:51:23,070
this is the regression version of that and it's meant to be nonparametric so we're

1690
01:51:23,070 --> 01:51:29,220
going to do this with the words the buzz so my parametrically ie the disturbed

1691
01:51:29,220 --> 01:51:34,030
the regression of y on x is allowed to be arbitrary i mean i a

1692
01:51:35,460 --> 01:51:39,210
we want to estimate now will call the parameter as it's the subspace it has

1693
01:51:39,210 --> 01:51:43,490
the finite dimensional object so it's really a parameter estimate that

1694
01:51:43,550 --> 01:51:46,460
under any arbitrary regression of

1695
01:51:46,490 --> 01:51:49,030
x y and x

1696
01:51:49,030 --> 01:51:53,320
OK so that's the problem cos some i parametric because we have a nonparametric component

1697
01:51:53,550 --> 01:51:57,420
which is really a nuisance parameter for this problem trying to estimate the regression

1698
01:51:57,420 --> 01:52:01,490
i'm just trying to get SOS is the primary ever nonparametric nuisance parameters usually called

1699
01:52:01,490 --> 01:52:03,860
semiparametric problem

1700
01:52:03,880 --> 01:52:15,440
OK so the main is just to hear the main approach to this literature which

1701
01:52:15,440 --> 01:52:17,570
is called sufficient dimension reduction

1702
01:52:17,590 --> 01:52:19,690
trying to find a subspace s

1703
01:52:19,710 --> 01:52:27,330
has done by inverse regression so it turns the problem around tries to try to

1704
01:52:27,330 --> 01:52:30,500
go from x to y where x is really high dimensional y is low dimensional

1705
01:52:30,870 --> 01:52:33,250
turnaround why might be one-dimensional

1706
01:52:33,300 --> 01:52:35,460
and go from y to x

1707
01:52:35,510 --> 01:52:40,440
because of white's one-dimensional you from wide access to the curve in the x space

1708
01:52:40,440 --> 01:52:44,530
OK so you could think about estimating that and then maybe doing PCA or something

1709
01:52:44,530 --> 01:52:47,300
to find the subspace woodpecker flies

1710
01:52:47,320 --> 01:52:50,790
and then turning around saying well that occur in which the data line that relevant

1711
01:52:50,790 --> 01:52:52,010
to y

1712
01:52:52,040 --> 01:52:54,320
because i was able to regression from y to x

1713
01:52:54,330 --> 01:52:58,400
for this kind of relationship and maybe i'll recover a reasonable subspace there so it's

1714
01:52:58,400 --> 01:53:03,650
kind of an intuition and that's so that's been done but what to do this

1715
01:53:03,660 --> 01:53:07,410
to an inverse regression from y to x now x is acting as the response

1716
01:53:07,410 --> 01:53:11,000
in regression you have to probabilistic assumptions in the response you don't put on the

1717
01:53:11,000 --> 01:53:13,170
covariance put spots

1718
01:53:13,170 --> 01:53:15,820
nine i'm not looking

1719
01:53:17,620 --> 01:53:21,720
twenty one point three six

1720
01:53:21,770 --> 01:53:24,170
twenty one point

1721
01:53:26,450 --> 01:53:28,670
in round of four if you want to

1722
01:53:28,750 --> 01:53:31,170
you see that the agreement is spectacular

1723
01:53:31,180 --> 01:53:33,300
within the uncertainty of my measurements

1724
01:53:33,320 --> 01:53:36,910
comes out amazingly well you could have removed this too of course

1725
01:53:36,970 --> 01:53:39,910
because if you have an uncertainty of point two here a little silly

1726
01:53:39,920 --> 01:53:41,620
you have little to hanging there

1727
01:53:41,680 --> 01:53:42,570
you see

1728
01:53:42,570 --> 01:53:48,120
indeed this spring is very close to an ideal spring it obeys hooks law

1729
01:53:48,160 --> 01:53:49,450
and it is also

1730
01:53:51,820 --> 01:53:54,150
here's the pendulum

1731
01:53:54,170 --> 01:53:55,740
here's the mass

1732
01:53:56,980 --> 01:53:59,460
offset at angle theta

1733
01:53:59,480 --> 01:54:01,850
the length of the pendulum is l

1734
01:54:01,850 --> 01:54:03,940
the length of the string

1735
01:54:04,030 --> 01:54:06,250
there's gravity here

1736
01:54:08,220 --> 01:54:11,130
and the other force on the

1737
01:54:12,200 --> 01:54:15,650
the only other force is the tension t

1738
01:54:15,750 --> 01:54:19,390
don't confuse that with period t this tension senators

1739
01:54:19,420 --> 01:54:23,080
was the only two forces there's nothing else

1740
01:54:23,090 --> 01:54:26,480
the thing is going to arc around

1741
01:54:26,500 --> 01:54:27,670
like this

1742
01:54:27,710 --> 01:54:29,660
and going to also made

1743
01:54:31,390 --> 01:54:33,250
i call this the y direction

1744
01:54:33,340 --> 01:54:35,450
and i call this

1745
01:54:35,510 --> 01:54:41,490
x direction and here x equals zero

1746
01:54:41,500 --> 01:54:44,570
well i'm going to decompose the

1747
01:54:44,570 --> 01:54:46,310
tension into

1748
01:54:46,360 --> 01:54:47,800
the wired into the

1749
01:54:47,850 --> 01:54:49,610
x direction as we have

1750
01:54:49,620 --> 01:54:52,810
done before

1751
01:54:52,900 --> 01:54:54,960
this is going to be the y component

1752
01:54:56,120 --> 01:54:58,110
x component

1753
01:54:58,410 --> 01:55:01,600
so this is why component

1754
01:55:01,690 --> 01:55:03,440
calls t

1755
01:55:03,530 --> 01:55:05,210
cosine theta

1756
01:55:05,250 --> 01:55:07,190
the x component

1757
01:55:07,280 --> 01:55:10,080
equals qi

1758
01:55:10,110 --> 01:55:12,090
find things

1759
01:55:12,100 --> 01:55:15,250
and now i'm going to write down the differential equations of motion

1760
01:55:16,150 --> 01:55:17,400
x erection

1761
01:55:17,550 --> 01:55:22,280
second newton's second law

1762
01:55:24,340 --> 01:55:25,560
he calls

1763
01:55:25,570 --> 01:55:27,840
this is the only force in the x direction

1764
01:55:27,860 --> 01:55:31,390
it's a restoring force just like with the spring and if we have to give

1765
01:55:31,390 --> 01:55:32,950
it a minus sign

1766
01:55:33,120 --> 01:55:34,430
he calls minus

1767
01:55:34,480 --> 01:55:35,970
ten times

1768
01:55:35,980 --> 01:55:38,720
the sign of the

1769
01:55:38,740 --> 01:55:42,550
he itself could easily be a function of data so i have to allow for

1770
01:55:43,590 --> 01:55:44,970
the sign of theta

1771
01:55:45,030 --> 01:55:48,020
he calls x if it's here at position x

1772
01:55:48,030 --> 01:55:49,690
divided by l

1773
01:55:49,700 --> 01:55:54,240
so can i can write for this minus p which may be a function of

1774
01:55:54,240 --> 01:55:57,800
theta and x defined by l

1775
01:55:57,800 --> 01:56:00,140
that's my differential equation

1776
01:56:00,150 --> 01:56:04,980
in the x direction and i prefer always forty is eight right down

1777
01:56:05,620 --> 01:56:07,200
o double bond

1778
01:56:07,260 --> 01:56:10,140
now the y direction

1779
01:56:10,180 --> 01:56:11,720
in one direction i have

1780
01:56:12,910 --> 01:56:14,640
why double dot

1781
01:56:16,380 --> 01:56:20,120
this is what plug direction so i have the cosine fade-out

1782
01:56:20,170 --> 01:56:24,340
minus mg

1783
01:56:24,420 --> 01:56:26,750
this equation one

1784
01:56:26,800 --> 01:56:28,560
and this equation two

1785
01:56:28,640 --> 01:56:30,410
so now we have to solve

1786
01:56:30,960 --> 01:56:34,170
coupled differential equations which is

1787
01:56:34,180 --> 01:56:37,030
hopeless tasks it looks like issue

1788
01:56:37,130 --> 01:56:38,800
and it is it

1789
01:56:38,910 --> 01:56:40,590
and now we're going to make

1790
01:56:40,600 --> 01:56:42,330
some approximations

1791
01:56:42,470 --> 01:56:45,900
and the approximations and we'll make which you often see

1792
01:56:45,960 --> 01:56:47,180
in physics

1793
01:56:47,190 --> 01:56:48,850
when something oscillates

1794
01:56:48,870 --> 01:56:49,960
is what we call

1795
01:56:49,990 --> 01:56:51,910
the small angle

1796
01:56:54,930 --> 01:56:57,820
small angle we will not allow theta

1797
01:56:57,820 --> 01:56:58,880
two become

1798
01:56:58,890 --> 01:57:00,510
two large

1799
01:57:00,530 --> 01:57:02,080
the qualitative

1800
01:57:02,140 --> 01:57:04,280
what i mean by two large

1801
01:57:04,300 --> 01:57:09,380
when fate which is in radians equals much much less than one we call that

1802
01:57:09,380 --> 01:57:11,170
a small angle

1803
01:57:11,200 --> 01:57:14,630
if that's the case the cosine of theta

1804
01:57:14,720 --> 01:57:17,670
is very close to what

1805
01:57:17,690 --> 01:57:22,900
you will say well what apply close to one o k five degrees

1806
01:57:22,900 --> 01:57:25,390
the cosine is o point nine

1807
01:57:25,400 --> 01:57:27,440
nine six

1808
01:57:27,500 --> 01:57:29,020
that's close to one

1809
01:57:29,070 --> 01:57:30,250
ten degrees

1810
01:57:30,270 --> 01:57:33,600
the cosine is o point nine eight five

1811
01:57:33,600 --> 01:57:36,720
that's only one and a half percent different from one

1812
01:57:36,920 --> 01:57:41,210
even the ten degrees doing extremely well

1813
01:57:41,260 --> 01:57:42,870
so this is

1814
01:57:42,910 --> 01:57:44,750
consequence number one

1815
01:57:44,810 --> 01:57:47,100
of the small angle approximation

1816
01:57:47,120 --> 01:57:48,870
a second consequence

1817
01:57:48,920 --> 01:57:52,190
of the small angle approximation

1818
01:57:52,250 --> 01:57:56,610
look at the extrusion that this object made from equilibrium

1819
01:57:56,620 --> 01:57:58,840
in fact direction that this big

1820
01:57:58,850 --> 01:58:03,690
look at the excursion makes in the y direction is this small

1821
01:58:03,750 --> 01:58:05,420
and where a smaller

1822
01:58:05,500 --> 01:58:11,310
then the excursion in the x direction provided the two angle is small

1823
01:58:11,310 --> 01:58:13,490
i give you an example

1824
01:58:13,520 --> 01:58:15,320
at five degrees

1825
01:58:15,320 --> 01:58:16,830
this excursion

1826
01:58:16,850 --> 01:58:20,030
is only four percent of this excursions

1827
01:58:20,050 --> 01:58:21,210
ten degrees

1828
01:58:21,230 --> 01:58:23,580
this excursion is only nine percent

1829
01:58:23,580 --> 01:58:24,880
of these excursions

1830
01:58:24,940 --> 01:58:28,930
and since the excursion in the y direction is so much smaller than the x

1831
01:58:30,560 --> 01:58:34,330
we say that the acceleration in the y direction

1832
01:58:34,390 --> 01:58:35,900
can be approximated

1833
01:58:35,920 --> 01:58:36,760
to be

1834
01:58:36,810 --> 01:58:37,980
roughly zero

1835
01:58:38,030 --> 01:58:40,500
there is almost no acceleration in

1836
01:58:40,510 --> 01:58:42,410
the y direction

1837
01:58:42,410 --> 01:58:44,470
with these two

1838
01:58:44,480 --> 01:58:47,640
conclusions which follow from the small angle approximation

1839
01:58:47,640 --> 01:58:48,670
i go back

1840
01:58:48,680 --> 01:58:51,260
the migration number two

1841
01:58:51,300 --> 01:58:53,510
and i find zero

1842
01:58:55,490 --> 01:58:57,410
which could be a function of theta

1843
01:58:57,430 --> 01:58:59,630
cosine data is one

1844
01:59:00,590 --> 01:59:01,530
and g

1845
01:59:01,570 --> 01:59:02,950
so i find

1846
01:59:03,800 --> 01:59:05,450
equals and g

1847
01:59:05,450 --> 01:59:08,610
notice it's no longer even the function of the data

1848
01:59:08,650 --> 01:59:11,820
so i simply have in my small angle approximation

1849
01:59:11,840 --> 01:59:13,240
but i can

1850
01:59:13,240 --> 01:59:14,410
make t

1851
01:59:14,470 --> 01:59:15,820
the same as mg

1852
01:59:15,820 --> 01:59:18,780
it's approximately i feel put an equals sign

1853
01:59:18,840 --> 01:59:23,180
i substituted back in my equations number one

1854
01:59:23,180 --> 01:59:24,360
so now i get

1855
01:59:25,760 --> 01:59:28,650
x double dot

1856
01:59:28,670 --> 01:59:31,010
and now bring it on the other side

1857
01:59:32,320 --> 01:59:34,260
he's now and g

1858
01:59:34,320 --> 01:59:36,400
and g

1859
01:59:36,680 --> 01:59:38,590
x divided by l

1860
01:59:38,610 --> 01:59:41,110
equals zero

1861
01:59:42,360 --> 01:59:45,650
comes the wonderful results x double dot

1862
01:59:45,670 --> 01:59:47,130
please g of l

1863
01:59:47,150 --> 01:59:48,490
times x

1864
01:59:48,530 --> 01:59:50,180
equals zero

1865
01:59:50,260 --> 01:59:52,300
and this is such a beautiful results

1866
01:59:52,410 --> 01:59:55,240
it almost makes me cry

1867
01:59:56,510 --> 01:59:57,740
a simple

1868
01:59:57,760 --> 01:59:59,590
harmonic oscillations

1869
01:59:59,630 --> 02:00:01,180
this equation

1870
02:00:01,950 --> 02:00:04,010
like a carbon copy

1871
02:00:04,010 --> 02:00:05,280
of the one

1872
02:00:05,280 --> 02:00:07,470
but we there

1873
02:00:07,470 --> 02:00:09,760
here we have k over n

1874
02:00:10,990 --> 02:00:13,300
we have g overall that's all

1875
02:00:13,320 --> 02:00:14,380
other than that

1876
02:00:14,400 --> 02:00:15,840
there is no difference

1877
02:00:15,880 --> 02:00:18,570
so you can write down immediately solution

1878
02:00:18,590 --> 02:00:21,360
two this differential equation

1879
02:00:22,570 --> 02:00:24,860
will be some amplitude

1880
02:00:24,880 --> 02:00:26,200
times the cosine

1881
02:00:26,220 --> 02:00:29,930
of omega t plus five just as we had before

1882
02:00:29,950 --> 02:00:31,220
and omega

1883
02:00:31,240 --> 02:00:33,050
i will now be the square root

1884
02:00:33,090 --> 02:00:34,780
of g of l

1885
02:00:34,780 --> 02:00:36,800
so the period of the pendulum

1886
02:00:36,840 --> 02:00:38,220
will be two pi

1887
02:00:38,260 --> 02:00:40,130
times square root of l

1888
02:00:40,150 --> 02:00:43,680
of the gene

1889
02:00:43,700 --> 02:00:47,970
just falls into our lab because we did all the work

1890
02:00:47,970 --> 02:00:51,340
i want you to realize that these results for pendulum

1891
02:00:51,400 --> 02:00:56,740
have their restrictions small angles and we discussed qualitatively how small you would like to

1892
02:00:57,590 --> 02:00:58,630
and also

1893
02:00:58,630 --> 02:01:03,300
the mass has to be exclusively in here and not in the string we call

