1
00:00:00,000 --> 00:00:02,420
this is part of the

2
00:00:02,420 --> 00:00:07,580
approach which is the discriminative which is that we don't care about the joint distribution

3
00:00:07,580 --> 00:00:08,380
of modelling

4
00:00:08,990 --> 00:00:12,290
i think we just want some function f of y given x

5
00:00:12,700 --> 00:00:17,430
we the inputs and and give me some estimate whether it's spam and then we

6
00:00:17,430 --> 00:00:19,450
went directly

7
00:00:19,470 --> 00:00:25,930
so the generative approaches related to conditional density estimation probabilistic approach in order to

8
00:00:27,110 --> 00:00:30,010
sometimes rest in in sense

9
00:00:32,040 --> 00:00:33,750
so lets us zoom in on the

10
00:00:33,760 --> 00:00:40,250
first case since this is a probabilistic talk and how to write down the generative

11
00:00:41,290 --> 00:00:44,470
that's why the prior probability and the conditional

12
00:00:45,020 --> 00:00:46,430
class priors and these are called

13
00:00:46,430 --> 00:00:49,050
class conditional distributions condition

14
00:00:49,070 --> 00:00:53,350
and the prior we usually use

15
00:00:53,410 --> 00:00:56,610
promoting so say that the probability of life

16
00:00:57,850 --> 00:01:00,020
if k is one

17
00:01:00,050 --> 00:01:01,200
this just

18
00:01:01,200 --> 00:01:04,910
constant and this one

19
00:01:04,930 --> 00:01:09,940
and of course the posterior probability which were interested in is the

20
00:01:09,950 --> 00:01:13,760
the posterior probability of y given x and our this is going to be the

21
00:01:13,760 --> 00:01:16,470
most likely value of y given x

22
00:01:16,480 --> 00:01:18,740
so this is the

23
00:01:18,750 --> 00:01:22,400
in terms of the likelihood the data given the model

24
00:01:22,400 --> 00:01:25,490
and the prior probability that two different one

25
00:01:25,570 --> 00:01:27,370
this really don't want to use

26
00:01:27,380 --> 00:01:34,000
the maximum likelihood classification which is a particular class which scientists

27
00:01:35,210 --> 00:01:37,800
you can convince yourself

28
00:01:37,820 --> 00:01:41,780
you know this is hard to make sure

29
00:01:41,790 --> 00:01:47,970
you know i think situations in which extremely unlikely

30
00:01:48,010 --> 00:01:52,150
you know that's a classic example of this the earthquake

31
00:01:52,230 --> 00:01:55,330
you know in the third world

32
00:01:58,110 --> 00:02:00,120
j for over the night

33
00:02:00,260 --> 00:02:04,930
probability in artificial intelligence and when he wrote the book you think california is still

34
00:02:04,930 --> 00:02:06,980
there is still a and so on

35
00:02:07,000 --> 00:02:08,540
you know california

36
00:02:08,560 --> 00:02:13,930
thanks chris so he said well let's say you have a nice house and the

37
00:02:13,930 --> 00:02:20,850
house has an alarm alarm i activities earthquake shakes the hands are somewhat greater endurance

38
00:02:20,880 --> 00:02:25,740
and you come home and you see that the alarm is ringing and you want

39
00:02:25,740 --> 00:02:26,360
to know

40
00:02:27,860 --> 00:02:31,010
you know it wasn't what i was working

41
00:02:31,030 --> 00:02:36,830
and here you might have different likelihood of the people are going give was very

42
00:02:36,920 --> 00:02:39,980
quickly and you know it was

43
00:02:40,000 --> 00:02:42,120
so we also

44
00:02:45,040 --> 00:02:46,650
even california

45
00:02:48,420 --> 00:02:54,440
it is still much lower than prior to breaking has in the prior probabilities using

46
00:02:54,440 --> 00:02:56,520
the series

47
00:02:57,210 --> 00:03:02,660
so this is all in bed we're is by maximum likelihood

48
00:03:03,460 --> 00:03:07,290
maximum likelihood in this case it's very easy to sort of the data into batches

49
00:03:07,290 --> 00:03:11,010
by the class to take all the spam e-mail one box

50
00:03:11,120 --> 00:03:13,360
all the songs you

51
00:03:13,410 --> 00:03:15,670
fox and you know

52
00:03:15,750 --> 00:03:19,390
prior probability by counting the size of the factors

53
00:03:20,050 --> 00:03:25,150
in my training set i saw seven hundred spam messages and two thousand nine spam

54
00:03:25,150 --> 00:03:28,940
messages so i think that the prior probability

55
00:03:29,010 --> 00:03:32,500
you know spam is you know seven divided by

56
00:03:32,520 --> 00:03:40,500
and then estimate the conditional probability of x given y separately within each patch

57
00:03:41,250 --> 00:03:42,490
the spam emails

58
00:03:42,510 --> 00:03:45,830
and we estimate our model x given spam

59
00:03:45,850 --> 00:03:49,040
and we take all the non spam email we treat it as a completely separate

60
00:03:49,040 --> 00:03:51,470
from the main effects given on

61
00:03:51,500 --> 00:03:52,710
and the

62
00:03:52,710 --> 00:03:57,560
estimate using maximum points and for each of these steps

63
00:03:57,610 --> 00:04:03,270
prior and the conditional distributions we need to use some kind of regularisation parameters so

64
00:04:03,350 --> 00:04:08,820
example with the points you don't want to the maximum likelihood estimate because i can

65
00:04:08,820 --> 00:04:12,930
be very useful for a lot more to

66
00:04:12,950 --> 00:04:16,580
so that's the general scheme here and

67
00:04:16,650 --> 00:04:17,880
now i want to

68
00:04:17,940 --> 00:04:25,490
tell about this idea of a relation which will help you avoid overfitting

69
00:04:25,510 --> 00:04:28,040
so to avoid overfitting what you can do is essentially

70
00:04:28,050 --> 00:04:33,310
priors on the the parameters of the class and the class conditional distributions

71
00:04:33,330 --> 00:04:38,070
so the simplest thing to do is say oh i think some parameters are unlikely

72
00:04:38,070 --> 00:04:43,280
in the parameters are likely to penalize parameters that were more likely

73
00:04:43,370 --> 00:04:47,250
but the important thing you can do which is like putting a prior on but

74
00:04:47,250 --> 00:04:51,200
it's a little bit more complicated in it's time some parameters together

75
00:04:51,230 --> 00:04:55,710
the same of for example i think this parameter estimates

76
00:04:55,730 --> 00:05:01,490
some probability some feature model is the same in five zero cost one

77
00:05:01,500 --> 00:05:05,040
so what does that gives you more data for estimating the parameters you only have

78
00:05:05,040 --> 00:05:11,790
one value announced separate value of course one so all the data for anything lots

79
00:05:11,790 --> 00:05:16,710
of parameters and lots of classes in this parameter computer

80
00:05:16,760 --> 00:05:23,620
and the third generalizations regularisation excuse me is that you can make factorizations independence assumption

81
00:05:23,620 --> 00:05:27,030
it is so this is the regularisation we focus on

82
00:05:27,050 --> 00:05:30,560
beginning saying well don't want to allow arbitrary

83
00:05:30,620 --> 00:05:36,900
distribution of these distributions here q x y and the factorized so that says is

84
00:05:36,900 --> 00:05:38,550
that this approach can

85
00:05:38,560 --> 00:05:43,450
it's time to choose the distribution qx given y instead of putting in general

86
00:05:43,450 --> 00:05:45,470
i think it is

87
00:05:45,930 --> 00:05:49,010
it would was be talking about

88
00:05:49,130 --> 00:05:52,680
this is the only way

89
00:05:54,190 --> 00:05:55,900
OK hello everybody so

90
00:05:56,220 --> 00:06:03,330
basically my my fundamental goal is to talk about independent component analysis but to begin

91
00:06:03,330 --> 00:06:08,800
with i will first use some slides stolen from my co-author you are whole going

92
00:06:08,990 --> 00:06:13,830
to talk about some that some kind of background things like principal components analysis which

93
00:06:13,830 --> 00:06:18,230
is often confused with ICA and other kinds of stuff

94
00:06:20,710 --> 00:06:25,850
this is the contents of this introductory part which takes something like one or two

95
00:06:25,850 --> 00:06:30,950
hours actually have no idea perhaps depending also on your questions you may to interrupt

96
00:06:30,950 --> 00:06:35,470
me at any time and ask because well i suppose i have an already well

97
00:06:35,470 --> 00:06:41,890
what back what what's your back level of background knowledge on this i these subjects

98
00:06:41,900 --> 00:06:44,240
i suppose it is also quite variable

99
00:06:44,300 --> 00:06:48,350
so first i will show something some basic ICA examples of what is i see

100
00:06:48,360 --> 00:06:49,470
all about

101
00:06:49,480 --> 00:06:55,020
and then i will go into this kind of stuff like motivated optimisation especially multivariate

102
00:06:55,020 --> 00:07:00,810
statistics in estimation theory that are necessary theoretical background

103
00:07:01,190 --> 00:07:05,990
so it's a very nice illustration and probably i think also very impressive illustration of

104
00:07:05,990 --> 00:07:07,540
what ICA does

105
00:07:07,560 --> 00:07:10,530
so what we have we have settled

106
00:07:10,540 --> 00:07:16,100
we originally had six images that you was see while but here we see some

107
00:07:16,100 --> 00:07:21,060
mixtures of those images so i mean this make this grayscale images have simply been

108
00:07:21,060 --> 00:07:26,820
linearly mixed by adding the grayscale values together with some certain random coefficients that we

109
00:07:26,820 --> 00:07:31,740
don't know that the point is that it is possible to find the original images

110
00:07:32,060 --> 00:07:35,750
without knowing the coefficients of all that mixing

111
00:07:35,800 --> 00:07:39,530
so is we did that we have is this thing here

112
00:07:39,560 --> 00:07:45,330
so six makes images and nothing else at all so can we find the original

113
00:07:46,640 --> 00:07:52,550
well you may has you have made this with can this when compute do i

114
00:07:52,550 --> 00:07:55,510
see you compute the independent components

115
00:07:55,510 --> 00:08:00,140
so i see a means independent component analysis what you get is these six original

116
00:08:00,140 --> 00:08:07,430
images which was some of them may look a bit weird that's because they have

117
00:08:07,430 --> 00:08:11,840
been inverted the polarity has been invented and white have been

118
00:08:11,850 --> 00:08:16,220
but still if you know that if you convert them back

119
00:08:16,230 --> 00:08:20,320
and then well you will see that actually yes you you get

120
00:08:20,340 --> 00:08:23,790
all of those original six original image

121
00:08:23,910 --> 00:08:30,970
so in terms of statistical estimation what we have here is is a latent variable

122
00:08:30,970 --> 00:08:37,910
model and these independent components are latent variables meaning here we will have something about

123
00:08:37,910 --> 00:08:40,170
that a bit later on

124
00:08:40,180 --> 00:08:42,790
and so this mixing was completely linear

125
00:08:42,800 --> 00:08:48,140
now see a ICSI we got to consider here is completely linear methods in the

126
00:08:48,140 --> 00:08:48,960
sense of

127
00:08:49,150 --> 00:08:52,360
having completely new mixing

128
00:08:52,730 --> 00:08:56,380
and here is another example of a bit more practical example

129
00:08:56,390 --> 00:09:00,580
because you may not encounter that kind of linear mixing some images every day

130
00:09:00,650 --> 00:09:06,240
but this is really about practical example where we have recorded brain activity electrical or

131
00:09:06,240 --> 00:09:09,300
electromagnetic brain activity using a sensor

132
00:09:09,350 --> 00:09:15,240
there has been a large number of sensors basically something like one or two hundred

133
00:09:15,240 --> 00:09:19,020
sensors that are located on just outside of the head

134
00:09:21,670 --> 00:09:23,530
so basically

135
00:09:23,840 --> 00:09:28,420
these three last ones are just for reference i mean they are not really recorded

136
00:09:28,610 --> 00:09:35,230
they are not really of interest but the point is that of these signals here

137
00:09:35,240 --> 00:09:42,080
given some of the outputs of the sensors and what you can see is that

138
00:09:42,580 --> 00:09:47,860
there's some kind of some kind some kind of mixing of certain kinds of phenomena

139
00:09:47,860 --> 00:09:50,670
you see that's like this kind of stuff here

140
00:09:50,670 --> 00:09:53,400
OK in many different senses

141
00:09:53,420 --> 00:09:58,230
well into video you would think that this kind of me these signals make sure

142
00:09:58,230 --> 00:10:02,520
lots the different kinds of interesting original signals

143
00:10:02,540 --> 00:10:07,330
laser pointer

144
00:10:07,340 --> 00:10:09,150
this OK

145
00:10:09,170 --> 00:10:13,850
this is also very not

146
00:10:13,880 --> 00:10:16,080
and a phallic symbols

147
00:10:20,010 --> 00:10:23,660
this is actually a decision rather that practical application of ICA

148
00:10:23,690 --> 00:10:26,830
people have done this kind of stuff for a couple of years and as an

149
00:10:26,830 --> 00:10:31,750
example of what you might get here are some nine independent components that have been

150
00:10:31,750 --> 00:10:36,770
estimated from that and if you analyse the for you will see that there are

151
00:10:36,770 --> 00:10:44,390
actually some some of the independent components correspond to something like really independent sources of

152
00:10:45,980 --> 00:10:50,290
well not necessarily in the brain some of these correspond to for example heart beat

153
00:10:50,290 --> 00:10:52,280
and so on but then again

154
00:10:52,380 --> 00:10:57,700
i see you will find some kind of underlying underlying sources of activity in

155
00:10:57,710 --> 00:11:01,760
in this brain signals we will have more about this in that towards and so

156
00:11:01,880 --> 00:11:04,030
and of this lecture

157
00:11:05,940 --> 00:11:10,710
i see it has found application in many different application areas recently

158
00:11:10,730 --> 00:11:15,770
here's a rather arbitrary list of those well for most probably the

159
00:11:15,770 --> 00:11:18,100
and conscientious verses

160
00:11:18,110 --> 00:11:23,080
not conscientious and careful is careless reliable versus undependable

161
00:11:23,470 --> 00:11:28,010
a good way to think about these things is in terms of the of the

162
00:11:28,010 --> 00:11:29,990
word oceans

163
00:11:30,010 --> 00:11:32,430
o CEA and

164
00:11:32,480 --> 00:11:37,830
the first letter captures openness conscientiousness extraversion

165
00:11:37,840 --> 00:11:40,520
agreeableness and neuroticism

166
00:11:40,540 --> 00:11:45,980
and the claim is those are four to five fundamental ways in which people

167
00:11:46,050 --> 00:11:48,360
differ from one another

168
00:11:48,370 --> 00:11:51,050
well why should we believe this why should we take this

169
00:11:51,060 --> 00:11:52,800
very seriously

170
00:11:52,850 --> 00:11:55,530
well there is actually some evidence for it

171
00:11:55,540 --> 00:11:58,350
it seems

172
00:11:58,380 --> 00:12:00,650
that some reliability

173
00:12:00,700 --> 00:12:03,060
in that it's stable

174
00:12:03,110 --> 00:12:06,670
over time

175
00:12:07,380 --> 00:12:08,700
if you test people

176
00:12:08,710 --> 00:12:13,390
over years by test your personality now under five traits and test you five years

177
00:12:13,390 --> 00:12:14,070
from now

178
00:12:14,180 --> 00:12:16,070
it will not change much

179
00:12:16,080 --> 00:12:18,620
and once you pass the age of thirty

180
00:12:18,690 --> 00:12:21,280
it's very stable indeed

181
00:12:21,290 --> 00:12:23,610
if you think about your parents

182
00:12:23,620 --> 00:12:27,900
and then give mum and dad a mental test on where they stand in each

183
00:12:27,900 --> 00:12:31,720
of the five traits ten years from that moment that will still be there

184
00:12:32,730 --> 00:12:33,720
it also

185
00:12:33,750 --> 00:12:37,260
seems to get agreement across multiple observers

186
00:12:38,040 --> 00:12:42,900
if i ask three to five traits

187
00:12:42,920 --> 00:12:44,310
if i ask your roommate

188
00:12:44,320 --> 00:12:45,960
what he or she thinks of you

189
00:12:45,970 --> 00:12:50,190
they ask your professor what he or she thinks of you and your mum what

190
00:12:50,190 --> 00:12:52,850
here with what she thinks of you

191
00:12:52,880 --> 00:12:56,950
how would back to gender how would

192
00:12:56,980 --> 00:12:58,740
the matchups

193
00:12:59,430 --> 00:13:01,360
and the overlap alot

194
00:13:01,390 --> 00:13:06,290
you walk around new leaving your personality to trail in the minds of people around

195
00:13:06,840 --> 00:13:11,130
and this trail is characterized in terms of these five dimensions

196
00:13:12,990 --> 00:13:14,160
it seems to be

197
00:13:14,180 --> 00:13:19,330
predict real-world behavior it is the number of the real world you wouldn't be very

198
00:13:19,330 --> 00:13:24,750
happy calling it valid you would take it seriously is that but it does so

199
00:13:24,750 --> 00:13:28,340
it's so conscientiousness how he's going to conscientiousness scale

200
00:13:28,410 --> 00:13:32,080
it relates to help pay for you or your spouse

201
00:13:32,100 --> 00:13:33,340
on how

202
00:13:33,340 --> 00:13:38,760
openness open you are are and psychological personality test relates to how likely you are

203
00:13:38,780 --> 00:13:40,130
to change job

204
00:13:41,630 --> 00:13:45,390
extroverts were people and i'm more and have more sexual partners

205
00:13:45,440 --> 00:13:46,840
because they are experts

206
00:13:46,850 --> 00:13:52,840
so these are real scales the batman hulk wonder woman doesn't correspond to anything in

207
00:13:52,840 --> 00:13:57,080
the real world but where you stand in each of these five dimensions the seem

208
00:13:57,080 --> 00:13:58,360
to capture it

209
00:13:58,740 --> 00:14:00,480
as an example

210
00:14:00,480 --> 00:14:02,340
the agreement by the way

211
00:14:02,530 --> 00:14:04,760
somebody this study

212
00:14:06,050 --> 00:14:09,040
several of the characters on the television show

213
00:14:09,090 --> 00:14:10,240
this instance

214
00:14:10,240 --> 00:14:13,170
because they want to find characters which everybody knew

215
00:14:13,190 --> 00:14:16,530
and at thirteen subjects judge

216
00:14:16,790 --> 00:14:20,530
these in turn characters on each of the five dimensions

217
00:14:20,580 --> 00:14:25,590
his openness neuroticism conscientiousness and extraversion

218
00:14:27,920 --> 00:14:29,330
they found

219
00:14:29,330 --> 00:14:31,400
considerable agreement

220
00:14:31,430 --> 00:14:35,380
and this is actually what i covered up is the agreeableness

221
00:14:36,010 --> 00:14:39,740
for pelosi you've never seen a television show this is all going to be

222
00:14:40,920 --> 00:14:42,640
but those people have

223
00:14:42,690 --> 00:14:48,640
can you guess which characters would be particularly agreeable

224
00:14:48,650 --> 00:14:50,320
anybody guess

225
00:14:52,880 --> 00:14:54,560
you right

226
00:14:54,570 --> 00:14:58,630
the most agreeable people are flanders in march

227
00:14:58,650 --> 00:15:00,420
who would be not so

228
00:15:01,900 --> 00:15:08,580
krusty is actually present the complicated cases

229
00:15:08,610 --> 00:15:13,300
but but mister burns but also

230
00:15:13,310 --> 00:15:15,400
where is a

231
00:15:15,540 --> 00:15:17,120
there's no

232
00:15:17,130 --> 00:15:19,390
nelson was nelson

233
00:15:20,180 --> 00:15:21,220
there's nothing

234
00:15:22,380 --> 00:15:26,170
you get strong consensus that ned ned flanders

235
00:15:26,190 --> 00:15:29,290
and march simpson are highly agreeable people

236
00:15:29,290 --> 00:15:34,080
six point two seven and five point four six one mister burns and nelson are

237
00:15:34,080 --> 00:15:39,060
very low nelson schoolkid one trouble happens eagles high and that's

238
00:15:39,080 --> 00:15:43,030
that's the psychological signed for military ability OK

239
00:15:43,050 --> 00:15:46,760
that's what i want to say this point about personality and how we measure it

240
00:15:46,770 --> 00:15:49,900
and again we're going to get back to it later when we talk about differences

241
00:15:49,900 --> 00:15:53,010
in personality now i want to deal

242
00:15:53,060 --> 00:15:54,620
with the second big difference

243
00:15:54,650 --> 00:15:58,450
the second big differences intelligence

244
00:15:59,180 --> 00:16:04,130
how do you define intelligence as no easy definition like personality is kind of difficult

245
00:16:04,130 --> 00:16:06,850
to get your fingers on what we're talking about here

246
00:16:07,030 --> 00:16:11,070
in one survey they have thousand experts to define intelligence

247
00:16:11,080 --> 00:16:13,830
and some answers showed up over and over again

248
00:16:13,840 --> 00:16:19,910
so just about everybody said intelligence involves abstract reasoning problem solving and the capacity to

249
00:16:19,910 --> 00:16:22,260
acquire knowledge

250
00:16:22,270 --> 00:16:24,770
that's the core being smart

251
00:16:24,780 --> 00:16:27,330
other people mentioned things like memory

252
00:16:27,340 --> 00:16:29,980
mental speed language mass

253
00:16:30,030 --> 00:16:33,360
members meet again knowledge and creativity

254
00:16:33,380 --> 00:16:36,570
also as hallmarks for intelligence

255
00:16:36,570 --> 00:16:38,500
and again

256
00:16:38,510 --> 00:16:42,130
it might be difficult to define it but you have a good feeling about what

257
00:16:42,130 --> 00:16:43,430
it is so

258
00:16:43,450 --> 00:16:44,570
you homework

259
00:16:44,670 --> 00:16:49,450
is actually in this is part of the show is actually of limited intelligence

260
00:16:49,470 --> 00:16:50,800
my colleagues

261
00:16:50,840 --> 00:16:52,850
is very high intelligence

262
00:16:52,870 --> 00:16:54,470
wonderful followed

263
00:16:56,050 --> 00:16:58,510
but is probably not as smart as that guy

264
00:16:58,520 --> 00:17:00,670
was like really really smart

265
00:17:01,500 --> 00:17:02,480
this guy

266
00:17:02,580 --> 00:17:05,200
ralph wiggum is particularly stupid

267
00:17:05,230 --> 00:17:09,810
so you have range and and and it's important to figure that characterized as well

268
00:17:09,810 --> 00:17:12,420
researched as but the feeling

269
00:17:12,450 --> 00:17:14,640
that there are some people who are smart

270
00:17:14,650 --> 00:17:18,270
and other people who are very smart and some people would come and others who

271
00:17:18,270 --> 00:17:22,870
are very that we want to do it do from a scientific standpoint

272
00:17:22,890 --> 00:17:27,360
it is characterized this in a more robust and interesting fashion

273
00:17:27,490 --> 00:17:29,790
the textbook has a nice review

274
00:17:29,920 --> 00:17:33,840
the history of attempts to define and measure intelligence

275
00:17:33,860 --> 00:17:36,480
but there's a couple of ideas i want to focus on

276
00:17:36,490 --> 00:17:41,070
one is an idea developed by by experiment

277
00:17:41,120 --> 00:17:43,600
which is there's two types of intelligence

278
00:17:45,800 --> 00:17:48,100
and there's s

279
00:17:48,130 --> 00:17:52,680
and this is your ability on specific tests

280
00:17:52,740 --> 00:17:55,610
so if there's ten tests they are given

281
00:17:55,630 --> 00:17:57,220
as part of an IQ

282
00:17:57,230 --> 00:18:00,520
an IQ test tensile tests

283
00:18:00,560 --> 00:18:03,530
you get a different score in each of the sub to be a math test

284
00:18:03,530 --> 00:18:08,740
in reading test and the spatial test in the different scores

285
00:18:10,510 --> 00:18:13,100
refers to general intelligence

286
00:18:14,050 --> 00:18:19,780
the general intelligence is something you bring to each of the tests in common

287
00:18:19,790 --> 00:18:21,950
so this is diagram here

288
00:18:22,150 --> 00:18:26,060
the six test for each of them there's an as

289
00:18:26,070 --> 00:18:28,940
and then above that there a g

290
00:18:28,990 --> 00:18:33,790
now g is very important notion the term g is used by psychologists lot even

291
00:18:33,790 --> 00:18:40,630
i now integrate everything from minus to save so i don't endlessly we copy all

292
00:18:40,630 --> 00:18:44,730
integrate by putting it up in a yellow chalk and you are left here on

293
00:18:51,500 --> 00:18:53,940
this is definitely a color and type of course

294
00:18:58,730 --> 00:19:03,250
so you want to integrate from my point quite good just integrate everything

295
00:19:03,270 --> 00:19:10,840
everything on the right-hand side also from minus five to five

296
00:19:10,880 --> 00:19:12,840
plus these other guys

297
00:19:12,860 --> 00:19:15,270
just indicate i have

298
00:19:15,290 --> 00:19:16,540
there are two

299
00:19:16,650 --> 00:19:21,590
analysis what happens

300
00:19:21,630 --> 00:19:23,440
what's this

301
00:19:27,040 --> 00:19:29,110
every term is zero

302
00:19:29,130 --> 00:19:35,630
because of the orthogonality relations the all of the form a constant times cosine in

303
00:19:35,820 --> 00:19:40,480
time something different from cosine and he signed katie

304
00:19:42,570 --> 00:19:45,230
that constant terms

305
00:19:45,270 --> 00:19:50,500
all the other terms is zero and the only one which survives is this one

306
00:19:50,650 --> 00:19:53,190
and what its value

307
00:19:53,190 --> 00:19:58,020
the integral from my despite apply of cosine square and i put that up somewhere

308
00:19:58,040 --> 00:19:59,150
it's right here

309
00:20:01,040 --> 00:20:02,790
it is part

310
00:20:02,880 --> 00:20:07,520
so this term turns into a and y

311
00:20:07,570 --> 00:20:08,570
a and

312
00:20:08,590 --> 00:20:14,110
that belong but this is where the integrals where the cosine to find

313
00:20:14,170 --> 00:20:16,570
so the end result is

314
00:20:16,630 --> 00:20:20,790
that we get a formula for

315
00:20:20,880 --> 00:20:24,090
they and what is a n

316
00:20:24,110 --> 00:20:25,710
a and is

317
00:20:25,710 --> 00:20:28,090
well a n times pi

318
00:20:28,110 --> 00:20:30,070
all these terms is zero

319
00:20:30,090 --> 00:20:35,650
and nothing is left but this left hand side and therefore and times pi i

320
00:20:35,670 --> 00:20:37,730
is the integral from

321
00:20:37,750 --> 00:20:39,340
negative for

322
00:20:40,090 --> 00:20:42,250
phi of f of t e

323
00:20:42,270 --> 00:20:44,590
times cosine t

324
00:20:47,960 --> 00:20:49,820
but that's a n times part

325
00:20:49,820 --> 00:20:53,860
therefore if i want to stay and i have to divide divided by pi

326
00:20:53,860 --> 00:20:58,900
and that's the formula for the coefficients a

327
00:20:59,000 --> 00:21:03,190
arguments exactly the same if you want to be an but all right it down

328
00:21:03,190 --> 00:21:04,460
for the sake of

329
00:21:04,480 --> 00:21:06,440
completeness as they say

330
00:21:06,480 --> 00:21:10,360
and to give you a chance to digest what i've done

331
00:21:10,770 --> 00:21:12,540
thirty seconds just

332
00:21:18,020 --> 00:21:23,610
that's because the arguments the same and the integral of science square density is also

333
00:21:24,730 --> 00:21:27,230
so there is no difference there

334
00:21:27,270 --> 00:21:29,980
now there's only one little caution

335
00:21:30,420 --> 00:21:33,520
i have to be a little careful

336
00:21:33,540 --> 00:21:37,670
this is an one two and so on and this is also in one two

337
00:21:38,460 --> 00:21:42,300
unfortunately the constant term is slight exception

338
00:21:42,320 --> 00:21:44,570
we've got to look at that specifically

339
00:21:44,590 --> 00:21:49,670
because if you forget it you can make it into gross gross gross errors

340
00:21:49,670 --> 00:21:51,770
well how about the constant term

341
00:21:51,770 --> 00:21:55,320
suppose i repeat the argument for that in miniature

342
00:21:55,480 --> 00:21:57,170
a constant term

343
00:21:57,190 --> 00:21:59,110
plus other stuff

344
00:21:59,300 --> 00:22:04,400
a typical of this stuff

345
00:22:04,500 --> 00:22:08,400
a and cosine was

346
00:22:08,430 --> 00:22:14,300
how my gay to get that constant term well if you think of this is

347
00:22:14,300 --> 00:22:16,110
sort of like

348
00:22:16,230 --> 00:22:23,630
constant times the reason is the constant is because being multiplied by cosine zero t

349
00:22:23,690 --> 00:22:28,400
because i zero one so that suggests i should

350
00:22:28,440 --> 00:22:33,130
i should multiply by i should multiply by

351
00:22:34,270 --> 00:22:38,320
in other words what i should do is simply integrate this from negative

352
00:22:38,420 --> 00:22:39,440
support i

353
00:22:39,460 --> 00:22:47,040
fifty dt what's the answer well this integrated from my despite apply is how much

354
00:22:47,110 --> 00:22:50,960
is c zero times two pi

355
00:22:51,920 --> 00:22:55,000
and the other terms all give me

356
00:23:01,880 --> 00:23:03,500
every other term is zero

357
00:23:03,500 --> 00:23:10,290
because if you integrate cosine nt you're sign overcomplete period you always get zero

358
00:23:10,300 --> 00:23:13,920
as much area above the axis of below where you can look at two special

359
00:23:13,920 --> 00:23:15,440
cases you know

360
00:23:17,590 --> 00:23:21,150
you always get zero the same thing with the site

361
00:23:23,840 --> 00:23:25,460
so the answer is

362
00:23:25,520 --> 00:23:27,500
that c zero

363
00:23:27,560 --> 00:23:30,880
is equal to is a little special

364
00:23:30,920 --> 00:23:35,000
it's not you don't just put an equal zero here

365
00:23:35,040 --> 00:23:38,210
because then you would lose the factor so

366
00:23:38,920 --> 00:23:42,000
see zero should be one over two pi

367
00:23:42,000 --> 00:23:44,210
finds this integral

368
00:23:44,230 --> 00:23:46,960
now there are two kinds of people in the world

369
00:23:47,070 --> 00:23:56,560
once learn to separate formulas and the ones who just want to separate stations

370
00:23:56,610 --> 00:23:57,630
so what

371
00:23:57,650 --> 00:23:59,880
most people do

372
00:23:59,960 --> 00:24:04,650
is they say look i want a i want this to be always the formula

373
00:24:04,650 --> 00:24:06,920
for a zero

374
00:24:06,920 --> 00:24:10,110
well for forty five minutes

375
00:24:13,320 --> 00:24:17,480
so here is the key staff assume that you have six

376
00:24:17,530 --> 00:24:19,460
we have lines

377
00:24:19,510 --> 00:24:21,460
in three space

378
00:24:21,480 --> 00:24:25,400
in each one of them you randomly select unit that

379
00:24:25,420 --> 00:24:28,550
either one of the two possible directions

380
00:24:28,630 --> 00:24:30,070
the new form

381
00:24:30,090 --> 00:24:32,070
the gram schmidt matrix

382
00:24:32,090 --> 00:24:39,090
i think only know products of these factors so clearly you get one along the

383
00:24:39,150 --> 00:24:42,840
two unit vectors

384
00:24:42,880 --> 00:24:47,030
it would be even with the small angle between them

385
00:24:47,150 --> 00:24:48,780
or its complement

386
00:24:48,880 --> 00:24:52,760
in other words what you will get in the off diagonal either plus out of

387
00:24:52,760 --> 00:24:55,510
myself while the cosine

388
00:24:55,510 --> 00:24:57,420
between the what

389
00:24:59,030 --> 00:25:02,460
this matrix being question matrix is

390
00:25:02,510 --> 00:25:04,230
positive semi definite

391
00:25:04,240 --> 00:25:07,400
it's right is three

392
00:25:07,400 --> 00:25:10,400
which means it's smallest i can value zero

393
00:25:10,420 --> 00:25:14,710
and it's multiplicities

394
00:25:16,800 --> 00:25:20,880
let's take a look as an example

395
00:25:20,940 --> 00:25:25,260
of this class which consists of the pentagon plus an isolated words

396
00:25:25,300 --> 00:25:27,420
so i call favourite graph

397
00:25:27,420 --> 00:25:29,840
and you see in the second

398
00:25:29,920 --> 00:25:39,530
OK if you don't mind OK let's we this little sequence of matrices in people

399
00:25:39,630 --> 00:25:41,170
thing from right to left

400
00:25:43,940 --> 00:25:46,820
let's look first this matrix

401
00:25:46,820 --> 00:25:51,260
this matrix is defined as follows you start with this craft

402
00:25:51,520 --> 00:25:56,000
and if two vertices are connected by an an edge

403
00:25:56,010 --> 00:25:58,780
put minus one in the matrix

404
00:25:58,820 --> 00:26:00,840
and if they are not connected by an edge

405
00:26:00,860 --> 00:26:02,320
put i one

406
00:26:02,360 --> 00:26:05,860
so for instance the forest for the first

407
00:26:05,920 --> 00:26:08,920
correspond to the isolated vertex

408
00:26:11,610 --> 00:26:13,210
this is covered by the way

409
00:26:13,230 --> 00:26:17,030
there's no reason in the world to define the adjacency matrix of the graph to

410
00:26:17,030 --> 00:26:18,530
its use in once

411
00:26:18,550 --> 00:26:21,130
one minus one one

412
00:26:21,130 --> 00:26:23,530
there's a lot of interesting information

413
00:26:23,530 --> 00:26:25,420
in this particular presentation

414
00:26:25,440 --> 00:26:27,960
this was introduced by

415
00:26:27,980 --> 00:26:32,590
dutch mathematician side and we call them decide to make this is of the

416
00:26:32,650 --> 00:26:36,900
so if you look at this matrix you can know

417
00:26:36,940 --> 00:26:41,070
can't really exciting things

418
00:26:41,070 --> 00:26:47,240
lo and behold it's smallest igon value is minus five

419
00:26:47,260 --> 00:26:50,570
and it's multiplicities

420
00:26:52,090 --> 00:26:54,110
if you just

421
00:26:54,130 --> 00:26:55,840
one of the most

422
00:26:56,710 --> 00:27:02,800
you'll get this matrix

423
00:27:02,820 --> 00:27:06,190
you can multiply by

424
00:27:08,010 --> 00:27:11,070
take o five get this matrix

425
00:27:11,090 --> 00:27:14,760
so this matrix is positive semi definite OK

426
00:27:14,780 --> 00:27:18,610
it's smallest like value zero it is multiplicity free

427
00:27:18,650 --> 00:27:24,150
and then use standard linear algebra to produce this expect that will actually give you

428
00:27:24,150 --> 00:27:28,440
the six lines on which you can produce the i'm sorry

429
00:27:34,840 --> 00:27:36,210
if you start with this

430
00:27:36,230 --> 00:27:40,010
six that was or any number of draws on the

431
00:27:40,050 --> 00:27:42,210
born in england

432
00:27:43,230 --> 00:27:44,050
you can

433
00:27:44,070 --> 00:27:50,610
produced of this system this side of matrix of the graph

434
00:27:50,610 --> 00:27:53,500
in other words with every

435
00:27:53,550 --> 00:27:58,500
system of a single lines we can associate the

436
00:27:58,510 --> 00:28:02,610
and vice versa with every graph with

437
00:28:02,630 --> 00:28:04,240
sick and i showed you

438
00:28:04,300 --> 00:28:08,460
i can correspond to a set of equivalence

439
00:28:10,820 --> 00:28:15,280
now this correspondence is not unique and it's very easy to see why

440
00:28:15,280 --> 00:28:17,980
if instead of choosing that

441
00:28:18,000 --> 00:28:19,260
in this direction

442
00:28:19,380 --> 00:28:22,420
i choose on the line of battle in the opposite direction

443
00:28:22,550 --> 00:28:24,280
it will we first

444
00:28:24,300 --> 00:28:27,380
all the inner products of the other factors

445
00:28:27,380 --> 00:28:29,170
in other words this is equivalent

446
00:28:29,170 --> 00:28:34,240
two taking a role in the car and divided by minus one

447
00:28:34,260 --> 00:28:35,630
if you

448
00:28:35,690 --> 00:28:39,380
what does it mean in the graph that is you take vertex

449
00:28:39,440 --> 00:28:40,690
and you do this

450
00:28:40,690 --> 00:28:44,480
you actually switch its relation to all the other approaches

451
00:28:44,530 --> 00:28:47,320
towards the people's connected by an edge

452
00:28:47,340 --> 00:28:49,760
it will not be connected by an edge

453
00:28:49,800 --> 00:28:51,070
and vice versa

454
00:28:51,110 --> 00:28:55,730
we call this operation switching was introduced by side

455
00:28:55,730 --> 00:28:57,570
it is defined as follows

456
00:28:57,570 --> 00:28:58,880
either way

457
00:28:58,920 --> 00:29:01,440
i mean if you switch repeatedly with this

458
00:29:01,440 --> 00:29:03,500
with respect to a sequence of axes

459
00:29:03,510 --> 00:29:08,130
it is the same as switching with respect to the whole set the same time

460
00:29:08,230 --> 00:29:12,030
this is defined as follows a subset of

461
00:29:12,110 --> 00:29:16,840
switching g with respect to you means that if you knew x

462
00:29:16,900 --> 00:29:22,480
is nothing new then remove the edge u x if it was an age and

463
00:29:22,510 --> 00:29:24,340
if it wasn't

464
00:29:24,340 --> 00:29:29,530
switching is an equivalence relation

465
00:29:30,530 --> 00:29:34,130
the number of of the number of women lines

466
00:29:34,190 --> 00:29:36,010
is equal to the number

467
00:29:36,010 --> 00:29:38,610
of switching equivalence classes

468
00:29:38,610 --> 00:29:44,460
well it's sort science

469
00:29:44,480 --> 00:29:50,260
so far forces

470
00:29:50,260 --> 00:29:53,030
there are only three switching losses

471
00:29:53,050 --> 00:29:54,900
OK for

472
00:29:54,900 --> 00:29:56,900
and and for the set to be open you have to be able to choose

473
00:29:56,900 --> 00:30:01,020
an epsilon small enough that the whole the whole ball of radius absolute around points

474
00:30:01,020 --> 00:30:02,390
to the set

475
00:30:07,670 --> 00:30:11,970
so one intuition about open sets is is is what you get if you take

476
00:30:11,970 --> 00:30:16,000
a close said like a closed ball and you remove the surface

477
00:30:16,000 --> 00:30:17,710
that's an open set

478
00:30:17,730 --> 00:30:22,050
but what's really important here for for for this why we want to what i

479
00:30:22,050 --> 00:30:23,560
want to use this image in

480
00:30:23,610 --> 00:30:26,580
integration is has nothing to do with the surface

481
00:30:32,320 --> 00:30:33,880
the idea is basically that

482
00:30:33,910 --> 00:30:37,130
that integration corresponds to something like assigning of all u

483
00:30:37,140 --> 00:30:40,680
right and if you think of something like say a hyperplane

484
00:30:40,690 --> 00:30:45,640
or a flat disk in three-dimensional space a that doesn't have what you write because

485
00:30:45,640 --> 00:30:47,760
it's flat in one direction

486
00:30:47,770 --> 00:30:51,270
and this definition of an open set which says that every point has a neighborhood

487
00:30:52,020 --> 00:30:53,240
still in this

488
00:30:53,260 --> 00:30:56,900
it means that this is the the set cannot be flat completely degenerate in any

489
00:30:58,960 --> 00:31:02,120
and even even in some small region of it cannot

490
00:31:02,150 --> 00:31:05,000
even locally if you if you focus in on one point

491
00:31:05,030 --> 00:31:06,110
there's always

492
00:31:06,130 --> 00:31:09,080
the set around it always extends in all directions

493
00:31:09,130 --> 00:31:12,660
so that's the set that has a meaningful notion of all u

494
00:31:12,680 --> 00:31:17,620
associated with it

495
00:31:20,270 --> 00:31:25,560
in in abstract mathematics this the system of all open sets of of a given

496
00:31:25,560 --> 00:31:27,830
space is called the topology of this

497
00:31:27,880 --> 00:31:32,420
so you've all heard the term topology i'm sure and

498
00:31:32,430 --> 00:31:35,040
in in more like

499
00:31:35,040 --> 00:31:39,650
calculus or another this kind of mathematics it's not like in the graph right has

500
00:31:39,650 --> 00:31:41,800
basically the neighborhood structure of the graph

501
00:31:41,850 --> 00:31:46,310
but what it tells you is which sets your space is like a lookup table

502
00:31:46,430 --> 00:31:49,770
politics like a lookup table you can look up OK the episode

503
00:31:52,170 --> 00:31:57,730
and so the general method for constructing these sigma algebras this these domains for integration

504
00:31:57,730 --> 00:32:01,420
is to start with all the open sets on your space

505
00:32:01,430 --> 00:32:05,080
and then you take the smaller sigma which contains all these open sets and that's

506
00:32:05,080 --> 00:32:07,920
denoted in this way so this is the system of sets

507
00:32:07,940 --> 00:32:12,310
then taking this closure the smallest sigma which contains all these sets

508
00:32:12,390 --> 00:32:14,420
is denoted sigma of

509
00:32:18,520 --> 00:32:22,610
that's called this kind of sigma algebras are called borel sigma algebra

510
00:32:23,320 --> 00:32:27,240
so you know to be of forming as the borel sigma algebra a sigma algebra

511
00:32:27,240 --> 00:32:31,820
generated by the open sets of this space omega

512
00:32:36,490 --> 00:32:41,860
basically the reason why why why everybody uses sigma arises once you have defined what

513
00:32:41,860 --> 00:32:43,370
you open set

514
00:32:43,390 --> 00:32:47,160
there are different ways how you can define sigmod but it might be meaningful

515
00:32:47,360 --> 00:32:52,580
so you can make different requirements like all the open sets should be in there

516
00:32:53,310 --> 00:32:57,020
i haven't said what a measurable function is but all the continuous functions should be

517
00:32:57,020 --> 00:33:00,620
measurable different kind of different kind of things that you could could require that all

518
00:33:00,620 --> 00:33:03,080
also meaningful one context another

519
00:33:03,110 --> 00:33:04,530
and it turns out that

520
00:33:04,570 --> 00:33:09,260
the sigma algebras defined by these requirements all end up being the same

521
00:33:10,780 --> 00:33:15,560
the the real question is what are your concern and once it settled

522
00:33:15,580 --> 00:33:17,910
one says that always get the same sigma algebra

523
00:33:17,930 --> 00:33:21,540
number matter how you go about it

524
00:33:22,900 --> 00:33:26,030
and just just two

525
00:33:28,930 --> 00:33:33,040
to clarify the so the signature will contain all the open sets

526
00:33:33,040 --> 00:33:38,690
but it will also contain all closed sets because closed sets complements of open sets

527
00:33:38,710 --> 00:33:41,840
and it contains a lot of sets that are neither open or close to take

528
00:33:41,840 --> 00:33:45,040
for example to think of taking an open interval

529
00:33:45,050 --> 00:33:49,100
so in an interview with the two boundary points removed and closed interval and then

530
00:33:49,100 --> 00:33:50,740
you take the union of the two

531
00:33:50,790 --> 00:33:54,120
there will also be in the sigma algebra but neither open nor closed

532
00:33:58,630 --> 00:34:01,070
this is

533
00:34:03,030 --> 00:34:05,790
month long

534
00:34:07,370 --> 00:34:11,820
six months small OK so

535
00:34:12,210 --> 00:34:15,900
that that's a good question it's not not maybe

536
00:34:17,570 --> 00:34:20,580
so it is not very intuitive and actually even if you want to prove that

537
00:34:20,580 --> 00:34:26,520
it can be huge problems what actually is the smallest sigma contain something

538
00:34:28,900 --> 00:34:31,780
they they if you have any system of sets

539
00:34:31,810 --> 00:34:36,890
down in your in your space is always smaller sigma algebra so small as system

540
00:34:36,890 --> 00:34:39,370
of sets that satisfies these these

541
00:34:39,390 --> 00:34:43,080
conditions and contains all these sets so that's unique

542
00:34:44,820 --> 00:34:47,120
actually finding that sigma algebra

543
00:34:47,160 --> 00:34:50,190
can practice if you actually want to to do them as it can be huge

544
00:34:51,190 --> 00:34:55,970
proving that something is the smallest technology for given systems that can also be huge

545
00:35:11,540 --> 00:35:15,060
OK so the question is why do you want to work with the smallest sigmod

546
00:35:15,060 --> 00:35:20,290
about why don't you take a minor injuries kevin some surplus that's in there so

547
00:35:20,310 --> 00:35:23,460
remember that the original problem was that that

548
00:35:23,560 --> 00:35:26,040
if the sigma algebra gets too large

549
00:35:26,060 --> 00:35:28,150
then we have trouble defining and measuring

550
00:35:29,020 --> 00:35:31,060
that's that's not

551
00:35:32,310 --> 00:35:35,690
only scratches the surface of the answer so generally

552
00:35:35,710 --> 00:35:37,710
generally if you

553
00:35:37,770 --> 00:35:41,320
you want your signature us to be as small as possible

554
00:35:42,380 --> 00:35:43,810
in the sense that

555
00:35:43,820 --> 00:35:50,770
you can think of that have it as

556
00:35:50,810 --> 00:35:54,020
as the resolution

557
00:35:54,340 --> 00:35:58,880
if you have the the larger sigma algebras defined as the resolution is

558
00:35:58,900 --> 00:36:02,290
what kind of sets you can integrate or or tree

559
00:36:02,670 --> 00:36:06,860
and and as a general rule of thumb

560
00:36:06,880 --> 00:36:11,270
for whatever problem you request you wanted to choose the causes resolution

561
00:36:12,290 --> 00:36:14,060
gives you all the information you need

562
00:36:16,360 --> 00:36:22,150
it is smallest

563
00:36:25,380 --> 00:36:31,480
yeah exactly so one one thing is is such properties for example in statistical estimation

564
00:36:31,540 --> 00:36:33,770
but i

565
00:36:33,810 --> 00:36:35,690
i i would love to talk about it

566
00:36:35,770 --> 00:36:39,590
but i was simply one of the time the second problem is that

567
00:36:39,650 --> 00:36:42,900
if you just make it too large then we get back to this problem of

568
00:36:42,900 --> 00:36:47,340
the power set which is so complicated that we can't even define the problem measure

569
00:36:50,090 --> 00:36:51,820
the reason for

570
00:36:51,840 --> 00:36:55,520
here's something four

571
00:37:03,500 --> 00:37:06,560
some example of constructing is ignored algebra

572
00:37:09,110 --> 00:37:11,560
if you want to define say integration

573
00:37:11,590 --> 00:37:12,900
on the real line

574
00:37:13,230 --> 00:37:14,880
in this way

575
00:37:16,790 --> 00:37:23,110
you want to set up something like

576
00:37:23,210 --> 00:37:26,880
a measure that defines volume on the real line or maybe in euclidean space

577
00:37:26,940 --> 00:37:29,770
but measures cornerback come to the limit

578
00:37:29,810 --> 00:37:33,460
so that gives you get taken to take a subset of euclidean space

579
00:37:33,480 --> 00:37:37,250
and you want to have a function that takes its subsidies an argument and science

580
00:37:37,250 --> 00:37:38,480
that's what

581
00:37:39,770 --> 00:37:41,610
in order to do that so

582
00:37:41,610 --> 00:37:44,940
we want to set up to measure and we need to define the set of

583
00:37:44,940 --> 00:37:50,210
so what happens to my fancy you absolutely think the the node with the largest

584
00:37:50,210 --> 00:37:53,070
mass not with the highest peak of the largest man

585
00:37:53,080 --> 00:37:57,590
and then the variational bayes is doing something more like

586
00:37:57,610 --> 00:38:01,250
and then as you move through two alpha positive infinity

587
00:38:01,260 --> 00:38:03,070
you get an an upper envelope

588
00:38:03,270 --> 00:38:07,580
c trying to do this for an unnormalised q distribution and you get up in

589
00:38:07,590 --> 00:38:10,150
the late on your approximation

590
00:38:10,240 --> 00:38:17,950
and that's going to try and include all of the mass in the distribution

591
00:38:17,970 --> 00:38:24,890
similarly you can look at the behaviour different evidences with respect to

592
00:38:24,940 --> 00:38:30,810
zero is in the in the p distribution and you move from the exclusive zero

593
00:38:30,870 --> 00:38:35,430
forcing behavior with zero impedance must be there in q

594
00:38:35,450 --> 00:38:39,490
so that you have regions where

595
00:38:40,060 --> 00:38:44,600
which will mean we tend to underestimate the variance of

596
00:38:46,440 --> 00:38:48,840
moving food to the other behaviour

597
00:38:50,330 --> 00:38:53,700
he will try and cover

598
00:38:53,710 --> 00:38:56,240
all of the mass of p

599
00:38:56,250 --> 00:38:58,940
even if there are serious in p

600
00:38:59,850 --> 00:39:01,450
finally if you look at the

601
00:39:02,090 --> 00:39:06,250
unnormalized q distribution that minimizes the divergence you'll see

602
00:39:06,270 --> 00:39:09,410
that the posterior mass

603
00:39:10,360 --> 00:39:15,720
moves from being estimate alpha equals minus infinity

604
00:39:15,740 --> 00:39:20,000
through actually the correct alpha equals one

605
00:39:20,010 --> 00:39:21,370
over two b

606
00:39:21,390 --> 00:39:22,710
over estimate

607
00:39:22,720 --> 00:39:25,700
a alpha equals positive infinity

608
00:39:25,750 --> 00:39:29,810
as i think as you the reasons for moving between different alpha the not really

609
00:39:29,810 --> 00:39:30,960
going to address that

610
00:39:30,980 --> 00:39:34,590
in this talk but some sort of reasons might be

611
00:39:34,610 --> 00:39:38,890
one is going to be tractability when these computations are going to be a valid

612
00:39:38,890 --> 00:39:43,290
for different graphs there is what sort of approximation you're looking to get

613
00:39:43,300 --> 00:39:46,590
which parts of the posterior actually interested in

614
00:39:55,450 --> 00:39:57,540
but that exact minimisation is not

615
00:40:09,270 --> 00:40:14,360
OK so that was just brief activities thing and then only going to look at

616
00:40:14,360 --> 00:40:23,220
the two settings africa's there in africa's one for variational bayes and expectation propagation

617
00:40:23,230 --> 00:40:26,040
so the nice thing is that we can

618
00:40:26,380 --> 00:40:27,410
point is

619
00:40:27,430 --> 00:40:34,570
the minimum divergence minimisation by message passing graph so many the factor graph notation

620
00:40:34,580 --> 00:40:39,040
we're able in the we have a factor and we're just going to look at

621
00:40:39,040 --> 00:40:40,190
the message

622
00:40:40,380 --> 00:40:45,160
eight from the message from f to be

623
00:40:46,200 --> 00:40:51,560
just to remind everyone here you might not be familiar this is the

624
00:40:51,800 --> 00:40:55,480
update equation for expectation propagation

625
00:40:55,560 --> 00:41:01,870
and so what you have you ever some product like to just something

626
00:41:01,890 --> 00:41:04,550
from then the incoming message from a

627
00:41:04,690 --> 00:41:08,680
over the fact that modelling marginalizing a something of a

628
00:41:08,700 --> 00:41:15,070
and then what EP is going to do take the backwards message from b to

629
00:41:15,120 --> 00:41:17,270
multiply that in

630
00:41:17,280 --> 00:41:23,410
and projection which is going to project the resulting distribution back into the family again

631
00:41:23,520 --> 00:41:25,370
allow for messages

632
00:41:25,500 --> 00:41:27,750
and then again divide again by

633
00:41:28,140 --> 00:41:30,770
that backwards reverse methods

634
00:41:30,780 --> 00:41:31,610
and so

635
00:41:31,650 --> 00:41:35,060
one way of thinking about this is that

636
00:41:35,070 --> 00:41:38,870
well first you should know that if projection is exact if it turns out that

637
00:41:38,870 --> 00:41:43,870
the results of the qualification is in the family of distributions which are going to

638
00:41:43,870 --> 00:41:46,700
be able to approximate then what you get

639
00:41:46,720 --> 00:41:48,500
it's just BP

640
00:41:48,520 --> 00:41:51,530
you have a belief propagation because

641
00:41:51,550 --> 00:41:53,800
these two to simply can't fly

642
00:41:53,810 --> 00:41:57,790
so what about the case where you're going to have to make that approximation you

643
00:41:57,790 --> 00:42:01,520
can have to take this possibly unpleasantly formed

644
00:42:01,700 --> 00:42:07,130
distribution and projected back into say the gas family whatever using then how is this

645
00:42:07,130 --> 00:42:10,360
reverse method influencing that projection

646
00:42:10,380 --> 00:42:14,180
one nice way of thinking about it the hint two

647
00:42:15,160 --> 00:42:17,870
algorithm is to where it should make that

648
00:42:17,890 --> 00:42:19,660
the approximation accurate

649
00:42:19,680 --> 00:42:22,750
so the that is going to say

650
00:42:22,770 --> 00:42:26,930
and i suggest that you try and make the projection accurate in this sort of

651
00:42:26,940 --> 00:42:28,290
region of space

652
00:42:28,310 --> 00:42:33,770
it's quite a nice way of thinking is as we move towards richer family messages

653
00:42:33,770 --> 00:42:39,230
then that message will get increasingly ignored let me try and simplify

654
00:42:39,240 --> 00:42:44,800
message family then it's going to have to use more information to try and find

655
00:42:44,800 --> 00:42:49,190
out where best to to approximate this awkward

656
00:42:49,420 --> 00:42:51,240
message distribution

657
00:42:53,460 --> 00:43:02,180
variational message passing doesn't use that this method always uses the same form of the

658
00:43:02,190 --> 00:43:03,750
message computations

659
00:43:03,770 --> 00:43:06,110
which is as we already being covered

660
00:43:08,450 --> 00:43:10,710
log form of the

661
00:43:10,830 --> 00:43:14,430
belief propagation

662
00:43:18,080 --> 00:43:20,750
OK so

663
00:43:20,770 --> 00:43:24,440
and it's going to take those message passing computation

664
00:43:24,450 --> 00:43:28,660
and apply them to a number of cases so we can see

665
00:43:28,680 --> 00:43:29,810
the behaviour

666
00:43:29,830 --> 00:43:32,320
of these two algorithms

667
00:43:32,340 --> 00:43:36,130
the first is is a very simple example

668
00:43:37,040 --> 00:43:41,340
all like graphs are going to have exactly one but almost all have one but

669
00:43:41,620 --> 00:43:43,750
very few variables keeping very

670
00:43:45,240 --> 00:43:49,100
so here it just going to have the factor in the

671
00:43:51,670 --> 00:43:54,930
with looking in the sense the output messages here so

672
00:43:54,940 --> 00:43:58,470
is going to be one of zero

673
00:43:58,480 --> 00:44:01,260
b is the probability of being

674
00:44:01,260 --> 00:44:04,740
into the pieces of pre-defined length

675
00:44:05,700 --> 00:44:10,050
annealing again but not simulated one so

676
00:44:10,070 --> 00:44:16,260
the point is that we are cooling this still here in the device called the

677
00:44:16,260 --> 00:44:22,010
malt primary cooling is performed by water here we have water sprays and in the

678
00:44:22,030 --> 00:44:25,810
support roles there's also water to perform schooling

679
00:44:28,880 --> 00:44:34,560
the problem is OK this is just practical interest illustrations from the plant this is

680
00:44:35,310 --> 00:44:36,890
it comes

681
00:44:37,240 --> 00:44:38,960
into for the production

682
00:44:38,970 --> 00:44:41,030
the main problem here is

683
00:44:41,040 --> 00:44:43,810
different defects

684
00:44:43,840 --> 00:44:45,550
we want to a wider

685
00:44:45,560 --> 00:44:51,410
mythologies are unhappy with it cracks

686
00:44:51,460 --> 00:44:53,840
we'd internal

687
00:44:53,960 --> 00:44:56,550
inhomogeneities and so on

688
00:44:56,610 --> 00:44:59,360
this is something they want to a white

689
00:44:59,380 --> 00:45:00,960
now how

690
00:45:00,960 --> 00:45:02,320
what is the way of

691
00:45:02,340 --> 00:45:08,980
preventing these unwanted effects this process

692
00:45:08,990 --> 00:45:11,060
it is actually controlled

693
00:45:11,090 --> 00:45:12,690
by a number of

694
00:45:12,700 --> 00:45:19,930
so called process parameters including the speed of the moving this to these devices and

695
00:45:19,930 --> 00:45:24,510
certain flows so you so that's praising the second recording zone

696
00:45:24,530 --> 00:45:27,350
and the engineer of the plant has two

697
00:45:27,370 --> 00:45:30,010
two during these parameters in way

698
00:45:30,060 --> 00:45:33,430
you can imagine that this is not a trivial problem

699
00:45:33,430 --> 00:45:39,640
because the number of parameters and possible settings is rather high

700
00:45:39,660 --> 00:45:42,810
in this case and how a particular

701
00:45:42,870 --> 00:45:49,760
that's a subspace definition and help for each parameter set minimum and maximum value and

702
00:45:49,760 --> 00:45:54,930
certain step size it makes no sense to search for the right to value continuously

703
00:45:54,930 --> 00:46:01,440
because these parameters can be set in a discrete way so this is a discretisation

704
00:46:01,680 --> 00:46:07,790
for each parameter which hosts a number of possible values if you multiply these

705
00:46:07,810 --> 00:46:13,100
you would again get the number of possible settings and in this case is the

706
00:46:13,100 --> 00:46:14,100
order of

707
00:46:15,830 --> 00:46:18,240
too much to check them all

708
00:46:18,240 --> 00:46:24,740
not possible to experiment in practice because in if we are very unlucky could even

709
00:46:24,740 --> 00:46:28,780
result in some accident and so on so

710
00:46:28,790 --> 00:46:34,950
in this case is the only way is to use a simulator of the process

711
00:46:34,950 --> 00:46:36,990
in america simulator

712
00:46:36,990 --> 00:46:38,120
the work

713
00:46:38,140 --> 00:46:43,140
lucky enough that in this case it exists for this process

714
00:46:43,160 --> 00:46:48,640
the numerical simulations that are reliable enough

715
00:46:48,700 --> 00:46:51,290
exist so the idea is

716
00:46:51,310 --> 00:46:56,990
to perform or to integrate an optimisation algorithm work which was the genetic code written

717
00:46:56,990 --> 00:47:00,470
in this case we don't cost to simulate

718
00:47:00,470 --> 00:47:04,600
and to evaluate each procedure each

719
00:47:04,660 --> 00:47:08,160
parameter setting according to some quality function

720
00:47:09,080 --> 00:47:11,390
the idea is

721
00:47:11,510 --> 00:47:15,100
rather straightforward this is the loop of the

722
00:47:15,120 --> 00:47:18,140
our iterative optimisation procedure

723
00:47:18,160 --> 00:47:19,740
this is the algorithm that

724
00:47:21,490 --> 00:47:29,350
o operations on the parameter settings and this is the way of evaluating them how

725
00:47:29,350 --> 00:47:31,850
this is done this simulator

726
00:47:31,850 --> 00:47:32,990
for each

727
00:47:33,010 --> 00:47:36,580
but parramatta vector performed the simulation

728
00:47:36,640 --> 00:47:41,560
calculate the temperature field in certain calling criteria that needs to be satisfied they are

729
00:47:41,600 --> 00:47:48,430
they do they are defined mathematically you're not going to these detectors and the result

730
00:47:48,430 --> 00:47:50,790
is a certain numerical value

731
00:47:50,810 --> 00:47:52,180
which tells us

732
00:47:53,260 --> 00:47:55,430
good the process

733
00:47:55,490 --> 00:47:59,390
is performed what is the quality of the estimated quality

734
00:47:59,410 --> 00:48:01,260
this may

735
00:48:01,280 --> 00:48:05,240
look to you but it's actually

736
00:48:05,370 --> 00:48:11,260
decades of development before behind the simulation procedure and these functions so this was done

737
00:48:11,260 --> 00:48:14,870
by people in different

738
00:48:14,890 --> 00:48:22,680
universities and steel plants it's the combination of theoretical development and empirical practice

739
00:48:22,680 --> 00:48:27,160
and this is what we did in our case so

740
00:48:27,220 --> 00:48:34,220
the genetic algorithm was coupled with the numerical simulator that performed evaluations again the here

741
00:48:34,390 --> 00:48:36,280
we how certain

742
00:48:36,310 --> 00:48:39,100
what we call performance traces here

743
00:48:39,260 --> 00:48:43,870
this is one of the initial studies performed for this problem

744
00:48:43,870 --> 00:48:46,450
the discrete variable

745
00:48:46,510 --> 00:48:51,570
indicating to which cluster data point belongs and it's not known in the training data

746
00:48:51,570 --> 00:48:55,410
and test data but we don't want to be in a specific right now about

747
00:48:56,640 --> 00:49:01,570
the first thing in bayes modelling is to think about the joint probability distribution of

748
00:49:01,570 --> 00:49:06,370
all these quantities involved and decompose it into a smart way

749
00:49:06,410 --> 00:49:11,300
typically you start with the parameters you assume there is just the application of this

750
00:49:11,300 --> 00:49:16,660
product decomposition to this problem the set of probability distribution of the parameters

751
00:49:16,680 --> 00:49:19,440
then you say OK in the next one is so the data and the hidden

752
00:49:19,440 --> 00:49:24,850
there was in the data given the parameters so this is still completely general

753
00:49:24,860 --> 00:49:28,820
and then you say OK i want to put the of x h given and

754
00:49:28,820 --> 00:49:31,340
then i should series of the whole

755
00:49:31,350 --> 00:49:35,680
pennsylvania that data HD

756
00:49:35,700 --> 00:49:42,410
but the assumption is that the observed data independent from one another given the parameters

757
00:49:42,410 --> 00:49:46,510
are known so that's why we can drop of these terms out of this expression

758
00:49:46,520 --> 00:49:50,900
and you end up with this more compact representation

759
00:49:50,910 --> 00:49:53,770
so p of ten is the

760
00:49:53,790 --> 00:49:56,630
prior distribution of parameters

761
00:49:56,640 --> 00:50:03,270
p of d HD given theta is the complete data likelihood

762
00:50:03,290 --> 00:50:07,470
but of course actually is not known so we have to

763
00:50:07,480 --> 00:50:13,300
integrate out the next step the complete that means

764
00:50:13,320 --> 00:50:16,550
assuming you could have made everything what is the likely

765
00:50:16,580 --> 00:50:21,980
the like is always the probability of the observed ovations given the parameters

766
00:50:22,010 --> 00:50:26,500
so typically have you might be interested in predicting x given the data so we

767
00:50:26,500 --> 00:50:31,770
are really interested in all these other quantities here we want to calculate this quantity

768
00:50:31,790 --> 00:50:36,670
so the first thing is as i said we don't know we don't know HD

769
00:50:36,670 --> 00:50:40,950
so the first thing is to marginalize out to make this expression

770
00:50:40,960 --> 00:50:45,050
without the d so this was one of the operations we've discussed before shifting to

771
00:50:45,050 --> 00:50:47,930
grant great out HD and get this

772
00:50:49,550 --> 00:50:56,360
the likelihood of the likelihood of the observed data depending on the amount of data

773
00:50:56,360 --> 00:50:58,590
this is the first operation to do

774
00:51:01,090 --> 00:51:05,730
next operation is then we can calculate the probability distribution of the parameters given the

775
00:51:05,730 --> 00:51:11,390
data by simply using bayes formula this was specified in the model this is that

776
00:51:11,400 --> 00:51:12,370
we have just

777
00:51:12,390 --> 00:51:15,450
a calculated that is the normalisation here

778
00:51:15,480 --> 00:51:19,720
the next thing is that we end up with the product of p of x

779
00:51:19,720 --> 00:51:22,020
is given that p of ten

780
00:51:22,290 --> 00:51:23,600
given the

781
00:51:23,610 --> 00:51:28,280
but we are not interested in that at also we have to integrate out and

782
00:51:28,280 --> 00:51:34,040
obtain the simple expression p of x on h given the data and finally we

783
00:51:34,040 --> 00:51:37,600
also not interested in age but in p of x given the

784
00:51:37,620 --> 00:51:43,440
so we also have to integrate out hidden variable in the data point so these

785
00:51:43,760 --> 00:51:45,620
should go through this may be

786
00:51:45,640 --> 00:51:52,020
when you review the lecture something really not be difficult operations derive from the basic

787
00:51:52,020 --> 00:51:55,130
principle of the technicalities involved

788
00:51:55,150 --> 00:52:03,110
all these integrals and and and the sums over here everything as sort of

789
00:52:07,820 --> 00:52:13,840
this i had this

790
00:52:13,840 --> 00:52:16,720
minus point eighty one

791
00:52:16,770 --> 00:52:20,380
which makes minus point eight i hope

792
00:52:20,440 --> 00:52:27,790
check it out your calculators with them out

793
00:52:27,800 --> 00:52:28,870
press the button

794
00:52:28,880 --> 00:52:32,860
now multiply that by age

795
00:52:32,900 --> 00:52:38,490
and which means it's going to minus point o eight

796
00:52:38,530 --> 00:52:43,540
perhaps with the zero after i didn't tell you how many decimal places was carried

797
00:52:43,540 --> 00:52:48,820
out to two decimal places i think that will be good enough

798
00:52:48,830 --> 00:52:55,890
finally the last step two here and one another one ten so the value of

799
00:52:55,890 --> 00:52:57,770
x is now two tens

800
00:52:57,780 --> 00:53:01,250
and finally what's the value of y what i didn't tell you where to stop

801
00:53:01,720 --> 00:53:07,620
let's stop that y of point let's stop y point two because there's no more

802
00:53:07,620 --> 00:53:09,490
room on the blackboard

803
00:53:09,590 --> 00:53:12,020
seems like an excellent

804
00:53:12,070 --> 00:53:15,000
about approximately how big is that in other words

805
00:53:15,070 --> 00:53:18,420
then this is going to be

806
00:53:23,000 --> 00:53:24,700
the old y

807
00:53:24,710 --> 00:53:28,200
plus this number which seems to be

808
00:53:28,210 --> 00:53:34,120
pointing to to be

809
00:53:34,140 --> 00:53:35,490
so the answer is

810
00:53:35,530 --> 00:53:38,320
new values five eighty two

811
00:53:38,360 --> 00:53:41,350
OK well we got to number we did what we were supposed to do we

812
00:53:41,350 --> 00:53:43,980
we got to number next question well

813
00:53:43,990 --> 00:53:47,050
i was asked a few questions of one of the first most basic thing is

814
00:53:47,050 --> 00:53:47,940
you know

815
00:53:48,120 --> 00:53:52,450
how right is this

816
00:53:52,460 --> 00:53:56,000
how can i answer such a question if i can

817
00:53:56,050 --> 00:53:59,240
now if i have no explicit formula for the solution

818
00:53:59,250 --> 00:54:00,630
that's the basic

819
00:54:00,690 --> 00:54:03,750
problem with numerical calculation

820
00:54:03,750 --> 00:54:08,400
in other words i have to wonder around in the dark to some extent yet

821
00:54:08,410 --> 00:54:12,800
have some idea when i've arrived at the place that i want to go

822
00:54:12,820 --> 00:54:15,110
well the first question i would like to answer

823
00:54:15,120 --> 00:54:18,870
the question is is this too high or too low is is oiler

824
00:54:18,970 --> 00:54:24,140
i'm sorry he will forgive me in heaven i will use him

825
00:54:24,200 --> 00:54:26,880
by this i mean is the result

826
00:54:26,900 --> 00:54:32,110
when you say something first and then i'll criticize it is allowed to high

827
00:54:32,120 --> 00:54:34,010
four two

828
00:54:34,210 --> 00:54:41,390
in other words it is the result of using oil is that i e is

829
00:54:41,390 --> 00:54:45,960
this number too high or too low is higher than the right answer what it

830
00:54:45,960 --> 00:54:46,840
should be

831
00:54:46,920 --> 00:54:52,260
or is it a lower than the right answer or god forbid is exactly right

832
00:54:52,270 --> 00:54:58,300
it's almost never exactly right that's not an option

833
00:54:58,420 --> 00:55:02,840
how do you answer that question well the sensory geometrically

834
00:55:04,770 --> 00:55:06,600
basically if

835
00:55:06,610 --> 00:55:09,520
the solution world lying

836
00:55:09,570 --> 00:55:11,420
four straight line

837
00:55:11,430 --> 00:55:17,960
then the oil and method would be exactly right all the time

838
00:55:18,010 --> 00:55:21,130
but it's not aligned that it's a it's a curve

839
00:55:21,200 --> 00:55:24,730
well the critical question is is the curve

840
00:55:24,780 --> 00:55:29,580
is the solution so here's the solution let's call y one of x let's say

841
00:55:29,590 --> 00:55:32,250
here was the starting point

842
00:55:32,250 --> 00:55:36,660
here the solution is convex

843
00:55:36,700 --> 00:55:38,820
and here the solution is

844
00:55:42,260 --> 00:55:47,410
i concave up concave down if you learn those words i think those are by

845
00:55:47,440 --> 00:55:50,590
now free michael pretty well disappear from the curriculum

846
00:55:50,640 --> 00:55:54,150
call it if you have an upper now what mathematicians call it

847
00:55:54,190 --> 00:55:57,390
convex is that the other one is concave

848
00:55:57,400 --> 00:56:02,250
well how to the oilers solutions look well just sketch i think from this you

849
00:56:02,250 --> 00:56:04,030
can see already

850
00:56:04,090 --> 00:56:07,990
when you start out on the oil solution is going to go like that now

851
00:56:07,990 --> 00:56:09,650
you're too low

852
00:56:09,660 --> 00:56:11,990
well let's suppose after that

853
00:56:12,080 --> 00:56:16,860
the line element here is approximately the same as what it is there

854
00:56:16,910 --> 00:56:21,520
you know roughly parallel after all there are not too far apart and the direction

855
00:56:21,520 --> 00:56:27,200
field is continuous at the directions don't change drastically from one point to another

856
00:56:27,250 --> 00:56:29,220
well there also of

857
00:56:29,260 --> 00:56:32,770
but now you see is still too long it even more

858
00:56:32,780 --> 00:56:35,490
as it pathetically tries to follow

859
00:56:35,490 --> 00:56:40,420
it's losing territory and that's basically because the curve is convex

860
00:56:40,540 --> 00:56:45,900
exactly the opposite would happen if the curve or concave the solution curve concave now

861
00:56:45,900 --> 00:56:47,430
it's too high

862
00:56:47,440 --> 00:56:50,970
and it's not going to be able to correct that

863
00:56:50,970 --> 00:56:56,930
check over the microsoft instant messenger so it's like more than a billion conversations with

864
00:56:56,930 --> 00:57:01,070
a two hundred forty million people from the whole world talking at the same time

865
00:57:01,070 --> 00:57:03,020
exchanging information and so on

866
00:57:03,090 --> 00:57:05,520
so scale is going

867
00:57:05,570 --> 00:57:08,960
but scale gives you more or less

868
00:57:09,990 --> 00:57:13,950
what when i when i what i mean by more is that we observe some

869
00:57:13,950 --> 00:57:17,520
global phenomena that are genuine and it would be

870
00:57:17,540 --> 00:57:22,270
invisible at smaller scales for example heavy tailed degree distribution thank you won't see them

871
00:57:22,280 --> 00:57:25,230
if you have if your network is small or if you have a network of

872
00:57:25,230 --> 00:57:26,540
few hundred people

873
00:57:26,580 --> 00:57:32,420
it is very hard for you to measure any statistical significance as we bigger networks

874
00:57:32,480 --> 00:57:36,730
that counts for sort of what you get less of is that we don't really

875
00:57:36,730 --> 00:57:40,090
know the the role of the nodes or we don't know the

876
00:57:40,140 --> 00:57:40,890
let's say

877
00:57:40,900 --> 00:57:43,940
if i say no that is maybe strong but we don't know

878
00:57:43,960 --> 00:57:47,820
we don't know the semantics right we have the observed things that such a large

879
00:57:47,820 --> 00:57:51,980
scale we can't you don't know the rules of particular nodes and i know the

880
00:57:51,980 --> 00:57:53,470
backgrounds and so on

881
00:57:53,480 --> 00:57:59,480
and the goal which is to find to find problems that lies somewhere between where

882
00:57:59,480 --> 00:58:02,670
we can sort of exploit both things

883
00:58:04,130 --> 00:58:07,210
and sort of moving towards what i'm going to talk about right so what do

884
00:58:07,210 --> 00:58:11,030
we know about networks i think we know a lot about the structure

885
00:58:11,080 --> 00:58:11,930
of the

886
00:58:11,940 --> 00:58:16,130
of the networks and here i have like list of of buzz words or terms

887
00:58:17,660 --> 00:58:20,660
when structure properties that were discovered in

888
00:58:20,710 --> 00:58:25,480
in large networks in the last ten years and you probably recognise a lot of

889
00:58:25,480 --> 00:58:29,210
the right to be talking about power law and scale free networks small world and

890
00:58:29,210 --> 00:58:33,610
so on but what i think is that we know much less about processes and

891
00:58:33,610 --> 00:58:38,480
dynamics that take place in networks and this is the topic of my talk today

892
00:58:38,530 --> 00:58:40,560
just to sort of

893
00:58:40,990 --> 00:58:43,970
what if i told you so here i have two

894
00:58:44,930 --> 00:58:48,230
the features of networks are one of them is

895
00:58:48,330 --> 00:58:49,490
is a network of

896
00:58:49,510 --> 00:58:54,420
traces of how disease was spreading and the other one is how people who recommended

897
00:58:54,470 --> 00:58:57,870
the DVD challenge and i want to tell you which one is which but this

898
00:58:57,870 --> 00:58:59,960
is what study this

899
00:59:00,020 --> 00:59:05,260
so what i'll talk about that diffusion and cascading behavior in networks and i was

900
00:59:05,260 --> 00:59:07,850
that the the top you have park

901
00:59:07,900 --> 00:59:14,000
first i will talk about basic mathematical models like virus propagation and diffusion or cascading

902
00:59:14,000 --> 00:59:19,350
behavior sort of two traditional topics and then we'll see some great consequences in finding

903
00:59:19,360 --> 00:59:23,520
the most influential nodes in the network then i'll show you some empirical studies on

904
00:59:23,520 --> 00:59:24,960
large networks

905
00:59:24,980 --> 00:59:30,010
radio radio cannot going to measure the cost the cascades and the behaviour of how

906
00:59:30,010 --> 00:59:34,340
things spread over the network and sharing the publications come from by the marketing and

907
00:59:34,340 --> 00:59:39,100
blogs and the like in the last part i show more and more algorithms and

908
00:59:39,100 --> 00:59:43,240
consequences come out from the real and then you can

909
00:59:43,540 --> 00:59:48,370
so what i mean by diffusion in social networks this would be at the behaviour

910
00:59:48,390 --> 00:59:53,200
of the cascades spread from node to node like an epidemic in in the network

911
00:59:53,230 --> 00:59:57,550
and there are now numerous examples of this for example you can have news propagation

912
00:59:57,560 --> 01:00:02,450
gravity some events in the people get aware of this event by hearing about it

913
01:00:02,450 --> 01:00:06,380
from the friends i'd like opinions rumors and things like that we also have like

914
01:00:06,380 --> 01:00:10,330
this one of the most effective way to here that something is good from from

915
01:00:10,330 --> 01:00:14,720
someone else like the rise of new websites free web based services and so on

916
01:00:15,050 --> 01:00:19,980
of course also like diseases like flu propagates via right i get the flow through

917
01:00:19,990 --> 01:00:24,820
contact with some other person that's the same that we have i know some kind

918
01:00:25,320 --> 01:00:31,500
changes in social principles like smoking maybe you really somebody may stop or start smoking

919
01:00:31,500 --> 01:00:35,450
given the pressure from their friends or whether they smoke or not and so on

920
01:00:37,500 --> 01:00:40,820
then similar for financial markets and so

921
01:00:40,990 --> 01:00:46,260
then there is a long history of the study diffusion and here i want to

922
01:00:46,260 --> 01:00:48,090
measure mentioned the two

923
01:00:48,100 --> 01:00:52,090
the most famous one was for the for the first one i want to quickly

924
01:00:52,090 --> 01:00:58,100
talk about is the spread of new agricultural practices in between two hundred sixty i'll

925
01:00:58,240 --> 01:01:04,330
farmers right so random crossing forty three they went ends and study how this new

926
01:01:04,330 --> 01:01:09,990
type of course see the hybrid corn that would increase the

927
01:01:10,010 --> 01:01:14,630
the game of i know how much you harvest every year by by significant amount

928
01:01:14,630 --> 01:01:20,000
of how this how this adoption of this new seat was adopted by function basically

929
01:01:20,000 --> 01:01:24,400
what they found was that as far as got aware of this new

930
01:01:24,440 --> 01:01:28,960
the miraculous it very quickly but what happened is that even later when they are

931
01:01:28,960 --> 01:01:30,250
sold this

932
01:01:30,300 --> 01:01:32,440
actually works they would go

933
01:01:32,460 --> 01:01:37,320
and by using this is so what what that sort of child was the diffusion

934
01:01:37,340 --> 01:01:42,980
is like this is a social process right it's a social network interpersonal communication that

935
01:01:42,990 --> 01:01:46,780
made these people involved or by businesses and

936
01:01:46,880 --> 01:01:50,170
the second classical study is spread of new

937
01:01:50,630 --> 01:01:56,930
medical practices when they beat all this studied the adoption of a new drug between

938
01:01:56,930 --> 01:02:01,640
doctors in illinois so there must some new and abiotic that

939
01:02:01,650 --> 01:02:05,140
that was very good at that point in time and what happened is that it

940
01:02:05,140 --> 01:02:09,060
was not like people became aware of the of the neuron and biotic or the

941
01:02:09,060 --> 01:02:10,260
new drugs like

942
01:02:10,360 --> 01:02:14,630
because studies or in sense trying to sell the new drugs but they wouldn't prescribe

943
01:02:14,680 --> 01:02:20,440
until their peers i from the same hospital and someone with this drug actually works

944
01:02:20,440 --> 01:02:24,850
so again it was like the social power of the year that made people start

945
01:02:24,900 --> 01:02:26,790
describe this new

946
01:02:26,920 --> 01:02:30,940
so again it's like it's just diffusion is like a social process and also there

947
01:02:30,940 --> 01:02:35,560
are many other studies of diffusion that can go in many interesting ways so here

948
01:02:35,560 --> 01:02:40,810
are two the more interesting one one is that there was like that

949
01:02:40,910 --> 01:02:42,860
obesity which is basically like

950
01:02:42,900 --> 01:02:45,600
if somebody is always the that this sort of

951
01:02:45,820 --> 01:02:48,350
while or

952
01:02:48,400 --> 01:02:52,590
diffuses through the network and they found some evidence that if we can friends then

953
01:02:52,600 --> 01:02:54,070
your probability of

954
01:02:54,120 --> 01:02:57,420
yourself go away like significant cases

955
01:02:58,260 --> 01:03:03,120
another thing that is sort can also one can think of us up pressure of

956
01:03:03,120 --> 01:03:09,040
on on the person is is this experiment from this from the sixties with a

957
01:03:09,040 --> 01:03:12,380
group of five to ten people they were put into the room and the like

958
01:03:12,580 --> 01:03:15,570
every person had to say which line is the closest way

959
01:03:15,620 --> 01:03:21,260
and what happened was the first line of them they were like they paid daughter

960
01:03:21,850 --> 01:03:25,090
they were set up in the last one was the genuine experiment and what actually

961
01:03:25,090 --> 01:03:29,850
the bayesian methodology and within that you you have

962
01:03:29,890 --> 01:03:32,110
algorithms which help you too

963
01:03:32,120 --> 01:03:33,760
decide how many clusters

964
01:03:33,770 --> 01:03:35,120
should one

965
01:03:35,130 --> 01:03:41,990
his plan yes

966
01:03:53,210 --> 01:04:00,490
oh i see so that's come to two dimensions right so each data point could

967
01:04:00,490 --> 01:04:02,290
be high dimensional

968
01:04:02,310 --> 01:04:04,620
and then you can have lots of data points

969
01:04:04,700 --> 01:04:09,880
in terms of the high dimensionality of the data points yes you could do dimensionality

970
01:04:10,950 --> 01:04:11,990
in fact

971
01:04:12,040 --> 01:04:14,380
one of the examples which are given in the end

972
01:04:14,450 --> 01:04:19,360
is on hand handwritten digits and what i did was i used PCA to reduce

973
01:04:19,360 --> 01:04:25,950
it to a low dimensional space and then hierarchical clustering in the multinationals

974
01:04:27,660 --> 01:04:28,840
that's the difference

975
01:04:29,120 --> 01:04:30,990
dimension to the

976
01:04:31,000 --> 01:04:34,410
the to the different hierarchical clustering approaches of that

977
01:04:34,440 --> 01:04:35,740
o which is

978
01:04:35,740 --> 01:04:37,150
along the lines of

979
01:04:37,160 --> 01:04:42,650
more or less bayesian algorithms so let's start with the traditional ones which i call

980
01:04:42,660 --> 01:04:44,140
linkage algorithms

981
01:04:44,150 --> 01:04:49,440
so this you might have heard of them like single linkage average linkage complete linkage

982
01:04:49,600 --> 01:04:55,580
and this up algorithms which in a sense this prescription prescriptive so

983
01:04:57,850 --> 01:05:00,020
if you simply given the algorithm

984
01:05:00,080 --> 01:05:03,350
which produces value a free hierarchy

985
01:05:03,390 --> 01:05:04,750
and it's not clear

986
01:05:04,810 --> 01:05:07,340
in from a probabilistic

987
01:05:07,690 --> 01:05:11,700
perspective is not clear what this tree actually means

988
01:05:12,130 --> 01:05:16,880
but you know it's very easy to implement as very simple to that to use

989
01:05:17,140 --> 01:05:18,810
this is very popular

990
01:05:19,270 --> 01:05:23,850
coming being more bayesian on so

991
01:05:25,240 --> 01:05:26,620
the next set of

992
01:05:26,630 --> 01:05:30,990
hierarchical clustering algorithms is based on probabilistic models because

993
01:05:31,090 --> 01:05:35,030
the probabilistic models gives you a way in which

994
01:05:35,040 --> 01:05:38,690
to judge how good your hierarchy is

995
01:05:38,740 --> 01:05:40,810
based on how good it is

996
01:05:40,820 --> 01:05:42,100
a generative process

997
01:05:42,110 --> 01:05:44,040
of the data

998
01:05:44,100 --> 01:05:47,310
and they could be more bayesian and you could do things like

999
01:05:47,360 --> 01:05:51,660
so this a probabilistic approaches which we tend to use a single tree

1000
01:05:51,670 --> 01:05:55,920
and if you are interested in recovering a tree from your data

1001
01:05:56,100 --> 01:05:57,760
a hierarchy from the data

1002
01:05:58,710 --> 01:06:00,510
if you don't have enough data

1003
01:06:00,520 --> 01:06:03,570
you have to be uncertain about what the tree is

1004
01:06:03,580 --> 01:06:07,080
and in order to capture the uncertainty you what you like to do is to

1005
01:06:07,080 --> 01:06:09,360
take a more bayesian approach in which

1006
01:06:09,400 --> 01:06:12,230
you have the whole distribution over trees and then

1007
01:06:12,380 --> 01:06:15,080
some ways higher probability than others

1008
01:06:15,090 --> 01:06:18,490
but you want to actually report the distribution of the trees

1009
01:06:19,230 --> 01:06:21,230
the tell was basically going to go through

1010
01:06:21,240 --> 01:06:23,290
this series of different

1011
01:06:23,350 --> 01:06:26,190
hierarchical clustering algorithms

1012
01:06:26,240 --> 01:06:30,260
so let's start with the link showrooms is very easy to just tell you the

1013
01:06:30,260 --> 01:06:32,990
algorithm and then we'll go from there

1014
01:06:33,000 --> 01:06:34,940
so the input

1015
01:06:34,990 --> 01:06:40,310
is you're given a set of data points x one two x

1016
01:06:40,320 --> 01:06:41,770
you also

1017
01:06:41,780 --> 01:06:47,470
you also need to tell the was some distance measure to use so this distance

1018
01:06:47,490 --> 01:06:52,020
this is the function is simply a function which returns to the distance between the

1019
01:06:52,030 --> 01:06:53,740
points x and y

1020
01:06:53,780 --> 01:06:57,380
so this could be things like l one distance euclidean distance

1021
01:06:57,840 --> 01:07:03,130
it could be either so distance measures as well

1022
01:07:03,190 --> 01:07:05,620
and then given this distance measures

1023
01:07:05,670 --> 01:07:08,960
you also want some way of combining

1024
01:07:08,990 --> 01:07:13,610
distances so the distance between two clusters so c and d here are subsets of

1025
01:07:13,750 --> 01:07:15,080
data points

1026
01:07:16,350 --> 01:07:20,620
the distance between two subsets two clusters here

1027
01:07:20,620 --> 01:07:23,560
is some function of the distance between

1028
01:07:24,210 --> 01:07:26,850
data points within each subset

1029
01:07:26,890 --> 01:07:31,840
then the algorithm is very simple to use but you initialise each data point in

1030
01:07:31,840 --> 01:07:36,990
its own separate clusters of initialise x i in cluster c i which contains only

1031
01:07:38,620 --> 01:07:42,150
and then we iterate forty and minus one iterations

1032
01:07:42,230 --> 01:07:46,740
in each iteration we simply find a pair of clusters c and d

1033
01:07:46,740 --> 01:07:50,610
that minimizes the distance between them so the idea is that if two clusters are

1034
01:07:50,610 --> 01:07:53,120
similar to each other that you want to much

1035
01:07:53,160 --> 01:07:54,880
and the more similar they are

1036
01:07:54,940 --> 01:07:58,810
the more you want to prioritize merging them

1037
01:07:58,820 --> 01:08:03,700
so you pick a pair of clusters with the minimum distance

1038
01:08:03,710 --> 01:08:07,330
and that simply much this two clusters together by basically

1039
01:08:07,370 --> 01:08:09,340
removing cnt

1040
01:08:09,340 --> 01:08:11,120
from your list of clusters

1041
01:08:11,220 --> 01:08:13,060
and then adding the union

1042
01:08:13,890 --> 01:08:15,230
two classes together

1043
01:08:15,230 --> 01:08:17,630
into your list of clusters and you just repeat this

1044
01:08:18,750 --> 01:08:22,260
in each iteration so you start off with and clusters

1045
01:08:22,280 --> 01:08:26,440
in each iteration we remove two clusters at one so we subtract the number of

1046
01:08:26,440 --> 01:08:27,750
clusters by one

1047
01:08:27,790 --> 01:08:28,840
and after

1048
01:08:28,860 --> 01:08:29,640
and like

1049
01:08:29,880 --> 01:08:33,860
after n minus one iterations all of the you only have one class the left

1050
01:08:33,860 --> 01:08:38,290
and this is this cluster contains all the data

1051
01:08:38,340 --> 01:08:40,380
so this is of

1052
01:08:45,720 --> 01:08:46,760
class of

1053
01:08:46,760 --> 01:08:48,420
hierarchical clustering algorithms

1054
01:08:48,440 --> 01:08:53,140
so that's the different linkage algorithms you have to use a single company and average

1055
01:08:53,220 --> 01:08:55,780
and they simply correspond to different

1056
01:08:55,820 --> 01:08:58,070
distance measures on clusters

1057
01:08:58,080 --> 01:09:00,720
so a different function to define

1058
01:09:00,780 --> 01:09:04,170
distances between classes given distances between data points

1059
01:09:04,220 --> 01:09:06,050
so in single linkage

1060
01:09:06,050 --> 01:09:10,330
the distance between two clusters is simply the minimum distance between

1061
01:09:10,340 --> 01:09:12,480
have data points one from each cluster

1062
01:09:12,540 --> 01:09:13,760
of the distance

1063
01:09:15,570 --> 01:09:20,950
complete linkage the distance between two clusters is the maximum distance between data points

1064
01:09:21,010 --> 01:09:22,610
in the cluster

1065
01:09:22,620 --> 01:09:26,420
and then in average linkage is simply the average

1066
01:09:28,170 --> 01:09:31,420
that's of course in many different ways of of

1067
01:09:31,470 --> 01:09:33,430
defining distance between

1068
01:09:33,440 --> 01:09:36,580
two clusters given distances between data points so

1069
01:09:37,190 --> 01:09:43,470
this i some of the of examples mean centroid ward weighted versions of this so

1070
01:09:43,470 --> 01:09:48,730
but the most popular history in fact the most of the probably everything

1071
01:09:49,220 --> 01:09:52,110
just to give you the intuition about what this too

1072
01:09:52,120 --> 01:09:53,860
history algorithms this

1073
01:09:53,910 --> 01:09:59,150
so in single linkage saying that the distance between two classes say where one class

1074
01:09:59,160 --> 01:10:00,930
the here and another the cost that

1075
01:10:01,060 --> 01:10:03,510
then the distance between these two clusters simply

1076
01:10:03,550 --> 01:10:06,040
the minimum distance of data points

1077
01:10:06,050 --> 01:10:07,310
one from each

1078
01:10:07,390 --> 01:10:10,000
of pairs of data points one from each cluster

1079
01:10:10,690 --> 01:10:12,360
notice that

1080
01:10:12,360 --> 01:10:13,800
according to this

1081
01:10:13,840 --> 01:10:18,770
distance measure this two clusters are very close to a very close together

1082
01:10:18,820 --> 01:10:21,150
if they have two data points which are close together

1083
01:10:21,150 --> 01:10:23,170
two you can write it has a some

1084
01:10:24,010 --> 01:10:25,030
over data points

1085
01:10:25,880 --> 01:10:26,440
of terms

1086
01:10:29,000 --> 01:10:29,720
what region

1087
01:10:30,470 --> 01:10:33,010
okay so we keep this one because it grows linearly with then

1088
01:10:33,700 --> 01:10:36,120
dear to logpi therefore it doesn't grow then

1089
01:10:36,530 --> 01:10:37,510
there is a growing anything

1090
01:10:38,210 --> 01:10:39,870
the grows with pi pi doesn't grow

1091
01:10:41,120 --> 01:10:43,170
okay grows with the number parameters right

1092
01:10:45,180 --> 01:10:47,650
okay but that we're keeping the number parameters fixed here

1093
01:10:48,700 --> 01:10:49,040
and now

1094
01:10:49,510 --> 01:10:52,770
this one half log of the determinant of any

1095
01:10:53,880 --> 01:10:55,270
that's grows the end

1096
01:10:56,520 --> 01:11:03,170
and i it's with the it's a couple lies derive it but essentially the hessian matrix

1097
01:11:05,370 --> 01:11:08,640
is the matrix of second derivatives of the log likelihood

1098
01:11:09,420 --> 01:11:10,090
you get

1099
01:11:11,570 --> 01:11:12,240
one term

1100
01:11:12,670 --> 01:11:14,250
for each data point again

1101
01:11:15,690 --> 01:11:17,210
so i can actually write

1102
01:11:21,610 --> 01:11:25,140
so sorry i can write the matrix any roughly has

1103
01:11:25,590 --> 01:11:26,590
end times

1104
01:11:27,430 --> 01:11:29,160
some matrix called they not

1105
01:11:29,820 --> 01:11:34,660
okay which doesn't grow them so the matrix eighty elements that they grow linearly with ann

1106
01:11:36,960 --> 01:11:41,190
if you have a debugging matrix with elements grow linearly with end

1107
01:11:41,550 --> 01:11:42,990
the determinant will grow

1108
01:11:43,520 --> 01:11:45,030
with enter the dean

1109
01:11:47,890 --> 01:11:49,860
and so when you take the logo that's

1110
01:11:50,160 --> 01:11:51,270
would you get is

1111
01:11:53,030 --> 01:11:56,000
sorry log enter the deity is dlog again

1112
01:11:58,110 --> 01:12:00,550
and the other term is a constant that doesn't grow them

1113
01:12:00,960 --> 01:12:05,730
so basically when we examine this term with a couple of lines map what we

1114
01:12:05,730 --> 01:12:08,540
get is a term dealer to log in

1115
01:12:10,170 --> 01:12:16,300
this is an amazingly simple equation is and it's an approximation to a marginal likelihood

1116
01:12:17,100 --> 01:12:18,810
which says computer

1117
01:12:19,490 --> 01:12:21,510
the maximum of the log likelihood

1118
01:12:22,550 --> 01:12:23,640
and penalized by

1119
01:12:24,320 --> 01:12:25,760
the number of parameters

1120
01:12:26,420 --> 01:12:26,910
over to

1121
01:12:27,770 --> 01:12:30,060
times log of the number of data points

1122
01:12:30,930 --> 01:12:34,250
this is a simple number that use the penalized maximum likelihood

1123
01:12:37,410 --> 01:12:38,190
super cheap

1124
01:12:38,840 --> 01:12:41,700
quick and easy to compute doesn't even depend on the prior

1125
01:12:42,390 --> 01:12:43,310
some of them went away

1126
01:12:45,860 --> 01:12:48,430
and we can use the maximum because it doesn't depend on the prior we can

1127
01:12:48,430 --> 01:12:51,490
use the maximum likelihood estimate is that the map estimate for example

1128
01:12:52,810 --> 01:12:56,020
it's equivalent to the minimum description length criterion

1129
01:12:56,710 --> 01:12:57,540
by people like

1130
01:12:58,000 --> 01:12:58,920
your memories and the

1131
01:13:00,220 --> 01:13:01,490
wallace and so on

1132
01:13:03,200 --> 01:13:08,170
although there is a lot more to the minimum description length literature than just this one one criterion

1133
01:13:10,250 --> 01:13:16,180
in the limit as n goes to infinity assumes that all parameters are well determined and the model is identifiable

1134
01:13:16,970 --> 01:13:20,980
otherwise the number parameters these should be the number of well determined parameters

1135
01:13:21,460 --> 01:13:22,910
and just as a word of caution

1136
01:13:23,630 --> 01:13:26,200
basically this encourages parameter counting

1137
01:13:26,740 --> 01:13:27,380
it says

1138
01:13:27,840 --> 01:13:31,340
the complexity of the model is going to be determined by the number of parameters

1139
01:13:31,650 --> 01:13:34,190
and that's a very very dangerous thing to do

1140
01:13:35,170 --> 01:13:37,970
there are simple models with only

1141
01:13:38,510 --> 01:13:39,420
one parameter

1142
01:13:39,900 --> 01:13:40,800
and there r

1143
01:13:40,900 --> 01:13:43,990
sorry there are complicated models the only one parameter

1144
01:13:44,530 --> 01:13:49,590
and there are simple models with infinitely many parameters so parameter accounting

1145
01:13:50,170 --> 01:13:55,720
is very dangerous but in this asymptotic regime for identifiable will determine models

1146
01:13:56,220 --> 01:13:58,340
this is a valid way of approximating the marginal

1147
01:14:00,720 --> 01:14:01,300
this was

1148
01:14:08,920 --> 01:14:10,630
yeah we removed e um

1149
01:14:11,070 --> 01:14:13,150
we remove the fisher information matrix

1150
01:14:14,220 --> 01:14:17,800
but we kept the way in which the fisher information matrix grows

1151
01:14:17,800 --> 01:14:20,720
right so

1152
01:14:20,730 --> 01:14:25,270
disingenious myself my name is there is always it is and i've been with accenture

1153
01:14:25,270 --> 01:14:27,850
just about a bit more than two years

1154
01:14:28,520 --> 01:14:36,910
and my previous experiences information retrieval we are actually i'll talk about because we actually

1155
01:14:36,910 --> 01:14:44,030
tech labs so accenture is the big consulting company and tech labs are small organisation

1156
01:14:44,030 --> 01:14:49,350
doing applied research and therefore four locations who are in north america in chicago in

1157
01:14:49,350 --> 01:14:54,510
silicon valley and one instance of young people is france where i come from and

1158
01:14:54,510 --> 01:14:59,110
we have one in india so the type of things we do is applied research

1159
01:14:59,110 --> 01:15:04,130
that motivated with real business problems and some focus areas machine learning in that to

1160
01:15:04,140 --> 01:15:10,450
mining that's my group so engineering collaboration knowledge management and a number of other topics

1161
01:15:10,470 --> 01:15:15,430
what i'm trying to do today is give you another view of

1162
01:15:15,970 --> 01:15:22,070
are not the case study what use cases how did we get the requirements and

1163
01:15:22,070 --> 01:15:24,720
one to do it shows the use cases that we did

1164
01:15:25,010 --> 01:15:30,270
as well as i try to present some technologies that we develop in scope of

1165
01:15:30,570 --> 01:15:36,910
active and how they're being used and if i have sometimes aluminum example of an

1166
01:15:36,910 --> 01:15:43,370
application that's not necessarily part of active but it deals with context switching and it

1167
01:15:43,370 --> 01:15:49,330
is actually deployed application which is inspired by real business problems

1168
01:15:51,250 --> 01:15:56,430
so it is you can probably make a stew heard probably by now activities in

1169
01:15:56,460 --> 01:15:59,830
the end aiming to increase the productivity of knowledge workers and is trying to do

1170
01:15:59,830 --> 01:16:07,350
this in a contextual in process backs task aware way and it's using a number

1171
01:16:07,350 --> 01:16:12,530
of technologies for this but what we are trying to the next being active accenture

1172
01:16:13,030 --> 01:16:18,270
we are trying to develop prototypes that integrate active technologies and some other technologies that

1173
01:16:18,270 --> 01:16:22,230
we develop the necessary with the end goal too

1174
01:16:22,310 --> 01:16:27,050
do exactly these invisibility we give knowledge workers so we are aiming at our own

1175
01:16:27,050 --> 01:16:32,910
workforce as well as in some long around we're aiming at their clients and the

1176
01:16:32,910 --> 01:16:38,150
goal is to assure that the technologies developed in active are in line we need

1177
01:16:38,160 --> 01:16:41,930
real world business problems

1178
01:16:42,270 --> 01:16:45,950
OK so i'll talk about the use cases

1179
01:16:45,970 --> 01:16:49,110
so um

1180
01:16:49,970 --> 01:16:55,410
big companies like accenture there are number of business processes and tasks to perform every

1181
01:16:55,410 --> 01:17:01,630
day but by by the employees and their range in types like proposal right so

1182
01:17:01,630 --> 01:17:10,850
all big companies consulting companies specifically to do proposal writing then marketing selling products for

1183
01:17:10,850 --> 01:17:17,730
instance requirement analysis software companies need to have a process of how requirements analysed in

1184
01:17:17,730 --> 01:17:24,350
order to develop the products the this the same goes for some protesting then companies

1185
01:17:24,350 --> 01:17:26,250
need to know how

1186
01:17:26,290 --> 01:17:31,570
they approach training their own workers their own employees so all this about to these

1187
01:17:31,570 --> 01:17:35,170
different business processes that because it's in company

1188
01:17:37,810 --> 01:17:43,470
there is different and specialised interfaces in company for each one of these

1189
01:17:43,690 --> 01:17:49,910
processes that for application that support this process is what is common to most of

1190
01:17:49,910 --> 01:17:55,270
them is that in why one way or another either directly or indirectly there's need

1191
01:17:55,530 --> 01:18:01,250
to access the central repository why central repository because this is the main it's into

1192
01:18:01,320 --> 01:18:08,850
the document centric knowledge management system that most company have implemented and

1193
01:18:08,870 --> 01:18:15,490
for instance for proposal writing the process will ask whoever is writing the document to

1194
01:18:15,490 --> 01:18:21,150
go to the repository to find similar documents it will ask to find people

1195
01:18:21,650 --> 01:18:25,450
it will ask to find similar vendors with which we have worked in the past

1196
01:18:25,450 --> 01:18:31,150
so in why when one way or another our employees are actually faced with having

1197
01:18:31,170 --> 01:18:37,970
to the central repository numerous times for different tasks so we decided to make

1198
01:18:38,190 --> 01:18:44,430
it's one of the processes this of interest for us and not that specific knowledge

1199
01:18:44,430 --> 01:18:49,810
enquiry what does this mean this this is basically

1200
01:18:49,830 --> 01:18:55,630
supporting all of these acts of the accenture employees that go to why central repository

1201
01:18:55,630 --> 01:19:02,190
numerous times for different reasons and for example that could be is no executive that

1202
01:19:02,190 --> 01:19:08,250
is writing proposals and he might you know all about his particular small domain which

1203
01:19:08,250 --> 01:19:13,570
is an expert but you might not know what's happening what is accenture doing within

1204
01:19:13,570 --> 01:19:18,670
the whole area in the industry so he would go to the repository find information

1205
01:19:19,170 --> 01:19:23,050
he will look at what similar products have been done for the same client maybe

1206
01:19:23,050 --> 01:19:30,010
he would look at what's what's new what's new products going happening and

1207
01:19:30,030 --> 01:19:32,050
usually people refer to

1208
01:19:32,070 --> 01:19:36,460
the internet repository and when they cannot find any information to go to the person

1209
01:19:36,460 --> 01:19:39,230
that person that always people for help

1210
01:19:41,030 --> 01:19:43,830
another very important process

1211
01:19:43,870 --> 01:19:50,150
actually go through the processes probably happening the most often in the consultants consulting companies

1212
01:19:50,150 --> 01:19:53,150
proposal writing and

1213
01:19:53,250 --> 01:19:58,380
this is an and this is a very important part of accenture's work because

1214
01:19:58,450 --> 01:20:03,470
most of our work is project based which means that

1215
01:20:03,490 --> 01:20:10,170
a client has the problem he writes a request for information request proposals and accenture

1216
01:20:10,170 --> 01:20:15,950
responds with a document and this document should contain it's the proposal is it's why

1217
01:20:15,950 --> 01:20:18,990
are we the best people to be chosen to do this work

1218
01:20:19,330 --> 01:20:23,960
what kind of credentials we haven't kind similar work we had that we have done

1219
01:20:23,970 --> 01:20:28,550
in this area in the past how well that would how however solve the problem

1220
01:20:28,550 --> 01:20:29,530
so i guess

1221
01:20:29,540 --> 01:20:33,890
because that is yes with respect to the original formulation the problem where we have

1222
01:20:33,890 --> 01:20:36,150
multiple levels in the graph

1223
01:20:36,200 --> 01:20:37,690
corresponding to

1224
01:20:37,720 --> 01:20:38,810
different labels

1225
01:20:38,820 --> 01:20:43,330
yes of course they in now the l labels and then graphs the the new

1226
01:20:43,330 --> 01:20:44,780
graph has an l

1227
01:20:44,840 --> 01:20:48,890
plus two nodes the the source and sink so yes it certainly be graph so

1228
01:20:48,890 --> 01:20:50,590
that the more work to do

1229
01:20:50,640 --> 01:20:54,260
and i guess what you'd expect in alpha expansion

1230
01:20:54,280 --> 01:20:55,700
no because

1231
01:20:55,750 --> 01:20:57,900
at every step of for expansion

1232
01:20:57,910 --> 01:21:01,430
you just have a binary labeling problem on the original graph to do so each

1233
01:21:02,230 --> 01:21:06,770
as the same complexity as one segmentation if you like in the previous one but

1234
01:21:06,770 --> 01:21:09,860
of course we now have to many steps and in order to get the results

1235
01:21:09,900 --> 01:21:10,790
you have to

1236
01:21:10,870 --> 01:21:13,580
cycle through the labels in some order

1237
01:21:13,590 --> 01:21:15,430
several times

1238
01:21:15,480 --> 01:21:18,740
where several turns out to be a few right you know

1239
01:21:18,750 --> 01:21:23,440
less than ten if not hundreds of thousands in some iterative methods and the reason

1240
01:21:23,440 --> 01:21:27,450
for that is i guess intuitively because OK this is the beauty of alpha expansion

1241
01:21:27,780 --> 01:21:33,320
compared with other iterative methods like belief propagation is that each iteration is doing something

1242
01:21:34,140 --> 01:21:40,390
the belief propagation and iteration can only extend you know the influence of the modification

1243
01:21:40,390 --> 01:21:42,060
of the solution if you like

1244
01:21:42,100 --> 01:21:43,470
like away from

1245
01:21:43,480 --> 01:21:47,380
you know it moves like away from one one set of the time to move

1246
01:21:47,380 --> 01:21:52,110
along the way have an influence over the entire image you have many many iterations

1247
01:21:52,120 --> 01:21:57,200
with rock that's not the case although it's not you know where alpha expansion is

1248
01:21:57,200 --> 01:21:59,670
the method itself is not finding a global optimum

1249
01:21:59,730 --> 01:22:00,780
each step

1250
01:22:00,790 --> 01:22:05,540
it's finding a global optimum and that global optimum involves long-range interactions so you know

1251
01:22:05,540 --> 01:22:10,100
intuitively that's why alpha expansion might be really good in cases where belief propagation is

1252
01:22:16,240 --> 01:22:19,220
is another thing which i think probably skip as well but just to say

1253
01:22:19,240 --> 01:22:21,580
well there's is an energy bound you can get

1254
01:22:21,590 --> 01:22:25,930
four alpha expansion so you know even though i'm saying the strong guarantees about global

1255
01:22:25,930 --> 01:22:30,850
optimality only apply to individuals that's not the problem as a whole there is a

1256
01:22:30,850 --> 01:22:34,490
guarantee that applies the problem as a whole which says that the energy that you

1257
01:22:34,490 --> 01:22:36,840
achieve is within a factor of two

1258
01:22:36,860 --> 01:22:38,720
of the optimal energy

1259
01:22:38,720 --> 01:22:41,230
or actually a factor two and that

1260
01:22:41,280 --> 01:22:44,440
depends on the details of the problem but

1261
01:22:44,910 --> 01:22:46,750
it turns out

1262
01:22:47,000 --> 01:22:50,360
you know when you look at the examples that's not useful that

1263
01:22:50,420 --> 01:22:55,620
so you know the difference in energy and energy in real problems are you know

1264
01:22:55,630 --> 01:22:58,190
relatively tiny far less than a factor of two

1265
01:22:59,620 --> 01:23:03,910
but yet those small energy differences matter in other words we look what happened to

1266
01:23:03,930 --> 01:23:05,060
the solution

1267
01:23:05,070 --> 01:23:09,610
two putative solution is energy is some some small facts much less than to greater

1268
01:23:09,610 --> 01:23:13,200
than the optimality the solution is all wrong all over the place

1269
01:23:13,320 --> 01:23:17,370
we care about energy difference is much much less than a factor of two so

1270
01:23:17,370 --> 01:23:22,060
you technically there's no guarantee but not going to get you anywhere sorry

1271
01:23:22,060 --> 01:23:26,740
again you know that's like you probably had experienced many times in other domains

1272
01:23:30,020 --> 01:23:33,780
let's get back to stereo because one of the things we really want to do

1273
01:23:33,830 --> 01:23:35,750
i'm not going to be able to deal with

1274
01:23:35,760 --> 01:23:41,630
the stereo problem in the generality it deserves but promised me no i promise you

1275
01:23:41,840 --> 01:23:46,560
say i promise you you can use graph cuts to solve stereo in quite some

1276
01:23:46,560 --> 01:23:51,770
detail that respects all of the detailed geometric constraints the there are that argue that

1277
01:23:51,890 --> 01:23:57,870
labour that but i can't quite tell you the entire thing very simplest formulation of

1278
01:23:57,900 --> 01:24:02,220
stereo which is not really satisfactory but it's a natural extension of what we discussed

1279
01:24:02,610 --> 01:24:04,720
in dynamic programming remember we said

1280
01:24:04,770 --> 01:24:06,570
dynamic programming

1281
01:24:06,580 --> 01:24:07,210
but should be

1282
01:24:07,250 --> 01:24:10,330
dynamic programming patient problem here as well

1283
01:24:10,370 --> 01:24:15,340
it looks just like this but the gene function they dealt only with neighbouring pixels

1284
01:24:15,340 --> 01:24:18,340
along an epipolar line

1285
01:24:18,350 --> 01:24:21,560
what we said looking at the solutions to that

1286
01:24:21,560 --> 01:24:26,040
the program problem is and this is a pity because neighbouring epipolar lines don't get

1287
01:24:26,040 --> 01:24:32,190
along quite frequently see artifacts arising where an entire epipolar line gets misaligned if only

1288
01:24:32,190 --> 01:24:35,060
so forth who gives you adapt graph right

1289
01:24:35,610 --> 01:24:39,650
so as i think there be at least one talk on friday which which actually

1290
01:24:39,650 --> 01:24:41,580
gets into one of the popular methods

1291
01:24:42,310 --> 01:24:43,310
so if you stick around

1292
01:24:44,670 --> 01:24:46,230
one of my favourite methods using

1293
01:24:46,940 --> 01:24:50,540
so that we can actually recover structures of these graphical model the dependencies

1294
01:24:50,940 --> 01:24:54,830
i'm unable to find new then variables and ask you which things depend on which other things

1295
01:24:55,270 --> 01:24:58,060
we have made progress as a community on on those as well

1296
01:24:59,710 --> 01:25:01,440
so i'm going to sort of our

1297
01:25:01,860 --> 01:25:03,000
give an example

1298
01:25:04,290 --> 01:25:06,130
of an undirected graphical model

1299
01:25:07,580 --> 01:25:11,500
we illustrate how much these algorithms scale and what they can do for you

1300
01:25:12,000 --> 01:25:14,650
right so so we look at the problem of drug detection

1301
01:25:16,730 --> 01:25:20,290
we are going to look at so so detecting about you know a couple of

1302
01:25:20,290 --> 01:25:24,850
my climate colleagues are very interested in specially significant doubts which last for like five

1303
01:25:24,850 --> 01:25:25,730
years or more

1304
01:25:26,250 --> 01:25:28,940
or period of time and spatial

1305
01:25:30,250 --> 01:25:35,690
because that's unusual that shouldn't have happened they want to understand the climate conditions under which the sort of

1306
01:25:36,150 --> 01:25:39,540
happening our is probably the most widely studied dropping it

1307
01:25:39,960 --> 01:25:43,290
it started in the late sixties and went on for a very very long time

1308
01:25:44,130 --> 01:25:47,500
this is the region in between the sahara and the south are

1309
01:25:49,060 --> 01:25:54,480
that that's what is another popular example in in the in north america and so on and so on

1310
01:25:54,920 --> 01:25:55,900
what we did was

1311
01:25:57,330 --> 01:26:02,130
we talk about this he argued it has this is the precipitation data it's difficult

1312
01:26:02,130 --> 01:26:05,860
to get precipitation data is what i'm understanding but this is one of the good

1313
01:26:05,860 --> 01:26:07,360
ones that people use

1314
01:26:07,810 --> 01:26:12,380
and we have data from nineteen one to two thousand six is the said we

1315
01:26:12,380 --> 01:26:16,500
used to have a resolution of twenty five degrees latitude longitude right so

1316
01:26:18,540 --> 01:26:23,940
this actually gives u precipitation value at orlando locations and we use the dataset

1317
01:26:25,380 --> 01:26:27,810
what we're going to try to do using this dataset

1318
01:26:28,210 --> 01:26:29,810
is we want to try to detect

1319
01:26:30,750 --> 01:26:32,480
these significant droughts

1320
01:26:33,170 --> 01:26:35,500
using a markov enough random field framework

1321
01:26:35,900 --> 01:26:36,210
right so

1322
01:26:36,900 --> 01:26:38,860
so at any given point of time

1323
01:26:39,580 --> 01:26:43,170
you can assume that you have this lattice latitude longitude

1324
01:26:44,150 --> 01:26:45,130
this is each location

1325
01:26:45,900 --> 01:26:48,750
and each location can be in one of two states

1326
01:26:49,250 --> 01:26:49,650
right so

1327
01:26:50,310 --> 01:26:52,350
its location can be in one of two states

1328
01:26:54,460 --> 01:26:55,060
are dropped

1329
01:26:55,980 --> 01:26:57,690
right it's it's it's it's a billion

1330
01:26:58,590 --> 01:26:59,090
be able

1331
01:27:00,080 --> 01:27:05,610
i'm saying i'm contacting them because this is normal and actually have some dependency depending

1332
01:27:05,610 --> 01:27:08,460
on what the condition this thing so that you so i don't want to make

1333
01:27:08,460 --> 01:27:10,520
independent predictions on each one of them

1334
01:27:11,040 --> 01:27:11,460
right and

1335
01:27:12,360 --> 01:27:16,080
i'm not going to show you the results we have tested at u just get

1336
01:27:16,380 --> 01:27:18,000
a very nice results if you're doing

1337
01:27:19,810 --> 01:27:23,320
so we are trying to maintain some dependencies at any point of time this is

1338
01:27:23,320 --> 01:27:27,810
sort of a snapshot of the great but each variable is the latent variable which

1339
01:27:27,810 --> 01:27:30,520
i don't know about his as you go along with it's it's

1340
01:27:31,150 --> 01:27:33,610
it's it's normal are it's drought conditions

1341
01:27:34,810 --> 01:27:37,730
then they replicate this thing or one hundred and six years

1342
01:27:40,040 --> 01:27:46,210
so far this toy example i'm i'm cooking up a markov random fields which have the statistical dependencies

1343
01:27:47,560 --> 01:27:53,790
i have no arrows and and columns and three and so that's basically each such

1344
01:27:54,670 --> 01:27:58,040
the time scale by a twelve five timesteps

1345
01:27:58,540 --> 01:28:00,020
that's a total of sixty

1346
01:28:00,860 --> 01:28:01,960
william variables

1347
01:28:02,420 --> 01:28:07,130
which can be in one of hundred sixty states right so of sixty billion variables

1348
01:28:07,670 --> 01:28:09,650
and which can be in one of two to sixty state

1349
01:28:10,400 --> 01:28:13,520
given observations on these points one set of observations

1350
01:28:13,980 --> 01:28:17,520
i'm trying to figure out which one of these two to the sixty states it

1351
01:28:17,520 --> 01:28:21,170
is in what what was likely explains what you've just rub salt

1352
01:28:23,610 --> 01:28:24,480
that's a hard problem

1353
01:28:25,750 --> 01:28:28,110
to make matters worse we actually using this year

1354
01:28:28,630 --> 01:28:35,230
he said well they developed number don't be seven million states which means that all of other seven million

1355
01:28:36,150 --> 01:28:40,210
seven million william labels which means that the total of total seven million states and

1356
01:28:40,210 --> 01:28:42,630
i'm trying to solve the maximization problem on this

1357
01:28:43,690 --> 01:28:45,420
i'm give you a sense of the scale

1358
01:28:46,080 --> 01:28:47,560
of what we can handle

1359
01:28:48,000 --> 01:28:52,060
using distributive law methods right so so this is not

1360
01:28:52,560 --> 01:28:58,080
your car use i acid run one hundred data points we can handle set up

1361
01:28:58,120 --> 01:29:01,490
an integer programme with seven million william variables

1362
01:29:02,440 --> 01:29:03,980
and the way we do this

1363
01:29:04,740 --> 01:29:08,030
is a bit complicated but it uses the distributive law

1364
01:29:08,490 --> 01:29:13,320
we essentially to a linear programming relaxation in a sense we are actually solving a linear programme

1365
01:29:14,490 --> 01:29:20,060
with seven million variables and while solving the linear program we use the message passing algo

1366
01:29:20,660 --> 01:29:24,350
right and that's why the fastest way of doing it if you use a commercial

1367
01:29:24,350 --> 01:29:29,130
package for doing this it probably not far we got to and then died because

1368
01:29:29,130 --> 01:29:33,450
it's it's not designed to handle this class somebody this scale of problems

1369
01:29:35,310 --> 01:29:39,360
something new with graphical model inference is something we can do this is the scale

1370
01:29:39,370 --> 01:29:41,090
we can do i'm actually going to show you know

1371
01:29:41,570 --> 01:29:44,240
sort of the f one of the outputs we got

1372
01:29:45,170 --> 01:29:49,580
so we can you know we can do this for droughts we can do it for other things as well

1373
01:29:50,240 --> 01:29:55,040
some other people are interested in sort of heat waves are precipitation extremes of things

1374
01:29:55,040 --> 01:29:57,620
so that's are and we can sort of handle bad

1375
01:29:58,120 --> 01:30:01,620
now we have reached a point that we've got all of these results and this

1376
01:30:01,620 --> 01:30:04,530
is sort of what we call the decoding the map inference that has happened

1377
01:30:05,080 --> 01:30:06,160
think of it as has

1378
01:30:06,630 --> 01:30:13,280
like a big big version of viterbi decoding but run on on bad many millions of variables

1379
01:30:13,770 --> 01:30:17,410
and if you want to sort of look at individual things whether it makes sense or not

1380
01:30:17,950 --> 01:30:19,480
this is what the water fountains there

1381
01:30:20,480 --> 01:30:24,670
it starting in the nineteen seventies and this is the reason you can go back and look up

1382
01:30:25,350 --> 01:30:29,020
we know that this the span nutritional so here this is exactly what it far right

1383
01:30:29,870 --> 01:30:32,130
you can add more variables like

1384
01:30:32,530 --> 01:30:37,740
five much content and other things that you think are relevant to improving the prediction of this problem

1385
01:30:38,360 --> 01:30:42,740
one thing you have to what is that we are not getting those red dots here and there

1386
01:30:43,360 --> 01:30:46,520
which is what will get if you just short based on the precipitation

1387
01:30:47,090 --> 01:30:51,570
right one month you want a little bit of low precipitation a purely thresholding based

1388
01:30:51,570 --> 01:30:53,210
approach will give you a dead

1389
01:30:53,780 --> 01:30:55,150
dots all over the place

1390
01:30:56,230 --> 01:31:00,060
and what this model actually doesn't that you don't have to worry about it if

1391
01:31:00,070 --> 01:31:02,120
you know if you don't have to worry about it

1392
01:31:02,770 --> 01:31:07,280
okay so so we have plenty of results and i'm not going to bore you you know sort of what

1393
01:31:08,060 --> 01:31:11,200
going to all of these but we found almost all of the major ones that

1394
01:31:11,200 --> 01:31:17,240
are known some ones which are pretty interesting which we're looking at in closer detail

1395
01:31:18,130 --> 01:31:21,670
and but this is what you can do so must be wondering seven million variables

1396
01:31:23,120 --> 01:31:27,030
half long it takes it takes about hour and i think we should be able

1397
01:31:27,030 --> 01:31:28,700
to bring it down to about a couple of minutes

1398
01:31:29,540 --> 01:31:32,160
within a month or so we are working on another variant of it

1399
01:31:32,160 --> 01:31:33,520
high confidence

1400
01:31:33,530 --> 01:31:37,000
it is compatible with negative correlation

1401
01:31:37,010 --> 01:31:39,360
we will see this later on

1402
01:31:39,410 --> 01:31:44,930
this is because you have to normalize obviously you've going to taken as it used

1403
01:31:44,930 --> 01:31:49,450
to normalize these two normalise new lows the orientation of the implications

1404
01:31:50,120 --> 01:31:52,730
and then what and then

1405
01:31:52,780 --> 01:31:54,370
about fifty two

1406
01:31:54,390 --> 01:31:59,890
one hundred substitute measures have been proposed it and no one is a clear winner

1407
01:32:00,080 --> 01:32:05,960
sticking to confidence but because for

1408
01:32:08,980 --> 01:32:12,830
do we mean by data mining this is famous for that you can read from

1409
01:32:12,830 --> 01:32:18,830
the distance of the query them the patterns that appear in there i was showing

1410
01:32:18,830 --> 01:32:23,930
i mean this is one first difference of machine learning with data mining to an

1411
01:32:23,930 --> 01:32:25,460
audience of

1412
01:32:25,470 --> 01:32:28,160
data mining and you have to show it this way

1413
01:32:28,230 --> 01:32:31,650
know this of machine learning you have to show in the way

1414
01:32:31,680 --> 01:32:33,870
and the content is the same

1415
01:32:33,930 --> 01:32:40,240
a few names and this to but they want these images they want something that

1416
01:32:40,240 --> 01:32:42,300
speaks to the

1417
01:32:42,330 --> 01:32:45,730
right brain hemisphere

1418
01:32:45,770 --> 01:32:50,600
and this is constant in many

1419
01:32:50,610 --> 01:32:54,590
i steps this is also my fiance proportions

1420
01:32:54,600 --> 01:32:56,730
selection of the process

1421
01:32:56,740 --> 01:33:00,570
preprocessing of the transformation modelling

1422
01:33:00,650 --> 01:33:03,110
obtaining models are part

1423
01:33:03,160 --> 01:33:07,560
validation and professional degree of the mode something and here with me

1424
01:33:10,240 --> 01:33:12,140
modelling point

1425
01:33:12,150 --> 01:33:17,350
which is where the machine learning algorithms are going to play and therefore this is

1426
01:33:17,350 --> 01:33:22,210
the area where i would be mostly meeting

1427
01:33:22,220 --> 01:33:25,510
some people call data mining these whole things

1428
01:33:25,520 --> 01:33:31,270
some people call this whole thing knowledge discovery and for these data mining

1429
01:33:31,610 --> 01:33:37,570
there is not for general agreement so i only used the data mining

1430
01:33:37,590 --> 01:33:39,740
as the places

1431
01:33:39,750 --> 01:33:41,960
for instance in the title of today's talk

1432
01:33:41,980 --> 01:33:46,270
but not in the real things i prefer to use just modelling for this and

1433
01:33:46,270 --> 01:33:51,160
knowledge discovery process for the whole thing

1434
01:33:51,230 --> 01:33:56,140
this is not the only diagram there is this whole thing i'm going to create

1435
01:33:56,200 --> 01:33:58,200
a is similar

1436
01:33:58,210 --> 01:34:01,230
and only that it

1437
01:34:01,230 --> 01:34:06,200
it makes it into a senior books the data preparation the process of selection and

1438
01:34:06,200 --> 01:34:08,370
preprocessing and transformation

1439
01:34:09,900 --> 01:34:14,700
or maybe we don't rush at copying these you we have this and the

1440
01:34:14,730 --> 01:34:19,310
late we're going pdf

1441
01:34:19,320 --> 01:34:23,760
but they and a number of things first the business understanding and the understanding for

1442
01:34:26,280 --> 01:34:27,380
they should

1443
01:34:27,390 --> 01:34:28,660
and then

1444
01:34:28,690 --> 01:34:29,880
we need

1445
01:34:29,950 --> 01:34:36,730
which is what one wishes not to do very often

1446
01:34:36,740 --> 01:34:38,600
that's very good

1447
01:34:38,610 --> 01:34:42,020
as i say that i will be concentrating in

1448
01:34:42,030 --> 01:34:43,120
this works

1449
01:34:43,160 --> 01:34:54,030
but let me just spend one slide in the processing transformation that everybody

1450
01:34:55,910 --> 01:34:59,180
four the actual data mining projects

1451
01:35:00,280 --> 01:35:04,880
part of the time is devoted to the preparation data transformation data selection

1452
01:35:05,360 --> 01:35:10,160
and evaluation and the tiny portuguese devoted to modelling

1453
01:35:10,210 --> 01:35:12,930
but in any lecture on data mining like mine

1454
01:35:12,940 --> 01:35:16,690
or in any book in data mining as you might find around

1455
01:35:16,700 --> 01:35:19,990
ninety percent of it is devoted to modelling and just

1456
01:35:20,010 --> 01:35:23,210
tiny comments are given on the other face

1457
01:35:23,400 --> 01:35:25,580
and this is not a contradiction

1458
01:35:25,590 --> 01:35:30,250
why do you have to spend so much on the faces because there is that

1459
01:35:30,250 --> 01:35:32,780
there is no from the experience of others

1460
01:35:32,800 --> 01:35:35,120
and when she

1461
01:35:35,150 --> 01:35:39,850
the reason for which it is difficult to make the first it's difficult to communicate

1462
01:35:39,850 --> 01:35:42,640
with ideas and second it's difficult to go

1463
01:35:42,690 --> 01:35:44,350
the weight in practice

1464
01:35:45,270 --> 01:35:48,630
just one more of these and i really

1465
01:35:48,650 --> 01:35:49,720
i have a lot of

1466
01:35:49,930 --> 01:35:56,770
stories that they were can we talk a lot of stuff but

1467
01:35:56,820 --> 01:36:01,110
but what do we learn not that much

1468
01:36:01,160 --> 01:36:03,800
so there is collection at the moment

1469
01:36:05,350 --> 01:36:07,390
axillary or

1470
01:36:07,410 --> 01:36:09,300
the processing of information

1471
01:36:09,300 --> 01:36:11,840
the most we can do and this is what i am attempting at in this

1472
01:36:11,840 --> 01:36:15,310
slide is to warn you that

1473
01:36:15,330 --> 01:36:19,310
it doesn't it is not sufficient to say they will analyse

1474
01:36:19,310 --> 01:36:23,570
the interview with indian entrepreneurs who wanted to do my taxes from bangalore r

1475
01:36:23,760 --> 01:36:26,220
and the one wonder right my new

1476
01:36:26,850 --> 01:36:30,710
software from bangalore r and the one who wanted to read my x-rays from bangalore

1477
01:36:30,710 --> 01:36:34,800
r and the one wanted trace my lost luggage on delta airlines from bangalore

1478
01:36:35,590 --> 01:36:36,780
that's while i had

1479
01:36:37,170 --> 01:36:38,020
been sleeping

1480
01:36:38,790 --> 01:36:40,910
something really big had happened

1481
01:36:41,740 --> 01:36:42,820
and i couldn't explain

1482
01:36:43,860 --> 01:36:48,470
in fact i wish i could show you out-takes and interviews so be sitting across the table from jerry rho

1483
01:36:49,030 --> 01:36:50,660
on a company called emphasised that

1484
01:36:51,060 --> 01:36:55,490
it's part of a group based on four hundred thousand american tax returns from india last year

1485
01:36:56,280 --> 01:36:57,460
and i say that injury

1486
01:36:58,640 --> 01:36:59,520
what happened

1487
01:37:00,080 --> 01:37:00,940
i missed something

1488
01:37:01,450 --> 01:37:02,950
how can you be doing this

1489
01:37:04,140 --> 01:37:04,880
what happened

1490
01:37:06,400 --> 01:37:12,440
well the last interview we did was with a friend mine on the nilekani who heads emphasises sio which is

1491
01:37:13,620 --> 01:37:17,240
one of the premier indian high-tech companies and now and then i was sitting on

1492
01:37:17,240 --> 01:37:21,510
the couch outside his office before the filming began doing kind pre-interview with them

1493
01:37:22,080 --> 01:37:24,300
and one point is that any time i gotta tell you

1494
01:37:25,300 --> 01:37:29,320
the global economic playing field is being leveled

1495
01:37:30,410 --> 01:37:33,320
the global economic playing field is being leveled

1496
01:37:33,960 --> 01:37:35,080
and you're americans

1497
01:37:35,680 --> 01:37:36,430
are not ready

1498
01:37:38,090 --> 01:37:40,060
all i wrote that down my little notebook

1499
01:37:40,790 --> 01:37:44,100
the global economic playing field is being leveled

1500
01:37:45,290 --> 01:37:47,830
well after the interview i got my jeep o

1501
01:37:48,270 --> 01:37:52,590
and then went back to my hotel and the whole time back in the right

1502
01:37:52,870 --> 01:37:57,060
i kept rolling over in my head what inundated said that global

1503
01:37:57,570 --> 01:37:59,370
economic playing field

1504
01:37:59,920 --> 01:38:01,270
is being level

1505
01:38:02,560 --> 01:38:07,870
and finally occurred me when i'm done with saying was that the global economic playing field was being flattened

1506
01:38:10,210 --> 01:38:10,580
and then

1507
01:38:12,030 --> 01:38:12,950
and i thought of this book

1508
01:38:13,470 --> 01:38:15,050
i said to myself my god

1509
01:38:16,090 --> 01:38:18,030
he's telling me the world is flat

1510
01:38:19,120 --> 01:38:22,380
he's telling me the world is flat and he's citing this

1511
01:38:23,100 --> 01:38:26,650
as a great achievement in human development

1512
01:38:27,700 --> 01:38:29,120
that we've made the world

1513
01:38:29,900 --> 01:38:30,410
in fact

1514
01:38:31,760 --> 01:38:34,440
well i went back to my hotel i called my wife is exactly how it

1515
01:38:34,440 --> 01:38:37,320
happens that honey i'm gonna read a book called the world is flat

1516
01:38:39,450 --> 01:38:41,960
she thought i was stark raving mad

1517
01:38:43,640 --> 01:38:50,730
but i came home i called the publisher the new york times in my boss columns perpetrators i only

1518
01:38:51,600 --> 01:38:53,230
i gotta go on leave immediately

1519
01:38:53,740 --> 01:38:55,020
because my software

1520
01:38:55,710 --> 01:38:59,400
these updating the framework through which i am analyzing foreign affairs

1521
01:38:59,900 --> 01:39:04,340
these updated i'm seeing things out there i cannot understand and explain

1522
01:39:05,060 --> 01:39:06,600
if i don't go on leave immediately

1523
01:39:07,130 --> 01:39:10,570
i'm gonna write something really stupid and the new york times

1524
01:39:14,030 --> 01:39:15,580
it's a great way to get only if i have

1525
01:39:18,090 --> 01:39:19,320
very hard to say no so

1526
01:39:20,700 --> 01:39:22,810
so we worked out basically and down

1527
01:39:23,260 --> 01:39:25,320
i've eventually got three months' leave and don

1528
01:39:27,250 --> 01:39:29,060
during that time basically

1529
01:39:29,650 --> 01:39:35,310
between march fifteen on december fifteenth in a complete frenzy ov on

1530
01:39:35,870 --> 01:39:37,910
obsession really i wrote this book

1531
01:39:38,910 --> 01:39:43,520
and dumb let me just go through the first three chapters very quickly on

1532
01:39:44,430 --> 01:39:45,640
alpha chapters called

1533
01:39:46,150 --> 01:39:48,540
i think probably not while you were sleeping

1534
01:39:49,890 --> 01:39:55,980
end a big in the book by noting that christopher columbus set sail in fourteen ninety two looking fourie

1535
01:39:56,580 --> 01:39:57,820
shorter route to india

1536
01:39:58,810 --> 01:40:00,940
that's what columbus was going in fourteen ninety two

1537
01:40:01,570 --> 01:40:04,270
the muslim power so that they had blocked the overland routes

1538
01:40:04,680 --> 01:40:08,820
end i don't want to go around the horn of africa so columbus sailed west

1539
01:40:09,800 --> 01:40:11,750
he had then you know that enter the santa maria

1540
01:40:12,340 --> 01:40:13,730
he never defined indeed

1541
01:40:14,690 --> 01:40:17,360
but he calls the people he ran into indians

1542
01:40:17,980 --> 01:40:19,730
and we call them back to this day

1543
01:40:20,610 --> 01:40:24,560
and he came home and told his wife honey have accidentally discovered the world is round

1544
01:40:25,910 --> 01:40:28,860
i set of free india five hundred twelve years later

1545
01:40:29,930 --> 01:40:32,140
i knew just which direction i was going

1546
01:40:32,590 --> 01:40:34,200
i sailed east

1547
01:40:34,620 --> 01:40:36,950
i had moved tons business class

1548
01:40:37,320 --> 01:40:41,270
any cheap satellite that popped up in my seat told me exactly where it was

1549
01:40:41,270 --> 01:40:42,760
and i came home and told my wife

1550
01:40:43,330 --> 01:40:46,230
honey i've accidentally discovered the world is flat

1551
01:40:47,290 --> 01:40:51,210
and the first chapter really goes to all these encounters i had in india

1552
01:40:53,290 --> 01:40:54,770
let me to this conclusion

1553
01:40:55,380 --> 01:40:56,390
but unlike columbus

1554
01:40:57,110 --> 01:40:57,920
i kept going

1555
01:40:59,460 --> 01:41:01,130
i next went to delhi in china

1556
01:41:02,200 --> 01:41:02,760
the capital

1557
01:41:03,440 --> 01:41:04,060
for japan

1558
01:41:04,900 --> 01:41:08,030
town major town in northeastern china where thousands

1559
01:41:08,620 --> 01:41:09,760
upon thousands

1560
01:41:11,100 --> 01:41:13,030
japanese speaking chinese

1561
01:41:13,810 --> 01:41:16,650
are now running the backrooms writing the software

1562
01:41:17,650 --> 01:41:22,470
and doing the beat the business processes and from nature japanese multinationals

1563
01:41:22,930 --> 01:41:26,430
and major american multinationals formerly based in tokyo

1564
01:41:27,730 --> 01:41:33,900
let me repeat that in case you've missed heard me okay in light of the recent headlines from china

1565
01:41:34,600 --> 01:41:38,120
tens of thousands of japanese speaking chinese

1566
01:41:38,560 --> 01:41:39,120
we are today

1567
01:41:39,700 --> 01:41:46,500
running the backrooms of major japanese multinationals from delhi and wear it is now a

1568
01:41:46,500 --> 01:41:50,760
maybe would sign a one to the blue and is zero to the green and

1569
01:41:50,780 --> 01:41:53,250
finally squares fit

1570
01:41:53,260 --> 01:41:56,150
and then that would be a plane the third

1571
01:41:56,240 --> 01:41:59,200
the third dimension here would be the one of the euro and you have a

1572
01:41:59,200 --> 01:42:04,460
plane in failing threshold of sources point five that would provide a cut off point

1573
01:42:04,570 --> 01:42:09,750
decision boundary if the two classes have different misclassification cost the boundary might move

1574
01:42:09,760 --> 01:42:12,730
but it would still be the plane would provide the fit

1575
01:42:13,420 --> 01:42:18,620
if is a false alarm false dismissals problem where false alarm seven times more expensive

1576
01:42:19,780 --> 01:42:26,130
forces was also expensive false alarm that you might move the decision boundary to minimise

1577
01:42:26,130 --> 01:42:31,720
cost but it will come from that single fit playing in the model would be

1578
01:42:31,730 --> 01:42:34,070
your left hand is

1579
01:42:34,100 --> 01:42:39,730
the a function of x the constant weighted linear terms

1580
01:42:39,730 --> 01:42:43,280
and here i guess you have to label one minus one they have decision boundary

1581
01:42:43,280 --> 01:42:46,920
zero that's mentioned in the

1582
01:42:47,000 --> 01:42:51,750
now this is a straight line has its has its value people can do wonderful

1583
01:42:51,750 --> 01:42:54,610
things with aggression if you use the right input

1584
01:42:54,630 --> 01:42:59,830
but more flexibility would be very nice and decision trees provide

1585
01:42:59,840 --> 01:43:03,960
almost the completely opposite way of looking at things where

1586
01:43:03,980 --> 01:43:06,610
the first make split

1587
01:43:06,680 --> 01:43:11,620
and say on first solve the whole problem with one single question and typically a

1588
01:43:11,620 --> 01:43:18,270
single variable single threshold and try to see what split amongst all the variables

1589
01:43:18,280 --> 01:43:23,240
with best separates two classes so here t i o five

1590
01:43:23,240 --> 01:43:26,440
we're fit greater than five we call it

1591
01:43:27,140 --> 01:43:30,700
and it's less than five you would the

1592
01:43:35,470 --> 01:43:36,870
so read this

1593
01:43:36,890 --> 01:43:40,240
the yes goes to the left to read this

1594
01:43:41,220 --> 01:43:49,440
and then you continue iteratively looking at those two sides sort of independently so the

1595
01:43:49,440 --> 01:43:53,320
data is fresh look at the first look at all the variables and all spots

1596
01:43:53,320 --> 01:43:55,830
are possible on the two sides of the tree

1597
01:43:55,840 --> 01:43:59,740
and continued to carve the space of classifiers

1598
01:43:59,780 --> 01:44:01,290
with different

1599
01:44:03,400 --> 01:44:07,280
you get at the end something that's equivalent to an expert system rule

1600
01:44:07,380 --> 01:44:11,070
that if tn is in the range two five and PE is one of the

1601
01:44:11,070 --> 01:44:14,980
sets in one and two and three it's bad otherwise it's good would be a

1602
01:44:14,980 --> 01:44:18,650
summary of the decision tree

1603
01:44:19,500 --> 01:44:23,730
normally be given training data cap in the number of cases

1604
01:44:23,730 --> 01:44:26,810
here the output variable y for all x

1605
01:44:26,840 --> 01:44:28,480
former vector

1606
01:44:28,500 --> 01:44:34,530
the measured values of attributes winding response out x being predictors

1607
01:44:36,140 --> 01:44:37,280
your data

1608
01:44:37,330 --> 01:44:38,970
this is where

1609
01:44:38,970 --> 01:44:44,000
none statisticians suspicions disagree how to look at things statisticians think of there being the

1610
01:44:44,010 --> 01:44:49,490
true underlying distribution there and that you've somehow draw a random sample from the distribution

1611
01:44:49,500 --> 01:44:51,880
and i think it's a useful way looking at things

1612
01:44:53,070 --> 01:44:54,860
the computer scientist

1613
01:44:55,110 --> 01:44:59,010
as a rule don't have as much

1614
01:44:59,020 --> 01:45:03,110
until the work that they don't have as much respect for noise or uncertainty in

1615
01:45:03,120 --> 01:45:08,880
the number number of machine learning techniques remember early on would push their products there

1616
01:45:08,900 --> 01:45:12,780
the models until they could exactly answer all the training data

1617
01:45:12,850 --> 01:45:16,560
but it is you know that leads to overfit because you fitting the noise as

1618
01:45:16,560 --> 01:45:17,780
well as the

1619
01:45:18,970 --> 01:45:23,170
so there seems to be a much more common understanding of not wanting to model

1620
01:45:23,170 --> 01:45:27,710
to be too complex at some point in eating regularisation so

1621
01:45:31,260 --> 01:45:33,130
the two so the two methods

1622
01:45:33,190 --> 01:45:35,810
that will try to minimise the risk

1623
01:45:36,610 --> 01:45:39,390
there is a loss function averaged over the

1624
01:45:39,460 --> 01:45:41,700
the output variables and the fitted model

1625
01:45:41,710 --> 01:45:43,340
ordinary least

1626
01:45:43,370 --> 01:45:48,300
ordinal regression is the least squared error loss function

1627
01:45:49,130 --> 01:45:51,350
the minimum can be found

1628
01:45:51,360 --> 01:45:56,050
instantly with over all are with regression so that's one reason so popular is the

1629
01:45:56,050 --> 01:45:58,380
math good this kind of thing

1630
01:45:59,800 --> 01:46:05,550
drunk looking for skis outside the street lamps they ask him why

1631
01:46:05,570 --> 01:46:07,070
you probably lost in

1632
01:46:07,080 --> 01:46:12,340
in the bard inches the the lights better here so sometimes that's what happens with

1633
01:46:12,340 --> 01:46:15,280
using regression the methods so beautiful

1634
01:46:15,300 --> 01:46:19,880
that's used in places one it's not appropriate the trees are

1635
01:46:19,930 --> 01:46:26,540
and neural nets and clustering and support use heuristic algorithms to iterative nature

1636
01:46:26,630 --> 01:46:30,610
so there are much slower compared to

1637
01:46:30,620 --> 01:46:32,830
so let's look at his

1638
01:46:32,990 --> 01:46:34,400
here's an example

1639
01:46:34,400 --> 01:46:36,650
the third stage

1640
01:46:36,800 --> 01:46:38,990
take an x four

1641
01:46:38,990 --> 01:46:42,240
we use all this lot here to train with

1642
01:46:42,490 --> 01:46:45,820
we try to test rx four

1643
01:46:46,130 --> 01:46:48,110
we don't need for

1644
01:46:48,160 --> 01:46:51,430
type should we test using x four

1645
01:46:51,470 --> 01:46:53,990
how we produce

1646
01:46:54,050 --> 01:46:54,990
y four

1647
01:46:57,280 --> 01:47:02,150
from this trained estimator using that block of the data

1648
01:47:03,450 --> 01:47:06,320
so we now have finished the fourth stage

1649
01:47:06,400 --> 01:47:07,820
we carry on

1650
01:47:07,860 --> 01:47:10,110
the same thing for the fifth stage

1651
01:47:10,950 --> 01:47:13,610
we have a complete

1652
01:47:13,660 --> 01:47:14,740
set of

1653
01:47:16,240 --> 01:47:19,010
based on every

1654
01:47:19,030 --> 01:47:23,400
member of the sample

1655
01:47:27,110 --> 01:47:30,010
we can compare in some way

1656
01:47:30,010 --> 01:47:32,400
which is convenient to ask why

1657
01:47:32,470 --> 01:47:34,240
and z

1658
01:47:34,240 --> 01:47:40,610
OK that will give us some estimate of our generally called generalisation error

1659
01:47:40,650 --> 01:47:45,150
but it can be any measure performance you like

1660
01:47:45,160 --> 01:47:47,490
so that's what happens with

1661
01:47:47,490 --> 01:47:51,860
cross validation

1662
01:47:56,610 --> 01:47:58,450
it should be

1663
01:47:58,470 --> 01:48:00,070
that are not

1664
01:48:00,260 --> 01:48:02,840
it doesn't

1665
01:48:02,860 --> 01:48:05,220
i mean there is good practice you

1666
01:48:05,240 --> 01:48:08,700
come across bits of this in the lab this afternoon for instance

1667
01:48:09,320 --> 01:48:12,450
it's very important that there is no

1668
01:48:12,450 --> 01:48:14,900
irrelevant ordering

1669
01:48:14,900 --> 01:48:16,760
in your training samples

1670
01:48:16,780 --> 01:48:18,430
if you put everything in

1671
01:48:18,430 --> 01:48:23,740
if let's say it was a classification problem and you put all the class eighty

1672
01:48:23,740 --> 01:48:26,740
ones in the first half of the sample and order to be one the the

1673
01:48:26,740 --> 01:48:29,240
second of the sample

1674
01:48:29,380 --> 01:48:31,090
the whole thing will break down

1675
01:48:31,860 --> 01:48:34,240
typically one assumes

1676
01:48:34,260 --> 01:48:36,650
these are independent samples that here

1677
01:48:36,720 --> 01:48:37,570
and you would

1678
01:48:37,610 --> 01:48:40,550
randomly ordered them to stop in order to

1679
01:48:42,340 --> 01:48:44,800
artificial ordering

1680
01:48:44,840 --> 01:48:47,200
within the sample

1681
01:48:50,920 --> 01:48:56,240
we talk about in the lab this afternoon because you'll see this in action

1682
01:48:56,280 --> 01:48:58,450
OK so

1683
01:48:58,510 --> 01:49:02,110
all that fits very nicely into the linear in the parameters

1684
01:49:02,160 --> 01:49:04,700
type problem

1685
01:49:04,740 --> 01:49:09,010
mainly because we have a very nice property there was ten is already mentioned that's

1686
01:49:09,010 --> 01:49:11,680
this this idea of a unique minimum

1687
01:49:11,720 --> 01:49:16,530
so there's only one right answer for any given structure

1688
01:49:16,590 --> 01:49:20,780
any given choice of basis functions

1689
01:49:20,860 --> 01:49:21,860
you get

1690
01:49:21,880 --> 01:49:22,900
one answer

1691
01:49:22,900 --> 01:49:25,780
it's the best answer

1692
01:49:25,880 --> 01:49:30,180
but the question of how do we choose these basis functions which has already been

1693
01:49:31,470 --> 01:49:33,130
and there's been

1694
01:49:33,220 --> 01:49:39,010
answered by the fact that it's hard we don't know what the right answer is

1695
01:49:39,050 --> 01:49:40,280
OK so

1696
01:49:40,340 --> 01:49:42,720
one way we can do is to

1697
01:49:42,760 --> 01:49:44,550
try to adapt them

1698
01:49:44,570 --> 01:49:50,400
linear models essentially based on an idea of taking out data axes

1699
01:49:50,420 --> 01:49:52,530
pre processing them in some way

1700
01:49:52,550 --> 01:49:54,240
through the phys

1701
01:49:56,150 --> 01:49:59,130
that's a fixed function we treat it as fixed

1702
01:49:59,160 --> 01:50:00,650
o operation

1703
01:50:00,660 --> 01:50:02,470
and then

1704
01:50:02,510 --> 01:50:05,010
we estimate the parameters

1705
01:50:05,760 --> 01:50:10,240
typically the cost function is in some way benign that is

1706
01:50:10,260 --> 01:50:13,180
it has this nice property of the unique

1707
01:50:13,220 --> 01:50:15,800
that's why we like to

1708
01:50:16,570 --> 01:50:21,200
because it is easy optimisation problem and no surprises

1709
01:50:23,150 --> 01:50:25,920
as the dimension goes up

1710
01:50:25,970 --> 01:50:27,630
these problems can become

1711
01:50:28,920 --> 01:50:32,240
OK particularly if we look at the polynomial problem

1712
01:50:32,260 --> 01:50:34,150
then you have this

1713
01:50:34,210 --> 01:50:38,180
massive explosion in the number of terms as the dimension goes up so if you

1714
01:50:39,630 --> 01:50:44,220
example often give you a sixteen by sixteen bit map

1715
01:50:44,220 --> 01:50:46,200
OK and you want to

1716
01:50:46,590 --> 01:50:52,320
true true that in some picture processing problem with a cubic polynomial function

1717
01:50:52,360 --> 01:50:55,950
three million terms of nearly three million

1718
01:50:55,970 --> 01:50:57,760
OK most of which will be

1719
01:50:57,800 --> 01:50:59,880
totally redundant

1720
01:51:01,320 --> 01:51:04,200
you want it with a very very big

1721
01:51:05,090 --> 01:51:09,780
starting with a relatively modest picture processing problems

1722
01:51:10,510 --> 01:51:14,680
there is a downside to doing these linear things typically wind up with lots and

1723
01:51:14,680 --> 01:51:20,090
lots and lots of things that maybe you don't need

1724
01:51:20,090 --> 01:51:22,680
and we've got the arbitrary choices

1725
01:51:22,740 --> 01:51:23,930
things like

1726
01:51:24,490 --> 01:51:28,930
things like the which is that such degrees and we have to

1727
01:51:28,950 --> 01:51:32,610
i work with that maybe in cross validation strategy two to

1728
01:51:32,650 --> 01:51:34,950
to to make these choices

1729
01:51:34,950 --> 01:51:38,200
the question what is the best preprocessor

1730
01:51:38,260 --> 01:51:42,200
we think about it that way to choose

1731
01:51:47,160 --> 01:51:51,340
this is what we got as a kind of control really so this is what

1732
01:51:51,340 --> 01:51:52,880
i tend to think things

1733
01:51:52,900 --> 01:51:55,550
signal flow flow through block diagram

1734
01:51:55,630 --> 01:52:00,590
what we have is the input data going into some block which does

1735
01:52:00,650 --> 01:52:04,010
something five there

1736
01:52:04,050 --> 01:52:09,010
that goes into an effective layer which generates an output which is compared to the

1737
01:52:10,280 --> 01:52:11,700
generates an error

1738
01:52:11,700 --> 01:52:14,880
the air is fed back into the adaptive layer there

1739
01:52:14,930 --> 01:52:17,630
to find those w this way

1740
01:52:19,660 --> 01:52:21,650
one way forward

1741
01:52:21,740 --> 01:52:24,360
be instead of trying to pick

1742
01:52:24,470 --> 01:52:26,510
fixed pre processing layer

1743
01:52:26,570 --> 01:52:28,420
using some

1744
01:52:29,740 --> 01:52:30,570
this go

1745
01:52:30,570 --> 01:52:32,430
straight for the jugular

1746
01:52:32,470 --> 01:52:35,570
and try to adapt it

1747
01:52:35,590 --> 01:52:38,950
directly in the same way as you're adapting

1748
01:52:38,990 --> 01:52:40,760
the output layer

1749
01:52:40,780 --> 01:52:43,090
optimizing the output layer

1750
01:52:43,180 --> 01:52:46,050
what about adapting or optimizing

1751
01:52:46,110 --> 01:52:50,340
this preprocessing

1752
01:52:50,380 --> 01:52:53,680
and that's what we talk about the rest of this

1753
01:52:53,740 --> 01:52:58,220
OK the multilayer perceptron is the archetypal

1754
01:52:58,260 --> 01:53:02,360
version of this approach is not the only one but it's the most

