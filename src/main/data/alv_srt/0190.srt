1
00:00:00,000 --> 00:00:02,270
OK so now we have these

2
00:00:02,290 --> 00:00:06,020
i suppose q r so are

3
00:00:06,040 --> 00:00:11,020
formulation we use a vector and matrix formulation instead of summations

4
00:00:11,040 --> 00:00:13,710
and this is really much of

5
00:00:14,390 --> 00:00:20,210
but there's also linear term has been used to you

6
00:00:20,230 --> 00:00:22,080
we also that

7
00:00:22,100 --> 00:00:23,210
in two days

8
00:00:23,230 --> 00:00:25,330
but if you look at these

9
00:00:25,330 --> 00:00:26,250
so if you

10
00:00:26,270 --> 00:00:31,060
if you move w into this position will not be here

11
00:00:31,080 --> 00:00:36,160
so this is what is is is an agent of what exactly what was it

12
00:00:36,790 --> 00:00:38,830
this is not about

13
00:00:38,850 --> 00:00:42,620
so what you have this relationship as you are minus

14
00:00:42,640 --> 00:00:47,710
my so you might start by transport stop

15
00:00:47,730 --> 00:00:56,230
he held that stop minus the the whole that transport that so hard his minus

16
00:00:56,250 --> 00:01:01,700
miners help that transport system so this is a new hundred

17
00:01:01,700 --> 00:01:03,930
but we also have a linear content

18
00:01:03,980 --> 00:01:12,500
this one is minus one minus summation so we have the summation of miners mines

19
00:01:12,500 --> 00:01:15,620
becomes class therefore

20
00:01:15,640 --> 00:01:19,520
we have is some mention of five minus the some

21
00:01:19,620 --> 00:01:24,980
so this is already very close

22
00:01:25,020 --> 00:01:30,560
two our new optimization problems is remember we are doing maximisation here

23
00:01:30,870 --> 00:01:33,830
so if you reverse it to minimisation

24
00:01:33,870 --> 00:01:41,430
then that's already held times up transpose q of minus some up really our the

25
00:01:41,430 --> 00:01:47,640
objective function of doing that we mention earlier so we almost

26
00:01:47,660 --> 00:01:51,750
so we still have to situations that

27
00:01:51,750 --> 00:01:58,540
well it was it was easier or what is the sum is not zero

28
00:01:59,180 --> 00:02:00,930
the tree using doing this

29
00:02:00,960 --> 00:02:05,810
maximisation is where you believe maximisation right

30
00:02:06,700 --> 00:02:09,810
then you can see that all different kinds of off

31
00:02:09,830 --> 00:02:12,440
labour force another they give you

32
00:02:12,500 --> 00:02:14,930
objective minus infinity

33
00:02:14,960 --> 00:02:21,770
maximisation so of course i was thinking there can never be your optimum objective

34
00:02:21,830 --> 00:02:27,520
the other thing is that that you do with the maximisation so of course minus

35
00:02:27,520 --> 00:02:31,290
infinity then it is not maximum over do

36
00:02:33,180 --> 00:02:34,580
so that means

37
00:02:34,600 --> 00:02:42,120
do optimum solution does not have which is something why principles have is zero

38
00:02:42,140 --> 00:02:45,100
therefore we only have two

39
00:02:45,160 --> 00:02:48,810
so we don't have to consider the situation at all

40
00:02:49,100 --> 00:02:54,200
we have to consider the situation we see the first situation

41
00:02:55,930 --> 00:03:01,750
you will be considered for this you becomes a kind of conditions so that we

42
00:03:01,930 --> 00:03:03,640
can strengthen over to do

43
00:03:03,660 --> 00:03:05,750
so we moved these to be

44
00:03:05,750 --> 00:03:07,870
the constraints of these

45
00:03:08,080 --> 00:03:14,120
i optimisation problem so we so writing these and also have greater or equal to

46
00:03:14,120 --> 00:03:15,500
zero then

47
00:03:15,520 --> 00:03:17,710
we have to deal with constraints

48
00:03:17,730 --> 00:03:23,580
and then we this maximisation with this is the same as the object the objective

49
00:03:23,580 --> 00:03:29,020
function that we mention already city already to do we get

50
00:03:29,040 --> 00:03:36,660
so this is the typical way of deriving the dual problem we want say i

51
00:03:36,660 --> 00:03:38,560
want to mention here is

52
00:03:40,120 --> 00:03:42,180
but if you look at some papers

53
00:03:42,200 --> 00:03:50,390
little use bill is a bill exactly the same way as i using here

54
00:03:50,440 --> 00:03:51,810
two weeks

55
00:03:51,830 --> 00:03:56,910
two explain that something quite in the dual problem is what principles are much should

56
00:03:56,910 --> 00:03:59,540
be easier for some people this

57
00:03:59,560 --> 00:04:02,600
so the the lagrangian function

58
00:04:03,600 --> 00:04:08,270
they know that they're going to minimize with respect to w and b so the

59
00:04:08,270 --> 00:04:11,750
partial derivative with respect to w and also be

60
00:04:11,980 --> 00:04:14,640
just like we do

61
00:04:14,660 --> 00:04:16,620
we took down here so

62
00:04:16,640 --> 00:04:23,850
they just show color the derivative with respect to be the linear ten meters

63
00:04:23,870 --> 00:04:25,250
the these the

64
00:04:25,250 --> 00:04:28,730
this these this is the linear tend to be

65
00:04:28,750 --> 00:04:33,210
so if the partial derivative you get some measure of i y and it is

66
00:04:33,210 --> 00:04:36,980
the oldest for that to be easier well that's not me

67
00:04:37,000 --> 00:04:43,350
right the following these these these derivations you can do that and the reason is

68
00:04:45,460 --> 00:04:50,230
because of this i mean if really is this is not the venue to minus

69
00:04:50,230 --> 00:04:55,820
infinity and later we have minus infinity is is not going to be looking at

70
00:04:55,820 --> 00:04:59,100
the problem do therefore it is not considered

71
00:04:59,310 --> 00:05:05,230
because two partial derivative you need to have a strictly convex function of the variables

72
00:05:05,520 --> 00:05:12,140
but our objective function is only street strictly convex on w but the if you

73
00:05:12,140 --> 00:05:17,770
have been used to not be it is not strictly convex so

74
00:05:17,790 --> 00:05:20,000
in this derivation usually

75
00:05:20,160 --> 00:05:24,410
if you see some people they just the partial derivative with respect to be aware

76
00:05:24,410 --> 00:05:25,980
that this not very

77
00:05:26,290 --> 00:05:30,080
very correct away

78
00:05:30,100 --> 00:05:39,810
then i want to say a little bit more about dual problems so it is

79
00:05:39,830 --> 00:05:44,310
it is a popular technique so quite a few people just think that for any

80
00:05:44,310 --> 00:05:48,200
optimisation problem is that do exist

81
00:05:48,200 --> 00:05:53,870
and also strong duality holds what is actually wrong

82
00:05:53,890 --> 00:05:59,310
in optimisation series there's nothing like that

83
00:05:59,330 --> 00:06:05,960
we only have serious saying that in this optimisation problem is a convex problem to

84
00:06:05,960 --> 00:06:12,290
convex programming makes the objective function is convex and also the feasible region feasible region

85
00:06:12,290 --> 00:06:18,370
is the set of points satisfying all constraints so this set is also

86
00:06:18,370 --> 00:06:24,120
is also convex we have a convex programming problems and not enough

87
00:06:25,660 --> 00:06:30,160
there's something called constraint qualification so you're constraints

88
00:06:30,180 --> 00:06:35,370
it must to satisfy certain conditions only for this kind of problems

89
00:06:35,370 --> 00:06:38,390
then you will have strong duality

90
00:06:38,410 --> 00:06:43,040
you can just write a new optimisation problem and then just write range in the

91
00:06:43,040 --> 00:06:45,580
USA all strong duality holds

92
00:06:45,820 --> 00:06:46,850
that's right

93
00:06:47,520 --> 00:06:49,100
so that we

94
00:06:49,100 --> 00:06:56,460
we will for our optimisation problem it satisfies all of these first this optimisation problem

95
00:06:56,460 --> 00:07:02,160
is a convex it is strictly convex and but also colleagues on the acidic variables

96
00:07:02,500 --> 00:07:08,830
so it's is convex so we are full of first condition than another important effect

97
00:07:08,830 --> 00:07:14,730
for constraint qualification is if you have a so-called being in constant

98
00:07:14,750 --> 00:07:19,520
then you're constant qualification is always satisfied

99
00:07:19,580 --> 00:07:21,200
this year

100
00:07:21,210 --> 00:07:24,310
and exception

101
00:07:24,330 --> 00:07:26,750
see the primal problem

102
00:07:26,790 --> 00:07:29,180
OK so here is the primary problem

103
00:07:29,230 --> 00:07:32,370
so what we have is

104
00:07:32,370 --> 00:07:36,720
that just uh it

105
00:07:36,730 --> 00:07:39,390
so i'd like to first the

106
00:07:40,980 --> 00:07:42,200
or get

107
00:07:42,330 --> 00:07:46,730
the rule

108
00:07:46,740 --> 00:07:50,340
why these optimality he holds

109
00:07:50,350 --> 00:07:51,660
and then

110
00:07:53,360 --> 00:07:56,110
the big problem that is how

111
00:08:00,190 --> 00:08:05,530
and at the convention of people of similar because its local optimality

112
00:08:09,610 --> 00:08:11,600
if you go back

113
00:08:11,710 --> 00:08:18,090
i had to calculate it at least in principle all these conditional expectation

114
00:08:18,140 --> 00:08:21,070
so the calculation of this such a

115
00:08:21,080 --> 00:08:24,370
conditional expectation the knowledge

116
00:08:24,430 --> 00:08:28,080
of the distribution of the sequence x one x two

117
00:08:28,970 --> 00:08:32,750
otherwise i cannot calculate the conditional expectation

118
00:08:32,780 --> 00:08:38,250
so the problem last for the second or they hall two

119
00:08:38,700 --> 00:08:41,450
create a bit of the world

120
00:08:41,460 --> 00:08:48,000
and then you could procedure with the same optimality prof feature here

121
00:08:48,590 --> 00:08:55,420
the approach should be as in the best possible asymptotic growth rate if we don't

122
00:08:55,440 --> 00:08:58,440
know anything on the process

123
00:08:58,480 --> 00:09:04,350
if i get the problem that i can make a pretty deal IBM and coca-cola

124
00:09:04,500 --> 00:09:07,380
then nobody we apparently what is the

125
00:09:07,390 --> 00:09:13,360
what are all the multidimensional distribution of the market

126
00:09:13,420 --> 00:09:16,070
but i have to happen and the code

127
00:09:16,170 --> 00:09:18,200
that's particularly optima

128
00:09:19,170 --> 00:09:21,930
for the proof i would like to

129
00:09:21,940 --> 00:09:22,830
the tu

130
00:09:22,850 --> 00:09:25,180
very nice the

131
00:09:27,990 --> 00:09:31,240
concept is is the martingale difference

132
00:09:31,280 --> 00:09:32,630
the choir

133
00:09:32,660 --> 00:09:38,310
because in on wednesday afternoon when i proved that for i i d

134
00:09:38,320 --> 00:09:43,580
one what is the best possible portfolio that

135
00:09:43,590 --> 00:09:53,160
then i proved optimality and the main key of the proof was law large number

136
00:09:53,210 --> 00:10:01,700
that is the average of independent identically distributed zero mean random variables the average goes

137
00:10:01,700 --> 00:10:04,360
to zero

138
00:10:04,440 --> 00:10:09,960
here i have just visionary but i feel i can handle the law of large

139
00:10:09,960 --> 00:10:12,210
numbers and this will be

140
00:10:12,240 --> 00:10:15,830
the law of large numbers are markedly different

141
00:10:15,870 --> 00:10:17,820
i have two sequences

142
00:10:18,160 --> 00:10:20,980
the and x

143
00:10:21,040 --> 00:10:26,350
and i have here two requirements for the definition of

144
00:10:26,360 --> 00:10:34,190
martingale difference one is that the and the function of x one x two xn

145
00:10:34,240 --> 00:10:38,880
and the other one is that the conditional expectation of the and

146
00:10:41,530 --> 00:10:43,550
preview axes

147
00:10:43,580 --> 00:10:45,540
is zero

148
00:10:45,550 --> 00:10:50,280
this is the definition of then i call the mach get different

149
00:10:50,330 --> 00:10:51,980
with respect to

150
00:10:51,990 --> 00:10:55,590
x that

151
00:10:56,170 --> 00:10:59,430
and it is very good

152
00:10:59,470 --> 00:11:04,350
because there is this year and i feel in

153
00:11:04,410 --> 00:11:09,130
in this year i will use large numbers but maybe

154
00:11:09,180 --> 00:11:11,610
you allowed to the data

155
00:11:11,620 --> 00:11:17,120
probably this theory that if you have a fuzzy and marking your different sequences

156
00:11:17,130 --> 00:11:18,460
such that the

157
00:11:18,470 --> 00:11:22,100
the thirtieth on its convergence then

158
00:11:22,120 --> 00:11:29,600
the average of is that the goes to zero we probably you one

159
00:11:29,610 --> 00:11:33,160
instead of these the strong law of large numbers

160
00:11:33,180 --> 00:11:34,820
i will

161
00:11:34,880 --> 00:11:38,140
sketch of of the proof

162
00:11:38,160 --> 00:11:45,090
not sketch the company proved that the average of marking differences goes to zero

163
00:11:45,110 --> 00:11:48,570
it means process or problem which

164
00:11:48,610 --> 00:11:55,720
the first step is that if we had a martingale difference sequence that it is

165
00:11:58,710 --> 00:12:00,830
we find

166
00:12:00,910 --> 00:12:06,000
forty four different it derive less than g

167
00:12:06,010 --> 00:12:12,490
and i would like to show that the product the i the j

168
00:12:12,490 --> 00:12:15,040
today we're going to talk about interference

169
00:12:15,060 --> 00:12:17,500
of electromagnetic radiation

170
00:12:17,610 --> 00:12:20,320
and i will start as a warm-up

171
00:12:20,390 --> 00:12:23,360
with famous historical experiment

172
00:12:23,420 --> 00:12:24,350
it was first

173
00:12:24,410 --> 00:12:27,840
young in eighteen one

174
00:12:27,850 --> 00:12:28,720
by the time

175
00:12:28,740 --> 00:12:31,740
the issue whether or not light

176
00:12:31,780 --> 00:12:36,270
was always whether those particles will still unresolved

177
00:12:36,350 --> 00:12:39,740
not always like to particles

178
00:12:39,780 --> 00:12:40,790
but the

179
00:12:40,850 --> 00:12:43,820
dutch physicist

180
00:12:44,820 --> 00:12:49,030
one of them to be weight issues unresolved

181
00:12:49,080 --> 00:12:50,820
let's agree that if

182
00:12:50,860 --> 00:12:52,880
like particles

183
00:12:52,940 --> 00:12:57,470
and you have have the screen has two openings

184
00:12:57,500 --> 00:12:59,030
and you will throw

185
00:12:59,080 --> 00:13:01,640
article four like tomato

186
00:13:01,720 --> 00:13:03,660
many the articles

187
00:13:03,690 --> 00:13:05,410
a you collect here

188
00:13:05,500 --> 00:13:09,250
and those made of don't get stuck on the screen but that make it

189
00:13:09,410 --> 00:13:13,440
former pioneer of tomatoes and then for the

190
00:13:13,500 --> 00:13:15,530
it's very difficult

191
00:13:15,630 --> 00:13:18,170
articles but the situation is different

192
00:13:18,190 --> 00:13:20,160
when we do with waves

193
00:13:20,220 --> 00:13:23,330
because the moment that you have were coming in

194
00:13:23,330 --> 00:13:28,050
say for instance we have plane waves coming in like this

195
00:13:28,080 --> 00:13:29,580
and the way can go

196
00:13:29,630 --> 00:13:36,020
both of these simultaneously that changes the picture quite dramatically

197
00:13:36,130 --> 00:13:38,360
well no was already in the

198
00:13:38,380 --> 00:13:40,170
seventeenth century

199
00:13:40,190 --> 00:13:42,830
that if you have a waterway

200
00:13:42,850 --> 00:13:45,190
going through a small opening

201
00:13:45,250 --> 00:13:47,330
so here waterways

202
00:13:47,360 --> 00:13:48,470
moving in

203
00:13:48,550 --> 00:13:50,880
like so he was also wore

204
00:13:50,920 --> 00:13:53,200
that which is seen coming out there

205
00:13:53,280 --> 00:13:56,000
are circular way

206
00:13:57,500 --> 00:14:00,070
look like this

207
00:14:00,110 --> 00:14:03,220
they propagating out circular

208
00:14:03,220 --> 00:14:04,720
and if the velocity

209
00:14:04,790 --> 00:14:07,940
of the way here is the same as the velocity there

210
00:14:07,940 --> 00:14:10,130
then the wavelength here

211
00:14:10,180 --> 00:14:11,530
it will be the same

212
00:14:11,570 --> 00:14:12,970
the way like

213
00:14:12,980 --> 00:14:16,190
but if the velocities different of course using change

214
00:14:16,190 --> 00:14:17,470
the difference in the

215
00:14:17,570 --> 00:14:20,720
we fly

216
00:14:20,780 --> 00:14:22,290
so happens

217
00:14:22,310 --> 00:14:24,750
my country my

218
00:14:24,780 --> 00:14:28,000
i suggest that in the seventeenth century new ideas

219
00:14:28,010 --> 00:14:32,820
which is now known as hamilton's principle

220
00:14:33,820 --> 00:14:36,320
we can think of this in very different ways

221
00:14:36,430 --> 00:14:38,000
and by the way is

222
00:14:38,030 --> 00:14:43,060
difficult to pronounce any one of you who knows how to say how things correctly

223
00:14:43,100 --> 00:14:44,320
is also that

224
00:14:44,380 --> 00:14:48,820
because you missed the new language you missed the your language

225
00:14:48,870 --> 00:14:53,380
and the combination of our and is complete kill you guys

226
00:14:53,400 --> 00:14:56,390
you know you don't get extra eight o three course credits if you can come

227
00:14:56,390 --> 00:14:59,250
to to my office and say how things but it was certainly put you into

228
00:14:59,250 --> 00:15:01,480
that category

229
00:15:01,530 --> 00:15:05,700
so in the seventeenth century the inhabitants came was an idea which was later amended

230
00:15:05,700 --> 00:15:09,190
by from l in the nineteenth century now known as the

231
00:15:09,200 --> 00:15:14,060
boykins i will pronounce it your way since final principle

232
00:15:14,070 --> 00:15:18,380
which works as follows if we have the playing monochromatic wave

233
00:15:18,430 --> 00:15:22,780
and it's incident on the screen with an aperture this be in aperture opening

234
00:15:22,790 --> 00:15:25,530
you have to have just two openings

235
00:15:25,560 --> 00:15:29,370
and avoidance final principle states the following all points

236
00:15:29,430 --> 00:15:32,130
in the aperture plane

237
00:15:33,310 --> 00:15:36,220
thought of secondary point sources

238
00:15:36,250 --> 00:15:38,600
of spherical waves

239
00:15:38,620 --> 00:15:42,000
and the point sources replaced the real source

240
00:15:42,000 --> 00:15:43,940
which is falling to screen

241
00:15:43,950 --> 00:15:49,090
and the stream itself is a perfect absorber of the radiation falling upon and so

242
00:15:49,090 --> 00:15:52,720
i will repeat the organ final principle which i will need to be

243
00:15:52,750 --> 00:15:54,280
all points

244
00:15:54,340 --> 00:15:56,410
in the aperture plane

245
00:15:56,450 --> 00:16:01,810
may be thought of as a secondary point sources of spherical wave

246
00:16:01,820 --> 00:16:05,050
in the case that we have one or this is a two dimensional surface there

247
00:16:05,050 --> 00:16:06,230
will be served

248
00:16:06,240 --> 00:16:09,080
but when you deal with light you can think of them as

249
00:16:09,150 --> 00:16:13,050
three-dimensional that means spherical which

250
00:16:13,060 --> 00:16:15,230
avoidance final principle

251
00:16:15,250 --> 00:16:16,770
is very powerful

252
00:16:17,540 --> 00:16:19,740
there is a wide range of

253
00:16:19,770 --> 00:16:23,940
opinion to its scientific merit

254
00:16:23,990 --> 00:16:28,840
and in a very famous book the principle of electrodynamics by melvin swartz

255
00:16:28,890 --> 00:16:33,880
i read the following i quote verbatim from his book

256
00:16:33,930 --> 00:16:38,300
he says hagens principle tells us to consider each point on the way from as

257
00:16:38,300 --> 00:16:40,400
a new source of radiation

258
00:16:40,450 --> 00:16:44,440
and at the radiation from all the news sources together

259
00:16:44,450 --> 00:16:47,070
physically this makes no sense at all

260
00:16:47,130 --> 00:16:49,300
light does not emit light

261
00:16:49,360 --> 00:16:52,620
only accelerating charges emit light

262
00:16:52,690 --> 00:16:57,650
thus we will begin by throwing out avoidance principle completely

263
00:16:57,750 --> 00:17:01,440
later we will see that it actually does give the right answer for the wrong

264
00:17:02,940 --> 00:17:06,380
so i will proceed my lecture today to get the right answer

265
00:17:07,680 --> 00:17:10,110
for the wrong reasons

266
00:17:10,150 --> 00:17:15,060
suppose now we have a

267
00:17:15,060 --> 00:17:18,710
electromagnetic wave i'm particularly thinking of light

268
00:17:18,800 --> 00:17:21,020
and i have here opening

269
00:17:21,040 --> 00:17:22,820
and i open

270
00:17:22,830 --> 00:17:25,020
plane waves are coming in

271
00:17:25,070 --> 00:17:27,610
on the left

272
00:17:27,620 --> 00:17:28,960
moving with the

273
00:17:30,300 --> 00:17:32,750
and that this opening a

274
00:17:32,760 --> 00:17:34,610
and that is opening the

275
00:17:34,640 --> 00:17:37,680
i knew the centre of the two ways

276
00:17:37,690 --> 00:17:40,830
this could be circular opening it could also be used it

277
00:17:40,880 --> 00:17:46,230
most of the experiments that i was let's which are perpendicular

278
00:17:46,240 --> 00:17:47,800
so there are very narrow

279
00:17:50,090 --> 00:17:52,250
imagine that you point

280
00:17:52,260 --> 00:17:55,010
and so

281
00:17:55,080 --> 00:17:58,580
the way from point a will reach that point p

282
00:17:58,670 --> 00:18:00,990
but away from source b

283
00:18:01,110 --> 00:18:05,060
also read the story elected vectors there

284
00:18:05,120 --> 00:18:09,080
i'm going to be and of course vectorial

285
00:18:09,090 --> 00:18:10,420
imagine now

286
00:18:10,440 --> 00:18:12,440
but the

287
00:18:12,490 --> 00:18:14,750
minus a

288
00:18:14,840 --> 00:18:19,640
imagine that there were one half wavelength

289
00:18:19,650 --> 00:18:21,650
then what you get is that the months

290
00:18:21,740 --> 00:18:27,140
of the vector he will coincide with the valley of effective from o one

291
00:18:27,150 --> 00:18:30,260
so you get the situation that like was like

292
00:18:30,270 --> 00:18:34,500
will give darkness we call that desperately interference

293
00:18:34,560 --> 00:18:35,630
the way from

294
00:18:35,640 --> 00:18:38,260
from there are one hundred eighty degrees

295
00:18:38,290 --> 00:18:40,640
out of face

296
00:18:40,650 --> 00:18:42,840
and if you take all the points

297
00:18:42,860 --> 00:18:45,650
for which the difference is one of

298
00:18:45,670 --> 00:18:48,360
that is a hyperbola will surface

299
00:18:48,370 --> 00:18:51,890
it's like a ball

300
00:18:51,940 --> 00:18:55,630
not only in the blackboard also comes out of the back like

301
00:18:55,640 --> 00:18:56,750
and you're

302
00:18:56,770 --> 00:18:58,320
five school days

303
00:18:58,370 --> 00:19:00,120
that is

304
00:19:00,130 --> 00:19:01,380
the sun

305
00:19:01,440 --> 00:19:05,170
of the distance in a and b p is constant

306
00:19:05,210 --> 00:19:07,020
then you get another

307
00:19:07,070 --> 00:19:11,330
but if the sum of the differences are constant you get across

308
00:19:11,420 --> 00:19:13,960
so you get an equivalent surface

309
00:19:13,980 --> 00:19:17,320
and all everywhere on that surface minus a

310
00:19:17,320 --> 00:19:19,890
what can be one lap

311
00:19:19,940 --> 00:19:21,560
and then you will get the

312
00:19:21,570 --> 00:19:24,730
the main consequence like like give darkness

313
00:19:24,740 --> 00:19:26,440
desperate to interfere

314
00:19:26,490 --> 00:19:28,420
the main roads don't do that

315
00:19:28,460 --> 00:19:29,760
one two mateo

316
00:19:29,790 --> 00:19:31,740
on top of another two may o

317
00:19:31,770 --> 00:19:37,450
does not you know to me that is distinctly different from particles

318
00:19:37,540 --> 00:19:40,250
and of course there also hyperbolic model surfaces

319
00:19:40,260 --> 00:19:41,500
for which there is

320
00:19:41,500 --> 00:19:48,440
a higher bias because they don't they don't have enough parameters to to be very expressive

321
00:19:48,440 --> 00:19:52,740
but at least the parameters will be well estimated so they have low variance well

322
00:19:52,750 --> 00:19:59,260
the models with more parameters like this one would have low low

323
00:19:59,270 --> 00:20:04,400
bias because they are more expressive but there would have higher variance in the

324
00:20:04,400 --> 00:20:08,540
estimation of the parameters so by averaging between the two then you cannot get the

325
00:20:08,540 --> 00:20:15,880
smaller bias and smaller variance as compared to to each of this models so

326
00:20:16,410 --> 00:20:23,400
so tends to work pretty well and there's been lots of different models that people

327
00:20:23,400 --> 00:20:28,000
have explored for for smoothing  and this is just taken from a review

328
00:20:28,000 --> 00:20:34,230
paper about ten years ago and they've compared whole bunch of smoothing algorithms at a

329
00:20:34,230 --> 00:20:39,800
time and found that there're two smoothing algorithms that seem to do

330
00:20:39,800 --> 00:20:44,560
much better than the other ones and and nowadays they're basically the the standard smoothing

331
00:20:44,560 --> 00:20:53,470
techniques that could be my phone sorry okay and they're called Kneser-Ney so interpolated

332
00:20:53,470 --> 00:21:00,520
interpolated Kneser-Ney and modified  Kneser-Ney what we would do instead is to

333
00:21:00,520 --> 00:21:07,840
use this idea of hierarchical Bayesian models to to build our language model okay and

334
00:21:07,840 --> 00:21:11,920
the idea is the following so if you look at the way smoothing works

335
00:21:11,920 --> 00:21:18,980
like this where we have the smooth probability is this average of this tree models you can kind of

336
00:21:18,980 --> 00:21:26,200
think of this algorithm as basically operating on a tree of contexts so

337
00:21:26,200 --> 00:21:30,280
the the tree is starts off at the root of the tree we have the

338
00:21:30,280 --> 00:21:38,120
context basically the empty context where we want to predict the next word without giving

339
00:21:38,120 --> 00:21:42,820
without being given any information about previous words and then at the next level down

340
00:21:42,980 --> 00:21:46,980
we'd like to predict the next would given only the previous word and then at

341
00:21:46,980 --> 00:21:52,580
the next level down we'd like to predict that next word given the previous two words and so

342
00:21:52,580 --> 00:21:58,940
forth and the idea of smoothing here is that you'd like to take a path down

343
00:21:58,940 --> 00:22:06,280
this context tree and average the probabilities along this path right that that's what this thing

344
00:22:06,280 --> 00:22:11,960
is doing and what that does is that it allows you to basically share information

345
00:22:11,960 --> 00:22:19,140
across the different the different context and the weight this context tree is constructed

346
00:22:19,140 --> 00:22:23,710
as you go up the tree you take a context and you drop the first

347
00:22:23,720 --> 00:22:27,380
word from the context so you start off with a long south parks drop the first

348
00:22:27,380 --> 00:22:31,720
word we get south parks drop the first word  we get parks drop the first word we

349
00:22:31,720 --> 00:22:38,020
get the empty context and the reason for this is basically that the later words in

350
00:22:38,020 --> 00:22:42,380
your context being closer to the word that you want to predict which is the one that

351
00:22:42,380 --> 00:22:51,580
comes next is more informative about the word that you wanna predict and because it's more informative

352
00:22:51,590 --> 00:22:54,600
you like to keep it in the context as you go up the tree as

353
00:22:54,600 --> 00:23:01,140
long as possible so this of course looks quite similar to a hierarchical bayesian model and

354
00:23:01,140 --> 00:23:07,600
we can do that so let's build a hierarchical Bayesian model on this context tree

355
00:23:07,600 --> 00:23:12,380
okay so at every node of our tree we're gonna have a set

356
00:23:12,380 --> 00:23:21,500
of parameters which basically describes the list of probabilities that for the list of

357
00:23:21,500 --> 00:23:28,500
probabilities for the following word given that context so this G of parks would

358
00:23:28,500 --> 00:23:32,080
aerodynamic coefficients that had to be there

359
00:23:32,240 --> 00:23:37,850
for that maneuver to look like it look that within itself has some + amount

360
00:23:37,850 --> 00:23:42,870
stuff goes you can change of validity still looks pretty good those 2 them were

361
00:23:42,880 --> 00:23:46,060
and then we had a set of reduced air management today when you go to

362
00:23:46,060 --> 00:23:50,180
the anemic Data Book which we hope to be able to get you copies of

363
00:23:50,460 --> 00:23:54,880
it will have not only the coefficients for all members and the the back angle

364
00:23:54,970 --> 00:23:59,060
such that it would tell you what the uncertainty in the function might number angle

365
00:23:59,060 --> 00:24:03,240
of attack is really in system should be able to work with host

366
00:24:03,400 --> 00:24:09,080
I will mention this ergonomic design substantiation report

367
00:24:10,040 --> 00:24:13,220
and you'll be able to get a copy of that

368
00:24:13,230 --> 00:24:16,520
place and all of that we're going to try to get a copy to to

369
00:24:16,540 --> 00:24:21,370
just for you and should be available and you find what that did that goes

370
00:24:21,370 --> 00:24:27,520
back him and for every coefficient Arango attack how we arrived at that specific value

371
00:24:27,520 --> 00:24:35,640
and specific variations using both wind tunnel flight tests and super document and not purity

372
00:24:36,210 --> 00:24:42,100
blood is an excellent technical document and that was done during the flight test program

373
00:24:42,100 --> 00:24:43,330
and inadequate

374
00:24:44,700 --> 00:24:52,560
I thought we had their land program that's primarily the backbone of the anemic database

375
00:24:52,570 --> 00:24:56,900
of the show before the arbitrarily integrated vehicle and

376
00:24:58,000 --> 00:25:07,740
mandatory testamentary vehicles 17 and 18 40 structural anatomy hours of 1 type and 17

377
00:25:07,740 --> 00:25:13,280
thousand and we have that compared with the Apollo is this time we get about

378
00:25:13,440 --> 00:25:21,060
45 thousand hours on the entry vehicle Apollo however Paolo didn't have all those movable

379
00:25:21,060 --> 00:25:23,660
surfaces move services nice

380
00:25:24,520 --> 00:25:28,080
and do a lot of things for you but you gotta test and make sure

381
00:25:28,110 --> 00:25:30,810
that there

382
00:25:31,140 --> 00:25:37,690
yes release of this facilities and country they still exists but a up away yesterday

383
00:25:37,710 --> 00:25:44,690
still exist in the other ones arranges to will once the still usable unfortunately those

384
00:25:44,690 --> 00:25:51,600
at Langley even though there there and they work and because of funding constraints Langley

385
00:25:51,620 --> 00:25:57,370
has a hard time and bring in being used and that's unfortunate going of some

386
00:25:57,370 --> 00:25:59,480
excellent facilities and

387
00:25:59,560 --> 00:26:07,190
should used that we've got in this wonderful test programming found after spatial encompassing programme

388
00:26:07,210 --> 00:26:13,100
summary uh that's that's about we can get this document pretty straightforwardly

389
00:26:15,040 --> 00:26:21,080
In previous his nascent vehicle of few US hours and that's because you don't with

390
00:26:21,080 --> 00:26:29,980
many surfaces primarily uh heating and that something and work on if you wanna get

391
00:26:29,980 --> 00:26:31,400
project to work on

392
00:26:31,460 --> 00:26:37,640
heating data and went

393
00:26:39,720 --> 00:26:41,760
on a small scale vehicle

394
00:26:42,180 --> 00:26:44,420
and make scale predictions

395
00:26:45,100 --> 00:26:50,140
reasons near and dear to my heart alright there is a test not too long

396
00:26:50,140 --> 00:26:56,700
ago and they still can match those 2 so good PhD thesis making figure out

397
00:26:56,700 --> 00:27:01,770
how to do that have the right not a scaling parameter like plumes enough which

398
00:27:01,780 --> 00:27:08,200
getting primarily really work with there's like data gallery images and it's like test data

399
00:27:08,200 --> 00:27:13,620
and went later and lot about about doesn't

400
00:27:13,980 --> 00:27:19,000
it doesn't bother us that much because we've got enough conservatism in the system and

401
00:27:19,000 --> 00:27:24,510
uncertainties that we know we're safe flat but it does hurt you when you say

402
00:27:24,850 --> 00:27:27,730
that heating radiators it should be

403
00:27:27,750 --> 00:27:32,170
non BTU before it's great per 2nd and applied to this dataset is not going

404
00:27:32,210 --> 00:27:40,190
to be united 1 where you have met another way but that I was looking

405
00:27:40,190 --> 00:27:46,850
for something to get my plough into analysis theoretical stuff that's probably arrive at the

406
00:27:47,520 --> 00:27:56,290
time of Toronto programmer and Aaron paid for that his budget by way and was

407
00:27:56,290 --> 00:28:01,550
there's nothing else

408
00:28:01,560 --> 00:28:04,680
i called is actually called zero

409
00:28:04,680 --> 00:28:07,310
and i call this displacement

410
00:28:09,150 --> 00:28:13,340
away from equilibrium

411
00:28:13,990 --> 00:28:16,720
for small angles

412
00:28:16,770 --> 00:28:18,720
i want to argue that he

413
00:28:19,300 --> 00:28:21,880
is very close to MG

414
00:28:21,960 --> 00:28:24,970
for one thing if you hold them vertically and you do nothing and there is

415
00:28:24,970 --> 00:28:29,640
no motion is always that he's ng two forces have to cancel each other that's

416
00:28:31,990 --> 00:28:35,430
but i can show you that even if the angles are modest

417
00:28:35,690 --> 00:28:39,220
that that should also be the case

418
00:28:39,250 --> 00:28:41,810
suppose i decompose t

419
00:28:41,860 --> 00:28:44,890
in two directions

420
00:28:45,020 --> 00:28:47,080
the vertical direction

421
00:28:47,720 --> 00:28:48,920
so this is t

422
00:28:48,930 --> 00:28:51,040
times the cosine of the that

423
00:28:51,090 --> 00:28:52,930
and in the horizontal direction

424
00:28:52,970 --> 00:28:54,210
so this is true

425
00:28:54,260 --> 00:28:56,010
times the sign of data

426
00:28:59,250 --> 00:29:01,210
if the angles are very small

427
00:29:01,220 --> 00:29:06,020
the object is hardly moving at all in this direction the motion is almost exclusively

428
00:29:06,020 --> 00:29:07,380
in this direction

429
00:29:07,420 --> 00:29:08,590
so there is no

430
00:29:09,930 --> 00:29:12,740
in the y direction or i should say

431
00:29:12,820 --> 00:29:16,000
the acceleration in the y direction is negligibly small

432
00:29:16,050 --> 00:29:21,510
for that means to high degree of accuracy the cosine theta is always the same

433
00:29:21,510 --> 00:29:23,360
as g

434
00:29:23,360 --> 00:29:28,790
i degree of accuracy but for small angles cosine theta itself is one

435
00:29:28,810 --> 00:29:30,070
therefore she

436
00:29:30,090 --> 00:29:32,590
equals MC

437
00:29:32,630 --> 00:29:34,500
and so the force

438
00:29:34,520 --> 00:29:36,210
that is driving

439
00:29:36,210 --> 00:29:38,950
this object back to equilibrium

440
00:29:38,960 --> 00:29:40,960
is t sin theta

441
00:29:41,010 --> 00:29:48,110
so that force is ng sin theta to high degree of accuracy

442
00:29:48,140 --> 00:29:50,260
i'm going to introduce again

443
00:29:50,280 --> 00:29:54,130
the gamma is be over and sort of these damping

444
00:29:54,170 --> 00:29:57,180
and i'm going to introduce that only gets squared

445
00:29:57,190 --> 00:29:58,260
calls g

446
00:29:58,290 --> 00:30:04,340
over alpha omega zero scratches g over LG over l being the

447
00:30:04,420 --> 00:30:06,800
square root of g over being the

448
00:30:06,830 --> 00:30:08,650
resonance frequency

449
00:30:08,690 --> 00:30:11,680
of a pendulum length l

450
00:30:13,230 --> 00:30:17,180
of the mass of the object as we have seen before

451
00:30:17,290 --> 00:30:20,770
so now i'm going to write down newton's second law

452
00:30:20,820 --> 00:30:22,740
so i get an axe

453
00:30:22,750 --> 00:30:25,520
double dot

454
00:30:25,530 --> 00:30:27,840
and then i get minus

455
00:30:27,890 --> 00:30:29,760
b acted dot

456
00:30:29,780 --> 00:30:31,180
that is the

457
00:30:31,220 --> 00:30:32,630
the damping

458
00:30:32,630 --> 00:30:34,640
is bx not

459
00:30:34,650 --> 00:30:36,240
and now comes this force

460
00:30:36,260 --> 00:30:38,220
which is the only one

461
00:30:38,910 --> 00:30:40,110
he wants to

462
00:30:40,150 --> 00:30:41,480
driving back to

463
00:30:41,540 --> 00:30:44,170
equilibrium it's the restoring force

464
00:30:44,180 --> 00:30:46,220
so that's force if you accept

465
00:30:46,240 --> 00:30:47,940
mighty being MG

466
00:30:47,940 --> 00:30:49,250
that is and

467
00:30:49,270 --> 00:30:53,000
times the size of data

468
00:30:53,050 --> 00:30:55,120
that's the difference in equation

469
00:30:55,170 --> 00:30:56,370
that i know

470
00:30:56,420 --> 00:30:59,880
i have to solve

471
00:30:59,890 --> 00:31:03,450
that is driven system

472
00:31:03,490 --> 00:31:05,470
no here i had driven system

473
00:31:05,480 --> 00:31:09,440
and boy i saw force here

474
00:31:09,490 --> 00:31:12,980
i don't see anything like that there

475
00:31:12,990 --> 00:31:16,430
well this will tell will come into this picture

476
00:31:16,470 --> 00:31:22,630
i always doing something

477
00:31:22,710 --> 00:31:26,860
i have i overlooked myself perhaps

478
00:31:34,550 --> 00:31:37,220
wouldn't change anything

479
00:31:37,240 --> 00:31:40,990
i've changed nothing but i don't see myself anymore

480
00:31:40,990 --> 00:31:43,710
so what's wrong is there anything wrong with this

481
00:31:43,760 --> 00:31:48,610
where do i show up in these equations

482
00:31:51,440 --> 00:31:55,530
way that equation two i show up

483
00:31:55,550 --> 00:31:59,070
what is the sign of data

484
00:31:59,130 --> 00:32:01,900
what is the sign of theta

485
00:32:02,010 --> 00:32:09,480
what is side of this angle

486
00:32:10,990 --> 00:32:15,320
mine is at that's walter ruin

487
00:32:15,320 --> 00:32:17,280
x minus at

488
00:32:17,320 --> 00:32:19,860
divided by l

489
00:32:19,880 --> 00:32:22,380
there are m

490
00:32:22,420 --> 00:32:27,110
so i'm going to substitute that in here and i'm going to divide by

491
00:32:27,990 --> 00:32:30,090
that's not defined by and yet

492
00:32:30,090 --> 00:32:31,010
just say

493
00:32:31,010 --> 00:32:32,490
x double dot

494
00:32:32,510 --> 00:32:33,610
a minus b

495
00:32:33,630 --> 00:32:34,740
next up

496
00:32:34,800 --> 00:32:36,380
and now we get

497
00:32:36,440 --> 00:32:39,070
minus and g

498
00:32:39,980 --> 00:32:42,550
x over l

499
00:32:42,610 --> 00:32:45,490
and now we bring will to live the other side

500
00:32:45,510 --> 00:32:49,380
and so we get plus and g

501
00:32:51,030 --> 00:32:53,630
finds at

502
00:32:53,630 --> 00:32:55,030
divided by

503
00:32:55,940 --> 00:32:58,090
and that ties at the zero

504
00:32:58,110 --> 00:33:01,990
the cosine omega t because i'm moving my hand

505
00:33:02,010 --> 00:33:05,260
at is a function of time

506
00:33:05,300 --> 00:33:07,460
right down and you here

507
00:33:07,490 --> 00:33:10,990
and then will check this

508
00:33:11,010 --> 00:33:15,480
so energy time sign fader has two terms it has an MG times x over

509
00:33:16,170 --> 00:33:20,780
but it has also an times at over l and i bring that over on

510
00:33:20,780 --> 00:33:23,420
this site but i know that at

511
00:33:23,460 --> 00:33:29,360
is changing in time and so you see now walter lewin is right there

512
00:33:29,400 --> 00:33:31,400
and i divide by n

513
00:33:31,440 --> 00:33:34,800
and i substitute omega zero screening here

514
00:33:34,820 --> 00:33:37,460
o i have been used to

515
00:33:37,510 --> 00:33:40,530
we should have screened there wasn't and there i decided not to divide by an

516
00:33:41,420 --> 00:33:43,610
now i'm going to divide by n

517
00:33:43,670 --> 00:33:48,400
i get x double dot

518
00:33:48,400 --> 00:33:50,570
in the ninety percent of patients

519
00:33:50,590 --> 00:33:52,060
where there was no

520
00:33:52,070 --> 00:33:54,110
effective response to the drug

521
00:33:54,270 --> 00:33:59,180
the EGF receptor it was wild type was present in the wild type configurations it

522
00:33:59,180 --> 00:34:04,800
might have been slightly overexpressed but it wasn't but it continued to function essentially as

523
00:34:04,870 --> 00:34:06,630
normal EGF receptor

524
00:34:06,650 --> 00:34:12,020
and this represents a major advance in cancer therapy because it suggests that one has

525
00:34:12,020 --> 00:34:16,710
to begin to understand what subset of patients one should treat

526
00:34:16,760 --> 00:34:20,030
with the drug which can happen on its own have quite toxic effects on the

527
00:34:20,940 --> 00:34:26,120
and from now on to state the obvious when one gets lung cancer patients one

528
00:34:26,120 --> 00:34:30,990
will check quickly using various reactions like the PCR reaction to see whether or not

529
00:34:31,000 --> 00:34:34,360
the cancer cells have mutated EGF receptor and if they do

530
00:34:34,850 --> 00:34:39,580
they will be candidates for rest treatment with the expectation that sixty eighty or even

531
00:34:39,580 --> 00:34:42,970
one hundred percent of them will have to resort respond

532
00:34:42,980 --> 00:34:47,260
and if they don't have mutated EGF receptor and they will not be subjected

533
00:34:48,340 --> 00:34:52,810
treatment by the drug this is the beginning of a new era of cancer drug

534
00:34:52,810 --> 00:34:57,990
treatment it's called rational drug design or rational treatment we don't just lump all the

535
00:34:57,990 --> 00:35:02,150
patients with a certain disease together so let's give them all this drug and throw

536
00:35:02,150 --> 00:35:04,060
things at me and see what happens

537
00:35:04,080 --> 00:35:10,230
here one begins to do it genetic diagnosis of the genomes of the patient's cancer

538
00:35:10,230 --> 00:35:13,360
cells in order to determine

539
00:35:13,380 --> 00:35:18,150
whether or not they have certain mutated genes in this case we're referring to one

540
00:35:18,150 --> 00:35:19,810
of these growth factor receptors

541
00:35:19,860 --> 00:35:24,540
by the way we're talking about one cancer today right

542
00:35:24,590 --> 00:35:29,010
if you are smoking now

543
00:35:29,020 --> 00:35:32,450
i always as the class how many people are smoking nobody

544
00:35:32,490 --> 00:35:36,510
has the

545
00:35:36,520 --> 00:35:39,340
has the moral fortitude to raise their hands

546
00:35:39,360 --> 00:35:42,400
but if you smoking now

547
00:35:42,450 --> 00:35:46,000
and you started this age and you continue and by the way if you start

548
00:35:46,000 --> 00:35:51,470
curating you continue small stopping smoking is actually a bit more difficult quite a bit

549
00:35:51,470 --> 00:35:53,450
more difficult than stopping heroine

550
00:35:53,500 --> 00:35:55,010
it's pretty interesting right

551
00:35:55,040 --> 00:35:58,770
it is so if you continue to smoke now

552
00:35:58,850 --> 00:36:01,730
you will be healthy for pretty long period of time

553
00:36:01,730 --> 00:36:05,600
probably another twenty or thirty years and for you that sounds like

554
00:36:06,830 --> 00:36:09,960
but when you get to be about forty or fifty things are going to start

555
00:36:09,960 --> 00:36:11,420
falling apart

556
00:36:11,420 --> 00:36:14,580
so you won't be able to do be very athletic centre lungs are going to

557
00:36:14,580 --> 00:36:17,690
be able to are going to be great and by the time you reach for

558
00:36:17,720 --> 00:36:19,790
fifty six years seventies

559
00:36:19,840 --> 00:36:24,200
what's going to happen is you will on average have a six to eight year

560
00:36:24,210 --> 00:36:26,200
shortened life expectancy

561
00:36:26,440 --> 00:36:30,000
ninety six sixty eight years is not that much but it really is you know

562
00:36:30,000 --> 00:36:33,590
when you get to be seventy and you think you're going to die next year

563
00:36:33,590 --> 00:36:38,460
uridine six to eight years makes a big difference sixty years is an enormous difference

564
00:36:38,460 --> 00:36:44,530
in life expectancy twenty percent of all people who died last year in this country

565
00:36:44,560 --> 00:36:46,270
twenty percent of all this

566
00:36:46,410 --> 00:36:48,590
cigarette smoking

567
00:36:48,600 --> 00:36:49,740
imagine that

568
00:36:49,900 --> 00:36:56,720
and when you die from cigarette smoke smoking sometimes you get lung cancer

569
00:36:56,770 --> 00:37:02,260
there probably were i think six hundred thousand people have died smoking last year

570
00:37:02,270 --> 00:37:06,810
six hundred thousand there were fifty five thousand american soldiers

571
00:37:06,830 --> 00:37:09,210
who died in vietnam in the whole war

572
00:37:09,270 --> 00:37:13,150
there were two hundred twenty american soldiers two thousand american soldiers who died all of

573
00:37:13,150 --> 00:37:14,290
world war two

574
00:37:14,310 --> 00:37:17,740
and last year in this and there were three thousand two thousand people died in

575
00:37:17,740 --> 00:37:19,280
the world trade center

576
00:37:19,300 --> 00:37:20,980
all right all those numbers

577
00:37:20,980 --> 00:37:26,530
so last year six hundred thousand people died premature deaths because they were smoking

578
00:37:26,550 --> 00:37:29,860
how many people died last year from smoking marijuana

579
00:37:30,800 --> 00:37:31,690
maybe two

580
00:37:31,690 --> 00:37:34,690
for three i don't know

581
00:37:34,730 --> 00:37:40,550
and my urging originated do any kind of smoking i'm not saying marijuana smoking is

582
00:37:40,550 --> 00:37:42,550
good for

583
00:37:42,570 --> 00:37:45,090
but i just want you to get these things in mind

584
00:37:45,110 --> 00:37:46,280
the perspective

585
00:37:49,500 --> 00:37:52,770
if you smoke

586
00:37:52,780 --> 00:37:57,570
you know in many countries including this one there's not much attention by given by

587
00:37:57,570 --> 00:38:01,690
the government press dissuading people from smoking and here's the reason why

588
00:38:01,730 --> 00:38:03,610
if you smoke

589
00:38:03,800 --> 00:38:08,530
and you get it get sick eventually eventually the countries all going to pay for

590
00:38:08,570 --> 00:38:10,480
medical costs right

591
00:38:10,500 --> 00:38:14,250
so really we all have to pay for the costs of people get sick

592
00:38:14,270 --> 00:38:16,530
it's all shared in one way or another

593
00:38:16,570 --> 00:38:20,980
but it's not such a big problem for government like the american government because if

594
00:38:20,980 --> 00:38:25,070
a lot of times we actually refer to the dimensions so think of this as

595
00:38:25,070 --> 00:38:26,860
the unit hypercube

596
00:38:26,890 --> 00:38:31,270
we have some measurements let's say they are point measurements of the function with some

597
00:38:32,360 --> 00:38:35,120
noise added to it

598
00:38:35,150 --> 00:38:38,470
that's it in a discussion like any other kind of noise

599
00:38:38,470 --> 00:38:43,850
can generalize this but this is because it is large and is independent of sample

600
00:38:43,890 --> 00:38:47,900
occasions otherwise the problem would be very very hard to but sometimes it not the

601
00:38:47,900 --> 00:38:50,640
case in practice

602
00:38:50,690 --> 00:38:54,720
and let's say this is the problem we have

603
00:38:56,690 --> 00:39:01,350
this is just what you have seen before passive versus active sampling passive sampling u

604
00:39:01,350 --> 00:39:04,440
s west of the questions first an active sampling u

605
00:39:04,450 --> 00:39:06,340
ask questions one at a time

606
00:39:06,340 --> 00:39:07,740
ben and

607
00:39:07,770 --> 00:39:10,730
based on what you've seen before

608
00:39:11,070 --> 00:39:13,950
so far this regression problems

609
00:39:13,970 --> 00:39:15,200
what is

610
00:39:15,220 --> 00:39:18,490
the goal of the goal is to construct a function

611
00:39:18,520 --> 00:39:19,610
another function

612
00:39:19,620 --> 00:39:23,890
that is somewhat closer to the true function as

613
00:39:24,860 --> 00:39:30,520
this function closeness in this case means the l two distance or the integral of

614
00:39:30,520 --> 00:39:33,600
the difference of the square of the functions

615
00:39:33,660 --> 00:39:35,270
at each point

616
00:39:35,280 --> 00:39:36,100
OK so

617
00:39:37,270 --> 00:39:38,730
squared error

618
00:39:40,770 --> 00:39:42,350
this expected

619
00:39:43,490 --> 00:39:46,520
this is your loss function now wanted to be small

620
00:39:46,520 --> 00:39:48,890
so there are two ingredients here

621
00:39:48,900 --> 00:39:51,560
as i said before one is the estimator

622
00:39:51,570 --> 00:39:55,720
so this function of the data that you're trying to

623
00:39:55,730 --> 00:39:58,740
use to approximate the true function

624
00:39:58,760 --> 00:40:00,910
the other one is the sampling strategy

625
00:40:00,970 --> 00:40:05,780
she's the rule used to choose where to sample next so in active learning those

626
00:40:05,780 --> 00:40:08,490
two things are very important in passive learning

627
00:40:08,510 --> 00:40:10,120
this would be

628
00:40:10,310 --> 00:40:13,050
the key to the design so it's not

629
00:40:13,100 --> 00:40:16,850
like this you're just saying OK i'm going to take examples here here here here

630
00:40:16,850 --> 00:40:19,480
to give the labels

631
00:40:21,350 --> 00:40:23,020
OK so

632
00:40:23,050 --> 00:40:24,100
let's see what

633
00:40:24,110 --> 00:40:26,890
what can you say about active learning setting

634
00:40:26,900 --> 00:40:29,270
but suppose you're trying to learn a smooth function

635
00:40:31,910 --> 00:40:35,100
temperature in the room or

636
00:40:35,110 --> 00:40:36,280
something like that

637
00:40:36,300 --> 00:40:42,650
which does not a lot of sharp changes that states although smooth function just as

638
00:40:42,650 --> 00:40:50,410
we've seen before outside of the smooth function two dimensions possibly high dimensional and

639
00:40:50,440 --> 00:40:53,280
what can we say about active learning well

640
00:40:53,310 --> 00:40:58,110
what pretentious saying this case that active learning some performance like this that x

641
00:40:58,120 --> 00:41:01,260
expected error decays like that

642
00:41:01,270 --> 00:41:04,450
and we can compare this with passive learning and see

643
00:41:04,480 --> 00:41:06,620
we have exactly the same rate

644
00:41:07,820 --> 00:41:13,100
what's going on active learning simply stop being helpful

645
00:41:13,200 --> 00:41:17,640
it is not helpful in this case at least in the minimax setting as i

646
00:41:17,640 --> 00:41:23,810
said minimax is very pessimistic way of looking at things the very worst case so

647
00:41:23,900 --> 00:41:25,720
the minimax setting

648
00:41:25,730 --> 00:41:28,220
sounds active learning

649
00:41:28,230 --> 00:41:29,870
for smooth functions

650
00:41:29,890 --> 00:41:30,870
just to help

651
00:41:30,890 --> 00:41:31,990
and why is this

652
00:41:33,220 --> 00:41:35,810
if you look at small functions they are

653
00:41:35,870 --> 00:41:40,180
smooth they are equally interesting are equally an interesting everywhere

654
00:41:40,180 --> 00:41:44,940
so there's nothing to really capitalize on no place where you can kind of sampling

655
00:41:44,940 --> 00:41:49,310
learn more about that function because they the face is kind of boring the

656
00:41:49,320 --> 00:41:50,980
quite onerous

657
00:41:51,650 --> 00:41:57,350
other types of functions in marginal inner margin is or for sure that have

658
00:41:57,370 --> 00:41:59,870
just like the step function or

659
00:41:59,890 --> 00:42:02,530
some boundaries like that

660
00:42:02,680 --> 00:42:07,980
below a little bit more interesting because the complexity of these functions is mostly concentrated

661
00:42:08,060 --> 00:42:14,990
in a low dimensional space here it's mostly one parameter this describes function here

662
00:42:15,010 --> 00:42:18,360
getting this maybe smooth function here and here

663
00:42:18,370 --> 00:42:20,810
but you have very sharp

664
00:42:20,810 --> 00:42:27,260
the boundary so there you might have a complex is kind of concentrated along this

665
00:42:27,280 --> 00:42:29,350
in this case the one-dimensional curve

666
00:42:31,720 --> 00:42:37,120
in this case active learning might have more problems in might work

667
00:42:37,810 --> 00:42:40,780
so this is because of classes of functions are interested in

668
00:42:40,800 --> 00:42:46,360
and let's focus only on piecewise constant function although some of these can be somewhat

669
00:42:46,360 --> 00:42:52,180
and you should get them both are available at labyrinth bookstore on york street or

670
00:42:52,180 --> 00:42:53,760
you get them online

671
00:42:53,780 --> 00:42:58,080
i should note that last time i talked of course i used the markets reader

672
00:42:58,130 --> 00:43:03,470
and when professor marvin chun taught his course last semester he used peter grace fifth

673
00:43:03,470 --> 00:43:08,080
edition text sort of maybe allowed use copies floating around you should feel free to

674
00:43:08,080 --> 00:43:10,800
try to get one of those

675
00:43:10,920 --> 00:43:12,950
evaluation goes like this

676
00:43:12,960 --> 00:43:14,300
there's a midterm

677
00:43:14,310 --> 00:43:15,620
and there's a final

678
00:43:15,630 --> 00:43:18,420
the final will not be held in exam period

679
00:43:18,440 --> 00:43:22,030
because i like to take long vacations it will be held on the last day

680
00:43:22,030 --> 00:43:23,580
of class

681
00:43:23,630 --> 00:43:29,430
example will will be multiple choice and short answer fill-in-the-blank that sort of thing prior

682
00:43:29,430 --> 00:43:32,940
to the exams i will post previous exams online

683
00:43:32,950 --> 00:43:36,150
so you have a feeling for how these exams were

684
00:43:36,160 --> 00:43:38,840
and so on there will also be review sessions

685
00:43:38,860 --> 00:43:43,050
starting at the beginning of the third week of class that is not the next

686
00:43:43,050 --> 00:43:44,490
week but the week after

687
00:43:44,540 --> 00:43:48,170
on one each one day i want to put up a brief question

688
00:43:48,220 --> 00:43:49,720
or set of questions

689
00:43:49,730 --> 00:43:51,280
which you have to answer

690
00:43:51,330 --> 00:43:55,350
and your answers need to be sent to teaching fellow you'll be given a teaching

691
00:43:55,350 --> 00:43:57,030
fellow assigned one

692
00:43:57,110 --> 00:43:59,330
by friday

693
00:43:59,340 --> 00:44:02,770
this is not meant to be difficult it's not meant to be more than five

694
00:44:02,770 --> 00:44:06,180
ten minutes of work but the point of the question

695
00:44:06,200 --> 00:44:10,950
fifteen twenty minutes of work but the point of the question is to motivate people

696
00:44:10,950 --> 00:44:12,710
to keep up with the material

697
00:44:12,720 --> 00:44:14,020
and do the redirect

698
00:44:14,030 --> 00:44:18,490
these questions will be marked pass fail i expect most everybody to pass all of

699
00:44:18,490 --> 00:44:19,510
the questions

700
00:44:19,520 --> 00:44:22,980
but it's just to keep you on track and keep going

701
00:44:23,000 --> 00:44:25,030
there's a book reviews

702
00:44:25,040 --> 00:44:28,520
a short book review to be written towards near the end of the class

703
00:44:28,530 --> 00:44:32,500
i'll give details about that later on in this semester and is also an experimental

704
00:44:32,500 --> 00:44:34,290
participation requirements

705
00:44:34,300 --> 00:44:39,450
and next week i'll handle piece of paper describing the requirements the point requirements to

706
00:44:39,450 --> 00:44:41,340
give you all experience

707
00:44:41,360 --> 00:44:46,020
actually seeing what psychological research is about as well as to give us hundreds of

708
00:44:46,020 --> 00:44:49,040
subjects to do experiments on

709
00:44:49,610 --> 00:44:55,420
the issue issues sometimes come about how well force

710
00:44:55,750 --> 00:44:58,060
here's how to do well

711
00:44:58,090 --> 00:45:01,040
ten all the classes keep up with the readings

712
00:45:01,050 --> 00:45:05,280
ideally keep of the readings before you come to class

713
00:45:05,290 --> 00:45:10,140
and one thing i would strongly suggest to form some sort of study was

714
00:45:10,180 --> 00:45:13,780
either formally or informally have people you talk to

715
00:45:13,790 --> 00:45:18,130
when the exact prior to the exams are he's palling something next to her

716
00:45:18,150 --> 00:45:21,830
i hope you know him

717
00:45:24,230 --> 00:45:26,220
and in fact what i want do

718
00:45:26,230 --> 00:45:29,540
not this class because the shopping period i don't know what's coming next last or

719
00:45:29,540 --> 00:45:34,040
what but also to a few minutes prior to beginning classes for people just introduce

720
00:45:34,070 --> 00:45:36,280
himself to the person next to them

721
00:45:36,300 --> 00:45:38,090
so to have some sort of resource

722
00:45:38,140 --> 00:45:41,180
in the class

723
00:45:42,410 --> 00:45:45,100
this is a large class

724
00:45:45,110 --> 00:45:48,310
and if you don't do anything about it can be very anonymous

725
00:45:48,350 --> 00:45:51,320
and some of you may choose to pursue it that way

726
00:45:51,370 --> 00:45:52,570
totally fine

727
00:45:52,580 --> 00:45:53,540
but what i was

728
00:45:53,550 --> 00:45:55,040
i suggest you do

729
00:45:55,060 --> 00:45:57,890
it established some contact with us

730
00:45:57,900 --> 00:45:59,070
if with me

731
00:45:59,080 --> 00:46:03,180
are we any of the teaching fellows and i'll introduce the teaching fellows sometime next

732
00:46:04,310 --> 00:46:07,600
you talk to us at the beginning or end of class

733
00:46:07,610 --> 00:46:10,580
unless you're special circumstances i always try to come

734
00:46:10,590 --> 00:46:13,890
at least ten minutes earlier i and i'm willing to stay late to talk to

735
00:46:13,890 --> 00:46:18,610
people you could come by the office hours which are on the syllabus and the

736
00:46:18,610 --> 00:46:20,770
sending email an appointment

737
00:46:20,780 --> 00:46:25,870
i'm very willing to talk to students about intellectual ideas about cause problems and so

738
00:46:25,870 --> 00:46:30,530
on and if you see me at some point just on campus you introduce yourself

739
00:46:30,530 --> 00:46:32,680
and i like me people

740
00:46:32,690 --> 00:46:33,840
from this class

741
00:46:34,710 --> 00:46:36,720
so again i want to stress

742
00:46:36,730 --> 00:46:40,250
you have the option of staying anonymous in this class we also have the option

743
00:46:40,250 --> 00:46:43,980
of seeking making some sort of contact with us

744
00:46:45,860 --> 00:46:48,310
that's the formal start of of course

745
00:46:48,320 --> 00:46:50,890
what course about

746
00:46:50,940 --> 00:46:58,650
unlike a lot of other courses some people come into psychology of some unusual motivations

747
00:46:58,650 --> 00:47:00,680
maybe you're crazy

748
00:47:00,750 --> 00:47:02,750
and hope to become less crazy

749
00:47:05,190 --> 00:47:08,690
maybe one to learn how to study better improve your sex life

750
00:47:08,700 --> 00:47:14,380
i interpret your dreams and win friends and influence people

751
00:47:14,400 --> 00:47:17,020
those are not necessarily better reasons to take

752
00:47:17,920 --> 00:47:19,090
this course

753
00:47:19,110 --> 00:47:23,090
and with the exception of the sex part of this course might actually help you

754
00:47:23,090 --> 00:47:27,740
will with some of these things the study of scientific psychology has a lot of

755
00:47:27,740 --> 00:47:33,260
insight of real world relevance to real problems that we face in our everyday lives

756
00:47:33,280 --> 00:47:35,590
and i want to try and when these issues come up when i try to

757
00:47:36,690 --> 00:47:41,030
and make you try to think about the extent to which the laboratory research i'll

758
00:47:41,030 --> 00:47:43,870
be talking about can affect your everyday life

759
00:47:43,880 --> 00:47:48,830
and study how you interact with people how you might try to persuade somebody of

760
00:47:48,830 --> 00:47:52,030
something else what sort of therapy works best for you

761
00:47:52,040 --> 00:47:56,680
but the general goals of the core

762
00:47:56,700 --> 00:47:57,970
are actually

763
00:47:57,980 --> 00:48:00,680
i think even more interesting than that

764
00:48:02,200 --> 00:48:03,370
what i wanna do

765
00:48:03,380 --> 00:48:08,370
is provided state of art introduction to the most important topic that there is

766
00:48:08,420 --> 00:48:13,060
that's how the human mind works how we think

767
00:48:13,070 --> 00:48:15,410
what makes us what we are

768
00:48:15,420 --> 00:48:20,020
i will be approaching this from a range of directions so traditionally psychology is often

769
00:48:20,020 --> 00:48:24,470
broken up into the following into five subareas

770
00:48:24,480 --> 00:48:30,340
neuroscience which is the study of the of the mind by looking at the brain

771
00:48:31,500 --> 00:48:35,200
which is the area which i focus mostly on which is trying to learn about

772
00:48:35,200 --> 00:48:37,100
how people develop and grow

773
00:48:37,110 --> 00:48:39,340
and learning

774
00:48:40,710 --> 00:48:44,200
which is the one term of the five might be unfamiliar some of you but

775
00:48:44,210 --> 00:48:50,400
refers to sort of computational approach to studying the mind often viewing the mind on

776
00:48:50,400 --> 00:48:56,100
analogy with the computer and looking at how people do things like understand language recognise

777
00:48:57,320 --> 00:48:59,620
playing games and so on

778
00:48:59,630 --> 00:49:01,510
there is social

779
00:49:01,530 --> 00:49:04,820
which is the study of how people act in groups how people act with other

780
00:49:06,270 --> 00:49:10,970
and there is clinical which is maybe the aspect of psychology people think of immediately

781
00:49:10,970 --> 00:49:15,610
when they psychology which is the study of mental health and mental illness

782
00:49:15,620 --> 00:49:19,010
and we'll be covering all of those areas will also be covering a set of

783
00:49:19,010 --> 00:49:20,630
related areas

784
00:49:20,640 --> 00:49:24,430
i am convinced that you can not study the mind

785
00:49:24,480 --> 00:49:30,300
solely by looking at the discipline of psychology the psychology spills over the issues of

786
00:49:30,370 --> 00:49:33,170
how the mind has evolved

787
00:49:33,190 --> 00:49:35,570
economics and game theory

788
00:49:35,620 --> 00:49:37,750
are now an essential tools

789
00:49:37,770 --> 00:49:40,480
for understanding human by human behaviour

790
00:49:40,520 --> 00:49:47,940
those issues connecting to philosophy computer science anthropology literature theology and many many other domains

791
00:49:47,980 --> 00:49:49,290
so this course will be

792
00:49:49,340 --> 00:49:52,290
wide ranging in that sense

793
00:49:52,300 --> 00:49:56,360
at this point i speaking in generalities so

794
00:49:56,380 --> 00:49:58,520
i want to post this introductory

795
00:49:58,540 --> 00:50:01,580
class by giving five examples

796
00:50:01,630 --> 00:50:03,830
other sorts of topics will be covered

797
00:50:03,850 --> 00:50:05,500
and all strata topic

798
00:50:05,550 --> 00:50:08,660
there will be covering next week on monday

799
00:50:08,660 --> 00:50:11,390
what is the acceleration any of the system

800
00:50:11,430 --> 00:50:14,410
i get them to minus one that is o point

801
00:50:14,420 --> 00:50:15,770
one five

802
00:50:15,820 --> 00:50:17,740
divided by the sun

803
00:50:17,780 --> 00:50:20,200
which is two point three five

804
00:50:20,280 --> 00:50:25,090
and that is approximately o point o six four g

805
00:50:25,120 --> 00:50:28,030
proximity o point o six four g

806
00:50:28,050 --> 00:50:31,560
it's about one sixty of the gravitational acceleration

807
00:50:31,620 --> 00:50:33,750
but a very modest

808
00:50:35,590 --> 00:50:36,860
what is the pension

809
00:50:38,200 --> 00:50:41,230
i substitute by numbers for one and two in there

810
00:50:41,290 --> 00:50:43,850
you can take for g ten if you like that

811
00:50:43,850 --> 00:50:45,900
you will find that the tension

812
00:50:45,920 --> 00:50:48,140
equals one point one

813
00:50:51,230 --> 00:50:52,530
and no

814
00:50:52,540 --> 00:50:54,680
look at what i predicted

815
00:50:54,770 --> 00:50:58,950
they both way one point one seven g that's non-negotiable

816
00:50:58,960 --> 00:51:01,630
that is my definition of weight the pension

817
00:51:01,640 --> 00:51:06,450
in both size is the same that's my definition of weight this is the weight

818
00:51:06,530 --> 00:51:10,030
this one had to wait one point two five

819
00:51:12,300 --> 00:51:13,910
without being accelerate

820
00:51:13,910 --> 00:51:14,780
you see

821
00:51:14,800 --> 00:51:16,110
it has lost weight

822
00:51:16,110 --> 00:51:18,170
as accelerated down

823
00:51:18,180 --> 00:51:22,260
this one had a weight of one point one gene

824
00:51:22,320 --> 00:51:26,000
see has gained weight because it accelerated up

825
00:51:26,010 --> 00:51:27,590
we see the whole picture

826
00:51:27,630 --> 00:51:29,660
vice together very neatly

827
00:51:29,670 --> 00:51:33,370
and it's important that you look at it that way

828
00:51:34,150 --> 00:51:35,980
i know want to return

829
00:51:37,030 --> 00:51:38,920
the idea of complete

830
00:51:40,620 --> 00:51:43,210
and i want to remind you if you like to go

831
00:51:43,270 --> 00:51:45,890
how i was swinging you at the end of history

832
00:51:45,930 --> 00:51:47,190
in the vertical

833
00:51:47,200 --> 00:51:49,050
i was thinking you like this

834
00:51:49,070 --> 00:51:50,730
and i was ringing

835
00:51:50,740 --> 00:51:52,970
a bucket of water like this

836
00:51:53,050 --> 00:51:55,940
and i want to return to that

837
00:51:55,980 --> 00:51:56,960
i want to

838
00:51:56,970 --> 00:51:58,750
look at you

839
00:51:58,760 --> 00:52:00,770
when you are a

840
00:52:00,780 --> 00:52:02,290
at the bottom of your circle

841
00:52:02,300 --> 00:52:05,230
and when you are at the very top

842
00:52:05,250 --> 00:52:09,110
of that's you

843
00:52:09,280 --> 00:52:10,900
you go

844
00:52:10,910 --> 00:52:15,130
around a circle which has radius are

845
00:52:15,180 --> 00:52:19,650
here is that circle

846
00:52:19,660 --> 00:52:23,160
a string here

847
00:52:23,200 --> 00:52:24,590
we're here

848
00:52:24,600 --> 00:52:26,420
and as a string here

849
00:52:26,470 --> 00:52:28,300
and at some point in time you there

850
00:52:28,310 --> 00:52:33,110
going around let's assume that you're going around was an angular velocity

851
00:52:33,120 --> 00:52:36,710
omega and for simplicity we keep omega constant

852
00:52:36,720 --> 00:52:39,900
that's really not that important

853
00:52:40,560 --> 00:52:42,450
this is point p

854
00:52:42,600 --> 00:52:44,960
and this is point as

855
00:52:45,010 --> 00:52:45,980
at first

856
00:52:46,020 --> 00:52:49,430
look at the situation at point p

857
00:52:49,430 --> 00:52:50,990
you have mass

858
00:52:51,120 --> 00:52:53,640
so gravity acts upon you

859
00:52:53,710 --> 00:52:54,850
and g

860
00:52:54,940 --> 00:52:57,340
there is tension in the string

861
00:52:59,320 --> 00:53:02,100
there must be this is non-negotiable

862
00:53:02,100 --> 00:53:04,700
centripetal acceleration upwards

863
00:53:04,710 --> 00:53:07,030
otherwise you could never do this

864
00:53:07,090 --> 00:53:09,540
number from the uniform circular motion

865
00:53:09,600 --> 00:53:12,300
so there must be here

866
00:53:12,350 --> 00:53:14,420
centripetal acceleration

867
00:53:14,460 --> 00:53:16,350
which is only gas great art

868
00:53:16,390 --> 00:53:18,340
or if you prefer

869
00:53:18,350 --> 00:53:20,340
this great divided by r

870
00:53:20,350 --> 00:53:24,420
if v is the speed and and gential speed at that point

871
00:53:24,550 --> 00:53:27,770
it must be there

872
00:53:27,820 --> 00:53:30,190
let's look here

873
00:53:30,230 --> 00:53:31,420
right there

874
00:53:31,420 --> 00:53:34,230
gravity is acting upon you

875
00:53:37,490 --> 00:53:41,210
let's assume the string is pulling on new let's assume that for now

876
00:53:42,010 --> 00:53:43,960
there's tension

877
00:53:44,020 --> 00:53:47,040
stream or not new

878
00:53:48,820 --> 00:53:51,370
non-negotiable when you make this

879
00:53:51,370 --> 00:53:52,620
curvature here

880
00:53:52,630 --> 00:53:53,640
there must be

881
00:53:53,650 --> 00:53:55,450
the centripetal acceleration

882
00:53:55,500 --> 00:53:59,770
and that's simply because the generation must be on because they are there is not

883
00:53:59,780 --> 00:54:02,700
negotiable has to be there

884
00:54:02,750 --> 00:54:03,870
that's no

885
00:54:03,880 --> 00:54:07,140
evaluate first the situation at p

886
00:54:07,270 --> 00:54:09,640
and i will call this class

887
00:54:09,650 --> 00:54:13,490
now call this minds

888
00:54:13,500 --> 00:54:16,890
so i get now is that t

889
00:54:16,920 --> 00:54:18,740
my name is mg

890
00:54:18,820 --> 00:54:20,780
must be

891
00:54:22,190 --> 00:54:24,400
times the centripetal acceleration

892
00:54:25,300 --> 00:54:27,190
i must be and

893
00:54:27,190 --> 00:54:30,210
centripetal acceleration g

894
00:54:31,450 --> 00:54:36,080
that looks very familiar looks like someone is b someone is being accelerated in know

895
00:54:36,080 --> 00:54:40,900
that almost the same equation

896
00:54:42,250 --> 00:54:45,960
the centripetal acceleration at this point

897
00:54:46,020 --> 00:54:47,510
for instance where

898
00:54:47,520 --> 00:54:49,780
ten meters per second squared

899
00:54:50,550 --> 00:54:53,200
you would weigh twice your normal weight

900
00:54:53,200 --> 00:54:54,940
tension here

901
00:54:54,970 --> 00:54:55,960
would be

902
00:54:59,210 --> 00:55:01,000
if this were five

903
00:55:01,020 --> 00:55:02,680
meters per second squared

904
00:55:02,680 --> 00:55:03,970
then you would

905
00:55:03,990 --> 00:55:07,560
one of the halftime show way

906
00:55:07,640 --> 00:55:08,950
let's now

907
00:55:08,970 --> 00:55:10,810
look at the situation

908
00:55:13,230 --> 00:55:15,470
point as

909
00:55:15,610 --> 00:55:17,110
i'm going to call

910
00:55:17,150 --> 00:55:19,490
this plus

911
00:55:19,590 --> 00:55:23,030
and that minus

912
00:55:23,090 --> 00:55:24,870
i'm going to find that

913
00:55:25,460 --> 00:55:27,920
plus and

914
00:55:27,960 --> 00:55:29,890
i must be and

915
00:55:29,900 --> 00:55:33,320
times the centripetal acceleration newton's second law

916
00:55:33,430 --> 00:55:35,760
so i find that the tension there

917
00:55:35,780 --> 00:55:37,200
he calls and

918
00:55:38,710 --> 00:55:41,000
minus g

919
00:55:42,550 --> 00:55:45,420
very similar to what i've seen before

920
00:55:45,420 --> 00:55:47,170
these objects

921
00:55:47,210 --> 00:55:51,280
is losing weight

922
00:55:51,340 --> 00:55:54,570
let's take the situation that a of c

923
00:55:54,600 --> 00:55:58,280
is exactly ten meters per second squared and we discussed the last time when we

924
00:55:58,280 --> 00:56:00,630
had the bucket of water in our hands

925
00:56:00,640 --> 00:56:03,320
if a of c if the centripetal acceleration

926
00:56:03,340 --> 00:56:04,120
when it

927
00:56:04,160 --> 00:56:06,450
make when i got to the top is ten

928
00:56:06,550 --> 00:56:08,410
and this is zero

929
00:56:08,540 --> 00:56:09,800
so the strange

930
00:56:09,850 --> 00:56:12,610
has no tension in the string of women

931
00:56:12,660 --> 00:56:17,470
and the bucket of water and you are weightless

932
00:56:17,500 --> 00:56:19,960
if the centripetal acceleration

933
00:56:19,980 --> 00:56:22,320
is larger than ten

934
00:56:22,380 --> 00:56:23,480
then of course

935
00:56:23,500 --> 00:56:27,000
this string will be tight there will be a force on you

936
00:56:27,000 --> 00:56:33,030
regression function classifier and then we use this variance founding idea is served intuitive way

937
00:56:33,030 --> 00:56:39,280
of understanding how to actually select is traumatic come things that verification

938
00:56:39,280 --> 00:56:44,410
OK in classification we're going to do the same thing some the purcell variances over

939
00:56:44,410 --> 00:56:48,400
the leaves in the tree times the probability falling in particular

940
00:56:48,460 --> 00:56:53,170
in this case these variances behave like one of the square root poems pl

941
00:56:53,170 --> 00:56:54,780
so at the end of the day

942
00:56:54,800 --> 00:56:58,000
down there you see what we end up with something like a bit more complicated

943
00:56:58,090 --> 00:57:01,650
the sum over all leaves in the tree of cells in the partition

944
00:57:01,670 --> 00:57:05,820
terms like the square root of peace developer

945
00:57:05,840 --> 00:57:06,940
in this type of

946
00:57:07,000 --> 00:57:13,030
very is found if you will for classification was first introduced by minister and david

947
00:57:13,030 --> 00:57:17,500
mcallister who is the director here here is very interesting idea and this is the

948
00:57:17,500 --> 00:57:20,400
basic intuition block behind why that might be

949
00:57:20,460 --> 00:57:22,210
a sensible thing

950
00:57:22,320 --> 00:57:25,670
we can get slightly weaker bond classification

951
00:57:25,670 --> 00:57:32,000
by viewing this summation here is an expectation this being the object of expectation being

952
00:57:32,000 --> 00:57:35,280
the probabilities that take expectation with respect to

953
00:57:35,340 --> 00:57:39,750
and then applying jensen's inequality and then we get something that may be a little

954
00:57:39,750 --> 00:57:41,650
bit more familiar to some people

955
00:57:41,690 --> 00:57:45,280
the bound on the variance is equal to the square root of the number of

956
00:57:45,280 --> 00:57:49,500
leaves in the tree divided by and this is exactly what you might obtain if

957
00:57:49,500 --> 00:57:56,440
you use the standard VC type of bound for trees

958
00:57:57,670 --> 00:58:02,980
this this this very sparse suggests the following rules for complexity regularisation

959
00:58:03,070 --> 00:58:06,800
now here's trying to be actually selecting trees and what i wanna do is i

960
00:58:06,800 --> 00:58:08,980
want to make sure that what i selected tree

961
00:58:08,980 --> 00:58:15,030
i struck some balance between giving a small empirical error and giving me not so

962
00:58:15,030 --> 00:58:16,940
large variance

963
00:58:17,050 --> 00:58:22,380
and so the lambda here in these equations could be viewed as the regularisation parameter

964
00:58:22,380 --> 00:58:27,320
that allows us to tweak the trade off between empirical risk invariances we like

965
00:58:27,360 --> 00:58:30,090
we'll see later how that can be theoretically

966
00:58:30,090 --> 00:58:35,500
shows in an optimal fashion but these are the basic classes were

967
00:58:35,550 --> 00:58:38,030
complexity regularisation rules in regression

968
00:58:38,030 --> 00:58:39,630
and in classification

969
00:58:39,650 --> 00:58:42,460
with the two different variance bounds whichever one you like

970
00:58:42,480 --> 00:58:45,630
the upper one being a little tighter than the lower one

971
00:58:47,590 --> 00:58:51,800
one thing i want to mention is an interesting feature here

972
00:58:51,820 --> 00:58:52,860
with the

973
00:58:52,880 --> 00:58:58,360
variance found for the classification case above there's is if you know is that if

974
00:58:58,360 --> 00:58:59,730
you think that the

975
00:58:59,750 --> 00:59:03,320
the distribution of the axis is more or less around

976
00:59:03,340 --> 00:59:06,570
spread around the feature space

977
00:59:06,610 --> 00:59:08,750
then a very small

978
00:59:08,900 --> 00:59:14,730
cells will have a very small probability of having features fall into that cell

979
00:59:15,460 --> 00:59:18,530
essentially small will contribute very little

980
00:59:18,550 --> 00:59:20,420
to this period found

981
00:59:20,480 --> 00:59:24,650
relative to large leaves and that is not happening with the other two parties which

982
00:59:24,650 --> 00:59:27,630
three or leaves the same and this is the really turns out to be very

983
00:59:27,630 --> 00:59:32,250
important feature of the first found in and we'll see that play out a little

984
00:59:32,250 --> 00:59:35,920
more detail in the second half of my lecture

985
00:59:35,980 --> 00:59:40,170
t and so in the second have focus on classification so i won't say more

986
00:59:40,170 --> 00:59:43,460
about that here but i want to give you an illustration of an example of

987
00:59:43,460 --> 00:59:45,460
this very kind of idea

988
00:59:45,520 --> 00:59:49,900
in something called image denoising so here's an image plus noise

989
00:59:49,900 --> 00:59:54,320
this in the middle here is a partition that we've learned directly from the noisy

990
00:59:54,320 --> 00:59:55,670
pixel values

991
00:59:55,690 --> 00:59:59,840
and and then at the right is the denoised image and that the image was

992
00:59:59,840 --> 01:00:01,630
obtained by solving

993
01:00:02,670 --> 01:00:05,860
this kind of complexity regularisation

994
01:00:05,880 --> 01:00:10,690
the value of lambda that's used in this kind of situation is usually something proportional

995
01:00:10,690 --> 01:00:14,130
to the variance of the noise level which you may or may not know if

996
01:00:14,130 --> 01:00:17,020
you don't know what you can estimate is ways to do that

997
01:00:17,070 --> 01:00:22,070
but all we're doing is is following that complexity regularisation scheme i just described in

998
01:00:22,070 --> 01:00:24,710
this is essentially a special case was

999
01:00:24,760 --> 01:00:30,750
usually called wavelet denoising using a particular type of wavelet haar wavelets

1000
01:00:30,750 --> 01:00:34,820
and this idea in the theory of it was really

1001
01:00:34,860 --> 01:00:41,230
pioneered and popularized by the time i went into stone in nineteen ninety four

1002
01:00:42,880 --> 01:00:44,940
when we talk about the second half of the

1003
01:00:45,530 --> 01:00:49,840
will take a break in middle is the theory behind complexity regularisation how do we

1004
01:00:49,840 --> 01:00:55,130
actually start formally justify what we just described in the previous slides

1005
01:00:55,190 --> 01:01:01,110
in the case of regression we get optimal denoising rules of that form

1006
01:01:01,230 --> 01:01:09,460
using various concentration inequalities per year telegraph concentration inequalities coupled with some approximation theory in

1007
01:01:09,460 --> 01:01:13,690
the classification case instead of using those concentration inequalities we can use

1008
01:01:13,750 --> 01:01:19,420
chernoff bounds hoeffding bounds and again approximation theory and more less the stories are very

1009
01:01:19,420 --> 01:01:23,460
similar and i'm going to focus on the classification theory to give you a good

1010
01:01:23,460 --> 01:01:25,710
insight as to what's actually going on

1011
01:01:25,760 --> 01:01:26,960
in a minute

1012
01:01:27,000 --> 01:01:30,250
and i think that we can take a little break now

1013
01:01:30,300 --> 01:01:49,590
and resume maybe in five minutes and i'll go into the second half of lecture

1014
01:01:49,650 --> 01:01:53,750
when you get on time

1015
01:02:00,610 --> 01:02:08,480
OK this is the my time

1016
01:02:08,500 --> 01:02:09,670
all right so

1017
01:02:09,670 --> 01:02:14,150
as i said in the second half the talk and we try to give you

1018
01:02:14,170 --> 01:02:16,070
a clear sense of real

1019
01:02:16,500 --> 01:02:20,920
theory behind some of the things i was trying to give intuition about the first

1020
01:02:20,920 --> 01:02:27,900
you know i was thinking more than this would be the end of

1021
01:02:27,920 --> 01:02:35,200
it is a strange fish and we try to

1022
01:02:35,220 --> 01:02:37,800
h type

1023
01:02:45,330 --> 01:02:47,210
this is

1024
01:02:47,250 --> 01:02:53,280
we are

1025
01:02:53,290 --> 01:02:55,530
which is the

1026
01:02:58,750 --> 01:03:01,310
you are

1027
01:03:05,330 --> 01:03:09,770
so usually after

1028
01:03:33,040 --> 01:03:36,470
if you can

1029
01:03:47,750 --> 01:03:51,220
she would later

1030
01:03:51,650 --> 01:03:55,910
he said

1031
01:03:56,450 --> 01:04:00,270
i don't want to the user

1032
01:04:10,910 --> 01:04:15,840
if we do that know he was well

1033
01:04:15,860 --> 01:04:20,580
well but three we had to

1034
01:04:20,600 --> 01:04:25,170
one of four

1035
01:04:25,180 --> 01:04:26,610
i mentioned

1036
01:04:27,100 --> 01:04:34,140
and the twenty two forty three

1037
01:04:34,150 --> 01:04:35,280
by using

1038
01:04:35,290 --> 01:04:36,470
i'm afraid

1039
01:04:36,490 --> 01:04:42,470
i don't know you hear it all

1040
01:04:47,510 --> 01:04:55,090
and all this

1041
01:04:55,100 --> 01:04:57,010
you think

1042
01:04:57,030 --> 01:04:59,620
three years

1043
01:05:05,240 --> 01:05:08,110
i think

1044
01:05:08,130 --> 01:05:17,630
which is to get

1045
01:05:19,210 --> 01:05:22,930
two or are

1046
01:05:22,960 --> 01:05:24,970
three years

1047
01:05:38,240 --> 01:05:43,520
it's very easy

1048
01:05:54,060 --> 01:05:58,330
we to you

1049
01:05:58,370 --> 01:06:01,130
each time we

1050
01:06:02,130 --> 01:06:13,710
you can easily show you just you

1051
01:06:13,730 --> 01:06:16,200
it's not for

1052
01:06:18,010 --> 01:06:21,300
but he

1053
01:06:21,330 --> 01:06:24,000
it just

1054
01:06:41,610 --> 01:06:46,340
in a finite

1055
01:06:55,810 --> 01:06:58,270
this one

1056
01:07:02,890 --> 01:07:05,250
now try to

1057
01:07:09,900 --> 01:07:12,150
it's two

1058
01:07:26,110 --> 01:07:30,480
the problem with a

1059
01:07:34,380 --> 01:07:36,360
one of the

1060
01:07:40,350 --> 01:07:44,360
the interesting thing is get

1061
01:07:46,610 --> 01:07:53,870
OK but that is not enough

1062
01:08:02,250 --> 01:08:07,120
two thousand three

1063
01:08:10,010 --> 01:08:12,770
the only

1064
01:08:14,120 --> 01:08:20,110
so it

1065
01:08:20,130 --> 01:08:21,220
so is

1066
01:08:21,230 --> 01:08:27,610
sure he is shown

1067
01:08:34,540 --> 01:08:43,530
you know you can

1068
01:08:54,540 --> 01:08:59,490
brown said

1069
01:09:08,440 --> 01:09:14,980
OK so

1070
01:09:15,000 --> 01:09:18,320
there are many

1071
01:09:20,560 --> 01:09:26,550
the fact that

1072
01:09:26,570 --> 01:09:29,060
you know my

1073
01:09:29,070 --> 01:09:38,680
you can look get for to days

1074
01:09:38,970 --> 01:09:43,770
a big

1075
01:09:46,990 --> 01:09:51,080
and these are

1076
01:10:10,150 --> 01:10:13,170
the last

1077
01:10:13,180 --> 01:10:17,740
the question is a you

1078
01:10:17,760 --> 01:10:21,560
it is

1079
01:10:22,250 --> 01:10:24,640
my we're

1080
01:10:24,740 --> 01:10:26,510
in the world

1081
01:10:26,830 --> 01:10:34,680
this would whole even if you don't

1082
01:10:34,680 --> 01:10:39,290
so this is again well of course we know that i see is very different

1083
01:10:39,290 --> 01:10:41,430
from PCA

1084
01:10:41,430 --> 01:10:45,970
but here again this is something that you might think is similar to PCA but

1085
01:10:45,970 --> 01:10:50,580
actually it is not at all similar to be so this is what this sparse

1086
01:10:50,580 --> 01:10:51,480
coding is

1087
01:10:51,510 --> 01:10:54,510
a kind of dimension reduction in the sense

1088
01:10:54,520 --> 01:10:59,430
that's so when you have let's say you have a one thousand dimensional data points

1089
01:10:59,430 --> 01:11:03,720
and the new input into what's possible and this is what the sparse coding give

1090
01:11:03,720 --> 01:11:09,750
the representation given that uses only ten of of ten active components and all the

1091
01:11:09,750 --> 01:11:14,140
sizes so instead the where your kind of reducing dimensions

1092
01:11:14,150 --> 01:11:19,480
but it's not at all the same distribution dimension by PCA because those components which

1093
01:11:19,590 --> 01:11:26,130
active always different from one point to another

1094
01:11:26,150 --> 01:11:29,350
so the idea is that you have a very large vocabulary

1095
01:11:29,400 --> 01:11:34,250
yes i tools from but then to represent any particular data points you will only

1096
01:11:34,250 --> 01:11:36,160
use a very small number

1097
01:11:36,180 --> 01:11:41,920
of those of active components it's a bit like in language actually so

1098
01:11:42,440 --> 01:11:46,670
in natural language i have tens of thousands of words that i can use but

1099
01:11:46,670 --> 01:11:50,360
if i want to describe as a single object for example

1100
01:11:50,370 --> 01:11:51,660
this computer here

1101
01:11:51,700 --> 01:11:55,090
i will is only a couple of years because only a couple of for example

1102
01:11:55,410 --> 01:11:58,310
this computer here

1103
01:11:59,020 --> 01:12:03,860
it's so that's actually people talk about what whatever believe is here so it is

1104
01:12:03,860 --> 01:12:07,890
you have potentially very large vocabulary but you will need is a couple of them

1105
01:12:07,980 --> 01:12:14,330
for any particular data point

1106
01:12:14,330 --> 01:12:16,000
and we finally get

1107
01:12:16,020 --> 01:12:18,380
so the results

1108
01:12:18,390 --> 01:12:21,780
so this is this is the basis vectors that you get when you do ICA

1109
01:12:21,780 --> 01:12:24,140
a or sparse coding on image with

1110
01:12:24,160 --> 01:12:27,690
so and so the point is that we have here

1111
01:12:27,700 --> 01:12:33,030
image windows of sixteen pixels five sixty six

1112
01:12:33,340 --> 01:12:34,770
and to get

1113
01:12:37,030 --> 01:12:43,970
from from the from which to estimate we we think we sample these small windows

1114
01:12:43,970 --> 01:12:46,020
from a large number of

1115
01:12:46,030 --> 01:12:47,700
of natural images

1116
01:12:47,720 --> 01:12:52,830
so natural images is bit rate is a term that is slightly vague what we

1117
01:12:52,830 --> 01:12:58,310
basically have is something of my life and loves and so on and then sample

1118
01:13:00,780 --> 01:13:01,680
and so

1119
01:13:01,750 --> 01:13:03,940
we they may be

1120
01:13:03,960 --> 01:13:06,540
something like ten to twenty thousand

1121
01:13:06,550 --> 01:13:12,630
some examples of these windows so we have to know twenty thousand data points

1122
01:13:13,550 --> 01:13:17,850
and so the original dimension of the data is two hundred fifty six because sixty

1123
01:13:17,850 --> 01:13:23,950
five sixty but then we do PCA for various technical reasons that we're not going

1124
01:13:23,950 --> 01:13:28,690
so as to reduce the dimension of the data to one hundred and sixty

1125
01:13:28,720 --> 01:13:32,250
and so that we do and then we do i see a and we get

1126
01:13:32,260 --> 01:13:36,370
we get one hundred sixty basis vectors each of these small squares is one of

1127
01:13:36,370 --> 01:13:43,840
the basis vectors and again called so that great is zero white positive and negative

1128
01:13:43,840 --> 01:13:45,810
so what you see here

1129
01:13:51,410 --> 01:13:55,560
well at least we provide well the first thing that

1130
01:13:55,630 --> 01:14:00,770
you would see is that most of these features are localized in space that is

1131
01:14:00,770 --> 01:14:07,590
most take any any single women who basis vectors most of it is is great

1132
01:14:07,600 --> 01:14:09,020
and only

1133
01:14:09,050 --> 01:14:10,400
this small part

1134
01:14:10,410 --> 01:14:11,870
of actually well

1135
01:14:11,880 --> 01:14:15,110
i wonder if you can see very well on this lighting conditions

1136
01:14:17,650 --> 01:14:22,030
so sixty pixels that sixteen

1137
01:14:22,090 --> 01:14:26,980
but i hope you can see most of the most of the feature is basically

1138
01:14:26,980 --> 01:14:28,690
grey that means zero

1139
01:14:28,710 --> 01:14:35,010
and you get non-zero values only in the typically in small spatially localized

1140
01:14:36,940 --> 01:14:40,980
of course the area will be different for each of these features

1141
01:14:41,040 --> 01:14:44,640
but and then the second property

1142
01:14:44,650 --> 01:14:49,880
it is that's not the most of these features oriented that means well into the

1143
01:14:49,880 --> 01:14:54,600
you can see that usually they have you know if have an orientation vertical horizontal

1144
01:14:54,600 --> 01:14:55,850
or anything in between

1145
01:14:56,420 --> 01:15:00,110
and the third property that is a bit more difficult to to see

1146
01:15:01,450 --> 01:15:08,050
but not necessarily well of the property because we call it multiresolution

1147
01:15:08,060 --> 01:15:09,800
which means that you have

1148
01:15:09,810 --> 01:15:15,700
which which which means that you have you have features that called for

1149
01:15:15,720 --> 01:15:17,510
very short small things

1150
01:15:17,520 --> 01:15:24,120
on in forty terms they have they are very high frequencies and then you have

1151
01:15:24,140 --> 01:15:29,900
the features that hold for larger things and work in a low dimensional frequencies

1152
01:15:29,900 --> 01:15:33,680
so for example well this is this is kind of coding for age of the

1153
01:15:33,690 --> 01:15:38,310
very low dimension where you know well most of these actually holding five which have

1154
01:15:38,670 --> 01:15:42,950
a much higher dimension and which are spatially much less

1155
01:15:42,970 --> 01:15:44,730
now these three properties

1156
01:15:44,900 --> 01:15:47,140
spatial localization orientation

1157
01:15:47,140 --> 01:15:49,760
and not a solution which is also

1158
01:15:49,780 --> 01:15:51,650
you would call it

1159
01:15:51,660 --> 01:15:55,290
localisation in fourier space

1160
01:15:55,310 --> 01:15:59,140
these three properties of atypical of the

1161
01:15:59,140 --> 01:16:02,540
so that explains data set sail to here

1162
01:16:02,680 --> 01:16:04,730
because this normalizes to one

1163
01:16:04,790 --> 01:16:06,650
it takes the form

1164
01:16:06,730 --> 01:16:10,290
so if optical data set on the design data

1165
01:16:10,340 --> 01:16:11,900
this medium alpha

1166
01:16:11,900 --> 01:16:14,220
model is more probable

1167
01:16:14,310 --> 01:16:18,940
simply due to this idea of normalizing which comes gain from this idea

1168
01:16:18,980 --> 01:16:21,020
having to look at all the other models

1169
01:16:22,680 --> 01:16:26,830
sorry all the other datasets these models could have predicted

1170
01:16:28,920 --> 01:16:31,650
if you go back to the sequence example

1171
01:16:31,670 --> 01:16:36,590
this high alpha model is equivalent if you like to six bit model this high

1172
01:16:36,590 --> 01:16:38,440
highly models of equivalent

1173
01:16:38,490 --> 01:16:39,720
to the

1174
01:16:39,720 --> 01:16:43,080
symbol eight single eight bit symbol

1175
01:16:43,140 --> 01:16:46,050
so it doesn't actually explain of data

1176
01:16:46,110 --> 01:16:48,950
if you had it would have given the very high probability that the next plane

1177
01:16:48,950 --> 01:16:49,920
of data

1178
01:16:49,930 --> 01:16:51,900
it's not the most probable

1179
01:16:51,950 --> 01:16:55,460
six bits explanation for that indicated sequence

1180
01:16:55,510 --> 01:17:00,330
well i did explain data but also explained allows generate a lot of other sequences

1181
01:17:00,380 --> 01:17:03,410
so it signed a bit low probability

1182
01:17:04,390 --> 01:17:08,030
model was just right well that explain the data

1183
01:17:08,960 --> 01:17:11,130
but it didn't explain much else

1184
01:17:11,180 --> 01:17:14,500
so that was the light so much that the most probable

1185
01:17:14,560 --> 01:17:16,480
that's occams razor work

1186
01:17:16,560 --> 01:17:17,760
both in

1187
01:17:17,810 --> 01:17:20,290
the discrete example i showed you the the very beginning

1188
01:17:20,340 --> 01:17:21,440
and also here

1189
01:17:21,440 --> 01:17:25,420
well we have sort of a continuous parameter space weights

1190
01:17:25,430 --> 01:17:28,070
if we integrate them out we can get this effect

1191
01:17:28,140 --> 01:17:31,670
in this case example by the marginal likelihood function

1192
01:17:31,720 --> 01:17:34,020
well we automatically penalized

1193
01:17:34,040 --> 01:17:36,060
we just one function calculation

1194
01:17:36,110 --> 01:17:37,690
models are not too simple

1195
01:17:37,700 --> 01:17:43,330
and models for the two complex

1196
01:17:44,160 --> 01:17:46,710
that would have been the end of the first

1197
01:17:46,720 --> 01:17:50,980
they have any questions at this point

1198
01:18:01,470 --> 01:18:09,670
this function

1199
01:18:10,000 --> 01:18:13,840
i have things quite fixed

1200
01:18:13,890 --> 01:18:16,300
same signal is not point one

1201
01:18:16,720 --> 01:18:18,740
that's the fixed basis matrix

1202
01:18:18,760 --> 01:18:20,760
that's the fixed data matrix

1203
01:18:20,800 --> 01:18:24,720
that only leaves out that i simply buried alpha

1204
01:18:24,720 --> 01:18:28,900
but in that actually to look at that indicated

1205
01:18:29,280 --> 01:18:31,970
so that's not too complex calculations

1206
01:18:33,130 --> 01:18:35,070
the negative log that

1207
01:18:35,090 --> 01:18:38,710
but as a function of alpha sigma squared fixed

1208
01:18:38,750 --> 01:18:40,630
all other variables are fixed

1209
01:18:42,030 --> 01:18:47,830
and that's the case again

1210
01:18:47,880 --> 01:18:50,920
OK let's move on

1211
01:19:06,080 --> 01:19:08,950
it's reasonable flexible talks to matter too much

1212
01:19:08,960 --> 01:19:11,920
running starting part two

1213
01:19:15,940 --> 01:19:18,610
so this is what part two would have be about

1214
01:19:18,950 --> 01:19:21,210
so i just want to extend this idea

1215
01:19:22,090 --> 01:19:27,770
how integrating and allows us to robustly of select model parameters in the model themselves

1216
01:19:27,820 --> 01:19:29,990
i will start off with another ten i

1217
01:19:30,920 --> 01:19:35,780
was going to talk about that sparse bayesian models and dynamically reconfigure talking at that

1218
01:19:37,690 --> 01:19:38,780
so hopefully convey

1219
01:19:38,810 --> 01:19:43,720
much useful information as possible and also shared in the end

1220
01:19:46,530 --> 01:19:50,770
eventually over spelling is that this is not a fresh in your mind

1221
01:19:50,820 --> 01:19:53,430
previously as in the moment again

1222
01:19:53,430 --> 01:19:55,180
we maximize this term

1223
01:19:55,220 --> 01:19:59,650
to estimate alpha things glad

1224
01:19:59,870 --> 01:20:02,790
actually estimates discovery fixed at the weekend

1225
01:20:02,810 --> 01:20:04,640
if you wanted to

1226
01:20:04,680 --> 01:20:06,310
it quite nice powerful feature

1227
01:20:06,460 --> 01:20:09,990
the bayesian approach we could have actually estimated the noise level itself

1228
01:20:10,020 --> 01:20:13,260
hopefully training example that later

1229
01:20:13,340 --> 01:20:15,000
but for now i want to look at

1230
01:20:15,010 --> 01:20:18,150
extending what we've done being even more basic

1231
01:20:18,160 --> 01:20:22,340
trying to choose a particular model

1232
01:20:22,400 --> 01:20:25,430
by model what i mean is show we have a look at

1233
01:20:25,440 --> 01:20:28,010
what the best set of basis functions as

1234
01:20:28,120 --> 01:20:31,190
and perhaps some basis functions of parameters

1235
01:20:31,240 --> 01:20:34,680
particular lbs function has the width parameter

1236
01:20:34,720 --> 01:20:38,120
can we automatically via bayesian procedure

1237
01:20:38,170 --> 01:20:39,770
choose those

1238
01:20:39,790 --> 01:20:44,120
this parameter and the basis set at the same time

1239
01:20:44,130 --> 01:20:48,970
so to do this we have to be a little bit more bayesian

1240
01:20:48,980 --> 01:20:54,000
and we're going to have to perform integrating at the earlier said we can do

1241
01:20:54,010 --> 01:20:56,020
but just start by

1242
01:20:56,070 --> 01:20:57,300
saying well

1243
01:20:57,310 --> 01:20:58,630
the model we're going to sort of

1244
01:20:58,630 --> 01:21:00,310
perform inference over

1245
01:21:00,360 --> 01:21:01,990
is simply

1246
01:21:02,000 --> 01:21:06,480
the collection of the basis functions and any parameters or just call it faces

1247
01:21:07,400 --> 01:21:09,660
and the parameter are

1248
01:21:09,670 --> 01:21:11,510
so we want to calculate the posterior

1249
01:21:11,530 --> 01:21:12,930
of the model

1250
01:21:13,040 --> 01:21:16,340
posterior probability the model given the data

1251
01:21:16,420 --> 01:21:18,830
this is the sort of essence

1252
01:21:18,950 --> 01:21:20,750
the bayesian approach

1253
01:21:20,770 --> 01:21:23,730
we simply calculate probabilities of what we want to know

1254
01:21:23,790 --> 01:21:26,550
conditioned and you know what we upset

1255
01:21:26,610 --> 01:21:29,720
ideally we don't want anything else on the right-hand side

1256
01:21:29,780 --> 01:21:32,680
that's some problems happen when we leave

1257
01:21:32,720 --> 01:21:39,670
parameters that need to be instantiated on the right-hand side instead of integrating that

1258
01:21:39,720 --> 01:21:43,830
if we assume all models are equally likely we have no prior prejudice and no

1259
01:21:43,830 --> 01:21:45,220
reason to

1260
01:21:45,220 --> 01:21:49,190
we can say that all models are equally likely this is flat

1261
01:21:49,300 --> 01:21:50,980
to find the most probable model

1262
01:21:50,990 --> 01:21:52,480
we need to find the model

1263
01:21:52,480 --> 01:21:54,820
it gives nice probability to the data

1264
01:21:55,940 --> 01:21:58,720
this commonly happens in the sort of bayesian manipulation

1265
01:21:58,780 --> 01:22:00,900
we assume if we assume a flat prior

1266
01:22:00,940 --> 01:22:02,120
we simply want to

1267
01:22:02,130 --> 01:22:05,410
find the maximum of the likelihood the likelihood of the data

1268
01:22:05,420 --> 01:22:08,070
condition on the model

1269
01:22:08,170 --> 01:22:11,550
so this is where the integrating outcomes in the game

1270
01:22:11,570 --> 01:22:15,720
because we don't want all these hyperparameters on the right-hand side we want to name

1271
01:22:15,720 --> 01:22:21,540
i have been given the derivation

1272
01:22:21,590 --> 01:22:25,640
so i've just sketched of the proof there is an asymptotic expression for the variance

1273
01:22:25,640 --> 01:22:27,720
of this estimator

1274
01:22:27,740 --> 01:22:31,500
you minimize the i think this is one of the things we can of large

1275
01:22:31,540 --> 01:22:34,810
u minimizes variance you get an expression for the q

1276
01:22:34,820 --> 01:22:38,450
and it happens to be that the constrained optimization problem

1277
01:22:38,510 --> 01:22:42,740
and because constrained because the some of the key over the whole space has that

1278
01:22:42,740 --> 01:22:47,680
up to one

1279
01:22:47,690 --> 01:22:52,200
o well the solution is that the solution to assess the effect

1280
01:22:52,240 --> 01:22:56,450
so the solution what you get is pfx times an exponential

1281
01:22:56,460 --> 01:22:58,310
now if you have a facts

1282
01:22:58,320 --> 01:23:00,280
written in exponential form

1283
01:23:00,290 --> 01:23:05,700
times another exponential you just add the exponent so you basically shifting the means

1284
01:23:05,700 --> 01:23:13,760
so i'm skipping the details here but that's what you will

1285
01:23:13,780 --> 01:23:16,370
so the true solution is impossible

1286
01:23:16,400 --> 01:23:18,630
so let's take it steps

1287
01:23:18,660 --> 01:23:23,330
you do an optimisation problem and you find out the optimal q

1288
01:23:23,340 --> 01:23:25,580
the optimal q requires this

1289
01:23:25,630 --> 01:23:30,140
this indicator functions so this is still impossible to do because direct

1290
01:23:30,160 --> 01:23:32,630
spiritual into

1291
01:23:34,140 --> 01:23:36,290
we're going to

1292
01:23:36,380 --> 01:23:41,790
do some large deviations theory which i haven't done

1293
01:23:41,830 --> 01:23:48,430
and this becomes e the feet some function of x

1294
01:23:48,450 --> 01:23:50,190
the i fx

1295
01:23:50,200 --> 01:23:52,190
is replaced by this kind of approach

1296
01:23:52,240 --> 01:23:56,210
and there's a lot of theoretical arguments as to why this is optimal

1297
01:23:56,260 --> 01:24:01,270
and then if this pfx is also say accounts in an exponential distribution

1298
01:24:01,320 --> 01:24:04,590
so x minus mu

1299
01:24:04,640 --> 01:24:07,460
square over sigma whatever

1300
01:24:07,530 --> 01:24:09,120
then you can add these two

1301
01:24:09,130 --> 01:24:12,580
and this will usually add up as i mean term so what you get is

1302
01:24:12,580 --> 01:24:15,850
in fact shift in the distribution

1303
01:24:17,380 --> 01:24:21,040
this flight details if you're an item in one d is just shift in high

1304
01:24:21,040 --> 01:24:23,200
dimensions this kind of

1305
01:24:23,210 --> 01:24:26,870
the way you shift to depends on the shape of heat you actually have to

1306
01:24:26,870 --> 01:24:30,080
solve another constrained optimisation problem

1307
01:24:30,090 --> 01:24:32,570
which can be very hard action

1308
01:24:43,690 --> 01:24:57,320
for full impact program for participation

1309
01:25:50,820 --> 01:26:05,620
that's correct

1310
01:26:05,640 --> 01:26:12,240
they affect the just brought because it's very specific example that people actually care about

1311
01:26:12,240 --> 01:26:12,940
a lot

1312
01:26:12,990 --> 01:26:15,020
first of all of

1313
01:26:15,040 --> 01:26:18,290
and it's also nice example because is one of those examples where you can crank

1314
01:26:18,290 --> 01:26:20,740
up variational theory and actually come up with

1315
01:26:20,780 --> 01:26:22,970
useful solutions

1316
01:26:23,020 --> 01:26:27,250
you are correct

1317
01:26:28,960 --> 01:26:33,490
you are correct

1318
01:26:33,550 --> 01:26:39,130
another thing also one more point about this is

1319
01:26:39,170 --> 01:26:40,550
quite often

1320
01:26:40,560 --> 01:26:44,190
people tend to also use the proposal distribution that just

1321
01:26:44,250 --> 01:26:49,440
that's the mode of the distribution of food optimisation and you put a proposal distribution

1322
01:26:49,440 --> 01:26:51,840
at the peak

1323
01:26:51,850 --> 01:26:55,810
that's fine if you what you care about the optimisation you care about integration that's

1324
01:26:55,810 --> 01:26:57,520
a stupid idea

1325
01:26:57,520 --> 01:27:00,140
i wrote a paper in two thousand three

1326
01:27:00,180 --> 01:27:02,630
called variational MCMC total rubbish

1327
01:27:02,640 --> 01:27:05,010
because i didn't see that then

1328
01:27:06,370 --> 01:27:09,790
and people so long see it now

1329
01:27:09,850 --> 01:27:13,420
so in my list of selected publications

1330
01:27:13,490 --> 01:27:16,610
it has to go

1331
01:27:16,620 --> 01:27:17,910
here's the catch

1332
01:27:17,920 --> 01:27:21,650
if you putting a proposal distribution with the high again

1333
01:27:21,660 --> 01:27:26,810
with the test high-density point is is not with the mass is the mass again

1334
01:27:26,810 --> 01:27:29,400
of the girls in high dimensions in the tail

1335
01:27:29,500 --> 01:27:33,530
if you and integrate units some of them as you counting the area

1336
01:27:33,540 --> 01:27:35,380
in high dimensions

1337
01:27:35,390 --> 01:27:40,110
coming up with heuristics putting the everest except the modes like some of these various

1338
01:27:40,110 --> 01:27:45,240
approximations on inside it these days people still used loopy BP is the proposal distribution

1339
01:27:45,240 --> 01:27:46,630
and so on

1340
01:27:46,640 --> 01:27:52,730
i mean this paper superior for any conferences with high probability of one on this

1341
01:27:53,270 --> 01:27:54,940
they're not so

1342
01:27:55,000 --> 01:27:57,000
because of

1343
01:27:57,340 --> 01:27:59,230
these high dimensional behaviors

1344
01:27:59,240 --> 01:28:04,560
this interesting counting to really care where the probability is not with the density is

1345
01:28:04,620 --> 01:28:06,670
that sort of more than

1346
01:28:06,720 --> 01:28:08,650
three key thing

1347
01:28:08,800 --> 01:28:11,180
so anyway so the estimator again

1348
01:28:11,690 --> 01:28:14,170
if you don't know the normalizing constant

1349
01:28:14,220 --> 01:28:17,050
it's just

1350
01:28:17,060 --> 01:28:21,940
the ratio of estimators and basically it's just one estimate if you just take

1351
01:28:21,990 --> 01:28:22,980
this site

1352
01:28:24,150 --> 01:28:29,140
and you should define w till the to be the normalised weights i w divided

1353
01:28:29,140 --> 01:28:31,310
by the sum of the top

1354
01:28:31,370 --> 01:28:36,340
so if you have the normalized weights with that

1355
01:28:37,590 --> 01:28:42,160
another thing that we do is so that will give you a sample version

1356
01:28:42,170 --> 01:28:43,410
that will give you

1357
01:28:43,430 --> 01:28:46,360
samples excited with the weight

1358
01:28:46,430 --> 01:28:50,180
and you might instead of having samples exile with the weight

1359
01:28:50,210 --> 01:28:51,690
you might want to have

1360
01:28:51,710 --> 01:28:54,230
a bunch of samples with weight one over n

1361
01:28:54,270 --> 01:28:55,910
like we had before

1362
01:28:55,920 --> 01:28:58,920
so here is something you can do

1363
01:28:58,930 --> 01:29:00,160
i suppose

1364
01:29:00,170 --> 01:29:02,760
so they inputs to this thing is

1365
01:29:02,800 --> 01:29:05,250
a bunch of samples x y

1366
01:29:05,260 --> 01:29:07,590
with the corresponding weight

1367
01:29:13,790 --> 01:29:22,150
this gives you an empirical density

1368
01:29:22,500 --> 01:29:24,190
this sort of exercise

1369
01:29:24,210 --> 01:29:25,970
this is w i

1370
01:29:26,060 --> 01:29:31,750
that's the approximation of the distribution

1371
01:29:31,760 --> 01:29:48,760
now you can build the cumulative

1372
01:29:50,140 --> 01:30:11,540
so this is the CDF

1373
01:30:13,040 --> 01:30:16,270
we're going what we wanted to do is we want more

1374
01:30:16,290 --> 01:30:19,820
after having some falls

1375
01:30:21,570 --> 01:30:24,350
with the new index j

1376
01:30:24,390 --> 01:30:27,750
and here i had i call one two and

1377
01:30:27,760 --> 01:30:32,940
and i'm going to want only and samples

1378
01:30:32,970 --> 01:30:38,260
and this is for example people the graphics to simulate light

1379
01:30:38,270 --> 01:30:40,270
with the ray tracing

1380
01:30:40,280 --> 01:30:43,590
with this the party the samples actually right

1381
01:30:45,690 --> 01:30:49,920
they can only afford very few of these evaluations because they're very expensive you have

1382
01:30:49,920 --> 01:30:52,520
to solve a lot of geometry

1383
01:30:54,010 --> 01:30:55,560
and so what you do is

1384
01:30:55,570 --> 01:30:58,260
after you've sampled with importance sampling

1385
01:30:58,340 --> 01:31:02,710
you resample to reduce the number of samples so you choose and to be a

1386
01:31:02,710 --> 01:31:05,370
lot less than n

1387
01:31:05,500 --> 01:31:09,140
so we want to have few samples and we want him to have

1388
01:31:09,150 --> 01:31:11,260
alright one over n

1389
01:31:11,310 --> 01:31:13,820
uniform weights in other words

1390
01:31:13,840 --> 01:31:15,620
how we do this

1391
01:31:15,660 --> 01:31:23,110
we use the trick of called resampling

1392
01:31:23,150 --> 01:31:25,040
and the way resampling works

1393
01:31:25,880 --> 01:31:32,630
you pick the uniform distribution

1394
01:31:32,630 --> 01:31:34,410
good morning

1395
01:31:34,430 --> 01:31:36,300
your colleagues and friends

1396
01:31:36,320 --> 01:31:38,300
it's my great pleasure

1397
01:31:38,300 --> 01:31:43,900
to introduce to you our today's keynote speaker get gigerenzer

1398
01:31:43,910 --> 01:31:49,150
he is director of the max planck institute for human development in berlin

1399
01:31:49,180 --> 01:31:50,490
many of us

1400
01:31:50,510 --> 01:31:57,580
remember his generous hospitality when the i a s e satellite conference in two thousand

1401
01:31:58,040 --> 01:32:01,110
three took place this institute

1402
01:32:02,150 --> 01:32:09,430
former policies include professorships in germany and in austria and at the university of chicago

1403
01:32:09,500 --> 01:32:15,190
which he gave up to become director of the max plank institute for simunic for

1404
01:32:15,190 --> 01:32:17,900
psychological research and then in berlin four

1405
01:32:17,970 --> 01:32:20,890
the max planck institute for human development

1406
01:32:20,900 --> 01:32:28,140
a critique of the work of daniel kahneman and amos tversky he argues

1407
01:32:28,150 --> 01:32:30,190
the heuristics

1408
01:32:30,220 --> 01:32:37,010
should not lead us to conceive of human being human thinking as riddled with cognitive

1409
01:32:38,470 --> 01:32:43,630
but rather consider rationality as an adaptive tool

1410
01:32:43,690 --> 01:32:47,820
that is bounded by constraints given through time and space

1411
01:32:47,820 --> 01:32:50,180
in the environment

1412
01:32:51,160 --> 01:32:55,550
one of the most quoted german psychologist

1413
01:32:55,570 --> 01:32:58,990
and also of quite a number of

1414
01:32:59,020 --> 01:33:01,360
pioneering research papers

1415
01:33:01,410 --> 01:33:03,830
as well as author of

1416
01:33:03,880 --> 01:33:06,550
number of books for the general public

1417
01:33:07,050 --> 01:33:09,650
he trains medical doctors

1418
01:33:10,790 --> 01:33:13,410
in financial managers

1419
01:33:13,430 --> 01:33:16,900
in their decision-making competencies

1420
01:33:16,910 --> 01:33:23,460
one of his recent books was entitled reckoning with risk

1421
01:33:23,470 --> 01:33:26,240
learning to to live with uncertainty

1422
01:33:26,240 --> 01:33:28,410
and it had such an impact

1423
01:33:28,430 --> 01:33:31,740
that's the british investment banker

1424
01:33:31,800 --> 01:33:36,740
david harding donated one point five million euros

1425
01:33:36,770 --> 01:33:43,250
two to get gigerenzer as kick off for what later became

1426
01:33:43,280 --> 01:33:48,760
what then became what now became the harding center for risk literacy

1427
01:33:50,850 --> 01:33:56,160
the vision to to our concept of statistical literacy

1428
01:33:56,190 --> 01:34:02,570
that we are very proud and feel honoured that you accepted the invitation to

1429
01:34:02,590 --> 01:34:07,440
to be keynote speaker at a conference in the we very much look forward to

1430
01:34:07,440 --> 01:34:08,880
your presentation

1431
01:34:20,100 --> 01:34:21,010
in early

1432
01:34:21,030 --> 01:34:22,410
twenty centuries

1433
01:34:22,420 --> 01:34:24,530
the science fiction author

1434
01:34:24,540 --> 01:34:26,310
herbert george wells

1435
01:34:26,390 --> 01:34:28,620
i made the following prediction

1436
01:34:28,630 --> 01:34:30,470
if we want

1437
01:34:30,480 --> 01:34:36,540
educated citizens in our modern technological world we need to teach them three

1438
01:34:41,100 --> 01:34:44,510
statistical thinking

1439
01:34:44,560 --> 01:34:46,890
that it's

1440
01:34:46,910 --> 01:34:47,670
i with

1441
01:34:47,690 --> 01:34:48,880
to deal with

1442
01:34:48,910 --> 01:34:50,350
the risks

1443
01:34:50,380 --> 01:34:57,910
in an uncertain world or more generally learning to live with an uncertain world

1444
01:34:57,910 --> 01:35:01,040
how far have we got now

1445
01:35:01,060 --> 01:35:03,600
in the early twenty first century

1446
01:35:03,670 --> 01:35:06,170
in most western countries

1447
01:35:06,190 --> 01:35:07,540
we have taught

1448
01:35:07,560 --> 01:35:09,040
practically everyone

1449
01:35:09,040 --> 01:35:10,750
reading and writing but not

1450
01:35:10,790 --> 01:35:12,340
statistical thinking

1451
01:35:12,350 --> 01:35:14,790
and american

1452
01:35:16,120 --> 01:35:20,040
new newspeak once announced the weather in the following way

1453
01:35:20,060 --> 01:35:25,380
the probability that it will rain on saturday he said is fifty percent

1454
01:35:25,440 --> 01:35:28,670
the probability that it will rain on sunday

1455
01:35:29,440 --> 01:35:32,450
also fifty percent therefore he concluded

1456
01:35:32,760 --> 01:35:38,790
but the probability that it will rain on the weekend is under percent

1457
01:35:40,500 --> 01:35:43,380
you are a sophisticated craft but

1458
01:35:43,420 --> 01:35:46,670
do you know what it means when you hear on the radio

1459
01:35:46,720 --> 01:35:50,050
that there is a thirty percent chance of rain tomorrow

1460
01:35:50,090 --> 01:35:53,700
thirty percent of what

1461
01:35:53,750 --> 01:35:55,070
i have done this study

1462
01:35:55,080 --> 01:36:02,640
in five large cities in five countries berlin new york amsterdam athens and black

1463
01:36:03,690 --> 01:36:10,050
most people in berlin and amsterdam believe that thirty percent chance of rain means that

1464
01:36:10,070 --> 01:36:12,440
it will rain tomorrow

1465
01:36:12,460 --> 01:36:14,930
in thirty percent of the time

1466
01:36:14,980 --> 01:36:17,960
that is seven to eight hours

1467
01:36:17,990 --> 01:36:23,020
others believe it will rain in thirty percent of the area

1468
01:36:23,030 --> 01:36:26,710
is most likely not where i live

1469
01:36:26,740 --> 01:36:31,880
most new yorkers believe neither of that is correct but it means that it will

1470
01:36:31,880 --> 01:36:35,940
rain on thirty percent of the base for which this prediction has been made that

1471
01:36:35,940 --> 01:36:39,080
means most likely not all

1472
01:36:39,250 --> 01:36:46,510
and people in athens basically still probability free when it comes to rest on one

1473
01:36:46,510 --> 01:36:51,140
lady there said OK i know what it means

1474
01:36:51,150 --> 01:36:56,820
three major role as you think it rains and seven not

1475
01:36:56,820 --> 01:37:00,360
this is an illustration of the general problem going to talk about

1476
01:37:00,370 --> 01:37:03,170
percentages or

1477
01:37:03,180 --> 01:37:05,260
according to some sources

1478
01:37:05,270 --> 01:37:08,460
the most frequent now in the media

1479
01:37:09,630 --> 01:37:12,770
few people understand and most importantly

1480
01:37:12,780 --> 01:37:14,150
if you notice

1481
01:37:14,240 --> 01:37:16,130
that they don't understand it

1482
01:37:16,140 --> 01:37:19,250
in this particular case the

1483
01:37:19,270 --> 01:37:22,780
cause and the solution is crystal clear

1484
01:37:22,920 --> 01:37:29,730
a probability of rain tomorrow is a single event probabilities tomorrow and by definition doesn't

1485
01:37:29,730 --> 01:37:31,510
specify the class

1486
01:37:32,310 --> 01:37:33,510
if you are

1487
01:37:33,560 --> 01:37:36,150
meteorologists days

1488
01:37:36,200 --> 01:37:39,940
and people naturally think in terms of classes

1489
01:37:40,000 --> 01:37:43,000
and they made up their make up something that sounds

1490
01:37:43,010 --> 01:37:47,640
that's to them so he is default clearly into communication

1491
01:37:47,690 --> 01:37:48,760
not so much

1492
01:37:48,780 --> 01:37:50,320
in the person

1493
01:37:50,380 --> 01:37:53,250
and much of what i will talk today about

1494
01:37:53,300 --> 01:37:57,430
medical about because communication is

1495
01:37:57,480 --> 01:38:01,840
and not about the false of patients or doctors but

1496
01:38:01,940 --> 01:38:03,260
the problem

1497
01:38:03,270 --> 01:38:05,200
that arises because

1498
01:38:05,230 --> 01:38:08,230
the information is communicated in the way

1499
01:38:08,250 --> 01:38:12,310
that the mind cannot easily understand

1500
01:38:12,360 --> 01:38:14,730
so there are always two ways

1501
01:38:15,880 --> 01:38:17,620
communicate the risks

1502
01:38:17,620 --> 01:38:20,870
and so the foundational results of compressed sensing

1503
01:38:20,890 --> 01:38:25,100
that it's possible to recover perfectly sparse signal if we have the a form of

1504
01:38:25,100 --> 01:38:30,730
incoherence but the problem in real life is that signals are very often exactly sparse

1505
01:38:31,870 --> 01:38:37,270
so the only exactly sparsity always nearly sparse so we need to address what happens

1506
01:38:37,270 --> 01:38:40,410
when the signal is now dispersed and also

1507
01:38:40,430 --> 01:38:45,250
unfortunately i cannot design sensors have an infinite amount of precision so i need to

1508
01:38:45,250 --> 01:38:49,370
be able to tell them to say what happens when i have no data

1509
01:38:49,390 --> 01:38:54,500
because if compressed sensing was the theory that is a bit analog to analog to

1510
01:38:54,500 --> 01:39:00,210
error correction where i realize enormously on the fact that the number of transmitted errors

1511
01:39:00,210 --> 01:39:06,500
missing only seventeen and eighteen or nineteen and infinite precision on the bits that have

1512
01:39:06,500 --> 01:39:11,640
received the compressed sensing will not be useful in signal processing applications because i'm not

1513
01:39:12,440 --> 01:39:14,100
of sensors that can

1514
01:39:15,640 --> 01:39:18,910
measured data was infinite precision

1515
01:39:18,930 --> 01:39:22,770
and it would be really ashame if we had a theory that relied on the

1516
01:39:22,770 --> 01:39:24,560
twenty six digits

1517
01:39:25,600 --> 01:39:29,100
the measured data to work

1518
01:39:30,640 --> 01:39:32,210
all right so

1519
01:39:32,230 --> 01:39:37,370
in real life images are not sparse they are nearly sparse you've seen before so

1520
01:39:37,370 --> 01:39:39,160
here's a picture of

1521
01:39:39,210 --> 01:39:43,440
the cameraman and when i look at this with the coefficient table i see that

1522
01:39:43,440 --> 01:39:46,710
most of the coefficient table is dark blue or black

1523
01:39:46,730 --> 01:39:51,480
which means that the coefficients of the world and i see coefficients indicated by

1524
01:39:51,500 --> 01:39:53,870
bright colors that are nonzero

1525
01:39:53,890 --> 01:39:58,120
at certain locations which of course have to do with the geometry of the object

1526
01:39:58,260 --> 01:39:59,600
so one of the

1527
01:39:59,600 --> 01:40:01,830
the point is weighted the coefficient table

1528
01:40:01,850 --> 01:40:05,770
is to show you that of course the locations of the wavelet coefficients are non-zero

1529
01:40:06,520 --> 01:40:11,330
highly image dependent depends on the geometry of the image and also for to look

1530
01:40:11,330 --> 01:40:15,440
at them all will see that none of them is actually exactly zero

1531
01:40:15,460 --> 01:40:21,200
OK so what is compressed sensing so the classical viewpoint about sampling

1532
01:40:21,210 --> 01:40:26,560
these as follows which is that you have a sneery sparse object a compressible object

1533
01:40:26,580 --> 01:40:30,890
what you do is you measure everything that we have very expensive camera we can

1534
01:40:30,890 --> 01:40:32,810
measure all the pixels

1535
01:40:32,830 --> 01:40:36,640
when you have all the pixels to compute all the wavelet coefficients and you give

1536
01:40:36,640 --> 01:40:38,180
the largest

1537
01:40:38,230 --> 01:40:42,540
so you compressed down the images by keeping the largest coefficient and when you do

1538
01:40:42,540 --> 01:40:49,290
this to pay certain distortion which is the difference between the original object and its

1539
01:40:49,290 --> 01:40:54,850
approximation that keeps the s largest coefficients in this expansion

1540
01:40:54,870 --> 01:40:58,100
compressed sensing is completely defined it says no

1541
01:40:58,100 --> 01:41:01,100
we don't measure everything and we don't

1542
01:41:01,210 --> 01:41:05,250
we don't throughout stuff

1543
01:41:05,290 --> 01:41:07,810
what we do is we actually measuring by

1544
01:41:07,810 --> 01:41:11,120
calculating inner products with incoherent waveforms

1545
01:41:11,140 --> 01:41:16,480
so we take n measurements and we we construct a linear programming

1546
01:41:16,580 --> 01:41:20,710
and the main results in the field say that if you do this or this

1547
01:41:20,730 --> 01:41:25,270
to get the same performance provided that the number of measurements you make

1548
01:41:25,330 --> 01:41:30,210
is essentially is the number of coefficients you keep in your best representations

1549
01:41:30,210 --> 01:41:32,120
times is logarithmic factors

1550
01:41:32,210 --> 01:41:34,750
and if in this approach you sense

1551
01:41:34,770 --> 01:41:39,750
the object very few times well how many times the number of

1552
01:41:39,770 --> 01:41:45,790
coefficients you keep in your compressed representation something like factor then what compressed sensing guarantees

1553
01:41:46,210 --> 01:41:49,140
is that when you reconstruct by linear programming is the error

1554
01:41:49,230 --> 01:41:50,770
is less than

1555
01:41:50,790 --> 01:41:56,230
the distortion you get by seeing everything in keeping the smartest coefficient

1556
01:41:56,250 --> 01:41:59,040
it does this result makes sense

1557
01:41:59,060 --> 01:42:00,410
in particular

1558
01:42:00,430 --> 01:42:05,520
if this actually happens to be exactly sparse then f is equal to f s

1559
01:42:05,520 --> 01:42:09,410
there's no difference and therefore the reconstruction error is zero

1560
01:42:09,430 --> 01:42:13,980
but in general what you get is you get the same distortion as somebody who

1561
01:42:13,980 --> 01:42:18,600
has seen everything and has kept the swan relations

1562
01:42:18,620 --> 01:42:20,540
and you get these out of

1563
01:42:20,560 --> 01:42:24,000
a number of measurements which is marginally bigger is an

1564
01:42:24,700 --> 01:42:28,100
essentially as times log and this

1565
01:42:32,600 --> 01:42:34,810
all right

1566
01:42:34,830 --> 01:42:38,820
OK so what he says is that if you can design sensors who take that

1567
01:42:38,820 --> 01:42:44,290
take incoherent measurement because of this result said that when you making cohen measurements you

1568
01:42:45,180 --> 01:42:49,160
a compressed version of the object in your hand or in other words do simultaneous

1569
01:42:49,160 --> 01:42:55,850
signal acquisition and compression and so this discovery was made independently by david owen hart

1570
01:42:55,850 --> 01:42:57,310
exactly the same time

1571
01:42:57,330 --> 01:42:59,850
in two thousand four

1572
01:42:59,850 --> 01:43:03,790
all right so you holdings is compressed version of the object by

1573
01:43:05,270 --> 01:43:10,680
incoherent things about an object is essentially getting you compress data about these object all

1574
01:43:10,700 --> 01:43:15,080
you need to do to actually display is to solve an optimisation problem

1575
01:43:15,080 --> 01:43:20,180
which is what the decompression step involves and so to paraphrase denis healy was my

1576
01:43:20,180 --> 01:43:26,390
program manager that he said that whilst compressed sensing such the possibility of compressed data

1577
01:43:26,390 --> 01:43:32,120
acquisition protocols which perform as if you could directly acquire just important information about the

1578
01:43:32,120 --> 01:43:35,520
image of interest and nothing else

1579
01:43:35,520 --> 01:43:38,770
right so a kind of resolve this product which is

1580
01:43:38,790 --> 01:43:46,020
perhaps compressed sensing is a way of getting information that we will not go away

1581
01:43:46,060 --> 01:43:50,910
it's an example is something like this so bali cannot see my pictures

1582
01:43:50,930 --> 01:43:56,580
so this is a compressed version i looked at everything i compress it down and

1583
01:43:56,580 --> 01:43:59,980
then i look at three times the number of coefficient like you to get this

1584
01:43:59,980 --> 01:44:04,790
picture i sort of want something a bit more sophisticated than n one and get

1585
01:44:04,790 --> 01:44:09,310
something as higher PSNR so roughly the same

1586
01:44:10,020 --> 01:44:13,830
OK but you can't see my pictures very well so

1587
01:44:13,830 --> 01:44:16,020
OK so now we try to kind of

1588
01:44:16,080 --> 01:44:22,250
express this main result mathematically because it wasn't a rigorous statement what i give you

1589
01:44:23,620 --> 01:44:26,540
so now i'm going to give you requested

1590
01:44:26,540 --> 01:44:30,640
so the problem that we're looking at is the problem of recovering

1591
01:44:30,660 --> 01:44:35,080
a vector x in r and from what i call phi x now x is

1592
01:44:35,080 --> 01:44:39,700
going to be a sparse coefficient sequence and so we can have this kind of

1593
01:44:39,710 --> 01:44:45,210
underdetermined problems to solve like all fights where an unknown an equations

1594
01:44:45,290 --> 01:44:46,580
all right

1595
01:44:46,600 --> 01:44:47,640
one of the

1596
01:44:47,640 --> 01:44:51,880
that's not in the other set I suppose it doesn't matter which way around we

1597
01:44:51,880 --> 01:44:55,180
do it but let's assume it's this way around so the point the first point

1598
01:44:55,180 --> 01:44:59,540
in the first set I can assume is the first point doesn't matter is not in the

1599
01:44:59,540 --> 01:45:06,960
second set well so first what we'll do is we will subtract this thing here

1600
01:45:06,960 --> 01:45:14,920
from this expression on both sides and so we bring it on the other

1601
01:45:14,920 --> 01:45:20,540
side and so we get a zero on one side and we get some kernel

1602
01:45:20,540 --> 01:45:25,440
expansion on the other side which might now be a lot of kernel expansion it

1603
01:45:25,440 --> 01:45:31,900
might have so on the previous slide it had N points on one side

1604
01:45:31,900 --> 01:45:35,380
and on the other side so I'll bring it on one side I might have N plus

1605
01:45:35,380 --> 01:45:38,840
N  points but I might have a little bit less than that because some points

1606
01:45:39,300 --> 01:45:42,420
might be identical some points from X might be in Y or vice versa

1607
01:45:42,460 --> 01:45:49,120
so we to make it a sum of pairwise distinct points

1608
01:45:49,120 --> 01:45:57,920
you get an expression like this where the first point is X one the

1609
01:45:57,920 --> 01:46:03,420
first coefficient is alpha one remember we're assuming the first point doesn't appear in

1610
01:46:03,420 --> 01:46:07,560
the second set so in this procedure of making this a sum of pairwise distinct

1611
01:46:07,560 --> 01:46:11,300
points the first point is not going to get changed the first point only

1612
01:46:11,300 --> 01:46:16,260
occurs in the set X so that means the first point is still there and

1613
01:46:16,260 --> 01:46:22,320
the first coefficient is still nonzero was nonzero to begin it hasn't changed all the

1614
01:46:22,320 --> 01:46:29,200
other points all the other coefficients I don't know okay so so the first point is still

1615
01:46:29,200 --> 01:46:33,970
there the other ones are somehow from the rest of this set and

1616
01:46:34,050 --> 01:46:38,980
could be the coefficients could be anything so then we have this nice expansion we

1617
01:46:38,980 --> 01:46:44,870
take the dot product with the same expansion dot product in in the R K H S

1618
01:46:44,870 --> 01:46:51,280
and then get an expression like this and this expression now probably looks

1619
01:46:51,280 --> 01:46:55,020
familiar to you is the expression that we usually check when we check for positive

1620
01:46:55,030 --> 01:47:00,960
definiteness and we know that the coefficient vector here is nonzero because we know

1621
01:47:00,960 --> 01:47:06,220
that the first entry is nonzero the first entry is the alpha one which we assumed to

1622
01:47:06,220 --> 01:47:11,980
be nonzero so we have an expression where this is zero even though the coefficient

1623
01:47:11,980 --> 01:47:19,420
vector's nonzero and that's exactly what's ruled out by the assumption of strict positive definiteness therefore

1624
01:47:19,420 --> 01:47:23,600
the kernel cannot be strictly positive definite but actually we were assuming that it's

1625
01:47:23,600 --> 01:47:29,780
strictly positive definite so we have a contradiction so therefore the two sets have to

1626
01:47:29,780 --> 01:47:40,080
be identical okay so the so now we know that if we take a  strictly positive definite kernel

1627
01:47:40,080 --> 01:47:44,780
set of points into reproducing kernels hibert space with it and we take

1628
01:47:44,780 --> 01:47:51,400
a mean of set of points we are not losing any information it remembers all the points

1629
01:47:51,400 --> 01:47:56,360
so let's look at this mean map a little bit more so it takes a

1630
01:47:56,360 --> 01:48:01,140
set of points maps mean into that space takes the mean it has a few

1631
01:48:01,140 --> 01:48:07,100
interesting properties one is if we take a dot product between this point mu of X

1632
01:48:07,100 --> 01:48:11,520
so these mean they reproducing kernel hilbert space kernel mean for short

1633
01:48:11,520 --> 01:48:16,680
take a dot product between the kernel mean and some arbitrary function F and you get this

1634
01:48:16,680 --> 01:48:22,140
thing now you use the fact that K represents point evaluation of F which means that we

1635
01:48:22,140 --> 01:48:29,180
get F of XI here for this thing so therefore this mu in sense represents

1636
01:48:29,180 --> 01:48:35,640
the operation of taking the average of a function over a sample X so it's it's

1637
01:48:35,640 --> 01:48:40,940
the average operator so it's a compact way of computing the average of a

1638
01:48:40,940 --> 01:48:48,120
function and another interesting thing to point out here is if we now compute

1639
01:48:48,140 --> 01:48:51,400
the distance between the mu of sum set X in the mu of sum set

1640
01:48:51,410 --> 01:48:58,320
Y so we can first rewrite this thing this is also essentially schwartz by

1641
01:48:58,330 --> 01:49:04,200
saying that this difference vector  is equal to the supremum that we get by taking

1642
01:49:04,410 --> 01:49:09,960
dot products between this spector so the norm of this spector is equal to taking dot products with arbitrary

1643
01:49:09,960 --> 01:49:14,600
vectors F that have length at most one and then taking the supremum over this

1644
01:49:14,600 --> 01:49:19,140
okay I think you probably are familiar with this and here again we

1645
01:49:19,140 --> 01:49:24,360
have something of this form so we can use this  here we write it like this which

1646
01:49:24,360 --> 01:49:31,980
means that  that two means of two data sets in the feature space will have

1647
01:49:32,330 --> 01:49:37,660
a distance between them or will be different if and only if we can find functions

1648
01:49:37,660 --> 01:49:43,260
a funk function F in the reproducing kernel Hibert space that distinguishes the two sets of

1649
01:49:43,260 --> 01:49:51,900
of points so so the functions in Hilbert space distinguish the two sets of points in this

1650
01:49:51,900 --> 01:49:57,200
in this mean sense so the functions will have different values in average on

1651
01:49:57,200 --> 01:50:01,930
the ones at this point on the one set of points from the values

1652
01:50:01,930 --> 01:50:08,640
on the other set of points and if we have two data sets we can

1653
01:50:08,640 --> 01:50:15,860
also look at which function best witnesses this difference so if you think

1654
01:50:15,860 --> 01:50:20,220
about here which function do I have to plug in to maximize this thing

1655
01:50:20,220 --> 01:50:25,340
well actually is the function that's that's parallel to this vector but then normalized

1656
01:50:25,340 --> 01:50:29,300
by the length of this vector because here we have this constraint the function should have

1657
01:50:29,300 --> 01:50:34,880
length one so let's take a look how how this function looks it's actually kind of

1658
01:50:34,880 --> 01:50:41,140
instructive because if we now take two sets of data points let's say we take

1659
01:50:41,140 --> 01:50:47,760
data from a gaussian and data from laplace distribution one-dimensional and to make

1660
01:50:47,760 --> 01:50:52,220
it a little bit more difficult we will match the the first two moments so we

1661
01:50:52,220 --> 01:50:57,900
will make sure that the gaussian leplace both have mean zero and variance one

1662
01:50:57,900 --> 01:51:02,700
so they are not they're not trivial to distinguish you could say but let's distinguish them in

1663
01:51:02,710 --> 01:51:07,020
the reproducing kernel space of a gaussian kernel it turns out the gaussian

1664
01:51:07,020 --> 01:51:11,300
is strictly positive definite haven't proved that but with the gaussian we can distinguish anything

1665
01:51:11,340 --> 01:51:15,540
in principle so let's see how the gaussian distinguishes this so the gaussian has to

1666
01:51:15,540 --> 01:51:21,760
distinguished this by finding a function that best distinguishes the samples a function that on

1667
01:51:21,760 --> 01:51:25,140
twenty for that is the difference in

1668
01:51:25,940 --> 01:51:28,700
next is the defensive

1669
01:51:28,720 --> 01:51:32,890
is called the body and the body

1670
01:51:34,680 --> 01:51:37,950
monumentally metric

1671
01:51:37,960 --> 01:51:40,200
we can define

1672
01:51:42,030 --> 01:51:47,990
five hundred children this this is called the liberty the connection

1673
01:51:48,030 --> 01:51:52,200
but in this the course genetic of find an excelled

1674
01:51:52,210 --> 01:51:54,890
so the joystick

1675
01:51:54,930 --> 01:52:06,570
is a dependent of the choice of a five connections

1676
01:52:06,580 --> 01:52:07,530
so all

1677
01:52:07,570 --> 01:52:10,740
it doesn't define studies good morning the

1678
01:52:11,390 --> 01:52:12,250
one four

1679
01:52:14,250 --> 01:52:17,550
what a b c mean when your money or the

1680
01:52:17,700 --> 01:52:18,740
the it

1681
01:52:18,780 --> 01:52:20,880
of the also free

1682
01:52:20,950 --> 01:52:25,850
one actual so that those something is abundant she's

1683
01:52:26,630 --> 01:52:28,450
so we could put it

1684
01:52:28,450 --> 01:52:33,010
you know what it's a statistical manifold if

1685
01:52:33,020 --> 01:52:37,260
covariant derivative with the metric is called antisymmetric

1686
01:52:39,560 --> 01:52:42,390
in this case we can define

1687
01:52:42,400 --> 01:52:44,250
find one actual

1688
01:52:45,800 --> 01:52:51,270
it is called the order of one actual on one actual by this

1689
01:52:53,030 --> 01:52:55,590
in this case the data is also

1690
01:52:55,720 --> 01:52:58,500
statistical manifolds

1691
01:52:58,710 --> 01:53:04,820
we call the one or that you want to that this statistical one

1692
01:53:06,250 --> 01:53:13,220
statistical little the studies going to one of the structure of course

1693
01:53:17,570 --> 01:53:21,780
it's be the so this is the set of

1694
01:53:21,800 --> 01:53:26,460
density functionals parameterized by

1695
01:53:28,930 --> 01:53:30,680
instead of on show

1696
01:53:31,510 --> 01:53:34,030
can be divided the money the ways

1697
01:53:34,050 --> 01:53:36,310
local coordinate system c

1698
01:53:36,320 --> 01:53:40,950
then we can define e minimum imagery

1699
01:53:40,950 --> 01:53:43,330
by the fish

1700
01:53:43,390 --> 01:53:45,260
and commissioned the thirty six

1701
01:53:47,450 --> 01:53:51,250
we can define

1702
01:53:52,090 --> 01:53:53,640
one parameter family of

1703
01:53:53,640 --> 01:53:57,440
well i mean if i mean you will find connections like this

1704
01:54:00,940 --> 01:54:01,940
in this case

1705
01:54:01,950 --> 01:54:03,830
the medical structure

1706
01:54:04,300 --> 01:54:07,300
that this kind of forced

1707
01:54:07,390 --> 01:54:13,950
on the north coast

1708
01:54:15,410 --> 01:54:17,130
but as the functional

1709
01:54:23,120 --> 01:54:24,880
the function of

1710
01:54:24,890 --> 01:54:26,700
what times saying

1711
01:54:26,710 --> 01:54:32,260
there then we define local black

1712
01:54:32,590 --> 01:54:36,510
by this photo so

1713
01:54:37,450 --> 01:54:39,260
two arguments so

1714
01:54:39,260 --> 01:54:41,620
OK that makes a

1715
01:54:41,660 --> 01:54:43,620
up to the FA

1716
01:54:46,430 --> 01:54:48,330
a second

1717
01:54:51,150 --> 01:54:54,600
they were part

1718
01:54:54,700 --> 01:54:55,990
then this is a

1719
01:54:56,030 --> 01:54:59,100
thanks to all

1720
01:55:01,590 --> 01:55:02,890
this is a rule

1721
01:55:02,900 --> 01:55:03,590
these are

1722
01:55:03,600 --> 01:55:05,880
the functional elements

1723
01:55:08,140 --> 01:55:13,480
because it owned by the city

1724
01:55:13,610 --> 01:55:16,950
the derivative vanishes

1725
01:55:17,000 --> 01:55:23,780
on the second day the but if you move the money in metric only in

1726
01:55:23,830 --> 01:55:31,000
on the floor

1727
01:55:31,060 --> 01:55:34,110
that the but all the

1728
01:55:34,120 --> 01:55:35,520
the functional

1729
01:55:35,540 --> 01:55:37,200
we can define

1730
01:55:37,210 --> 01:55:39,030
a bit of

1731
01:55:39,070 --> 01:55:42,230
find connections

1732
01:55:42,240 --> 01:55:43,880
and in this case

1733
01:55:43,900 --> 01:55:45,520
but it but it's

1734
01:55:46,490 --> 01:55:49,820
mean only do one statistical manifolds

1735
01:55:49,830 --> 01:55:52,230
so we said that these models

1736
01:55:52,250 --> 01:55:57,970
in is study could money flows from the contrast function law

1737
01:55:58,030 --> 01:56:05,290
and this is example of other the functions

1738
01:56:08,200 --> 01:56:09,820
euclidean space

1739
01:56:10,810 --> 01:56:11,500
the the way

1740
01:56:11,730 --> 01:56:15,620
deeds of the euclidean distance

1741
01:56:15,650 --> 01:56:16,520
so set

1742
01:56:20,240 --> 01:56:21,580
half of

1743
01:56:21,590 --> 01:56:23,600
the square of the

1744
01:56:23,610 --> 01:56:25,900
the euclidean distance

1745
01:56:25,910 --> 01:56:26,860
in this case

1746
01:56:26,870 --> 01:56:30,340
is a contrast function

1747
01:56:30,400 --> 01:56:34,830
and the role in this is stunned the euclidean structure

1748
01:56:34,970 --> 01:56:36,570
the and stuff

1749
01:56:36,620 --> 01:56:41,430
among the euclidean metric

1750
01:56:42,940 --> 01:56:44,220
the the second example

1751
01:56:44,910 --> 01:56:49,030
divergence canonical divergence

1752
01:56:49,070 --> 01:56:52,350
we are open to all

1753
01:56:55,550 --> 01:56:57,290
convex functions going

1754
01:56:59,220 --> 01:57:00,560
we defined

1755
01:57:06,450 --> 01:57:09,740
want the functional

1756
01:57:09,910 --> 01:57:13,580
this and all indices of the statistical models

1757
01:57:13,600 --> 01:57:15,850
so the covered

1758
01:57:15,860 --> 01:57:20,020
over this connection or responses

1759
01:57:20,070 --> 01:57:24,470
but of course the euclidean distance

1760
01:57:24,530 --> 01:57:26,190
this quantity

1761
01:57:26,230 --> 01:57:27,860
this function is

1762
01:57:27,950 --> 01:57:32,200
you squared euclidean distance

1763
01:57:32,210 --> 01:57:34,830
it is also putting one day versus

1764
01:57:34,930 --> 01:57:41,730
because the discourse

1765
01:57:41,820 --> 01:57:44,360
four my diagrams one

1766
01:57:47,150 --> 01:57:47,910
so what

1767
01:57:47,920 --> 01:57:50,330
literally the contrast function

1768
01:57:54,310 --> 01:57:57,860
yes it over in points

1769
01:57:58,950 --> 01:58:04,150
we can define the contrast voronoi detour

1770
01:58:04,190 --> 01:58:05,720
it is formed

1771
01:58:05,840 --> 01:58:12,750
and we assume that voronoi regions for p

1772
01:58:12,770 --> 01:58:13,700
but so

1773
01:58:13,700 --> 01:58:18,610
mean in regions so

1774
01:58:18,620 --> 01:58:20,200
the plot shows

1775
01:58:20,240 --> 01:58:22,060
we go to the parts you

1776
01:58:23,050 --> 01:58:29,060
contrast voronoi diagram of p with respect to the functional

1777
01:58:30,410 --> 01:58:31,730
and then we must

1778
01:58:31,730 --> 01:58:37,970
so initially we think about the value of this is just the value in school

1779
01:58:37,980 --> 01:58:39,880
an interval has a particular value

1780
01:58:40,040 --> 01:58:43,780
OK we don't know what the value is so you think about this in the

1781
01:58:43,780 --> 01:58:45,480
bayesian way that we think all

1782
01:58:45,530 --> 01:58:48,430
i don't know what it is and to me the random variables

1783
01:58:50,380 --> 01:58:53,150
although it although this is one of the examples where

1784
01:58:53,190 --> 01:58:58,400
there's no frequency interpretation of this you can argue whether this is really random but

1785
01:58:58,410 --> 01:59:01,570
you don't know something to treated as though it was random

1786
01:59:01,620 --> 01:59:05,010
so let's try this thing is totally random

1787
01:59:05,920 --> 01:59:09,380
but in fact we treat the whole function of those random

1788
01:59:09,390 --> 01:59:13,660
we don't know what the function does until we evaluated

1789
01:59:14,810 --> 01:59:18,080
until actually compute what is of x equals two

1790
01:59:19,290 --> 01:59:21,650
well i don't really know

1791
01:59:21,730 --> 01:59:24,690
if i could just evaluate the function at all

1792
01:59:24,910 --> 01:59:28,210
at all values x then there wouldn't be a problem into

1793
01:59:28,530 --> 01:59:32,270
and then you can just then you could have some kind of about that so

1794
01:59:32,270 --> 01:59:38,470
you can evaluate all possible that

1795
01:59:40,560 --> 01:59:43,250
we get the recipe

1796
01:59:44,040 --> 01:59:45,510
you specify

1797
01:59:46,760 --> 01:59:49,330
distribution over functions like to say well

1798
01:59:49,340 --> 01:59:54,240
i don't know exactly what the function is but i assume something over the function

1799
01:59:54,340 --> 01:59:55,860
about the function u

1800
01:59:56,100 --> 01:59:58,500
the constant process

1801
01:59:58,610 --> 02:00:00,660
distribution over functions

1802
02:00:00,710 --> 02:00:01,660
and then

1803
02:00:01,680 --> 02:00:02,520
you start

1804
02:00:02,530 --> 02:00:04,830
measuring what the function values are

1805
02:00:04,870 --> 02:00:06,740
at various places

1806
02:00:06,780 --> 02:00:12,110
in this case does not make matter how you how you choose to export

1807
02:00:12,120 --> 02:00:16,300
you could use them at random or you could choose an equally spaced or

1808
02:00:16,310 --> 02:00:18,210
you could get somebody to just for you

1809
02:00:18,220 --> 02:00:19,700
it doesn't matter

1810
02:00:20,740 --> 02:00:22,050
so now

1811
02:00:22,800 --> 02:00:24,400
prior distribution over

1812
02:00:24,450 --> 02:00:29,270
together with the data implies the posterior distribution over

1813
02:00:30,000 --> 02:00:35,570
and the posterior distribution over f remember the posterior distribution just the bundle functions

1814
02:00:35,580 --> 02:00:40,220
that implies a distribution over the quantity the incident

1815
02:00:40,230 --> 02:00:43,330
because every one of those functions in the bundle

1816
02:00:43,340 --> 02:00:46,530
with a particular value to the article

1817
02:00:46,540 --> 02:00:50,970
now if you if we look at all the possible functions inside the funnel give

1818
02:00:50,970 --> 02:00:52,250
you all distribution

1819
02:00:52,300 --> 02:00:54,970
over the thing that makes it

1820
02:00:56,060 --> 02:00:57,300
so let's try

1821
02:00:57,310 --> 02:00:58,920
and do that

1822
02:00:58,930 --> 02:00:59,840
OK so now

1823
02:01:00,300 --> 02:01:03,570
now interested in the interval

1824
02:01:03,580 --> 02:01:05,570
over all the function

1825
02:01:05,580 --> 02:01:06,540
with respect

1826
02:01:06,550 --> 02:01:08,280
he here

1827
02:01:08,290 --> 02:01:13,490
where is the posterior gaussianprocess process

1828
02:01:13,540 --> 02:01:18,570
because this is just this is just the protection of the golf courses onto the

1829
02:01:18,590 --> 02:01:20,750
this function here

1830
02:01:21,620 --> 02:01:25,780
the result they were going to get out we also have gas distribution

1831
02:01:25,890 --> 02:01:30,770
it's an infinite dimensional projection but is just the prediction of the

1832
02:01:30,830 --> 02:01:33,800
we can compute the mean

1833
02:01:33,850 --> 02:01:35,400
and the variance of the thing

1834
02:01:35,480 --> 02:01:37,410
OK so that will tell us that

1835
02:01:38,600 --> 02:01:41,150
the assumptions we had about the functions

1836
02:01:41,200 --> 02:01:42,110
and given

1837
02:01:42,120 --> 02:01:44,510
the measurements we make about the function

1838
02:01:44,520 --> 02:01:48,460
we can now come up with a with the best guess the mean

1839
02:01:48,510 --> 02:01:49,510
and we can also

1840
02:01:50,060 --> 02:01:53,200
well what is the variance of the estimate

1841
02:01:53,240 --> 02:01:56,190
it doesn't just give the point predictions that tells us how

1842
02:01:56,200 --> 02:01:58,530
sure can be about the value

1843
02:01:59,070 --> 02:02:01,280
let's look at the mean function

1844
02:02:01,330 --> 02:02:02,460
in detail

1845
02:02:03,060 --> 02:02:04,920
if the average

1846
02:02:05,080 --> 02:02:09,080
if the value of this of this interval

1847
02:02:09,130 --> 02:02:10,970
in our interval here

1848
02:02:11,040 --> 02:02:12,120
and then we have to

1849
02:02:12,560 --> 02:02:17,990
integrate that with respect to all possible choices of the function f

1850
02:02:18,000 --> 02:02:22,080
from the posterior distribution like the posterior distribution of f is

1851
02:02:22,240 --> 02:02:25,170
that given the observed data

1852
02:02:25,500 --> 02:02:30,750
now we can we can we can swap around these two integrations and that'll give

1853
02:02:30,750 --> 02:02:31,650
us now

1854
02:02:31,700 --> 02:02:33,480
the interval of the

1855
02:02:33,490 --> 02:02:38,190
of all the average of the function with respect to the posterior distribution

1856
02:02:38,240 --> 02:02:39,470
this is just the

1857
02:02:39,510 --> 02:02:41,790
this is just the average

1858
02:02:41,840 --> 02:02:42,960
of the function

1859
02:02:45,250 --> 02:02:46,670
average now

1860
02:02:46,680 --> 02:02:48,200
outside perspective

1861
02:02:48,460 --> 02:02:50,510
if you think about this

1862
02:02:50,530 --> 02:02:54,200
the average function on the golf process

1863
02:02:54,220 --> 02:02:56,410
vector p

1864
02:02:56,460 --> 02:02:57,900
sometimes is p

1865
02:02:57,910 --> 02:03:01,460
we some covariance functions we might be able to evaluate this

1866
02:03:01,790 --> 02:03:07,350
well it's not going to the exact details and similarly with a little bit more

1867
02:03:07,690 --> 02:03:13,460
little bit more information science here we can also write down the variances again for

1868
02:03:13,460 --> 02:03:17,400
some choices of p in some grand function is things can also be computed in

1869
02:03:17,400 --> 02:03:18,660
closed form

1870
02:03:21,040 --> 02:03:26,800
OK that's the details in interesting times

1871
02:03:26,910 --> 02:03:29,080
the but in the slides if you're interested

1872
02:03:29,090 --> 02:03:30,630
here's an example

