1
00:00:00,000 --> 00:00:01,530
i use my lecture notes

2
00:00:01,570 --> 00:00:05,690
same thing

3
00:00:05,710 --> 00:00:08,750
it's up to you

4
00:00:08,760 --> 00:00:15,460
you want me to hold it up for you

5
00:00:22,440 --> 00:00:26,590
that's what i sometimes do in the middle lecture across the world OK go ahead

6
00:00:35,130 --> 00:00:36,550
a show in which you do

7
00:00:50,500 --> 00:00:53,570
is moving string by

8
00:00:53,590 --> 00:00:54,590
we know

9
00:00:54,590 --> 00:00:59,800
by changing school in

10
00:01:23,000 --> 00:01:41,280
so the

11
00:01:55,780 --> 00:01:59,630
i would like to thank you

12
00:01:59,630 --> 00:02:03,000
very kind of you

13
00:02:04,590 --> 00:02:07,840
we have some other surprises for you

14
00:02:07,880 --> 00:02:12,230
my graduate student extract original miller you now working at the

15
00:02:12,250 --> 00:02:13,670
at harvard

16
00:02:13,670 --> 00:02:15,000
he has the didgeridoo

17
00:02:15,000 --> 00:02:17,440
in fact there was a story behind the data

18
00:02:17,730 --> 00:02:20,320
it's original research

19
00:02:20,340 --> 00:02:24,340
it's not so different from this

20
00:02:24,360 --> 00:02:26,820
it is open here

21
00:02:26,860 --> 00:02:27,710
it's open

22
00:02:27,710 --> 00:02:30,300
can look through it and see

23
00:02:30,320 --> 00:02:31,500
it is therefore

24
00:02:31,520 --> 00:02:33,110
also next benefits

25
00:02:34,320 --> 00:02:37,900
and they had better get the bot is i don't know where probably in australia

26
00:02:37,920 --> 00:02:41,360
and never finished first ever going to be the first

27
00:02:41,420 --> 00:02:48,360
got is that you don't walked away was

28
00:02:48,360 --> 00:02:51,190
now the price is in good faith

29
00:02:51,360 --> 00:02:55,860
i wanna see you for forty years john

30
00:02:55,880 --> 00:02:56,960
look at that

31
00:02:58,800 --> 00:03:01,840
that is close to forty

32
00:03:01,840 --> 00:03:03,670
in his right close

33
00:03:03,690 --> 00:03:08,440
so is fundamental respondents sixty eight together for forty gotta to get up

34
00:03:08,480 --> 00:03:16,760
by roughly two and a three times then good luck john

35
00:03:19,760 --> 00:03:22,670
that was good for the

36
00:03:22,690 --> 00:03:25,300
the in

37
00:03:25,320 --> 00:03:29,130
no way for

38
00:03:29,130 --> 00:03:31,340
well i don't believe

39
00:03:31,360 --> 00:03:33,960
why not try again

40
00:03:34,110 --> 00:03:37,250
i just want to spend up to

41
00:03:48,590 --> 00:03:52,440
i i think you get two hundred sixty eight so

42
00:03:52,460 --> 00:03:55,210
how can you say something very exciting

43
00:04:05,230 --> 00:04:06,900
we have only

44
00:04:06,940 --> 00:04:09,500
it is to my knowledge

45
00:04:09,530 --> 00:04:13,130
the physics of wave oscillations vibrations

46
00:04:13,230 --> 00:04:15,400
which he brought

47
00:04:15,420 --> 00:04:17,030
if you think this is problem

48
00:04:17,690 --> 00:04:19,900
so you see what you

49
00:04:19,940 --> 00:04:22,650
it's real from all

50
00:04:22,670 --> 00:04:40,050
and not too close to the

51
00:04:41,440 --> 00:04:44,710
they get the four forty roughly

52
00:04:44,730 --> 00:04:45,840
full OWL on

53
00:04:50,590 --> 00:04:53,070
are not

54
00:04:53,170 --> 00:04:57,150
no closely

55
00:04:57,430 --> 00:04:58,840
two twenty

56
00:04:58,860 --> 00:05:01,440
strong than four four

57
00:05:02,800 --> 00:05:06,210
so when she tries to get four forty six e

58
00:05:06,230 --> 00:05:07,750
there's still

59
00:05:07,750 --> 00:05:12,000
among lower than the four four you can change in practice that already before you

60
00:05:14,480 --> 00:05:17,340
i i lower lawns

61
00:05:19,170 --> 00:05:23,610
you may

62
00:05:23,860 --> 00:05:25,000
four forty

63
00:05:27,110 --> 00:05:29,880
you don't get to twenty four

64
00:05:30,500 --> 00:05:33,760
the way it's not your fault but trombone

65
00:05:33,780 --> 00:05:36,530
play something

66
00:05:36,550 --> 00:05:39,900
four were

67
00:06:14,400 --> 00:06:16,920
two more installments before will have to make

68
00:06:16,940 --> 00:06:20,190
right we have smith with friends for

69
00:06:20,210 --> 00:06:23,030
in space not

70
00:06:23,170 --> 00:06:26,150
so this french or you have to break earlier

71
00:06:26,170 --> 00:06:28,190
and then will have

72
00:06:32,940 --> 00:06:36,710
just six years old

73
00:06:36,730 --> 00:06:41,800
a guy named for keyboard which we have here

74
00:06:41,940 --> 00:06:49,960
i think you get something close to midway on piano which was for forty hertz

75
00:06:49,980 --> 00:06:51,050
three minutes

76
00:06:51,050 --> 00:07:01,780
go it

77
00:07:03,380 --> 00:07:15,480
well had

78
00:07:19,090 --> 00:07:21,860
go ahead

79
00:07:24,780 --> 00:07:26,360
nine or forty

80
00:07:32,590 --> 00:07:37,730
this is something you

81
00:07:37,760 --> 00:07:39,300
and we

82
00:07:40,690 --> 00:07:42,090
and we

83
00:07:43,670 --> 00:07:45,570
we were

84
00:08:10,760 --> 00:08:13,650
and senior producer for four

85
00:08:13,690 --> 00:08:17,380
anyone wants to volunteer singing

86
00:08:17,420 --> 00:08:19,300
yes try

87
00:08:19,340 --> 00:08:22,750
c help you can make it on out

88
00:08:22,760 --> 00:08:33,530
we have competition already

89
00:08:33,550 --> 00:08:34,940
what's your name

90
00:08:34,980 --> 00:08:37,530
because of

91
00:08:37,590 --> 00:08:42,090
the song can you try to sing for forty four close to that

92
00:08:42,110 --> 00:08:46,000
i think you listen with one support

93
00:08:55,210 --> 00:08:57,630
going to once again

94
00:09:00,320 --> 00:09:02,630
what you see in the future

95
00:09:02,650 --> 00:09:08,940
this is your time

96
00:09:09,020 --> 00:09:16,260
see something interesting anything you want

97
00:09:20,860 --> 00:09:25,450
beautiful overtones in your voice

98
00:09:25,480 --> 00:09:27,340
you want to try it

99
00:09:27,370 --> 00:09:29,540
you have competition

100
00:09:29,550 --> 00:09:32,040
get the four forty

101
00:09:40,650 --> 00:09:43,830
very now systems

102
00:09:50,910 --> 00:09:57,610
OK you back

103
00:09:57,990 --> 00:10:01,520
and it so you can just walk around the island structure like

104
00:10:51,370 --> 00:11:08,660
so you see the violin

105
00:11:08,700 --> 00:11:10,980
you seem to violate

106
00:11:10,990 --> 00:11:12,980
you may have noticed

107
00:11:12,980 --> 00:11:14,900
that the strings

108
00:11:14,900 --> 00:11:21,430
welcome to the final session of the conference portion of ICML thank you for coming

109
00:11:21,510 --> 00:11:23,100
so as the group had

110
00:11:23,110 --> 00:11:27,450
about one hundred and seventy rounds of play is about ten boat rides

111
00:11:27,470 --> 00:11:29,930
approximately three thousand cups of coffee

112
00:11:29,940 --> 00:11:33,870
and have a lot of inspiring and productive conversations

113
00:11:33,880 --> 00:11:37,760
so when you program chair of the conference you want to pick an event for

114
00:11:37,760 --> 00:11:41,210
the last session that really encourages everyone to stay to the very end of the

115
00:11:41,210 --> 00:11:43,980
conference something that nobody wants to mess

116
00:11:44,030 --> 00:11:46,680
and hence we come to an during

117
00:11:46,730 --> 00:11:49,740
it's my pleasure to introduce the last invited speaker

118
00:11:49,790 --> 00:11:53,750
i first met and in nineteen ninety seven when we were both trying to email

119
00:11:53,900 --> 00:11:58,460
into was working with andrew more and had just finished publishing three papers that year

120
00:11:58,460 --> 00:12:04,630
in model selection and cross validation and clustering i i believe and the two of

121
00:12:04,630 --> 00:12:11,530
us talk about collaboration in text classification and hierarchies of classes classes using shrinkage and

122
00:12:11,530 --> 00:12:12,840
andreas inside

123
00:12:12,850 --> 00:12:16,130
and scientific sensory delight throughout this

124
00:12:16,150 --> 00:12:19,530
and the next year and told me that he was leaving CMU and going to

125
00:12:19,530 --> 00:12:24,740
MIT and i was really surprised i was asked and said well graduating anybody to

126
00:12:24,740 --> 00:12:28,880
move on and see different part of the world and this is what you finished

127
00:12:28,880 --> 00:12:34,060
your phd already i know you don't agree with but i think he really quite

128
00:12:34,060 --> 00:12:35,280
finished so quickly

129
00:12:35,360 --> 00:12:42,940
and he said no i finished my undergrad i he was a phd student center

130
00:12:42,940 --> 00:12:46,940
went on to MIT to work with michael jordan and then moved with mike to

131
00:12:47,710 --> 00:12:54,110
and finished there is is now an assistant professor at stanford is published almost a

132
00:12:54,110 --> 00:13:00,020
hundred papers yes for best paper awards are spread across natural language processing computer vision

133
00:13:00,020 --> 00:13:06,370
information retrieval and machine learning which uses deep and incisive knowledge of machine learning

134
00:13:06,420 --> 00:13:10,470
but but among this is actually the thing that i admire and you for the

135
00:13:10,470 --> 00:13:13,280
most is that he has his eye on the big picture

136
00:13:13,510 --> 00:13:18,350
you know what i was talking venture copiers NIPS he told me that i didn't

137
00:13:18,350 --> 00:13:21,280
get into a i just to do statistics

138
00:13:21,390 --> 00:13:25,110
the time and i think much of the community has lost some of its big

139
00:13:25,110 --> 00:13:26,380
AI dreams

140
00:13:26,430 --> 00:13:31,700
andrews is focused on finding the common architecture for intelligence in the brain and building

141
00:13:31,700 --> 00:13:35,930
general purpose robots for the home and today we can hear more about the latter

142
00:13:35,950 --> 00:13:42,910
please try to be welcoming unless invited speaker and doing

143
00:13:55,570 --> 00:13:57,170
because on

144
00:13:59,640 --> 00:14:04,280
what it is today is

145
00:14:04,310 --> 00:14:08,200
tell you about

146
00:14:10,590 --> 00:14:17,270
so what does it say about sphere or the stanford AI robot knowledge

147
00:14:17,320 --> 00:14:19,450
and so on

148
00:14:19,460 --> 00:14:24,300
the theme of this talk will actually be application machine learning for robotics and describing

149
00:14:24,300 --> 00:14:25,860
this work too

150
00:14:25,880 --> 00:14:29,150
you can make use of my own excitement about the impact the machine learning is

151
00:14:29,150 --> 00:14:31,250
having robotics as well as some

152
00:14:31,270 --> 00:14:39,320
you should be well i think it's the most exciting open problems robotics

153
00:14:39,420 --> 00:14:43,540
the project was started about two-and-a-half hours for years ago

154
00:14:43,590 --> 00:14:48,980
motivated by the observation that in the in the last few decades has fragmented into

155
00:14:48,980 --> 00:14:50,390
many different

156
00:14:50,440 --> 00:14:51,320
and today

157
00:14:51,330 --> 00:14:56,790
each of these boxes is an almost entirely separately such community you've also entirely separate

158
00:14:56,790 --> 00:14:58,590
conferences and so on

159
00:14:58,610 --> 00:15:04,380
and so we want to define unified challenge problems require tied together these disparate threads

160
00:15:04,380 --> 00:15:06,550
of a back together again

161
00:15:06,600 --> 00:15:10,080
i'm glad to be stationary i

162
00:15:10,300 --> 00:15:13,420
see familiar with the history of a i i think of this as

163
00:15:13,460 --> 00:15:18,350
very much a project in the tradition sharing here they keep doing this with two

164
00:15:18,350 --> 00:15:23,640
thousand eight AI technology rather than say nineteen sixty six AI technology as was in

165
00:15:23,650 --> 00:15:30,950
the case of shaking the EU because the project this girls

166
00:15:31,030 --> 00:15:35,470
so long term challenge problems several cells onto a single robot able to do things

167
00:15:35,470 --> 00:15:38,050
like tidy room is the dishwasher

168
00:15:38,100 --> 00:15:42,370
fact of items assemble furniture prepare meals is thought that

169
00:15:42,420 --> 00:15:45,450
many others already actually have robots that holds today

170
00:15:45,470 --> 00:15:51,210
we call them the dishwasher washing machine and dryer and i think of evolutionary robotics

171
00:15:51,210 --> 00:15:54,960
become only when we can start to build a single robot they can do many

172
00:15:54,960 --> 00:16:02,470
different things rather than the highly specialized single purpose robots probably have a dishwasher

173
00:16:02,950 --> 00:16:07,200
short go and stop this talk was two

174
00:16:07,240 --> 00:16:12,310
because on the second of those challenge problems the robot able to fashion item office

175
00:16:12,320 --> 00:16:18,130
and so concretely arm was about they can understand the verbal commands like

176
00:16:18,140 --> 00:16:22,380
there is that the state from my office and be able to carry out that

177
00:16:23,250 --> 00:16:27,540
and so when talk about the five elements on just repeat on the slide

178
00:16:27,720 --> 00:16:31,700
we developed in order to build applications

179
00:16:31,760 --> 00:16:32,940
so creepy

180
00:16:32,950 --> 00:16:38,210
on the left after recognition system so the robot can say recognise this paper the

181
00:16:38,210 --> 00:16:39,550
oscar to fetch

182
00:16:39,600 --> 00:16:44,910
on mobile manipulation so that they can navigate into spaces open doors get inside your

183
00:16:44,910 --> 00:16:46,740
office because they

184
00:16:46,790 --> 00:16:52,150
for perception to give the robot the sensitive FET space around

185
00:16:52,160 --> 00:16:57,060
as being also they can pick up this paper that so you want to search

186
00:16:57,110 --> 00:17:01,660
and the is spoken dialogue systems to tie the whole thing together

187
00:17:02,890 --> 00:17:04,900
let's start by talking about

188
00:17:04,910 --> 00:17:13,200
opt recognition

189
00:17:15,550 --> 00:17:17,640
you image today is

190
00:17:17,650 --> 00:17:23,030
superior to that of any evaluation vision system and there are many reasons for this

191
00:17:23,030 --> 00:17:26,660
some people talk about context about commons and so on

192
00:17:27,890 --> 00:17:32,060
i think one of the reasons that human after recognition p of human object recognition

193
00:17:32,060 --> 00:17:33,760
is so it appears that

194
00:17:33,780 --> 00:17:35,540
humans use using here

195
00:17:35,550 --> 00:17:39,420
to the directly in object and does it in a very high resolution images

196
00:17:39,430 --> 00:17:44,770
and recognizing objects much easier from high resolution images low resolution ones

197
00:17:44,830 --> 00:17:46,590
so for example

198
00:17:46,630 --> 00:17:49,410
on the as you know what is an object

199
00:17:52,190 --> 00:17:54,120
two a vision people

200
00:17:54,130 --> 00:18:01,250
you to show a high-resolution image becomes it's easier to recognise what is on

201
00:18:01,270 --> 00:18:04,700
and the page and that that's where the coffee mug will like by the this

202
00:18:04,700 --> 00:18:08,990
this is the robot so it's no wonder that object recognition on the robot system

203
00:18:08,990 --> 00:18:10,450
work very well

204
00:18:10,460 --> 00:18:11,270
right so

205
00:18:11,280 --> 00:18:14,950
just be clear on this as i'm facing is looking directly at you i actually

206
00:18:14,950 --> 00:18:17,600
do not have enough pixels in my i

207
00:18:17,740 --> 00:18:20,390
to recognise it as the projector over there

208
00:18:20,400 --> 00:18:24,600
great recognises this project over all my rights i need to turn my eyes

209
00:18:24,690 --> 00:18:26,570
and look directly at it

210
00:18:28,220 --> 00:18:31,580
the high resolution version in michigan recognizes the project

211
00:18:31,630 --> 00:18:33,230
and i can tell the way

212
00:18:33,230 --> 00:18:37,850
as long as you are exploring all states and actions indefinitely

213
00:18:37,900 --> 00:18:42,750
and as long as the obvious smaller conditions are satisfied for the learning rate

214
00:18:42,770 --> 00:18:45,640
then these organisms converging is

215
00:18:45,640 --> 00:18:50,340
and bicycle master and you know what is going to fight

216
00:18:50,660 --> 00:18:52,410
and so

217
00:18:52,430 --> 00:18:56,000
this works with any policies so that's why it's called of matter

218
00:18:56,020 --> 00:19:00,800
on the other hand sources on policy methods and any method that avoids the policy

219
00:19:00,990 --> 00:19:03,740
on police and by following the policy

220
00:19:03,750 --> 00:19:05,280
then on policy matters

221
00:19:05,340 --> 00:19:07,990
so this section be will become

222
00:19:08,050 --> 00:19:10,220
important later

223
00:19:10,260 --> 00:19:14,180
so there very very and expect to start start to think of them have time

224
00:19:14,190 --> 00:19:15,170
to read

225
00:19:15,170 --> 00:19:19,770
so this figure is is the positive and maybe you want to think about it

226
00:19:19,800 --> 00:19:21,210
being the beach

227
00:19:21,230 --> 00:19:25,050
you see the battlements of q learning and sarsa over time

228
00:19:25,070 --> 00:19:29,810
and this is the work that is so then this is that the problem so

229
00:19:29,810 --> 00:19:33,710
there is likely an otherwise this is just amazing read

230
00:19:33,710 --> 00:19:37,820
you want to get to the goal state this is the problem and you start

231
00:19:37,820 --> 00:19:42,510
from this date and if you fall over the cliff then in one thousand five

232
00:19:42,750 --> 00:19:46,660
you get back to the start state and it's a very one of one hundred

233
00:19:47,800 --> 00:19:53,690
at every time step you receive reward of minus for maybe i should have mentioned

234
00:19:53,820 --> 00:19:58,170
have changed this to something more along

235
00:19:58,230 --> 00:19:59,230
but anyways

236
00:19:59,250 --> 00:20:03,590
so the positive like you learning is performing worse in this case

237
00:20:03,670 --> 00:20:04,960
that's herself

238
00:20:05,060 --> 00:20:08,320
i will give you the answer

239
00:20:08,430 --> 00:20:10,510
to think about it

240
00:20:10,590 --> 00:20:15,190
because this is the two learning is going matching to try to optimize

241
00:20:15,230 --> 00:20:18,070
why is that finding errors

242
00:20:18,090 --> 00:20:21,490
OK so one thing that i didn't specify about but but it's on the slide

243
00:20:21,490 --> 00:20:22,370
is then

244
00:20:22,380 --> 00:20:25,680
we are using by running these experiments epsilon greedy

245
00:20:28,830 --> 00:20:35,450
these are the size

246
00:20:35,520 --> 00:20:36,940
i don't know

247
00:20:36,950 --> 00:20:40,050
OK so i don't know if it's

248
00:20:40,100 --> 00:20:44,980
back to say it doesn't really matter

249
00:20:44,980 --> 00:20:46,850
OK so the next idea

250
00:20:46,890 --> 00:20:48,430
is that we

251
00:20:48,470 --> 00:20:53,080
the conclude previously that sometimes monte carlo is better sometimes better

252
00:20:53,180 --> 00:20:57,230
the next question is can we interpolate between these two target and so that we

253
00:20:57,230 --> 00:21:00,510
could change in the smallest way from monte carlo two

254
00:21:00,930 --> 00:21:03,470
two TV and vice versa

255
00:21:03,520 --> 00:21:05,730
and the idea is that

256
00:21:05,770 --> 00:21:10,540
you can do the bootstrapping after one time step or two thousand seven four

257
00:21:10,560 --> 00:21:13,090
many more time steps

258
00:21:13,100 --> 00:21:14,060
if you do

259
00:21:14,080 --> 00:21:14,810
if you

260
00:21:14,820 --> 00:21:15,990
in the limit

261
00:21:16,180 --> 00:21:20,770
you do bootstrapping after if it number of times that that corresponds to model that

262
00:21:20,820 --> 00:21:22,680
you're not doing bootstrapping at all

263
00:21:22,680 --> 00:21:25,800
so what bootstrapping is that you're replacing

264
00:21:27,710 --> 00:21:29,160
trot out this concert

265
00:21:29,260 --> 00:21:32,780
rewards from some sidestep

266
00:21:32,850 --> 00:21:36,230
is the value function estimate so this was run

267
00:21:36,240 --> 00:21:39,560
you could do and after one step two step mother

268
00:21:39,570 --> 00:21:42,370
it doesn't really change the computation

269
00:21:42,370 --> 00:21:45,000
what are the properties of the target and

270
00:21:45,620 --> 00:21:49,240
but this way at least you can interpolate between

271
00:21:49,650 --> 00:21:50,740
it all

272
00:21:51,050 --> 00:21:53,840
this basically the method i want to cuddle

273
00:21:53,890 --> 00:21:55,280
and then you could

274
00:21:55,300 --> 00:22:01,270
maybe you optimize these two are part of the problem tissue somebody powerful

275
00:22:01,280 --> 00:22:03,780
faster convergence based

276
00:22:03,830 --> 00:22:08,020
so this is just formalizing this idea so he

277
00:22:08,030 --> 00:22:10,210
compute everything up to

278
00:22:10,280 --> 00:22:12,990
the end of the episode

279
00:22:14,500 --> 00:22:17,940
into the zero you just the step after

280
00:22:17,970 --> 00:22:19,470
one thousand seven

281
00:22:19,480 --> 00:22:25,230
but you could do that after t time steps for many more

282
00:22:27,160 --> 00:22:32,170
and so then you could have this learning target and and with control how much

283
00:22:32,170 --> 00:22:34,660
she would strap

284
00:22:36,220 --> 00:22:40,650
so i sort of explained already that

285
00:22:41,450 --> 00:22:43,450
in some examples

286
00:22:43,520 --> 00:22:48,640
monte carlo can perform better than TV and then you can imagine that could be

287
00:22:48,640 --> 00:22:53,060
intermediate cases found it makes sense to bootstrap the debate but it doesn't make sense

288
00:22:53,060 --> 00:22:54,990
to which are too much

289
00:22:55,020 --> 00:22:59,930
and so this example is about that if you are interested i can spend later

290
00:22:59,940 --> 00:23:02,780
or this is sometimes just keep or that

291
00:23:02,800 --> 00:23:04,230
so this shows that

292
00:23:04,260 --> 00:23:05,380
if you

293
00:23:05,380 --> 00:23:08,200
this bootstrapping changing and

294
00:23:09,220 --> 00:23:11,970
so this is the root mean square and are

295
00:23:11,990 --> 00:23:17,010
approximating value function and so small that user better than you see that in this

296
00:23:17,010 --> 00:23:19,750
particular example but the reality is

297
00:23:19,780 --> 00:23:29,340
you actually have the best performance feature learning methods than any clues three or five

298
00:23:29,350 --> 00:23:30,260
so that's good

299
00:23:34,520 --> 00:23:36,910
so the next five years that may be

300
00:23:36,910 --> 00:23:38,690
if this works so well

301
00:23:38,740 --> 00:23:41,540
you could

302
00:23:41,560 --> 00:23:46,080
many children step preacher so instead of

303
00:23:46,090 --> 00:23:49,080
just using a single and maybe you want to

304
00:23:49,200 --> 00:23:54,790
some makes this different returns you do that

305
00:23:54,830 --> 00:23:57,890
it's called a complex backup

306
00:23:57,970 --> 00:24:00,890
and so the next idea that

307
00:24:00,900 --> 00:24:01,930
makes this

308
00:24:03,580 --> 00:24:08,510
is that well was university occur to you that you could have reach of these

309
00:24:08,510 --> 00:24:11,480
different bootstrapping models

310
00:24:11,490 --> 00:24:13,530
you could use an exponential

311
00:24:13,560 --> 00:24:16,130
you can use exponential ways that

312
00:24:16,130 --> 00:24:17,940
so basically you

313
00:24:17,950 --> 00:24:20,490
you define some long the value

314
00:24:20,500 --> 00:24:22,780
and the weight off

315
00:24:22,790 --> 00:24:26,350
the TV zero of this is going to be from iceland

316
00:24:26,370 --> 00:24:31,330
the weight of the second that there is one minus lambda times longer though

317
00:24:31,390 --> 00:24:35,580
the third one minus lambda times long the square and so on

318
00:24:35,600 --> 00:24:40,040
if the HDD and of the episode then you just use this

319
00:24:40,050 --> 00:24:41,460
the remaining

320
00:24:41,490 --> 00:24:43,270
value there

321
00:24:43,300 --> 00:24:47,180
and so the good thing about this is that these sums to one

322
00:24:47,180 --> 00:24:49,930
so this is the proper way of doing ratings

323
00:24:50,000 --> 00:24:51,920
uses weighted

324
00:24:53,990 --> 00:24:57,220
so use a little bit of this and that so

325
00:24:57,260 --> 00:24:59,630
and then changing the landau

326
00:24:59,670 --> 00:25:02,660
it gives you control over

327
00:25:02,670 --> 00:25:04,790
if you really want to concentrate

328
00:25:06,100 --> 00:25:12,800
when you're not doing something close to monte carlo but you're doing something close to

329
00:25:13,800 --> 00:25:18,490
in particle if lambda equals zero then have seen here it is in all

330
00:25:18,490 --> 00:25:22,900
and the probability of sunrise rising tomorrow how can i read

331
00:25:22,910 --> 00:25:24,550
so there are some limitations

332
00:25:26,100 --> 00:25:29,670
so these objective vists interpretation

333
00:25:29,730 --> 00:25:31,440
it's kind of

334
00:25:31,450 --> 00:25:32,370
you know

335
00:25:32,380 --> 00:25:35,360
it's kind of people things you say

336
00:25:36,460 --> 00:25:40,600
i can measure i mean i can try to measure this probability is but i'm

337
00:25:40,600 --> 00:25:44,700
assuming that they exist and this is some kind of intrinsic property of the word

338
00:25:44,750 --> 00:25:45,880
that there is

339
00:25:46,000 --> 00:25:51,590
like the ride this story the sun as actually two in

340
00:25:51,720 --> 00:25:56,020
actually it some probability of writing tomorrow and

341
00:25:56,080 --> 00:25:58,390
this is kind of a property

342
00:25:58,400 --> 00:26:00,320
of the many of nature

343
00:26:00,330 --> 00:26:02,100
and i i can only

344
00:26:02,120 --> 00:26:06,140
observe it's like the the mass sun some last but it also has some probability

345
00:26:06,140 --> 00:26:06,910
of writing

346
00:26:06,970 --> 00:26:09,320
and i just want to measure the

347
00:26:09,320 --> 00:26:13,530
so these probabilities pre exist

348
00:26:13,620 --> 00:26:17,360
and the last interpretation

349
00:26:17,370 --> 00:26:23,230
so for the subjectivist interpretation these probabilities they are just numbers that people

350
00:26:23,250 --> 00:26:28,170
a given person would assign to some evidence and that correspond to

351
00:26:28,210 --> 00:26:29,560
how likely or

352
00:26:29,560 --> 00:26:33,260
i mean how strong is belief is that this event will occur

353
00:26:34,330 --> 00:26:36,910
there is

354
00:26:36,930 --> 00:26:40,680
i mean you can naturally defined mean naturally assign

355
00:26:42,270 --> 00:26:46,820
i mean the same numbers these beliefs real numbers to these beliefs and then you

356
00:26:46,820 --> 00:26:50,130
know you can come up with the natural or input

357
00:26:50,130 --> 00:26:56,060
natural rules for combining is when you when you believe in one event in to

358
00:26:56,130 --> 00:26:58,930
be one event and or another even how you

359
00:26:58,980 --> 00:27:01,750
combine it and so on and

360
00:27:01,800 --> 00:27:04,530
you can show that if you

361
00:27:04,550 --> 00:27:08,450
use these natural rules then it will lead you to

362
00:27:08,480 --> 00:27:12,120
something that has the same properties as probabilities

363
00:27:15,110 --> 00:27:17,670
probabilities are the right way

364
00:27:17,710 --> 00:27:20,610
to formalise

365
00:27:21,130 --> 00:27:24,350
the natural notion of of belief in something

366
00:27:27,890 --> 00:27:31,950
then once you can of choose in one of these interpretations i mean you don't

367
00:27:31,950 --> 00:27:36,110
have to choose basically you can just run your calculation without even

368
00:27:36,160 --> 00:27:39,670
thinking about how to interpret these numbers

369
00:27:39,720 --> 00:27:42,580
and one way in which you can

370
00:27:42,620 --> 00:27:47,310
combining these numbers these being decisive action i mean you can derive that actions

371
00:27:47,600 --> 00:27:49,860
what is called the bayes rule and

372
00:27:49,910 --> 00:27:52,050
what is is just the way too

373
00:27:52,100 --> 00:27:56,430
abate your probabilities when you make observations so you have

374
00:27:56,510 --> 00:27:58,170
prior beliefs

375
00:27:58,170 --> 00:27:59,930
in some what

376
00:27:59,960 --> 00:28:01,610
your h

377
00:28:01,610 --> 00:28:02,600
you have

378
00:28:02,610 --> 00:28:04,940
in some way to measure how

379
00:28:05,340 --> 00:28:10,420
what is the conditional probability of observing some data given here but it is that's

380
00:28:10,420 --> 00:28:13,910
against the need to define so of some numbers

381
00:28:13,910 --> 00:28:16,910
and the natural way to

382
00:28:16,960 --> 00:28:21,430
derived from that's how much i believe in your i after you have seen the

383
00:28:22,320 --> 00:28:23,370
is two

384
00:28:23,480 --> 00:28:25,510
apply this rule and this is kind of

385
00:28:25,520 --> 00:28:28,730
if you want to be consistent with the axioms

386
00:28:28,770 --> 00:28:30,440
that's the only way you

387
00:28:30,450 --> 00:28:32,110
you should do it

388
00:28:32,120 --> 00:28:36,840
but again you can't really measure all these numbers but you know what you have

389
00:28:36,840 --> 00:28:40,370
agreed that this is the right thing to do you can just work but work

390
00:28:40,370 --> 00:28:43,170
of the details

391
00:28:45,800 --> 00:28:46,990
my point here

392
00:28:49,010 --> 00:28:52,900
what can we gain using probabilities is is this the right way

393
00:28:54,070 --> 00:28:54,950
you know

394
00:28:54,970 --> 00:28:56,550
doing inference or

395
00:28:59,360 --> 00:29:02,800
well i mean it's natural way to do it or it is a convenient way

396
00:29:02,800 --> 00:29:08,760
to do it you get this nice formula you get this nice

397
00:29:08,780 --> 00:29:11,200
number that you can combine but

398
00:29:11,240 --> 00:29:14,570
we don't gain anything right we we still cannot prove that

399
00:29:14,610 --> 00:29:16,840
what we are saying is true because

400
00:29:17,010 --> 00:29:20,760
we don't get any guarantee

401
00:29:20,800 --> 00:29:24,450
the probability is that just help us resulting but

402
00:29:24,470 --> 00:29:27,950
if we did not sit these numbers right in the first place

403
00:29:27,990 --> 00:29:30,670
but even if we set the right i mean because there are so many ways

404
00:29:30,670 --> 00:29:33,180
to interpret that you can't really prove that

405
00:29:33,200 --> 00:29:36,180
the probability of something is something because

406
00:29:36,240 --> 00:29:40,740
you know you can come up with all sorts of interpretations you can hear frequencies

407
00:29:41,340 --> 00:29:45,970
objectivist or subjectivist and none of these people with three

408
00:29:46,360 --> 00:29:53,010
so again it's kind of disappointing but even though you are using probability it doesn't

409
00:29:53,010 --> 00:29:57,880
mean that you're doing the right thing or that you justifying

410
00:29:58,320 --> 00:30:02,090
so we know that the problem is that we we need these assumptions there is

411
00:30:02,090 --> 00:30:04,340
no way we can escape this assumption

412
00:30:07,110 --> 00:30:09,240
so what can we do

413
00:30:09,430 --> 00:30:11,970
what we can do is

414
00:30:12,010 --> 00:30:13,340
you know

415
00:30:13,360 --> 00:30:17,110
implicitly assume something like the future looks like the that

416
00:30:17,110 --> 00:30:21,720
this is kind of reasonable and you know everyone would agree with that some degree

417
00:30:21,760 --> 00:30:25,550
or what we have not seen it looks like what we have seen

418
00:30:25,610 --> 00:30:29,300
and it's clear that if we don't make such assumptions

419
00:30:29,380 --> 00:30:31,970
then there is no point in doing in the

420
00:30:31,990 --> 00:30:36,840
you know in working learning theory or even working measuring at all because if you

421
00:30:36,840 --> 00:30:39,590
don't assume that there is some regularity in the world

422
00:30:39,630 --> 00:30:43,990
there is no hope that you can cooperate with these algorithms

423
00:30:44,380 --> 00:30:49,610
but still we want to make these assumptions you know as limited as possible we

424
00:30:49,610 --> 00:30:51,760
don't want to assume too much

425
00:30:51,780 --> 00:30:55,340
and we want to keep the minimum which means that

426
00:30:55,400 --> 00:31:00,700
if we can even completely get rid of this assumption actually you can a certain

427
00:31:00,740 --> 00:31:02,510
sense if you

428
00:31:02,550 --> 00:31:04,610
i don't aim at proving that

429
00:31:04,650 --> 00:31:09,430
some reason is optimal that if you want to compare algorithms like if you will

430
00:31:10,700 --> 00:31:12,720
i want to predict but

431
00:31:12,740 --> 00:31:15,550
two i mean to predict well but you want to predict as well as some

432
00:31:15,550 --> 00:31:19,450
of the and if you want to imitate the behavior of an organism

433
00:31:19,610 --> 00:31:22,860
independent of how good the reason is that you can

434
00:31:22,860 --> 00:31:25,590
you know somehow get rid of these assumptions

435
00:31:25,650 --> 00:31:30,010
so it's a matter of you know

436
00:31:30,050 --> 00:31:32,720
can we match some of the first

437
00:31:34,070 --> 00:31:35,800
like in when we

438
00:31:35,800 --> 00:31:38,800
i'm willing to go with assumptions in the question is

439
00:31:38,820 --> 00:31:42,860
and the question that the series would answer is

440
00:31:42,880 --> 00:31:44,240
given these assumptions

441
00:31:44,260 --> 00:31:45,470
what can we do

442
00:31:45,490 --> 00:31:47,970
at best threats what is the

443
00:31:47,970 --> 00:31:53,910
optimal algorithm with respect to this assumption that something we can

444
00:31:56,340 --> 00:32:00,070
now i want to have slowly go towards more

445
00:32:00,130 --> 00:32:01,280
one thing

446
00:32:02,490 --> 00:32:04,200
how it's not too slow for you

447
00:32:04,220 --> 00:32:08,300
i fixed myself put o five percent of people that should be easily by the

448
00:32:08,300 --> 00:32:09,590
end of the lecture

449
00:32:09,610 --> 00:32:12,510
so far it seems to be OK

450
00:32:12,570 --> 00:32:14,240
eighty three percent

451
00:32:14,260 --> 00:32:19,280
OK so

452
00:32:19,300 --> 00:32:21,630
maybe you need to it faster

453
00:32:21,910 --> 00:32:24,380
so let me introduce now

454
00:32:24,550 --> 00:32:26,950
and try to dissuade picture of

455
00:32:26,990 --> 00:32:29,240
the things

456
00:32:29,260 --> 00:32:31,070
it's a

457
00:32:31,130 --> 00:32:36,340
you know how to distinguish between different assumptions of frameworks because you probably have heard

458
00:32:36,340 --> 00:32:38,570
about you know online learning or

459
00:32:38,590 --> 00:32:44,450
semi supervised learning or bayesian inference and all these things you know i want to

460
00:32:44,450 --> 00:32:45,200
kind of

461
00:32:46,470 --> 00:32:48,070
chart where you can

462
00:32:48,070 --> 00:32:53,050
place these things and related to each other and understand what is an assumption and

463
00:32:53,050 --> 00:32:54,220
what is just

464
00:32:54,220 --> 00:32:58,050
matter of sitting the problem and so on and trying not to confuse all these

465
00:32:58,050 --> 00:33:01,380
and that have weights w i

466
00:33:01,430 --> 00:33:04,030
at time t minus one

467
00:33:07,590 --> 00:33:12,200
we then will propagate so we cannot say that xt

468
00:33:12,210 --> 00:33:14,430
it's going to be whatever the dynamics

469
00:33:14,440 --> 00:33:17,830
some function f x d minus one

470
00:33:20,840 --> 00:33:22,620
cases and you

471
00:33:22,630 --> 00:33:25,270
new sample

472
00:33:25,280 --> 00:33:27,780
xt i

473
00:33:27,820 --> 00:33:33,430
and then we got no way to get a new weights

474
00:33:33,450 --> 00:33:34,940
it's more or less how

475
00:33:34,950 --> 00:33:40,290
the algorithm is going to do it that's the picture

476
00:33:42,280 --> 00:33:46,690
quite often these distributions are moving with certain the hockey playing case with

477
00:33:46,700 --> 00:33:50,510
what you want to maintain is the distribution of the with the mode

478
00:33:50,550 --> 00:33:52,300
the player location

479
00:33:52,410 --> 00:33:55,080
but then the player moves and distribution system

480
00:33:55,140 --> 00:33:58,420
because the distribution is estimating the location of the play

481
00:33:58,460 --> 00:34:01,100
location and speed and so on

482
00:34:01,190 --> 00:34:05,100
so if the distribution is moving too quickly then what happens is you know a

483
00:34:05,100 --> 00:34:07,430
lot of these particles

484
00:34:07,440 --> 00:34:09,760
and that was very low weight

485
00:34:09,800 --> 00:34:13,980
and algorithmically what happens is you end up with only one part of a lot

486
00:34:13,980 --> 00:34:14,890
of time

487
00:34:14,950 --> 00:34:17,620
so what we're going to do is we're going to this about computation here but

488
00:34:17,620 --> 00:34:19,170
i forgot to say that

489
00:34:19,170 --> 00:34:21,630
with all the serial machine

490
00:34:21,640 --> 00:34:25,000
but instead of drawing samples from one to two three four

491
00:34:25,050 --> 00:34:28,850
what we're going to do is going to maintain and samples

492
00:34:29,070 --> 00:34:32,900
it just keep propagating and samples what

493
00:34:33,120 --> 00:34:35,810
so to eliminate this problem

494
00:34:35,830 --> 00:34:37,780
we introduce resampling

495
00:34:38,930 --> 00:34:42,380
we propagate things through the dynamical system

496
00:34:42,420 --> 00:34:45,180
we select the brightest samples

497
00:34:45,190 --> 00:34:47,650
and and

498
00:34:47,670 --> 00:34:51,650
propagate some again

499
00:34:54,330 --> 00:34:55,650
weight to

500
00:34:55,690 --> 00:34:59,170
so if the so in the distribution is moving

501
00:34:59,210 --> 00:35:03,540
you have to stay with the samples in order to be able to track

502
00:35:03,730 --> 00:35:08,560
without this algorithm would not work

503
00:35:10,620 --> 00:35:11,370
but this

504
00:35:11,380 --> 00:35:13,980
according to the importance weight

505
00:35:14,040 --> 00:35:17,520
i importance weights

506
00:35:17,540 --> 00:35:19,040
the story is about

507
00:35:19,050 --> 00:35:20,930
there's more to the story

508
00:35:21,050 --> 00:35:25,020
in terms of analysis of bias and variance in one is it good to example

509
00:35:25,020 --> 00:35:26,340
we're not

510
00:35:26,400 --> 00:35:29,760
but i'm going to leave it at this stage that suffices

511
00:35:29,800 --> 00:35:33,280
suffice it to say that if you implement the first part of the just missionaries

512
00:35:34,520 --> 00:35:38,710
because otherwise it will not work

513
00:35:38,940 --> 00:35:41,350
so this is how the algorithm works

514
00:35:41,380 --> 00:35:46,130
at the beginning to draw samples from the initial distribution

515
00:35:46,140 --> 00:35:48,630
we you have an initial model state

516
00:35:48,680 --> 00:35:52,100
if that model is not very good ones and worried because the idea of the

517
00:35:52,170 --> 00:35:53,880
dynamical system is that

518
00:35:53,920 --> 00:35:57,400
you start far off but soon you catch up with with

519
00:35:57,410 --> 00:36:03,280
players so the robot spoke about estimate but as it moves on along the world

520
00:36:03,360 --> 00:36:06,170
the estimate improves

521
00:36:06,790 --> 00:36:10,740
and then algorithm is as follows

522
00:36:10,750 --> 00:36:15,450
you draw a sample from the transition prior

523
00:36:15,520 --> 00:36:18,870
so you move forward just like we did with that right

524
00:36:18,930 --> 00:36:21,430
move forward one step

525
00:36:21,450 --> 00:36:26,570
and then evaluate importance weights in this case if you propose from the prior distribution

526
00:36:26,600 --> 00:36:29,370
importance weighted simply the likelihood

527
00:36:29,370 --> 00:36:31,370
these stationary point

528
00:36:31,390 --> 00:36:35,350
i mean so there there's a theorem for BP that that the goods stationary point

529
00:36:35,350 --> 00:36:36,850
a minimum

530
00:36:36,850 --> 00:36:41,100
four EP and know that they are not necessarily holds

531
00:36:44,350 --> 00:36:49,080
exactly so for belief propagation of people have done is they've developed alternative ways of

532
00:36:49,100 --> 00:36:50,640
minimizing the energy

533
00:36:50,660 --> 00:36:53,370
for example one they go directly downhill on the energy

534
00:36:53,430 --> 00:36:56,600
o or one to try to find the lowest minimum of the energy and so

535
00:36:56,600 --> 00:37:01,140
on and people have done similar things three but in my experience what i found

536
00:37:01,140 --> 00:37:04,680
is that finding the lowest EP model evidence

537
00:37:05,660 --> 00:37:08,490
or even the highest one is necessarily the most accurate one

538
00:37:08,490 --> 00:37:10,910
because it is in the bound in in any sort

539
00:37:12,120 --> 00:37:19,660
finding the the one that has the highest instances making sense

540
00:37:19,720 --> 00:37:23,810
o king

541
00:37:28,640 --> 00:37:31,970
that's right

542
00:37:31,990 --> 00:37:36,410
that's right

543
00:37:37,140 --> 00:37:39,870
this is essentially a rewriting of the free energy

544
00:37:41,620 --> 00:37:45,410
if if all the factors in your model

545
00:37:45,430 --> 00:37:46,450
are already

546
00:37:46,470 --> 00:37:50,450
in the right exponential family for example if all of the as artigas in but

547
00:37:50,450 --> 00:37:51,760
they just coupled

548
00:37:51,760 --> 00:37:57,620
and your after all there are fully factorized right venue in BP situation the definition

549
00:37:57,620 --> 00:37:58,890
of BP

550
00:37:59,080 --> 00:38:00,350
from my point of view

551
00:38:00,890 --> 00:38:03,970
if you're in that particular situation then this would reduce to

552
00:38:03,990 --> 00:38:07,490
the bit free energy essentially

553
00:38:14,180 --> 00:38:15,020
on the

554
00:38:15,040 --> 00:38:18,160
on the web page where you can get my thesis i have a paper which

555
00:38:18,180 --> 00:38:24,450
which shows exactly corresponds to bit free energy

556
00:38:24,450 --> 00:38:27,560
it's called the EP energy function

557
00:38:33,010 --> 00:38:36,010
but actually i should point out though that the

558
00:38:36,040 --> 00:38:38,970
the way i wrote the energy function that paper wasn't clean is this this is

559
00:38:38,970 --> 00:38:42,870
not my new favourite way of writing the energy function is and that if you

560
00:38:42,870 --> 00:38:46,010
look if you look in the literature you'll find various people who give different energy

561
00:38:46,010 --> 00:38:49,350
function three p and i think that this so this is like a million ways

562
00:38:49,350 --> 00:38:50,850
of writing this energy function

563
00:38:50,910 --> 00:38:53,160
i just find this to be the nicest way

564
00:38:53,160 --> 00:38:54,350
reverting it

565
00:38:54,430 --> 00:38:58,990
mainly because it's a function only of the message is the message you don't have

566
00:38:59,010 --> 00:39:00,120
to be normalized

567
00:39:00,140 --> 00:39:04,520
and indirectly gives to the model that was the nice properties that i like

568
00:39:04,560 --> 00:39:07,780
and in fact if you if you just take this

569
00:39:07,850 --> 00:39:11,720
take his energy and you differentiate the spectre the priors of the phi tells you

570
00:39:11,720 --> 00:39:13,450
really get DP update

571
00:39:13,490 --> 00:39:17,040
as you just have to green to zero and you have do so

572
00:39:17,060 --> 00:39:18,830
so it's a really nice wearing

573
00:39:23,600 --> 00:39:24,930
yes question

574
00:39:24,930 --> 00:39:33,370
that's right

575
00:39:36,310 --> 00:39:45,850
it's it's something that i don't really use except to compute the model evidence

576
00:39:46,760 --> 00:39:48,060
i mean some people

577
00:39:48,080 --> 00:39:51,560
people who study BP use the BP free energy

578
00:39:51,560 --> 00:39:54,510
two to understand the article properties of so this might be useful for that as

579
00:39:54,510 --> 00:39:57,390
well the there's been very limited work in that context

580
00:39:57,470 --> 00:40:01,450
i myself i always adjust to compute the moments once ironically

581
00:40:01,720 --> 00:40:05,560
the use of reveals

582
00:40:07,260 --> 00:40:12,560
future work by

583
00:40:12,850 --> 00:40:16,280
for the look at the paper

584
00:40:16,290 --> 00:40:22,490
so in the paper i i give intuition for why this is why is good

585
00:40:28,180 --> 00:40:32,080
so so by by giving the generalized view of what he in terms of minimising

586
00:40:32,080 --> 00:40:36,310
divergences we can all change it to be to become different algorithms particular i can

587
00:40:37,120 --> 00:40:40,620
the difference i minimize some of the divergence in different message passing

588
00:40:40,680 --> 00:40:46,080
and the paper said earlier divergence measures and message passing shows how you can get

589
00:40:46,280 --> 00:40:49,680
a variety of different message passing by changing the divergence is today i'm just going

590
00:40:49,680 --> 00:40:53,290
to give a little taste of that by showing one difference measured you can pick

591
00:40:53,370 --> 00:40:54,620
in particular

592
00:40:54,640 --> 00:40:55,600
you can

593
00:40:55,620 --> 00:40:59,370
you can choose the KL divergence where q and p are swapped

594
00:40:59,430 --> 00:41:01,060
that's one thing you can do you can say

595
00:41:01,080 --> 00:41:04,080
i don't want to minimize the killer peter q i want to minimize the KL

596
00:41:04,080 --> 00:41:05,660
of q

597
00:41:05,660 --> 00:41:09,390
and we can use you can take the recipe i just gave and just change

598
00:41:09,390 --> 00:41:12,620
all of the KL divergence is in there too one cycle from p

599
00:41:14,010 --> 00:41:15,720
run the exact same algorithm

600
00:41:15,760 --> 00:41:18,930
but but change the project operator essentially

601
00:41:18,930 --> 00:41:21,680
and what you get you get mean field approximation

602
00:41:21,700 --> 00:41:25,450
which i think is is an interesting relationship to others

603
00:41:25,450 --> 00:41:28,660
and i'll show you that works more generally

604
00:41:28,700 --> 00:41:32,140
you can use an alpha divergence rather than kill them as well for the into

605
00:41:32,140 --> 00:41:36,890
sort of a big family of divergences which killed one and you can get other

606
00:41:37,990 --> 00:41:41,140
you may have heard of for example you can get reweighted belief propagation

607
00:41:41,160 --> 00:41:45,470
by using bayes in the same recipe i just gave up with alpha divergence and

608
00:41:45,470 --> 00:41:49,390
you can get power EP is well that's all described in the paper i cited

609
00:41:50,660 --> 00:41:57,510
let's let's stick to KLQ to be OK so we interchange the arguments of

610
00:41:57,560 --> 00:42:00,700
to kill is not symmetric functions we do actually change

611
00:42:00,720 --> 00:42:03,290
we are measuring we swap two arguments

612
00:42:03,350 --> 00:42:06,350
and what do we gain by doing that right

613
00:42:07,810 --> 00:42:09,010
in my mind

614
00:42:09,020 --> 00:42:13,310
the sort of main the main good property of going from KLQP to p

615
00:42:13,330 --> 00:42:18,120
is this one is that when you minimize these local divergences

616
00:42:18,140 --> 00:42:21,220
you're you're equivalently minimising the global divergence

617
00:42:21,260 --> 00:42:22,540
so in particular

618
00:42:22,540 --> 00:42:27,290
that that approximation gap that was present in the because our context was wrong

619
00:42:27,310 --> 00:42:31,100
doesn't exist if you scale of q to p

620
00:42:31,120 --> 00:42:32,310
and i'll show you

621
00:42:32,330 --> 00:42:35,040
i give you a brief sketch of why that's the case

622
00:42:36,830 --> 00:42:40,760
all right on the global divergence right on the local divergence and show that there

623
00:42:40,950 --> 00:42:42,640
very equivalent

624
00:42:42,660 --> 00:42:46,250
this isn't a complete proof that the algorithm is the same you can find the

625
00:42:46,250 --> 00:42:49,160
proof in my different measures paper give sketch

626
00:42:49,210 --> 00:42:52,420
OK so what's the global divergence the goal divergences

627
00:42:52,460 --> 00:42:57,090
the KL fifteen q of xn pnx

628
00:42:58,390 --> 00:43:03,630
and just to reiterate what that is

629
00:43:03,660 --> 00:43:07,340
that is this

630
00:43:08,240 --> 00:43:09,540
and recall that

631
00:43:09,550 --> 00:43:15,660
q of x is the product of all of the approximate factors

632
00:43:15,660 --> 00:43:19,760
it one formula on the left in the formula on the right

633
00:43:19,770 --> 00:43:23,190
is this the form below yes i just past that to be a formula is

634
00:43:23,190 --> 00:43:26,170
a formula such as positive

635
00:43:26,210 --> 00:43:31,000
the right to draw formation tree work through

636
00:43:36,680 --> 00:43:40,700
a bracketed sic of the brecon in this way

637
00:43:40,730 --> 00:43:44,960
right rather than

638
00:43:45,030 --> 00:43:46,840
this way

639
00:43:46,930 --> 00:43:49,490
the box grabs onto its arguments

640
00:43:49,510 --> 00:43:50,680
more tightly

641
00:43:50,950 --> 00:43:53,550
i'm then implication does

642
00:43:53,630 --> 00:43:56,980
you will slowly pick up these things as the

643
00:43:57,010 --> 00:44:01,260
this lecture progresses anyway so what i'd like you to do is to convince yourself

644
00:44:01,260 --> 00:44:04,570
that some of these other things are a formula is the the

645
00:44:05,340 --> 00:44:09,970
in modal logic we really deal with actual formula we deal with shapes

646
00:44:10,000 --> 00:44:11,930
sometimes they call schemas

647
00:44:11,970 --> 00:44:15,110
and what i wanna do their say

648
00:44:15,180 --> 00:44:19,720
when you have the shape then you start using formula variables like five

649
00:44:23,190 --> 00:44:26,170
phi implies phi is the shape

650
00:44:26,180 --> 00:44:30,930
because i can substitute any formula i like so if i wanted to i could

651
00:44:30,930 --> 00:44:34,300
take this whole thing and put it in there as long as i put the

652
00:44:34,300 --> 00:44:36,430
same thing in getting

653
00:44:36,480 --> 00:44:38,270
the formula

654
00:44:38,430 --> 00:44:41,680
so what we're going to do is we can instantiate

655
00:44:42,690 --> 00:44:45,660
and the word is uniformly

656
00:44:45,740 --> 00:44:50,030
OK so the example box p flies p two

657
00:44:50,050 --> 00:44:51,470
so the box

658
00:44:51,480 --> 00:44:54,070
p nor implies p two

659
00:44:54,070 --> 00:44:57,890
i claim this is not an instance of that

660
00:45:01,860 --> 00:45:08,370
sorry i

661
00:45:09,170 --> 00:45:13,550
OK so uniformly the phi has to be the same thing

662
00:45:13,570 --> 00:45:18,000
but this is p not but now it's saying well actually we change their minds

663
00:45:18,000 --> 00:45:19,220
when we went from

664
00:45:19,410 --> 00:45:21,860
so the area to the

665
00:45:22,780 --> 00:45:27,500
that's all just wherever you see five has to be the same wherever you see

666
00:45:27,500 --> 00:45:31,200
side it can be different from five it can be the same as five you

667
00:45:31,200 --> 00:45:32,900
want as well

668
00:45:32,990 --> 00:45:38,590
so and convince yourself that this scheme of things work and sometimes all one two

669
00:45:38,600 --> 00:45:43,550
calculate the formula such is going to be that the number of symbols in assuming

670
00:45:43,550 --> 00:45:44,920
that these guys

671
00:45:44,940 --> 00:45:48,400
p not p two are all of length one

672
00:45:48,410 --> 00:45:52,430
so i want this to be to get right it's one two

673
00:45:52,460 --> 00:45:55,470
it's this is of length one this is of length one

674
00:45:56,110 --> 00:45:58,910
this is lovely to then

675
00:45:58,930 --> 00:46:01,970
this is of length four

676
00:46:09,430 --> 00:46:12,410
big party

677
00:46:12,420 --> 00:46:14,890
no it's not graph theory it's just

678
00:46:15,000 --> 00:46:17,020
traditionally called sky

679
00:46:17,030 --> 00:46:21,900
whereas if you have no background whatsoever in logic which is what sometimes happens here

680
00:46:21,910 --> 00:46:27,240
then what's the schema sounds very it all and by shape what i'm trying to

681
00:46:27,240 --> 00:46:31,110
get across is this idea of the uniformity right

682
00:46:31,340 --> 00:46:36,190
shape is something that's the same everywhere shape and if you see the same shape

683
00:46:36,190 --> 00:46:39,520
in the places he's got to instantiated in the same way

684
00:46:39,590 --> 00:46:42,830
it's just something that i think about which other people

685
00:46:42,900 --> 00:46:46,690
other people use it as well but you still see it more and more recently

686
00:46:46,690 --> 00:46:49,050
in introductory textbooks

687
00:46:49,110 --> 00:46:54,410
OK because not everyone has a background in say philosophy anymore so all of the

688
00:46:54,410 --> 00:46:56,570
old terms of schemas in

689
00:46:57,030 --> 00:46:59,950
substitution instances and things like that

690
00:47:00,970 --> 00:47:03,690
they just don't have the background

691
00:47:03,700 --> 00:47:06,410
OK any other question

692
00:47:06,420 --> 00:47:07,850
i find

693
00:47:07,870 --> 00:47:12,930
and now this lecture is all about the kripke semantics for modal logic stand after

694
00:47:12,990 --> 00:47:14,740
guy called so keep

695
00:47:15,530 --> 00:47:19,020
made major contributions to basically showed

696
00:47:19,030 --> 00:47:22,930
that modal logic has semantics in terms of graphs

697
00:47:22,970 --> 00:47:30,270
from nineteen twenty remember was more logic was healthy disciplines using only syntax

698
00:47:30,270 --> 00:47:31,880
is and

699
00:47:31,940 --> 00:47:36,430
i'm sure you guys as you goes no principal component analysis can essentially be interpreted

700
00:47:36,570 --> 00:47:38,820
as getting distribution

701
00:47:38,830 --> 00:47:42,860
of objects and you essentially if you ignore

702
00:47:42,880 --> 00:47:49,170
some of the similar low-energy organ components you essentially going some generalizations so like essentially

703
00:47:49,170 --> 00:47:52,180
what we do with PCA is we essentially get you can think of it this

704
00:47:52,180 --> 00:47:55,640
as a generative model we have the generative model of how

705
00:47:55,690 --> 00:47:57,160
a bunch of faces here

706
00:47:57,170 --> 00:48:00,890
given that the are already listed same way so given that a lot of weight

707
00:48:00,930 --> 00:48:03,420
on the eyes and nose of every subject

708
00:48:03,730 --> 00:48:09,480
and make sure that i made sure that you find so i've removed the final

709
00:48:09,480 --> 00:48:12,400
from each of them that they all go back to the same template

710
00:48:12,490 --> 00:48:13,830
and all of here

711
00:48:13,860 --> 00:48:18,500
is a measure of how that appearance changes as a function of these identities

712
00:48:18,530 --> 00:48:20,000
so really simple idea

713
00:48:21,660 --> 00:48:24,160
can i can go into it a little bit more detail

714
00:48:24,530 --> 00:48:29,810
the training examples for instance the way that we do that is said probably have

715
00:48:29,840 --> 00:48:33,970
always nice coordinates that define the box here and so

716
00:48:34,000 --> 00:48:37,390
this is where we get people at the university of pittsburgh or myself in this

717
00:48:38,060 --> 00:48:40,130
and clicking clicking points

718
00:48:40,150 --> 00:48:45,060
and we get these these what what values

719
00:48:45,060 --> 00:48:48,090
what we do then if we keep them with notation here is that of the

720
00:48:48,090 --> 00:48:51,270
magical does so what my source image

721
00:48:51,320 --> 00:48:55,330
and the magical w ensures that the size of all these images have the same

722
00:48:55,350 --> 00:49:00,360
to just say it's in by and another in square by one convex and square

723
00:49:00,500 --> 00:49:03,190
one called the on-screen one column vector

724
00:49:03,250 --> 00:49:04,830
and just for

725
00:49:04,950 --> 00:49:10,510
completeness i shall refer to my training data summer training examples given that the normalized

726
00:49:10,520 --> 00:49:14,670
for these for these grant reports is t one t two all the way through

727
00:49:15,450 --> 00:49:18,240
and this is what they look like one to normalise

728
00:49:18,310 --> 00:49:23,350
these are in happen that this column vectors of pixel intensity

729
00:49:23,410 --> 00:49:26,140
and essentially all then

730
00:49:26,160 --> 00:49:27,380
is but for

731
00:49:27,400 --> 00:49:31,380
put it into a big matrix of t one through t m

732
00:49:31,880 --> 00:49:35,090
and you see here so to say this is an squares

733
00:49:35,330 --> 00:49:37,140
in terms of

734
00:49:37,150 --> 00:49:40,850
we should be using the same thing that this is the site m squared them

735
00:49:40,930 --> 00:49:44,380
in terms of pixels and i've got in

736
00:49:44,420 --> 00:49:48,830
and images this one then of gordon and m squared by in

737
00:49:50,620 --> 00:49:56,750
what can do then typically is an assisted living in PCA remain so to say

738
00:49:56,750 --> 00:49:59,010
that you want to make sure things of the same origin

739
00:49:59,020 --> 00:50:01,700
and this is kind of like the ghostly

740
00:50:01,750 --> 00:50:07,780
i mean face things like being made in we actually we actually did a series

741
00:50:07,780 --> 00:50:11,830
for the discovery channel recently on like facebook stuff and things of that was being

742
00:50:11,830 --> 00:50:17,670
made in the media likely this idea of an average facts selectively like registering and

743
00:50:17,910 --> 00:50:22,610
registering face across many many many images an interesting thing is that a lot of

744
00:50:22,610 --> 00:50:28,000
people can claim that the average price is it's all it's it's much more beautiful

745
00:50:28,010 --> 00:50:33,610
which statically beautiful anyway then an individual because it has little symmetry

746
00:50:33,670 --> 00:50:39,030
and humans tend to like symmetry interface and also become smooth out lot the irregularities

747
00:50:39,030 --> 00:50:41,430
and things like that

748
00:50:46,140 --> 00:50:50,880
i i've heard of that affair that as well but from my money they look

749
00:50:50,880 --> 00:50:53,830
kind of quite closely things but the depending on the

750
00:50:53,850 --> 00:50:57,560
but there is and it's it's an interesting it's an interesting artifact the kind of

751
00:50:57,560 --> 00:51:02,270
but again that's perhaps some sort of redundancy in a human visual system perhaps so

752
00:51:02,320 --> 00:51:09,790
that we like we like people with with really small faces and symmetrical faces so

753
00:51:10,050 --> 00:51:14,340
there's not much i mean but the but you know they got the average faces

754
00:51:14,560 --> 00:51:15,480
and then we apply

755
00:51:15,900 --> 00:51:19,500
principal component analysis which i hope you're all familiar with

756
00:51:19,500 --> 00:51:21,530
you can either do

757
00:51:21,550 --> 00:51:25,770
classic organic decomposition or this to an SVD

758
00:51:25,790 --> 00:51:27,810
and essentially what we've got here

759
00:51:27,830 --> 00:51:28,640
is that

760
00:51:28,830 --> 00:51:35,490
i is essentially concatenation of these i can vectors

761
00:51:35,530 --> 00:51:38,290
up after up to the an organ vectors and if i look at these i

762
00:51:38,350 --> 00:51:44,100
vectors in as an image you can to get these ghostly looking images become the

763
00:51:44,130 --> 00:51:46,890
kind of thing like that's the kind of place like but they

764
00:51:46,950 --> 00:51:51,080
there there are some parts that so like parts it should be so positive values

765
00:51:51,080 --> 00:51:53,130
that are negative values and vice versa

766
00:51:53,220 --> 00:51:58,650
and and obviously we've got inside that we've got a diagonal matrix that's got the

767
00:51:58,650 --> 00:52:02,830
organ values that kind of gives me it's essentially the amount of energy in my

768
00:52:02,830 --> 00:52:07,030
training sample sets associated with each of these i are parts

769
00:52:07,040 --> 00:52:13,040
and so this is all quite rudimentary i mean this is classical stuff

770
00:52:13,050 --> 00:52:17,970
and have a point is essentially this they can go OK well

771
00:52:17,980 --> 00:52:20,140
we found the image matching the

772
00:52:20,420 --> 00:52:25,570
SST works quite well so let's use stay but instead of trying to solve for

773
00:52:25,600 --> 00:52:26,570
just p

774
00:52:26,580 --> 00:52:29,200
just trying to solve the wall let's try and solve

775
00:52:29,220 --> 00:52:32,080
for the appearance variation at the same time

776
00:52:32,090 --> 00:52:35,430
that's essentially what they do so it's kind of we can go get to the

777
00:52:35,640 --> 00:52:36,600
only whatever

778
00:52:36,620 --> 00:52:38,020
so this is

779
00:52:38,020 --> 00:52:41,060
this is my source image and i'm trying to match for

780
00:52:41,100 --> 00:52:44,380
this is my p i don't know MIP map is there some kind of searching

781
00:52:44,410 --> 00:52:48,930
life much likely and that the mining industry exhaustive search on this kind of light

782
00:52:48,930 --> 00:52:53,010
how to get the labelled data and how do we actually know something like politics

783
00:52:53,010 --> 00:52:54,950
at the moment i guess negative sentiment

784
00:52:55,530 --> 00:52:56,470
it's a hard problem

785
00:52:57,890 --> 00:53:00,700
we kind of cheated a little bit by using the emotive comes

786
00:53:01,580 --> 00:53:03,430
so the molecules smileys

787
00:53:04,060 --> 00:53:05,930
it's something people very often attached to

788
00:53:06,870 --> 00:53:08,680
personal messages or tweets what

789
00:53:09,470 --> 00:53:13,890
so we took the assumption that they uh smiley face happy face

790
00:53:14,720 --> 00:53:16,870
represents the positive sentiment and the

791
00:53:17,430 --> 00:53:19,200
south place negative sentiment

792
00:53:21,310 --> 00:53:22,490
so what the data

793
00:53:23,100 --> 00:53:23,470
and is the

794
00:53:25,220 --> 00:53:27,680
deep deep semantic analysis of

795
00:53:28,310 --> 00:53:31,700
what keywords are likely to be sentiment-bearing and so on and so forth

796
00:53:32,280 --> 00:53:32,830
well actually

797
00:53:33,510 --> 00:53:35,180
just extracted all foreground

798
00:53:36,010 --> 00:53:36,640
from the street

799
00:53:37,970 --> 00:53:41,700
by i mean character-based foreground not word level foreground

800
00:53:42,530 --> 00:53:43,890
and we build eh

801
00:53:44,310 --> 00:53:45,350
logistic regression model

802
00:53:46,010 --> 00:53:47,180
or a collection of such

803
00:53:49,680 --> 00:53:50,080
and the

804
00:53:51,100 --> 00:53:51,890
these are the results

805
00:53:53,700 --> 00:53:54,970
what it shows is about

806
00:53:55,760 --> 00:53:59,560
which trains a single logistic regression model over such a presentation

807
00:54:00,350 --> 00:54:01,200
using more data

808
00:54:01,910 --> 00:54:03,850
definitely improved the performance

809
00:54:04,430 --> 00:54:05,200
i mean clearly

810
00:54:06,290 --> 00:54:08,780
now interestingly given a small sample

811
00:54:10,760 --> 00:54:11,970
three classifiers

812
00:54:13,370 --> 00:54:15,280
vastly outperforms a single model

813
00:54:16,430 --> 00:54:19,510
using fewer data so this is trained with a hundred million

814
00:54:22,100 --> 00:54:25,740
hand although the trained with just ten million examples

815
00:54:26,240 --> 00:54:29,720
and this is the number of ensemble elements in the collection

816
00:54:30,530 --> 00:54:31,080
so clearly

817
00:54:32,030 --> 00:54:33,220
one element with

818
00:54:34,740 --> 00:54:38,240
one hundred million examples this is not as good as three

819
00:54:38,990 --> 00:54:40,290
trained with is ten million

820
00:54:43,970 --> 00:54:45,010
so some win

821
00:54:45,760 --> 00:54:49,010
and it's interesting that those ensembles of linear classifiers

822
00:54:49,600 --> 00:54:55,200
weights often find people which often people find to be not as effective as the linear classifiers

823
00:54:56,390 --> 00:54:57,830
in this case they work very well

824
00:54:59,010 --> 00:55:01,830
and there's a lot of diminishing returns by going into

825
00:55:05,890 --> 00:55:07,030
more than eight hours for

826
00:55:07,850 --> 00:55:13,990
with the ensembles sorry the user was samples was more important in this case the news of more data

827
00:55:14,660 --> 00:55:15,890
but more than just a helps

828
00:55:20,510 --> 00:55:21,490
some people may ask

829
00:55:22,310 --> 00:55:23,680
okay so proposing this

830
00:55:24,450 --> 00:55:26,660
the category this an online algorithm

831
00:55:28,470 --> 00:55:31,060
which streams the data about what they are

832
00:55:31,290 --> 00:55:33,870
what if there really is something which requires iterations

833
00:55:35,120 --> 00:55:36,200
so what do i do now

834
00:55:38,100 --> 00:55:40,850
so there's a few options and is not impossible

835
00:55:42,620 --> 00:55:46,410
so the number of iterations really small just like my we unroll them

836
00:55:46,970 --> 00:55:47,580
now we can just

837
00:55:48,060 --> 00:55:48,790
a look

838
00:55:50,780 --> 00:55:51,780
we can do is to pay

839
00:55:52,990 --> 00:55:55,100
well the problem is like we very often don't know

840
00:55:55,490 --> 00:55:57,560
the number of iterations ahead of time

841
00:55:58,200 --> 00:56:01,200
and even if we did you'd probably be fairly large

842
00:56:03,060 --> 00:56:04,180
it was like this too

843
00:56:06,080 --> 00:56:07,930
actually implement this thing in

844
00:56:09,680 --> 00:56:13,810
in something like a cascading framework in python sketch

845
00:56:15,620 --> 00:56:18,450
it's extra work it doesn't really fit well with what are we to

846
00:56:19,810 --> 00:56:21,850
recently that some options of having

847
00:56:22,330 --> 00:56:23,350
custom control flow

848
00:56:24,560 --> 00:56:25,140
to pick

849
00:56:25,560 --> 00:56:28,580
but i mean is very sensitive actually not fully baked yet

850
00:56:29,970 --> 00:56:31,830
so the only option is to

851
00:56:32,550 --> 00:56:34,200
right customer mapreduce job

852
00:56:35,140 --> 00:56:38,290
o use something like an external framework like mahout

853
00:56:38,930 --> 00:56:40,260
which can do it for us

854
00:56:42,720 --> 00:56:46,290
really phase one particular problem word this is interesting

855
00:56:47,120 --> 00:56:48,140
those topic modeling

856
00:56:49,240 --> 00:56:53,370
so the topic modeling is something a lot of people study for a variety of reasons

857
00:56:54,290 --> 00:56:55,370
we also find it useful

858
00:56:55,990 --> 00:56:56,990
for applications

859
00:56:57,550 --> 00:57:00,600
so what you see here is like what big one particular topic

860
00:57:00,600 --> 00:57:04,130
so now the question is what i'm trying to prove to you is that e

861
00:57:05,160 --> 00:57:09,610
e two is more negative than the one so ultimately what one is

862
00:57:09,630 --> 00:57:12,770
what's the relative magnitude of the two over one

863
00:57:12,790 --> 00:57:16,220
what is it is greater than one that's the question right i can prove that

864
00:57:16,220 --> 00:57:17,380
is greater than one

865
00:57:17,440 --> 00:57:19,910
that means that the line stable then

866
00:57:19,920 --> 00:57:22,030
the set of ion pairs and if you

867
00:57:22,080 --> 00:57:23,800
if you look at the series

868
00:57:23,810 --> 00:57:25,050
this series

869
00:57:25,070 --> 00:57:27,690
you know the series logo one

870
00:57:27,710 --> 00:57:30,860
with series expansion log of one class act

871
00:57:30,860 --> 00:57:37,350
is what is it's x minus x squared over two plus executed over three etcetera

872
00:57:37,350 --> 00:57:41,180
etcetera few plug in x equals one that's our series

873
00:57:41,210 --> 00:57:43,510
one minus the hapless the third

874
00:57:43,550 --> 00:57:46,570
it's all there so if you if if you're going work that out log two

875
00:57:46,570 --> 00:57:52,480
n two log two pi matthew can show that for the line are series

876
00:57:52,540 --> 00:57:54,790
and is equal to two

877
00:57:54,840 --> 00:58:00,380
the natural logarithm of two which is one point three eight six which is greater

878
00:58:00,380 --> 00:58:01,240
than one

879
00:58:01,270 --> 00:58:05,590
so what have i shown i've shown that by taking a big and number of

880
00:58:05,590 --> 00:58:11,370
individual ion pairs and putting them all together and align the system's energy

881
00:58:11,410 --> 00:58:12,880
became more negative

882
00:58:12,890 --> 00:58:16,650
so that's to say that

883
00:58:16,700 --> 00:58:18,810
it's favour so we could say

884
00:58:18,810 --> 00:58:21,580
and energy level diagram is to the energy

885
00:58:21,600 --> 00:58:24,570
here's zero if this is

886
00:58:24,620 --> 00:58:30,400
the energy of the pair let's make this he over the of the ion pair

887
00:58:30,490 --> 00:58:32,320
he over the for the pair

888
00:58:32,330 --> 00:58:33,700
so this is one

889
00:58:35,910 --> 00:58:37,740
we just showing

890
00:58:37,760 --> 00:58:41,910
that for the line so this is pairs ever get number of pairs this is

891
00:58:41,910 --> 00:58:43,470
as big as the number of

892
00:58:43,490 --> 00:58:45,470
i pairs in line

893
00:58:45,520 --> 00:58:48,940
that's one point three eight six and then if you go to the real sodium

894
00:58:48,940 --> 00:58:53,570
chloride crystal and you do the same calculation one in three dimensions so that you

895
00:58:53,570 --> 00:58:55,130
take every sodium

896
00:58:55,140 --> 00:58:56,930
and you count all of its

897
00:58:56,960 --> 00:59:02,260
nearest neighbours next nearest neighbours next next nearest neighbors not only online but in the

898
00:59:02,260 --> 00:59:08,120
plane and above and below in all three dimensions and go through that calculation it's

899
00:59:08,120 --> 00:59:12,670
even more negative it's one point seven four seven five

900
00:59:13,920 --> 00:59:16,940
if i start to react sodium with chlorine

901
00:59:16,950 --> 00:59:20,280
the result is the crystal a three-dimensional array

902
00:59:20,280 --> 00:59:25,340
because at every point first thing happens electron transfer y

903
00:59:25,390 --> 00:59:29,820
to get a stable octet the result i got ion pairs

904
00:59:29,860 --> 00:59:35,830
plus attract to minus minus is attractor pluses they start climbing together but they don't

905
00:59:35,830 --> 00:59:38,210
stop with one plus one

906
00:59:38,270 --> 00:59:42,320
one plus one and then the ion pair grabs another one another one and before

907
00:59:42,320 --> 00:59:45,890
you know it you have a crystal so this is simply the energetics of crystal

908
00:59:45,890 --> 00:59:51,190
formation so this is a three-dimensional crystal you can consider this line of one dimensional

909
00:59:51,190 --> 00:59:53,160
crystal because it's ordered

910
00:59:53,190 --> 00:59:58,460
it's ordered psychology one-dimensional crystal so this says that when it comes to iron

911
00:59:59,800 --> 01:00:01,350
what comes to ions

912
01:00:01,410 --> 01:00:02,780
comes to ions

913
01:00:03,790 --> 01:00:05,210
line dancing

914
01:00:05,230 --> 01:00:07,670
trump's ballroom dancing that's what it's a

915
01:00:07,790 --> 01:00:12,160
they they rather be in line then dance two by two

916
01:00:12,240 --> 01:00:15,030
that doesn't apply to all things but it does in this case

917
01:00:17,350 --> 01:00:20,730
now let's look at properties let's talk about properties in we've got a long way

918
01:00:20,740 --> 01:00:21,860
we started with this

919
01:00:21,870 --> 01:00:26,420
hypothesis of octet stability and talking about properties of crystals there is solid at room

920
01:00:26,420 --> 01:00:27,850
temperature why

921
01:00:27,860 --> 01:00:30,360
what's the what's these are huge energy

922
01:00:30,410 --> 01:00:32,080
these are huge energy

923
01:00:32,120 --> 01:00:33,620
coulombic forces

924
01:00:34,690 --> 01:00:38,920
high melting points and boiling points like to have a high melting point boiling point

925
01:00:39,040 --> 01:00:44,340
because when you ask yourself the question you should consider what is the thermal energy

926
01:00:44,430 --> 01:00:50,120
first the bonding energy if the binding energy is very strongly negative thermal energy

927
01:00:50,150 --> 01:00:54,700
isn't great enough to disrupt those bonds and allow those wants to be broken and

928
01:00:54,710 --> 01:00:56,420
half fluidity

929
01:00:57,080 --> 01:01:02,610
strong bonds means high melting points and boiling point transparent to visible light

930
01:01:02,670 --> 01:01:04,630
transferred to visible light

931
01:01:04,650 --> 01:01:09,760
crystals of the sodium chloride crystal is transparent to visible light table solved is white

932
01:01:09,760 --> 01:01:14,090
but that's because you've got powder and you've got multiple surface scattering large crystal of

933
01:01:14,090 --> 01:01:15,930
sodium chloride is

934
01:01:15,960 --> 01:01:17,640
clear and colorless

935
01:01:17,650 --> 01:01:21,570
and it's faceted you can see that it has it has the shape if you

936
01:01:21,570 --> 01:01:22,210
take it

937
01:01:22,300 --> 01:01:24,390
one of those individual

938
01:01:24,410 --> 01:01:31,610
grains of sodium chloride look at it carefully you'll see the edges look like this

939
01:01:31,680 --> 01:01:35,420
why is transferred to visible light here i ask that question when you want to

940
01:01:35,420 --> 01:01:38,090
know something transparent to visible light or not

941
01:01:38,130 --> 01:01:39,880
here's the ministry material

942
01:01:39,890 --> 01:01:41,640
and here's is visible light

943
01:01:41,650 --> 01:01:46,700
and what is going to allow for absorption or some interaction with visible light inside

944
01:01:46,700 --> 01:01:52,200
here you've got electronic states and you gotta ask yourself is the energy difference here

945
01:01:52,200 --> 01:01:57,620
the delta e in the electronic states how does that compare with the

946
01:01:58,130 --> 01:01:59,670
visible light

947
01:01:59,680 --> 01:02:03,170
what's the invisible like you know it's about two to three

948
01:02:03,230 --> 01:02:05,130
electron volts

949
01:02:05,140 --> 01:02:08,440
o two to three electron volts this has what

950
01:02:08,490 --> 01:02:14,840
sodium plastics isoelectronic with neon chloride minus isoelectronic with our got what do we know

951
01:02:14,840 --> 01:02:17,330
about the average valence electron energies

952
01:02:17,330 --> 01:02:20,840
which is sort the the first slide it's up around thirteen

953
01:02:20,850 --> 01:02:22,660
fourteen fifteen

954
01:02:22,720 --> 01:02:24,580
electron volts

955
01:02:24,620 --> 01:02:27,270
this thing is going to touch only global

956
01:02:27,280 --> 01:02:29,030
so what happens it goes through

957
01:02:31,350 --> 01:02:35,130
all that glitters is not gold but it must have free electrons

958
01:02:35,180 --> 01:02:37,370
this has no free electrons

959
01:02:37,410 --> 01:02:39,320
they're all bound

960
01:02:39,320 --> 01:02:42,180
so that's how you determine if something's transparent

961
01:02:42,220 --> 01:02:46,450
it's going to be electrically insulated is not for to move around all bound hard

962
01:02:46,450 --> 01:02:48,410
and brittle y

963
01:02:48,450 --> 01:02:52,820
because of its strong bond strong bonds means the material will resist

964
01:02:52,840 --> 01:02:54,550
an applied force

965
01:02:54,570 --> 01:02:59,110
you apply force to the material will resist until you have the force great enough

966
01:02:59,130 --> 01:03:02,990
to break the bonds so you don't have ductility which is what you have in

967
01:03:02,990 --> 01:03:09,070
metal that makes metal so fascinating because metallic bonding allows the atoms to glide over

968
01:03:09,070 --> 01:03:13,160
one another without resulting in catastrophic failure

969
01:03:13,180 --> 01:03:16,700
whereas ceramics ionic crystals

970
01:03:16,700 --> 01:03:18,140
i will resist

971
01:03:18,140 --> 01:03:24,760
until they shatter there's only two categories it's either elastic with full resistance or

972
01:03:28,130 --> 01:03:31,530
the bonds soluble in water and other polar solvents will have to come back to

973
01:03:31,530 --> 01:03:35,870
that later they melt to form ionic liquids ionic solids from liquids so that's

974
01:03:35,970 --> 01:03:38,280
that's i showed you last a what happens when they

975
01:03:40,010 --> 01:03:42,220
and we were able to electrolytes and make

976
01:03:42,260 --> 01:03:44,630
fantastic novels like like magnesium

977
01:03:44,700 --> 01:03:49,030
OK what we find what we find is ionic crystals where we look well you

978
01:03:49,030 --> 01:03:54,030
electron transfer senior the donor and union acceptors so we're the good donors donors are

979
01:03:54,030 --> 01:03:57,740
over here the on the left and the the periodic table

980
01:03:57,760 --> 01:04:01,010
and where the except on the right side of the periodic table

981
01:04:01,050 --> 01:04:04,090
so if you take some from column one or two when you mix it with

982
01:04:04,090 --> 01:04:07,280
some from column even nine bingo you've got

983
01:04:07,320 --> 01:04:12,360
reactions so here's an example that shows here's here's some very very metallic elements groups

984
01:04:12,360 --> 01:04:16,090
we're going to discuss today

985
01:04:16,110 --> 01:04:20,250
resistive forces and drag forces

986
01:04:20,270 --> 01:04:22,310
when you move an object

987
01:04:22,320 --> 01:04:27,330
through a medium whether to guess web the liquid experiences drag force

988
01:04:27,420 --> 01:04:31,360
this drag force depends on the shape of the object the size of the object

989
01:04:31,410 --> 01:04:33,420
the medium through which you remove it

990
01:04:34,240 --> 01:04:36,710
the speed of the object

991
01:04:36,760 --> 01:04:39,270
the medium is immediately obvious

992
01:04:39,330 --> 01:04:43,080
it's area you move through air you feel the wind for you here that the

993
01:04:43,080 --> 01:04:44,710
drag force

994
01:04:44,760 --> 01:04:47,600
if you swim in water you feel this drag force

995
01:04:47,650 --> 01:04:51,900
in oriel the drag force will be even larger

996
01:04:52,710 --> 01:04:53,810
direct force

997
01:04:53,820 --> 01:04:59,140
this resistive force is very very different from the fraction that we've discussed earlier

998
01:04:59,190 --> 01:05:00,620
when two surfaces

999
01:05:00,640 --> 01:05:03,470
move relative to each other there

1000
01:05:03,480 --> 01:05:07,290
the kinetic friction coefficient remains constant

1001
01:05:07,340 --> 01:05:09,610
independent of the speed

1002
01:05:10,590 --> 01:05:13,200
the drag forces and the resistive forces

1003
01:05:13,240 --> 01:05:16,800
they are not all independent of the speed

1004
01:05:16,810 --> 01:05:19,120
in very general terms

1005
01:05:19,160 --> 01:05:22,350
the resistive force

1006
01:05:22,370 --> 01:05:23,820
can be written

1007
01:05:25,990 --> 01:05:27,590
k one

1008
01:05:27,690 --> 01:05:31,130
times the velocity

1009
01:05:31,180 --> 01:05:33,000
was k two

1010
01:05:33,010 --> 01:05:35,950
times the velocities created

1011
01:05:35,970 --> 01:05:38,220
and always in the opposite direction

1012
01:05:38,270 --> 01:05:42,420
of the velocity vector this view is being made

1013
01:05:42,430 --> 01:05:45,150
so all the signs k one pnk two

1014
01:05:45,160 --> 01:05:46,870
and obviously the the squared

1015
01:05:46,880 --> 01:05:48,050
they all are

1016
01:05:48,080 --> 01:05:52,360
positive values and the k values depend on the shape

1017
01:05:52,400 --> 01:05:53,480
on the side

1018
01:05:53,500 --> 01:05:57,510
of the object and on the kind of media and that i have

1019
01:05:57,530 --> 01:06:00,280
today i will restrict myself exclusively

1020
01:06:00,320 --> 01:06:01,800
two spheres

1021
01:06:01,810 --> 01:06:05,470
and when we deal with years

1022
01:06:05,530 --> 01:06:09,210
we're going to get that the

1023
01:06:09,260 --> 01:06:11,320
the force

1024
01:06:11,350 --> 01:06:14,520
the magnitude of the force this part

1025
01:06:14,570 --> 01:06:17,490
equals c one

1026
01:06:17,540 --> 01:06:18,880
times are

1027
01:06:18,900 --> 01:06:21,920
time to speed policy two

1028
01:06:21,940 --> 01:06:23,700
times are spread

1029
01:06:23,710 --> 01:06:25,900
times the square

1030
01:06:25,950 --> 01:06:27,790
and again it's always opposing

1031
01:06:28,510 --> 01:06:30,670
velocity vector

1032
01:06:31,450 --> 01:06:32,340
c one

1033
01:06:32,360 --> 01:06:33,960
in our

1034
01:06:33,980 --> 01:06:35,870
units kilograms

1035
01:06:35,950 --> 01:06:37,890
four meters per second

1036
01:06:37,910 --> 01:06:39,570
and c two

1037
01:06:39,620 --> 01:06:41,480
as the dimension of density

1038
01:06:41,490 --> 01:06:45,380
kilogram per cubic metres

1039
01:06:45,390 --> 01:06:46,870
we call this

1040
01:06:46,910 --> 01:06:48,870
the viscous term

1041
01:06:48,890 --> 01:06:50,620
and we call this

1042
01:06:51,340 --> 01:06:53,350
pressure to

1043
01:06:54,080 --> 01:06:55,850
viscous term

1044
01:06:55,910 --> 01:06:59,880
has to do is the stickiness of the medium

1045
01:06:59,890 --> 01:07:01,400
if you take for instance

1046
01:07:01,420 --> 01:07:03,210
liquid water

1047
01:07:03,260 --> 01:07:08,520
and all antara is a huge difference in stickiness physicist also refer to that as

1048
01:07:10,580 --> 01:07:13,480
if you have a high viscosity is very sticky

1049
01:07:13,490 --> 01:07:14,750
then this

1050
01:07:14,760 --> 01:07:17,330
number c one will be very high

1051
01:07:17,380 --> 01:07:21,460
so this we call the viscous term

1052
01:07:21,470 --> 01:07:24,540
and this we call

1053
01:07:24,550 --> 01:07:27,170
the pressure to

1054
01:07:28,160 --> 01:07:31,260
c one is a strong function of temperature

1055
01:07:31,280 --> 01:07:34,290
we know that if you take part in you it

1056
01:07:35,370 --> 01:07:39,470
the viscosity goes down is way more sticky when it is cold

1057
01:07:39,590 --> 01:07:42,850
c two is not very dependent on

1058
01:07:42,930 --> 01:07:45,700
the temperature

1059
01:07:45,750 --> 01:07:47,370
it's not so easy to sing

1060
01:07:47,380 --> 01:07:49,260
why this pressure term

1061
01:07:49,270 --> 01:07:53,300
he has to be square later in the course when we deal with transfer of

1062
01:07:54,270 --> 01:07:57,370
we will understand why there is the squared term there

1063
01:07:57,390 --> 01:07:59,680
but they are square is very easy to see

1064
01:07:59,690 --> 01:08:01,840
because if you have a sphere

1065
01:08:02,690 --> 01:08:04,480
there is some fluids

1066
01:08:04,490 --> 01:08:07,530
gas or liquid streaming onto to it

1067
01:08:08,600 --> 01:08:10,400
this has a cross sectional area

1068
01:08:10,420 --> 01:08:15,190
which is proportional to are squared and so it's easy to see that the force

1069
01:08:15,210 --> 01:08:17,830
that is object experience we call it

1070
01:08:17,890 --> 01:08:20,420
the pressure term is proportional

1071
01:08:20,480 --> 01:08:23,610
two are square to that's easy to see two liquids

1072
01:08:23,610 --> 01:08:25,750
have factorizations

1073
01:08:25,790 --> 01:08:26,640
if you

1074
01:08:26,670 --> 01:08:33,270
read conditional independence you can read conditional independence by the concept of graph separation

1075
01:08:33,270 --> 01:08:36,280
but i i d separation

1076
01:08:36,580 --> 01:08:42,090
and likewise the opposite also holds and is especially important

1077
01:08:42,100 --> 01:08:44,010
because remember that use their actions

1078
01:08:44,030 --> 01:08:46,780
is the one that's most interesting to us

1079
01:08:46,830 --> 01:08:51,060
because usually the set of conditional independence statements is what you are listed from the

1080
01:08:51,060 --> 01:08:52,780
domain expert

1081
01:08:52,790 --> 01:08:57,200
the main expert knows nothing about factorizations of probability distribution

1082
01:08:58,020 --> 01:09:01,520
so these actions particularly for

1083
01:09:01,570 --> 01:09:05,860
so if you have a probability distribution that satisfies the conditional

1084
01:09:05,880 --> 01:09:11,640
independence statements implied by d separation

1085
01:09:11,650 --> 01:09:18,280
then it to factorize what they were

1086
01:09:19,630 --> 01:09:23,530
so in other words you should

1087
01:09:23,530 --> 01:09:26,060
from now on we we should think of

1088
01:09:30,230 --> 01:09:33,720
the separated

1089
01:09:33,810 --> 01:09:35,850
from b

1090
01:09:36,000 --> 01:09:38,110
by c

1091
01:09:38,900 --> 01:09:40,410
you should

1092
01:09:40,460 --> 01:09:46,440
now i know that this is basically means a

1093
01:09:46,460 --> 01:09:48,120
conditional independence

1094
01:09:52,360 --> 01:09:55,360
these are purely

1095
01:09:55,370 --> 01:09:56,690
graph theory

1096
01:09:57,900 --> 01:10:00,940
we define the most of the prediction

1097
01:10:00,950 --> 01:10:05,880
it is based on the definition of that unblock that

1098
01:10:07,340 --> 01:10:12,290
it's a purely probabilistic on

1099
01:10:12,360 --> 01:10:14,180
so here you have

1100
01:10:14,190 --> 01:10:16,680
two universities in the meat

1101
01:10:21,620 --> 01:10:23,550
it is very important

1102
01:10:26,460 --> 01:10:35,380
so again

1103
01:10:35,390 --> 01:10:41,290
if you see x two

1104
01:10:41,290 --> 01:10:43,120
x five and x six

1105
01:10:47,750 --> 01:10:51,780
so again

1106
01:10:52,620 --> 01:10:55,970
x five has head tail with respect to these

1107
01:10:56,070 --> 01:11:00,470
its head tail with respect to this but

1108
01:11:00,500 --> 01:11:04,770
so it is with respect all that cities head tail

1109
01:11:08,250 --> 01:11:13,050
yes these he is blocked

1110
01:11:13,090 --> 01:11:15,080
is but he is blocked

1111
01:11:16,670 --> 01:11:18,690
and any path that

1112
01:11:18,700 --> 01:11:19,880
goes through

1113
01:11:19,890 --> 01:11:21,530
x five

1114
01:11:21,540 --> 01:11:23,460
in this case

1115
01:11:23,480 --> 01:11:26,950
o particular this bad block this blocked

1116
01:11:29,010 --> 01:11:33,220
you see that there is more than one pattern there's more than two pass

1117
01:11:33,270 --> 01:11:36,700
that involve x five so this is one

1118
01:11:36,760 --> 01:11:39,820
which has had failed is another which had failed

1119
01:11:39,830 --> 01:11:43,900
but these but here in the those

1120
01:11:43,960 --> 01:11:46,780
involve the sewage is here

1121
01:11:46,780 --> 01:11:48,110
he x five

1122
01:11:48,120 --> 01:11:53,350
he's a head-to-head nodes with regard these

1123
01:11:53,480 --> 01:11:58,190
so again

1124
01:11:58,220 --> 01:12:02,610
so OK so you want to ask if in addition to x one

1125
01:12:02,620 --> 01:12:03,800
and x five

1126
01:12:03,800 --> 01:12:05,870
i also had x two

1127
01:12:07,700 --> 01:12:09,030
but also

1128
01:12:09,040 --> 01:12:11,340
is that the question

1129
01:12:11,360 --> 01:12:25,810
no it's not walk because when a as i said this is blocked

1130
01:12:25,850 --> 01:12:28,420
no this is wrong this is not a blog

1131
01:12:28,480 --> 01:12:32,090
there's not a lot because when they observe these here

1132
01:12:32,090 --> 01:12:36,530
i i created connection between extreme x six

1133
01:12:36,570 --> 01:12:38,590
so that's exactly what we have here

1134
01:12:38,600 --> 01:12:41,100
in this exercise

1135
01:12:41,110 --> 01:12:43,510
all i see that was one

1136
01:12:43,560 --> 01:12:47,760
this is OK so so basically

1137
01:12:47,780 --> 01:12:49,580
i mean well

1138
01:12:49,590 --> 01:12:50,890
well this

1139
01:12:50,920 --> 01:12:54,060
it's not the separate

1140
01:12:54,100 --> 01:13:00,550
because these that is what this but is blocked in this but it's not book

1141
01:13:00,600 --> 01:13:04,850
if this best for any reason i was blocked then it would be separate for

1142
01:13:04,850 --> 01:13:09,170
example the is viable what was observed

1143
01:13:09,230 --> 01:13:12,330
then it would be separate

1144
01:13:12,330 --> 01:13:14,690
because the first condition with be met

1145
01:13:14,790 --> 01:13:18,810
because this is the tail to tail node is that it doesn't matter if the

1146
01:13:18,810 --> 01:13:21,300
second condition is not met

1147
01:13:21,350 --> 01:13:25,250
as long as the first one

1148
01:13:25,380 --> 01:13:26,500
so again

1149
01:13:26,500 --> 01:13:28,110
just remembering

1150
01:13:28,120 --> 01:13:30,330
in this particular example

1151
01:13:30,420 --> 01:13:33,970
if i want to ask the question whether x three and x six o be

1152
01:13:36,340 --> 01:13:42,520
they are not be separated if we observe x one and x five only

1153
01:13:42,590 --> 01:13:45,360
because the spent here is not blocked

1154
01:13:45,440 --> 01:13:48,080
they are separate fields

1155
01:13:48,120 --> 01:13:51,170
x one x five and x

1156
01:13:51,190 --> 01:13:56,640
because including this that will also be one because of these which is observer in

1157
01:13:56,640 --> 01:13:59,950
its tail to tail node in the

1158
01:14:00,010 --> 01:14:01,900
it's not clear now

1159
01:14:01,920 --> 01:14:07,300
OK in your other question

1160
01:14:10,870 --> 01:14:13,590
OK let's keep going

1161
01:14:13,620 --> 01:14:17,270
so remember this two theorems which basically tell you that well

1162
01:14:17,290 --> 01:14:18,210
if you

1163
01:14:18,230 --> 01:14:21,220
if you have a bayesian network

1164
01:14:23,170 --> 01:14:27,360
the factorisation of that vision

1165
01:14:27,370 --> 01:14:30,110
willing to align precisely

1166
01:14:30,150 --> 01:14:35,710
which conditional independence is not only the conditional independences that we saw yesterday

1167
01:14:35,790 --> 01:14:40,120
because yesterday we saw that we constructed the bayesian networks

1168
01:14:40,140 --> 01:14:45,430
starting from very simple conditional independence so let's just

1169
01:14:45,440 --> 01:14:50,030
ignore some of the conditioning viable so because that's what we did right

1170
01:14:50,110 --> 01:14:55,390
so basically we started with the entire factorisation of the joint probability distribution in terms

1171
01:14:56,070 --> 01:14:58,370
conditional probability distributions

1172
01:14:58,390 --> 01:15:03,840
and then the forces some particular conditional independence statements that were kind of of string

1173
01:15:03,840 --> 01:15:05,400
handling that

1174
01:15:06,460 --> 01:15:11,190
and then there's obviously a lot of interesting challenges in deciding given

1175
01:15:11,230 --> 01:15:15,590
the results from each of these purposes what which ones are most relevant to show

1176
01:15:15,590 --> 01:15:19,770
for that particular query and what you should use to show them and so on

1177
01:15:19,800 --> 01:15:23,920
i think one of the things one of we have in the past had approach

1178
01:15:23,940 --> 01:15:28,420
this problem as we said g we don't have much capacity let's see if we

1179
01:15:28,420 --> 01:15:31,320
can find some queries that we'd like to send to

1180
01:15:31,340 --> 01:15:34,710
some of the other corpora that we believe will have good results but that so

1181
01:15:34,730 --> 01:15:38,710
much harder problem because you're actually trying to decide based on three words in the

1182
01:15:39,460 --> 01:15:41,630
is this going to be interesting

1183
01:15:41,630 --> 01:15:46,730
will the you know google groups corpus have interesting results that three query

1184
01:15:46,750 --> 01:15:50,570
and that's actually much harder problem and if you actually send the query down get

1185
01:15:50,570 --> 01:15:55,110
the scores back from that corpus and the relevant documents and then you can make

1186
01:15:55,110 --> 01:15:59,020
a decision saying oh yeah well the score from groups is twelve in the score

1187
01:15:59,020 --> 01:16:02,860
from printing five so i'm going to assume that one of them is more relevant

1188
01:16:03,000 --> 01:16:05,670
other of the scores may not be directly comparable you

1189
01:16:05,710 --> 01:16:09,380
over time over a set of queries understand

1190
01:16:09,420 --> 01:16:13,290
you know what when one corpus is going to be more relevant

1191
01:16:16,110 --> 01:16:17,570
the other big change

1192
01:16:17,590 --> 01:16:21,110
that's happened relatively recently is that

1193
01:16:22,270 --> 01:16:23,960
we now have the ability to

1194
01:16:23,960 --> 01:16:27,570
update our index in a matter of minutes here

1195
01:16:27,610 --> 01:16:29,320
for for given

1196
01:16:29,520 --> 01:16:34,290
page as opposed to waiting fairly long periods and during very large batch updates

1197
01:16:34,300 --> 01:16:37,670
and a lot of things that have to change your system in order to really

1198
01:16:37,670 --> 01:16:38,570
do right

1199
01:16:38,610 --> 01:16:43,300
first of all how do you know what pages you want update it's good to

1200
01:16:43,300 --> 01:16:44,730
have a system that can identify

1201
01:16:45,110 --> 01:16:47,920
are there interesting new pages on the web that just showing up in the last

1202
01:16:47,920 --> 01:16:48,900
minute or two

1203
01:16:48,900 --> 01:16:54,190
or are there important pages that have been updated that you ideally like to have

1204
01:16:54,190 --> 01:16:58,190
any page as soon as it changes be updated in your inject that's a very

1205
01:16:58,190 --> 01:17:03,000
difficult thing not of crawled into the world to really do that and some pages

1206
01:17:03,000 --> 01:17:07,980
change every time you crop because they have like the timestamp at the bottom so

1207
01:17:08,020 --> 01:17:11,840
you sort of have to pick and choose your battles and decide for given budget

1208
01:17:11,840 --> 01:17:15,530
of pages it adopted one of the most important one that you want updated so

1209
01:17:15,530 --> 01:17:16,710
the two indexes

1210
01:17:16,750 --> 01:17:19,150
fresh and useful users

1211
01:17:20,030 --> 01:17:25,050
so the crawling system needs to be able to accommodate these very fast update requests

1212
01:17:25,070 --> 01:17:28,880
like i want to update this page and i want now not here's a whole

1213
01:17:28,880 --> 01:17:31,110
bunch of urals and i want to

1214
01:17:31,110 --> 01:17:33,050
you know in an hour

1215
01:17:33,070 --> 01:17:34,520
the indexing system

1216
01:17:34,520 --> 01:17:39,090
it is challenging because it depends on global information

1217
01:17:39,090 --> 01:17:44,150
every structured for example where you depend on things like pagerank you depend on the

1218
01:17:44,150 --> 01:17:48,420
anchor text of pages that point to the to that page you need actually index

1219
01:17:48,570 --> 01:17:50,590
data with

1220
01:17:50,630 --> 01:17:52,900
the other with the target document

1221
01:17:52,980 --> 01:17:57,250
so you must have some way of getting this information or some approximation of it

1222
01:17:57,550 --> 01:18:00,670
in order to index the page within a minute or so

1223
01:18:00,690 --> 01:18:04,360
and serving system has to be re thought of it so that it can be

1224
01:18:04,360 --> 01:18:10,800
except updates four different documents while you're actually serving requests and accept updates fairly frequent

1225
01:18:12,190 --> 01:18:16,570
and the data structures you actually one user fairly different in that case they know

1226
01:18:16,650 --> 01:18:22,940
something we update things in batches of ten hours of

1227
01:18:23,360 --> 01:18:25,360
the other thing that i think is important in

1228
01:18:26,460 --> 01:18:27,710
and we've

1229
01:18:27,710 --> 01:18:30,480
i realized over time is that

1230
01:18:30,530 --> 01:18:35,170
it's really important when you're trying to make progress in improving ranking and and search

1231
01:18:35,170 --> 01:18:39,070
quality and so on is that you want to make it easy to do experiments

1232
01:18:40,150 --> 01:18:45,150
the faster you can make around africa experiment from when you have an idea to

1233
01:18:45,190 --> 01:18:47,840
when you actually can

1234
01:18:47,940 --> 01:18:52,150
demonstrate prototype that idea the better it's going to be

1235
01:18:52,150 --> 01:18:55,900
so we spent a lot of effort building tools that allow it

1236
01:18:55,920 --> 01:19:00,170
the make it fairly easy to do some kind of experiments in that they're not

1237
01:19:00,190 --> 01:19:01,610
hugely beneficial

1238
01:19:01,610 --> 01:19:05,440
some experiments you don't actually need to sort of do very much you just want

1239
01:19:05,480 --> 01:19:10,090
weight different ranking parameters and those you can actually do without really rolling out

1240
01:19:10,090 --> 01:19:13,440
anything in terms of new binaries or anything you can

1241
01:19:13,500 --> 01:19:16,690
just do them online are the ones that are more difficult to the ones that

1242
01:19:16,690 --> 01:19:18,690
need information that you didn't

1243
01:19:18,710 --> 01:19:21,550
i had the foresight to build a new production index

1244
01:19:21,630 --> 01:19:26,480
you need some way of computing and information generating it

1245
01:19:27,020 --> 01:19:31,170
after involves pass making a pass over all the documents are over some

1246
01:19:31,210 --> 01:19:35,940
derive information from from all the documents and be able to build that structure the

1247
01:19:35,940 --> 01:19:37,670
side that you can then use

1248
01:19:37,690 --> 01:19:39,770
to do your experiment

1249
01:19:42,130 --> 01:19:46,730
and not really talk very much about these various systems but p several of pieces

1250
01:19:46,730 --> 01:19:48,360
of infrastructure we built

1251
01:19:48,420 --> 01:19:52,150
are useful for doing some of these countries parents i already talk about a bit

1252
01:19:52,150 --> 01:19:56,820
idea fails which is just a big file system spread across thousands of machines

1253
01:19:56,840 --> 01:20:01,340
mapreduce is a abstraction that makes it easy to write and run large scale kind

1254
01:20:01,340 --> 01:20:02,920
of batch computations

1255
01:20:02,960 --> 01:20:07,920
i deals with like nasty things that happened like machine failures it automatically paralyzes things

1256
01:20:07,920 --> 01:20:09,980
across parts machines

1257
01:20:10,000 --> 01:20:12,400
basically it enables you to write

1258
01:20:12,420 --> 01:20:19,730
simple fairly simple programs that can extract information which is of interest to you for

1259
01:20:19,730 --> 01:20:24,090
your experiments and perform those ad-hoc experiments market

1260
01:20:24,230 --> 01:20:29,790
bigtable is a sort of semi structured storage system think of it as a very

1261
01:20:29,790 --> 01:20:33,610
large row and column oriented store

1262
01:20:33,670 --> 01:20:36,940
so for example one thing that gives us is online

1263
01:20:37,000 --> 01:20:40,980
efficient access to per document information any time you can say what is the current

1264
01:20:40,980 --> 01:20:42,820
information i know about

1265
01:20:42,860 --> 01:20:44,420
CNN dot com

1266
01:20:44,440 --> 01:20:49,290
and i will tell you OK pagerank is this will have profit this time here's

1267
01:20:49,290 --> 01:20:52,550
the current contents that we know about so on

1268
01:20:52,610 --> 01:20:57,000
and this is very useful for being able to update documents in a matter of

1269
01:20:57,000 --> 01:21:01,960
minutes as opposed to days

1270
01:21:02,000 --> 01:21:04,070
OK so the experimental cycle

1271
01:21:04,070 --> 01:21:07,980
you want to start with a new ranking idea you obviously use these tools to

1272
01:21:07,980 --> 01:21:11,980
generate interesting data and then you want to be able to

1273
01:21:12,000 --> 01:21:16,650
at first run off line experiment users are involved in this is actually just want

1274
01:21:17,380 --> 01:21:21,550
your say ranking change and look at what effect it has on how quickly how

1275
01:21:21,570 --> 01:21:23,500
results are ranked for

1276
01:21:23,520 --> 01:21:28,630
different results and some human rate increase that's another one maybe you'll do a random

1277
01:21:28,630 --> 01:21:32,270
selection of korean just look at what changes compared to the production ranking

1278
01:21:32,320 --> 01:21:36,420
so important thing to note here is that the latency or the throughput of this

1279
01:21:36,420 --> 01:21:38,000
prototype don't really matter

1280
01:21:38,050 --> 01:21:40,820
you know you can run ten thousand queries and it can take hours

1281
01:21:42,500 --> 01:21:44,420
not ideal but it's OK

1282
01:21:44,460 --> 01:21:47,880
and then you can iterate based on the results of this maybe you find that

1283
01:21:48,190 --> 01:21:53,790
she works well in this case but not this case i change occurred about

1284
01:21:53,790 --> 01:21:55,570
maurice pollen

1285
01:21:55,620 --> 01:21:58,980
and you have the size expected cost

1286
01:21:59,090 --> 01:22:01,450
more than that

1287
01:22:01,460 --> 01:22:05,370
well no really

1288
01:22:05,400 --> 01:22:08,260
i mean heuristic because the have two objectives

1289
01:22:08,380 --> 01:22:13,790
so the real problem is really by criterion probably two objective to minimize the expected

1290
01:22:14,220 --> 01:22:15,970
return expected mean

1291
01:22:16,010 --> 01:22:16,900
i mean

1292
01:22:16,960 --> 01:22:18,300
and the very

1293
01:22:18,430 --> 01:22:20,330
but you can minimize the same

1294
01:22:20,380 --> 01:22:21,820
but at the same time

1295
01:22:21,920 --> 01:22:25,170
so you really have

1296
01:22:25,180 --> 01:22:27,620
you have to define what we mean by the solution

1297
01:22:27,650 --> 01:22:33,300
and this is one possible definition of

1298
01:22:33,310 --> 01:22:35,740
so really

1299
01:22:35,810 --> 01:22:41,080
later converted by criterion problem into single about

1300
01:22:41,140 --> 01:22:43,660
or maybe it's only a partial

1301
01:22:43,710 --> 01:22:48,090
the real answer would be to look at the complete rail car between two objects

1302
01:22:48,730 --> 01:22:57,340
compete for me for doing this fixed come on it if you want point

1303
01:22:58,160 --> 01:23:02,220
but this is not quite as a function of four come as you know it

1304
01:23:02,220 --> 01:23:04,930
just LP with european bites means

1305
01:23:04,950 --> 01:23:07,880
but now we at quite experiments

1306
01:23:08,160 --> 01:23:16,840
another example is again very familiar and support vector machines

1307
01:23:16,890 --> 01:23:21,880
suppose now can we go back to this problem linearly separating two sets of points

1308
01:23:22,040 --> 01:23:25,880
we suppose we assume that there exactly separable

1309
01:23:25,900 --> 01:23:28,840
and we try to pick hyperplane that actually

1310
01:23:28,960 --> 01:23:36,080
it's better and more robust and more robust separation than other feasible hyperplane

1311
01:23:36,090 --> 01:23:41,060
well the most common way of defining that minimizes to maximize the margin between the

1312
01:23:43,560 --> 01:23:47,160
and that turns out to be one over the not relate to one of the

1313
01:23:47,160 --> 01:23:53,060
normal to get the maximum margin separating hyperplane minimize the norm of a lot of

1314
01:23:56,260 --> 01:23:59,580
that's qp p

1315
01:23:59,860 --> 01:24:05,330
or in the support vector classifier can combine the two

1316
01:24:05,380 --> 01:24:10,070
and look at the most general information and very have that's not separable

1317
01:24:10,150 --> 01:24:15,480
we tries to object if you want to maximize the margin and minimize the error

1318
01:24:15,480 --> 01:24:17,430
of the classification

1319
01:24:17,450 --> 01:24:20,290
expressed using this piecewise linear penalty

1320
01:24:20,330 --> 01:24:22,970
and again you can look at a weighted sum of the two

1321
01:24:23,080 --> 01:24:24,850
give support vector

1322
01:24:28,780 --> 01:24:33,200
so that's all the QP but it can be expressed as a QB you

1323
01:24:33,210 --> 01:24:40,790
convert back piecewise linear term in all office

1324
01:24:40,860 --> 01:24:46,130
and so this linear programming quadratic programming last a class of problems that been discussed

1325
01:24:46,130 --> 01:24:48,870
in this part geometric programming

1326
01:24:49,180 --> 01:24:55,030
i'm not sure that really want to use in machine learning but other applications

1327
01:24:55,070 --> 01:24:59,020
so it's again on the problem like this the minimize

1328
01:24:59,040 --> 01:25:04,300
the cost function and the constraint functions this specific form

1329
01:25:04,350 --> 01:25:07,130
it looks like a polynomial in x

1330
01:25:07,220 --> 01:25:09,180
except that the

1331
01:25:09,230 --> 01:25:11,840
exponent can be any

1332
01:25:11,900 --> 01:25:15,540
the positive or negative

1333
01:25:15,590 --> 01:25:18,310
and i don't have to be integers

1334
01:25:18,350 --> 01:25:23,300
but restrict x to be positive domain of the function of

1335
01:25:23,360 --> 01:25:26,270
and this coefficient from the also

1336
01:25:26,270 --> 01:25:29,080
that's called the posynomial

1337
01:25:29,100 --> 01:25:30,950
in this context

1338
01:25:30,950 --> 01:25:33,510
polynomial in n

1339
01:25:33,530 --> 01:25:35,070
the people

1340
01:25:35,120 --> 01:25:37,560
it cannot describe these

1341
01:25:37,750 --> 01:25:42,520
it looks like a polynomial but the core components that can be any

1342
01:25:43,760 --> 01:25:46,240
and your specific x to be

1343
01:25:46,310 --> 01:25:49,680
and then minimize that function subject to

1344
01:25:49,730 --> 01:25:53,610
posynomial function subject wasn't on the constraints ax it

1345
01:25:53,730 --> 01:25:56,810
it's called the genetic programming

1346
01:25:56,810 --> 01:26:00,030
now that's not a convex function of x

1347
01:26:00,080 --> 01:26:03,120
of what can be any number

1348
01:26:03,170 --> 01:26:05,720
in general functions of this form are not convex

1349
01:26:05,760 --> 01:26:08,930
but there is a simple trick that allows you to convert this into a convex

1350
01:26:10,120 --> 01:26:15,310
and the trick is that instead of using x positive components access variables use their

1351
01:26:15,470 --> 01:26:18,400
blogger variables

1352
01:26:18,420 --> 01:26:24,320
and then the problem transforms into one complex problem if you buy this function

1353
01:26:24,330 --> 01:26:29,480
function i make a change of variables x x with the exponent

1354
01:26:29,530 --> 01:26:31,060
financial of some

1355
01:26:31,230 --> 01:26:33,810
other the variable y make this change of variables

1356
01:26:33,830 --> 01:26:35,420
then you get this

1357
01:26:35,430 --> 01:26:38,260
because the problem

1358
01:26:38,300 --> 01:26:41,170
and you've seen these are convex functions of y

1359
01:26:41,420 --> 01:26:46,970
so log of a sum of exponentials of y

1360
01:26:46,980 --> 01:26:51,710
and then in this form you get a convex problem and can easily minimized this

1361
01:26:51,720 --> 01:26:59,210
this is genetic programming problem solved

1362
01:26:59,620 --> 01:27:03,570
the it's called him i mean there are several theories about how what's called genetic

1363
01:27:03,570 --> 01:27:06,090
programming which means

1364
01:27:06,150 --> 01:27:07,830
one of the theories is that it's

1365
01:27:08,240 --> 01:27:14,230
it comes up often in geometric optimization problems variables which of only in height

1366
01:27:14,230 --> 01:27:19,120
here are examples of loss functions for each of the problem

1367
01:27:19,130 --> 01:27:21,910
really of regression you take the

1368
01:27:21,930 --> 01:27:23,310
least square

1369
01:27:23,310 --> 01:27:27,690
measure which measure how far is my estimate

1370
01:27:27,690 --> 01:27:31,790
of this target from the true target

1371
01:27:31,800 --> 01:27:34,620
in the classification problem

1372
01:27:34,670 --> 01:27:38,250
i i i will have a

1373
01:27:38,270 --> 01:27:40,320
since last not telling me

1374
01:27:40,330 --> 01:27:41,940
it's not to me

1375
01:27:41,950 --> 01:27:45,010
zero if i had the good targets and

1376
01:27:45,020 --> 01:27:47,120
one of the way so i will have

1377
01:27:47,230 --> 01:27:52,190
penality for i didn't find that could be a good target

1378
01:27:55,250 --> 01:27:57,550
in in density estimation

1379
01:27:57,560 --> 01:28:00,960
usually what we do you think is the likelihood

1380
01:28:01,010 --> 01:28:04,120
but since we are in machine learning framework

1381
01:28:04,130 --> 01:28:09,390
and it's not a positive measure we have what we have is the negative measure

1382
01:28:09,520 --> 01:28:13,010
usually we take the negative log likelihood

1383
01:28:13,060 --> 01:28:18,700
because it's easier to compute than likelihood to

1384
01:28:18,710 --> 01:28:19,730
so that

1385
01:28:19,740 --> 01:28:23,520
instead of you you don't want to maximize the

1386
01:28:23,540 --> 01:28:26,610
the likelihood what you want to minimize the

1387
01:28:27,500 --> 01:28:31,550
look like likelihood which is the same person

1388
01:28:31,560 --> 01:28:37,390
so now let's put together finding searching for a good function in a in a

1389
01:28:37,420 --> 01:28:40,930
function space f

1390
01:28:41,920 --> 01:28:44,220
we take the

1391
01:28:44,230 --> 01:28:46,930
the risk of harm

1392
01:28:49,610 --> 01:28:55,720
of our function of a given function is the expected value if over the whole

1393
01:28:55,720 --> 01:28:59,600
domain the whole possible values of my name

1394
01:28:59,690 --> 01:29:02,600
of my my sample

1395
01:29:02,610 --> 01:29:04,720
of my examples

1396
01:29:04,740 --> 01:29:07,020
that will be the expected risk

1397
01:29:07,060 --> 01:29:10,880
and what want is we want to minimize so you have this because it's the

1398
01:29:11,040 --> 01:29:14,570
expected value of the loss function

1399
01:29:14,660 --> 01:29:17,920
over the entire domain

1400
01:29:18,790 --> 01:29:23,660
what i want to minimize this

1401
01:29:23,690 --> 01:29:26,860
so if a could had the

1402
01:29:26,920 --> 01:29:29,810
the function which minimize my laws

1403
01:29:29,850 --> 01:29:34,200
what does that mean over the entire pacific values of my domain

1404
01:29:34,310 --> 01:29:39,560
i will be happy the problem is that i don't know this density probability distribution

1405
01:29:39,600 --> 01:29:42,310
is similar

1406
01:29:42,320 --> 01:29:46,330
so we cannot compute that we can minimize it

1407
01:29:46,350 --> 01:29:47,360
so what we

1408
01:29:47,390 --> 01:29:50,280
do instead is we choose

1409
01:29:50,450 --> 01:29:53,140
to minimize the empirical risk

1410
01:29:53,150 --> 01:29:56,060
so we which is the risk is computed

1411
01:29:56,060 --> 01:29:58,860
only over our training set

1412
01:29:58,880 --> 01:30:01,040
and to take the

1413
01:30:01,050 --> 01:30:03,240
function which minimize

1414
01:30:04,450 --> 01:30:07,960
the risk of on this train set

1415
01:30:08,010 --> 01:30:11,580
OK but how good is the approximation

1416
01:30:11,690 --> 01:30:13,720
it's not exactly the same

1417
01:30:15,790 --> 01:30:19,840
over the whole domain only on a sample of system

1418
01:30:20,850 --> 01:30:22,320
we know

1419
01:30:22,330 --> 01:30:28,710
that is for any function so without being the what you know

1420
01:30:28,720 --> 01:30:33,400
without the need the one that we chosen choose on the training set we know

1421
01:30:34,090 --> 01:30:37,680
the empirical risk

1422
01:30:37,820 --> 01:30:43,630
is there is an unbiased estimate of the expected risk

1423
01:30:45,270 --> 01:30:49,690
sure i told you what that means that the index the

1424
01:30:49,710 --> 01:30:53,690
the expectation of the empirical risk is equal to the risk

1425
01:30:53,720 --> 01:30:55,470
and we know also that

1426
01:30:55,480 --> 01:30:59,450
it's quite consistent estimate that is if if my

1427
01:31:01,310 --> 01:31:06,680
then there it's it's in need goes it's big enough

1428
01:31:06,970 --> 01:31:11,080
i will take the risk would be a good estimate of the empirical risk would

1429
01:31:11,080 --> 01:31:12,720
be a good estimate of the

1430
01:31:13,660 --> 01:31:16,210
expected risk

1431
01:31:25,800 --> 01:31:32,590
the more samples the smaller

1432
01:31:36,220 --> 01:31:38,390
rolls walking

1433
01:31:38,430 --> 01:31:41,070
so was less

1434
01:31:41,080 --> 01:31:45,420
yeah i think you're not taking the integral over to

1435
01:31:47,080 --> 01:31:51,050
you know you're taking it over to you're in fact

1436
01:31:57,290 --> 01:32:00,310
for example the

1437
01:32:01,770 --> 01:32:02,650
this the source of

1438
01:32:04,510 --> 01:32:07,280
we saw the number

1439
01:32:07,400 --> 01:32:10,950
this list is

1440
01:32:10,960 --> 01:32:14,840
in fact this is this is the only related

1441
01:32:14,850 --> 01:32:18,660
two two these factors think again it says something about the

1442
01:32:18,670 --> 01:32:20,150
the fact that

1443
01:32:20,180 --> 01:32:23,340
are our instant factor mean

1444
01:32:23,700 --> 01:32:29,440
and the mean it's it's an unbiased and consistent estimate even i don't need to

1445
01:32:29,450 --> 01:32:31,640
know in fact that p

1446
01:32:31,650 --> 01:32:33,070
he said

1447
01:32:33,130 --> 01:32:35,220
something related to to the rule

1448
01:32:36,720 --> 01:32:41,160
i don't know the proof that can have it in mind but

1449
01:32:51,580 --> 01:32:53,140
even if we

1450
01:32:57,690 --> 01:33:06,380
it's a it's a it's related to the weak law of probabilities

1451
01:33:07,090 --> 01:33:08,110
so but

1452
01:33:08,130 --> 01:33:11,820
in fact we're not interested by this

1453
01:33:11,880 --> 01:33:14,550
this value what we were interested in city

1454
01:33:14,560 --> 01:33:19,850
this value so the value of the empirical risk for the

1455
01:33:19,860 --> 01:33:22,020
the optimum function

1456
01:33:22,480 --> 01:33:28,380
i cannot do that anymore because this time i have two dependencies on

1457
01:33:29,150 --> 01:33:30,690
so i i

1458
01:33:30,700 --> 01:33:32,810
it's not that easy

1459
01:33:35,020 --> 01:33:37,520
we want to know relationship between

1460
01:33:37,530 --> 01:33:41,220
the empirical risk of my optimum function on my

1461
01:33:41,230 --> 01:33:42,970
training set

1462
01:33:42,980 --> 01:33:49,080
what will be the expected risk of this particular function

1463
01:33:49,090 --> 01:33:51,700
if if i could take the whole the in

1464
01:33:51,710 --> 01:33:54,340
and it is it

1465
01:33:54,350 --> 01:33:57,610
close to the to the optimum risk or not

1466
01:33:57,630 --> 01:34:02,060
OK so that's out the question we tried to answer

1467
01:34:02,920 --> 01:34:05,190
so for example we want to know if

1468
01:34:05,200 --> 01:34:09,060
taking this particular functions

1469
01:34:09,060 --> 01:34:11,150
cold cold is still

1470
01:34:11,170 --> 01:34:14,250
eight times less likely than hot cold

1471
01:34:14,270 --> 01:34:19,290
so have a lot of cold cold then actually still be unlikely

1472
01:34:19,290 --> 01:34:22,960
so let's see

1473
01:34:23,020 --> 01:34:24,690
what the correct

1474
01:34:24,710 --> 01:34:27,750
with the model actually believes i'm going to show you another graph that we have

1475
01:34:27,800 --> 01:34:31,360
looked at before unfortunately the lines are kind of on top of each other here

1476
01:34:31,730 --> 01:34:35,560
but there are four kinds of lines and every day starting with a two

1477
01:34:35,560 --> 01:34:38,690
we have about the previous day and that day

1478
01:34:38,710 --> 01:34:42,340
so this pink here says it's pretty likely

1479
01:34:42,340 --> 01:34:43,590
that you know

1480
01:34:43,610 --> 01:34:47,840
a little less than half probability to be a hot day that was preceded by

1481
01:34:47,840 --> 01:34:49,190
a hot day

1482
01:34:49,210 --> 01:34:53,230
it's also pretty likely to be a cold day that was preceded by cold if

1483
01:34:53,230 --> 01:34:55,090
you look very carefully

1484
01:34:55,150 --> 01:34:59,340
at this you can see the this problem star behind

1485
01:34:59,400 --> 01:35:01,650
these two things are unlikely

1486
01:35:01,670 --> 01:35:03,750
a cold day followed by a hot day

1487
01:35:03,770 --> 01:35:08,750
try a hot day pursued by called a recall pursued by heart those are unlikely

1488
01:35:08,750 --> 01:35:09,940
these are like

1489
01:35:09,940 --> 01:35:13,040
and that's what you would expect from this picture

1490
01:35:13,060 --> 01:35:17,290
the higher parts are still more likely according to this model so even though we

1491
01:35:17,290 --> 01:35:19,840
can't tell what the weather is we still think of

1492
01:35:19,860 --> 01:35:21,590
it is more likely than hot called

1493
01:35:21,610 --> 01:35:24,690
and i'll show you how we reconstruct this minute

1494
01:35:24,710 --> 01:35:28,400
so let's go now to

1495
01:35:28,420 --> 01:35:31,540
two an alternative

1496
01:35:31,570 --> 01:35:38,070
which is what if we have an inertia

1497
01:35:38,090 --> 01:35:42,190
OK well it's still the case that this is totally flat because

1498
01:35:42,210 --> 01:35:47,250
this is the ice cream probabilities are still correlated we still have that exactly the

1499
01:35:47,250 --> 01:35:50,270
same on cold isn't

1500
01:35:50,290 --> 01:35:54,000
so i this is this of working

1501
01:35:54,020 --> 01:36:00,880
the same and called isn't this but if we look at the second order probabilities

1502
01:36:01,440 --> 01:36:05,420
the by gram probabilities will see now that now because the model start out by

1503
01:36:05,420 --> 01:36:07,630
assuming and inertia

1504
01:36:07,650 --> 01:36:10,190
these have switched

1505
01:36:10,230 --> 01:36:11,860
the yellow and light blue

1506
01:36:11,880 --> 01:36:14,940
are the problem ones hot to cold and cold heart because that's where the model

1507
01:36:14,940 --> 01:36:18,340
things the model things that should put more probability

1508
01:36:18,360 --> 01:36:22,940
on the patterns that alternate instead of the patterns that keeps the same way

1509
01:36:24,090 --> 01:36:27,770
the point is that this graph is exactly the same both times

1510
01:36:27,820 --> 01:36:29,420
but this gradually changed

1511
01:36:29,420 --> 01:36:34,190
therefore we can't get this second order probabilities just by looking at the first order

1512
01:36:35,800 --> 01:36:38,400
we're going to have to do something a little bit different

1513
01:36:38,400 --> 01:36:40,980
so let's go back to this picture

1514
01:36:40,980 --> 01:36:46,250
well we're interested in

1515
01:36:46,320 --> 01:36:50,090
what we're interested in is the person if we look at

1516
01:36:50,110 --> 01:36:54,900
all of the two to the thirty third has a quarter go through this article

1517
01:36:54,920 --> 01:37:00,540
about through that article to go through that article to go through that i

1518
01:37:01,770 --> 01:37:05,840
most if most of the probability goes on has to go through that are

1519
01:37:05,840 --> 01:37:10,150
then the day three is probably called a pursued by called

1520
01:37:10,210 --> 01:37:11,560
so if we look at the

1521
01:37:11,570 --> 01:37:17,170
probability paths going through particular are not just a particular state then we can see

1522
01:37:18,170 --> 01:37:22,130
we can see which transitions are likely

1523
01:37:22,400 --> 01:37:24,460
so how do we do that

1524
01:37:24,480 --> 01:37:27,290
well it's going to be similar to what we had before

1525
01:37:27,300 --> 01:37:42,340
so we've got a lot of paths coming into this called stay here a lot

1526
01:37:42,340 --> 01:37:45,000
of paths coming out of that called state here we have one are here which

1527
01:37:45,000 --> 01:37:48,500
has some costs

1528
01:37:48,520 --> 01:37:53,940
so what's the total probability i let's let's call that cost

1529
01:37:54,020 --> 01:37:57,460
can number x no we already have x we already have p

1530
01:37:57,650 --> 01:38:00,650
which is a

1531
01:38:00,670 --> 01:38:08,630
well actually we

1532
01:38:08,710 --> 01:38:12,520
so what's the total probability of all paths going through this art

1533
01:38:12,520 --> 01:38:16,570
well we have x times a times speed plus six times a times q

1534
01:38:16,590 --> 01:38:18,690
plus x times a times are

1535
01:38:18,710 --> 01:38:21,380
right lots of pads coming in lots paths going out

1536
01:38:21,540 --> 01:38:24,590
if we multiply these individually and some of them are going have to do thirty

1537
01:38:24,590 --> 01:38:28,610
one schumann's because there's two to thirty one paths going through this article a quarter

1538
01:38:28,610 --> 01:38:30,190
of all of them

1539
01:38:31,750 --> 01:38:34,840
but fortunately we have are often probability

1540
01:38:35,040 --> 01:38:37,690
so we can say it's just alpha of the state

1541
01:38:37,710 --> 01:38:40,190
times a times span of that state

1542
01:38:40,260 --> 01:38:42,730
the total probability of everything coming in

1543
01:38:42,730 --> 01:38:47,820
times this are times total probability of everything going out

1544
01:38:47,900 --> 01:38:53,690
so we actually do this on the spreadsheet

1545
01:38:53,690 --> 01:38:58,360
so here

1546
01:38:58,380 --> 01:39:00,440
here we have probability of

1547
01:39:00,440 --> 01:39:03,590
o site

1548
01:39:03,590 --> 01:39:11,650
go back here and change these back to original numbers

1549
01:39:11,670 --> 01:39:21,090
OK that's our original reconstructions so now you see the probability of a colder call

1550
01:39:21,110 --> 01:39:24,980
hot to cold cold heart and heart to heart the particular day

1551
01:39:24,980 --> 01:39:27,960
some to one so this is how the two to the

1552
01:39:29,460 --> 01:39:34,340
these four cells are how the two to the thirty third have got divided up

1553
01:39:34,690 --> 01:39:37,610
among the four arcs that could happen

1554
01:39:39,000 --> 01:39:40,650
twelve and thirteen

1555
01:39:40,880 --> 01:39:45,540
that's those numbers and that's what we're what planning in the graph

1556
01:39:45,560 --> 01:39:49,090
and i will click around the computation but it's basically just doing this with the

1557
01:39:49,090 --> 01:39:51,420
office embeds that we've already computed

1558
01:39:51,630 --> 01:39:54,090
so this is just acute dynamic programming track

1559
01:39:55,290 --> 01:39:59,040
being able to avoid summing over a trillion pounds

1560
01:39:59,040 --> 01:40:03,350
can we still do it for infinitely many functions and that's what this sometimes ideas

1561
01:40:03,350 --> 01:40:07,580
about the time we make is that made this problem solvable

1562
01:40:07,600 --> 01:40:10,100
the called symmetrisation

1563
01:40:10,140 --> 01:40:15,620
and is the introduction of capacity concepts the basic idea is that even if there

1564
01:40:15,620 --> 01:40:17,230
are infinitely many

1565
01:40:18,040 --> 01:40:19,520
if the capacity is

1566
01:40:19,520 --> 01:40:21,850
small in some sense then

1567
01:40:21,870 --> 01:40:25,770
on the finite sample of observations they will effectively behave as if there are only

1568
01:40:25,770 --> 01:40:28,100
finitely many or are they

1569
01:40:28,120 --> 01:40:33,200
actually a sub exponentially many but you see a little more about this later

1570
01:40:33,210 --> 01:40:36,830
so let's look at two functions

1571
01:40:39,370 --> 01:40:42,020
we have a function class containing two functions

1572
01:40:42,700 --> 01:40:49,410
we will provide right this thing here so remember this is what we are interested

1573
01:40:50,140 --> 01:40:52,640
the maximum of these two functions

1574
01:40:52,700 --> 01:40:54,270
this deviation

1575
01:40:55,080 --> 01:40:57,080
so when you

1576
01:40:57,100 --> 01:41:01,140
when is this

1577
01:41:01,160 --> 01:41:03,230
this equality here true

1578
01:41:03,270 --> 01:41:07,970
we could say we write s two events one is the

1579
01:41:07,980 --> 01:41:12,640
the event that the risks so the test around the training error for functions f

1580
01:41:13,810 --> 01:41:15,770
differ by one than epsilon

1581
01:41:15,790 --> 01:41:18,870
so i mean it's a i'm interested in whether

1582
01:41:19,290 --> 01:41:23,520
one of the two functions has the risk of the east side of the the

1583
01:41:23,640 --> 01:41:27,250
worst function has the risk of at least two or more than epsilon

1584
01:41:27,290 --> 01:41:28,750
and this will

1585
01:41:28,770 --> 01:41:30,890
with this will take place if

1586
01:41:30,950 --> 01:41:34,250
the first function has the risk more than epsilon all the second one has risk

1587
01:41:34,250 --> 01:41:38,160
more than epsilon this is not an excuse for the close to both the

1588
01:41:38,180 --> 01:41:39,710
the that

1589
01:41:39,730 --> 01:41:42,660
so c one is the event

1590
01:41:43,520 --> 01:41:45,200
the first function

1591
01:41:45,250 --> 01:41:50,850
we draw such training set such that the risk is the former site c two

1592
01:41:50,850 --> 01:41:52,750
is implicit functions

1593
01:41:52,770 --> 01:41:56,870
so that we can rewrite this quantity as the probability of the union of these

1594
01:41:56,870 --> 01:41:59,970
two events

1595
01:41:59,980 --> 01:42:04,700
we can rewrite this again some of these two probabilities minus the probability that both

1596
01:42:04,700 --> 01:42:06,230
take place

1597
01:42:06,250 --> 01:42:10,870
probabilities on the negative so if we drop this we get upper bound

1598
01:42:10,870 --> 01:42:13,290
which looks like this

1599
01:42:13,290 --> 01:42:20,370
so maybe you haven't personally i don't know how much mathematics computer scientists typically studies

1600
01:42:20,390 --> 01:42:25,350
maybe i can i can quickly ask how many of you are the cognitive scientists

1601
01:42:25,350 --> 01:42:28,580
all psychologists

1602
01:42:28,600 --> 01:42:29,870
OK so that's

1603
01:42:29,910 --> 01:42:32,350
i would say twenty five percent

1604
01:42:32,680 --> 01:42:34,430
how many of you are

1605
01:42:34,450 --> 01:42:39,970
mathematicians and physicists or mathematically minded computer scientists

1606
01:42:39,980 --> 01:42:43,830
so that's that's more so i think it's probably for you it would be easy

1607
01:42:43,850 --> 01:42:47,890
for the other ones i'm trying to also give some intuition so

1608
01:42:47,910 --> 01:42:52,410
so roughly speaking the short version is that we're interested in with

1609
01:42:52,430 --> 01:42:57,700
one of the two functions misleads us about the difference between training and test error

1610
01:42:57,700 --> 01:43:01,160
minimisation credited this will happen is upper bounded by

1611
01:43:01,200 --> 01:43:05,370
the probability that the first one is misleading

1612
01:43:05,410 --> 01:43:09,390
second one misleading us and is just about because you could be that both are

1613
01:43:09,390 --> 01:43:13,580
misleading as so i could be doing some double counting here

1614
01:43:13,970 --> 01:43:16,580
OK so if we have this

1615
01:43:16,600 --> 01:43:19,540
now we at this point these quantities now

1616
01:43:22,120 --> 01:43:27,560
so i can use the channel of these both these we get

1617
01:43:27,580 --> 01:43:31,200
the identical to the both of them so i will get the same results before

1618
01:43:31,200 --> 01:43:35,020
which was this one only with a factor of two here

1619
01:43:35,020 --> 01:43:39,080
the factor of two i don't i don't know if i have to because this

1620
01:43:39,080 --> 01:43:43,850
thing goes down exponentially with the number of salvation so this goes down so fast

1621
01:43:44,410 --> 01:43:45,810
it is about two

1622
01:43:45,830 --> 01:43:48,830
it doesn't cost us anything

1623
01:43:48,850 --> 01:43:53,600
now let's look at functions and i think you can imagine what's going to happen

1624
01:43:53,620 --> 01:43:55,620
even if i don't go the details

1625
01:43:55,640 --> 01:44:01,000
what we will get if we get an an extra factor n

1626
01:44:01,060 --> 01:44:02,980
so we came up with this

1627
01:44:02,980 --> 01:44:07,160
it's going to be like before this time some of n

1628
01:44:07,270 --> 01:44:09,250
quantities each to them

1629
01:44:09,270 --> 01:44:14,040
we can do with china we get an extra factor n

1630
01:44:15,730 --> 01:44:19,980
so again to make this a little bit intuitive

1631
01:44:19,980 --> 01:44:28,140
suppose you're you trained learning machine with your favourite method which is by said

1632
01:44:28,180 --> 01:44:30,730
suppose you're doing something that none of us would ever do

1633
01:44:30,910 --> 01:44:35,600
training several learning machines very all in the test set after was to take the

1634
01:44:35,600 --> 01:44:37,660
one that does based on the test set

1635
01:44:37,680 --> 01:44:43,120
you may be right in this paper about it so all how can that be

1636
01:44:43,140 --> 01:44:46,890
well actually if you take any functions and you do that

1637
01:44:46,910 --> 01:44:49,850
then again this plant sorry

1638
01:44:49,870 --> 01:44:53,310
it's not written down here but

1639
01:44:55,430 --> 01:44:58,640
so if you take two functions and choose the one that does that

1640
01:44:58,660 --> 01:45:00,540
then this volunteer

1641
01:45:00,560 --> 01:45:03,100
tells you how far you can be

1642
01:45:03,120 --> 01:45:07,230
of course you estimate of the test error

1643
01:45:07,250 --> 01:45:13,040
function that you're reporting units papers can be from the true test error

1644
01:45:13,060 --> 01:45:16,890
so it can be can be a waiver factor of two more

1645
01:45:16,910 --> 01:45:21,290
actually it's not that bad we have the largest set

1646
01:45:21,310 --> 01:45:25,450
again this factor of two is really pretty insignificant compared to pass this goes to

1647
01:45:25,450 --> 01:45:32,340
the first step that you do computing the CSP projection is to make the whitening

1648
01:45:32,380 --> 01:45:34,370
of this over all

1649
01:45:34,380 --> 01:45:36,480
covariance OK

1650
01:45:37,120 --> 01:45:38,760
it means that you

1651
01:45:39,910 --> 01:45:41,960
the data like that

1652
01:45:41,980 --> 01:45:43,180
in this case

1653
01:45:43,220 --> 01:45:45,270
the core covariance

1654
01:45:45,280 --> 01:45:47,180
this is

1655
01:45:47,200 --> 01:45:50,580
the uniform

1656
01:45:52,620 --> 01:45:53,900
OK so

1657
01:45:53,910 --> 01:45:55,120
so you

1658
01:45:55,180 --> 01:45:56,950
use of this problem

1659
01:45:56,960 --> 01:45:59,420
the projection matrix

1660
01:46:01,120 --> 01:46:04,840
OK so so the interesting thing is that here the

1661
01:46:04,880 --> 01:46:09,170
the class axis of semantic perpendicular now

1662
01:46:09,180 --> 01:46:10,680
OK and now

1663
01:46:10,720 --> 01:46:17,470
this is just one further rotation that because is required

1664
01:46:17,490 --> 01:46:19,950
and so

1665
01:46:19,960 --> 01:46:21,550
which is to take on

1666
01:46:21,880 --> 01:46:22,890
so we

1667
01:46:22,950 --> 01:46:25,840
intuitively speaking we would like to

1668
01:46:26,510 --> 01:46:28,820
a number of projections where

1669
01:46:29,320 --> 01:46:36,330
along one projection the one class has a very large variance whereas the other one

1670
01:46:36,330 --> 01:46:37,940
has small variance

1671
01:46:38,040 --> 01:46:40,430
and next projection has

1672
01:46:40,440 --> 01:46:44,560
this class have large variance this one small very

1673
01:46:44,600 --> 01:46:50,400
and if you use all these these type of projections they called the CSP projection

1674
01:46:52,720 --> 01:46:57,100
use them together and you can discriminate very well

1675
01:46:57,140 --> 01:47:00,600
use them as input for the fire

1676
01:47:00,620 --> 01:47:01,560
and so

1677
01:47:01,620 --> 01:47:05,830
the rotation matrix basically is

1678
01:47:06,230 --> 01:47:07,840
you know

1679
01:47:07,870 --> 01:47:10,650
by computing are fulfilled

1680
01:47:17,100 --> 01:47:21,450
this was the two class idea so how do we do this for multi for

1681
01:47:21,450 --> 01:47:29,190
many classes and one obvious way to do it which is the SVM way

1682
01:47:29,210 --> 01:47:32,330
one against the rest

1683
01:47:33,950 --> 01:47:38,440
but of course also known as we ends i think we learned that there's that

1684
01:47:38,440 --> 01:47:40,700
smarter ways to do

1685
01:47:40,710 --> 01:47:42,310
and one against the rest

1686
01:47:42,430 --> 01:47:44,300
and so

1687
01:47:44,320 --> 01:47:45,810
what we

1688
01:47:47,370 --> 01:47:52,710
he did on proposed this is that we

1689
01:47:52,730 --> 01:47:59,180
we bayes we compute the multiclass CSP based on the simultaneous diagonalisation

1690
01:47:59,230 --> 01:48:01,620
of covariance matrices

1691
01:48:02,150 --> 01:48:04,720
so after whitening he would

1692
01:48:05,190 --> 01:48:07,440
simultaneous diagonalisation

1693
01:48:07,450 --> 01:48:10,770
many corporate

1694
01:48:13,520 --> 01:48:15,710
and in fact

1695
01:48:24,390 --> 01:48:28,910
so here's

1696
01:48:28,930 --> 01:48:30,080
so you would have

1697
01:48:30,110 --> 01:48:33,170
i like to find these are matrix

1698
01:48:33,290 --> 01:48:37,930
such that all these covariance matrices class covariance matrices

1699
01:48:38,330 --> 01:48:41,760
you know we would get this

1700
01:48:41,800 --> 01:48:46,630
to be the and the sum of all these is is unity

1701
01:48:46,680 --> 01:48:51,060
and in the last slide just

1702
01:48:52,460 --> 01:48:55,090
so in fact

1703
01:48:55,120 --> 01:48:57,980
here i if i put index d i here

1704
01:48:58,010 --> 01:48:59,690
and if i

1705
01:48:59,760 --> 01:49:01,380
right this is DJ

1706
01:49:01,400 --> 01:49:09,330
then the i plus the days equal to one

1707
01:49:09,330 --> 01:49:12,910
and the first is that often like these are extremely but much bigger than a

1708
01:49:12,910 --> 01:49:14,480
single model for

1709
01:49:15,540 --> 01:49:20,250
one approach to solving libraries would be to apply the same approach here so when

1710
01:49:20,250 --> 01:49:24,630
you have something that is defined standard c library like live or the math library

1711
01:49:24,630 --> 01:49:29,710
likely then you just include the entire text of the library inside the building program

1712
01:49:29,710 --> 01:49:32,060
but that just makes things extremely bloated i mean

1713
01:49:32,090 --> 01:49:35,230
think about if you just want to use print of program like we had in

1714
01:49:35,230 --> 01:49:39,430
this example and you have to include the entire c library which is megabytes long

1715
01:49:39,430 --> 01:49:42,070
that seems like not a good way of of

1716
01:49:42,120 --> 01:49:43,860
doing the linking

1717
01:49:43,870 --> 01:49:46,400
so what do with resolution libraries is

1718
01:49:46,410 --> 01:49:49,350
it's actually the idea is to only include

1719
01:49:49,360 --> 01:49:54,360
on the door to all files in the library in which undefined symbols that were

1720
01:49:54,360 --> 01:49:55,840
previously encountered

1721
01:49:55,890 --> 01:49:57,940
i define

1722
01:49:59,180 --> 01:50:01,790
if you think a little bit about what i said that's one reason why it's

1723
01:50:01,790 --> 01:50:03,040
usually good idea

1724
01:50:03,060 --> 01:50:04,240
when you

1725
01:50:04,280 --> 01:50:05,790
you know do GCC

1726
01:50:05,810 --> 01:50:11,180
on and use the linker use the to specify the libraries at the end

1727
01:50:11,290 --> 01:50:13,220
because what we're going to do is

1728
01:50:13,270 --> 01:50:16,020
we're take all the waterfalls and then they are going to be things like das

1729
01:50:16,020 --> 01:50:20,750
lm mean the standard c libraries usually by default already included on

1730
01:50:20,800 --> 01:50:22,010
on this on this

1731
01:50:22,010 --> 01:50:23,010
command line

1732
01:50:23,030 --> 01:50:26,600
but if you have functions like square root and so on here what we're going

1733
01:50:26,610 --> 01:50:28,830
to do is we're going to build up what the link is going to do

1734
01:50:28,830 --> 01:50:32,340
is going to build up a set of undefined symbols until it gets to the

1735
01:50:33,250 --> 01:50:36,270
and this going to be undefined symbols remain many if use the square root program

1736
01:50:36,500 --> 01:50:40,450
a square root function and you can write your own square root function

1737
01:50:40,470 --> 01:50:42,310
it's in the math library then

1738
01:50:42,330 --> 01:50:45,040
you might ask for the math library to be included and you want to pull

1739
01:50:45,040 --> 01:50:48,710
the definition of square from the math library

1740
01:50:48,710 --> 01:50:53,530
the way in which the linker knows to pull the right down five is

1741
01:50:54,370 --> 01:50:58,900
the math library is going to have information that says we five contains all what

1742
01:51:01,100 --> 01:51:06,890
any time we see an undefined symbol at this stage regardless candice archives looking for

1743
01:51:06,890 --> 01:51:10,160
the symbol that's been fire in particular looking in this example for the square root

1744
01:51:11,870 --> 01:51:15,870
and when we find the square root symbol somewhere inside in some doubt o five

1745
01:51:16,110 --> 01:51:19,020
we're going to take that dodo file and not the rest of the library and

1746
01:51:20,100 --> 01:51:24,780
use this algorithm that we did so take that don't phylogeny alone and push that

1747
01:51:24,780 --> 01:51:26,050
as input to

1748
01:51:27,500 --> 01:51:29,970
and that's the way in which we're going to build up

1749
01:51:30,010 --> 01:51:31,060
so that's why

1750
01:51:31,080 --> 01:51:34,590
least in this particular implementation of out of images of the lincoln which is very

1751
01:51:35,700 --> 01:51:40,830
if you put the LM will confront you're problem because if s and otto's the

1752
01:51:40,830 --> 01:51:42,870
father actually use quite

1753
01:51:42,890 --> 01:51:46,330
in the math library was included well in front then ignore

1754
01:51:46,340 --> 01:51:50,390
square would not yet have been part of the undefined set of symbols on until

1755
01:51:52,640 --> 01:51:55,460
and you know there are other ways to deal with that you could have because

1756
01:51:55,460 --> 01:51:58,980
they're going more passes presumably can deal with this problem but this is just an

1757
01:51:58,980 --> 01:52:02,490
example of how the next does this and this just work

1758
01:52:04,050 --> 01:52:08,580
OK so this idea of linking and on you know some resolution and relocation has

1759
01:52:08,580 --> 01:52:10,550
been you know people worked on this

1760
01:52:10,570 --> 01:52:14,050
for a very long time in almost every software system used in fact if you

1761
01:52:14,050 --> 01:52:18,050
use latex to build you're of you know design papers and so on on it

1762
01:52:18,050 --> 01:52:21,490
doesn't version of simple as it does similar solutions well because there's all sorts of

1763
01:52:21,490 --> 01:52:26,060
cross references that later and uses essentially the same kinds of algorithms go over the

1764
01:52:26,060 --> 01:52:32,180
files on and all the documents in multiple passes the results pretty general algorithm

1765
01:52:32,200 --> 01:52:37,020
that we describe here of building these different sets to ultimately figure out there answer

1766
01:52:37,020 --> 01:52:42,770
find references remaining

1767
01:52:42,820 --> 01:52:56,280
so one step back for a minute and generalize on

1768
01:52:56,280 --> 01:53:01,290
the different approaches that we've seen so far for doing on simple resolution

1769
01:53:01,290 --> 01:53:05,090
and so no regret algorithms can potentially do actually my if you view this as

1770
01:53:05,100 --> 01:53:09,020
a game and we'll talk about you know this is a zero sum game no

1771
01:53:09,020 --> 01:53:14,030
relevance can potentially do much better than playing minimax optimal strategy which in this case

1772
01:53:14,040 --> 01:53:17,050
be fifty fifty depending on how the world is

1773
01:53:17,120 --> 01:53:21,400
OK but they'll never do much worse so well defined minimax optimal

1774
01:53:21,420 --> 01:53:23,830
minimax optimality later we get to it

1775
01:53:23,980 --> 01:53:28,230
in fact the existence of these algorithms gives an immediate proof of the minimax theorem

1776
01:53:28,240 --> 01:53:31,950
will see why don't talk about that later

1777
01:53:37,140 --> 01:53:41,250
i just one thing to mention is that our view of the world of life

1778
01:53:41,320 --> 01:53:46,300
they call it is it's some unknown sequence it's in the future you know there's

1779
01:53:46,300 --> 01:53:48,680
going to be some sort of traffic patterns the future we just don't know what

1780
01:53:48,680 --> 01:53:49,680
it is

1781
01:53:49,700 --> 01:53:54,140
and our goal is to do well no matter what their sequences

1782
01:53:54,170 --> 01:53:56,410
in expectation over

1783
01:53:56,460 --> 01:54:00,380
internal randomisation our algorithm can use

1784
01:54:00,380 --> 01:54:03,030
so in particular in this view

1785
01:54:03,050 --> 01:54:05,800
our algorithms are going to have to be randomized

1786
01:54:05,810 --> 01:54:08,390
so what do i mean by that what i mean is if you the deterministic

1787
01:54:09,870 --> 01:54:13,070
then there always exists a sequence in which every day

1788
01:54:13,080 --> 01:54:17,370
you're playing the bad guy you see that that's true to fewer terms it's ready

1789
01:54:17,370 --> 01:54:20,830
to go what you can do the first day of them what about the world

1790
01:54:21,960 --> 01:54:25,260
this way and then if given that the next thing you would have done is

1791
01:54:25,260 --> 01:54:28,600
go up with them whether the world the next is like this if you give

1792
01:54:28,600 --> 01:54:33,090
me a deterministic strategy i can always reverse engineer a world in which it paying

1793
01:54:33,090 --> 01:54:34,930
one every single time

1794
01:54:34,990 --> 01:54:38,700
and in hindsight one of the two choices must have been

1795
01:54:38,720 --> 01:54:42,800
you know paid only half

1796
01:54:42,850 --> 01:54:44,460
or refer about

1797
01:54:45,490 --> 01:54:47,750
the only hope we have against

1798
01:54:47,760 --> 01:54:55,190
they can of be an adversarial view of life is have to be random

1799
01:54:55,210 --> 01:55:01,040
OK so kind of you in the game is of our algorithm against the

1800
01:55:01,050 --> 01:55:03,700
now in practice you know of the world not really out to get you your

1801
01:55:05,340 --> 01:55:07,130
for analysis

1802
01:55:07,440 --> 01:55:12,910
OK so it's a little bit an abridged history and developments of this kind of

1803
01:55:12,910 --> 01:55:18,660
stuff goes back quite a ways so hunan in nineteen fifty seven building on the

1804
01:55:18,670 --> 01:55:23,890
work of blackwell the earlier in nineteen fifty six gave an algorithm with actually actually

1805
01:55:23,890 --> 01:55:25,490
solves this problem

1806
01:55:25,500 --> 01:55:27,620
and it has attracted

1807
01:55:27,630 --> 01:55:30,900
o and was the number of choices we had he was amount of time

1808
01:55:30,930 --> 01:55:33,730
it's where over t so as time goes

1809
01:55:33,750 --> 01:55:36,430
because of this is dropping down to zero

1810
01:55:36,510 --> 01:55:38,150
exactly what want

1811
01:55:38,200 --> 01:55:39,760
if we just kind of re

1812
01:55:39,800 --> 01:55:42,450
order this in a way which i i like if you have

1813
01:55:42,470 --> 01:55:44,730
you know how long do we have to play

1814
01:55:44,820 --> 01:55:50,040
to get our average regret down to some epsilon so down to

1815
01:55:50,090 --> 01:55:51,440
o point

1816
01:55:51,460 --> 01:55:53,180
one hour

1817
01:55:53,200 --> 01:55:56,920
so there you are then just setting that that's on

1818
01:55:58,150 --> 01:56:01,130
solving forty you get

1819
01:56:01,130 --> 01:56:03,670
and over at once

1820
01:56:03,700 --> 01:56:07,080
so that's how many times you have to play in order to get your average

1821
01:56:07,080 --> 01:56:09,110
regret bound

1822
01:56:09,110 --> 01:56:15,110
the time number time to take how how quickly or converge

1823
01:56:15,250 --> 01:56:20,360
and this actually is optimal in terms of its dependence on to europe

1824
01:56:21,030 --> 01:56:25,750
you can't be scored one of security and game theory view the number of rows

1825
01:56:25,750 --> 01:56:30,850
number choices constant not so important number of times that so pretty much the case

1826
01:56:30,850 --> 01:56:34,340
ninety seven people can stop thinking

1827
01:56:34,360 --> 01:56:40,890
o before i get to that has been worked since then so let me get

1828
01:56:40,890 --> 01:56:43,940
the white how came back in machine learning let me just kind of this a

1829
01:56:43,940 --> 01:56:45,200
little bit about why

1830
01:56:45,210 --> 01:56:47,230
this is the optimal

1831
01:56:47,240 --> 01:56:49,470
dependence you could hope to get in terms of the

1832
01:56:49,510 --> 01:56:51,080
in terms dependence on t

1833
01:56:51,140 --> 01:56:55,250
so so think about the following setting let's go back to this picture we had

1834
01:56:55,390 --> 01:57:00,140
two out and two possible worlds and imagine that the way life is his life

1835
01:57:00,140 --> 01:57:01,700
flips a coin

1836
01:57:01,720 --> 01:57:05,820
that every day a fifty fifty chance this is the expensive one is the chief

1837
01:57:05,820 --> 01:57:09,820
one four fifty fifty chance this is the key point is expensive

1838
01:57:09,820 --> 01:57:13,440
so life at court then it doesn't matter what the algorithm does

1839
01:57:13,460 --> 01:57:16,820
to see the world for a fair coin each day to determine which type of

1840
01:57:16,830 --> 01:57:18,670
the it's going to be

1841
01:57:18,680 --> 01:57:22,590
well whenever year algorithm does there's a fifty fifty chance you have to pay

1842
01:57:23,690 --> 01:57:30,160
what do we do something world record so whatever your algorithm in expectation pays one-half

1843
01:57:30,160 --> 01:57:34,390
everyday whatever you choose to do so in a few days you're expected cost

1844
01:57:34,430 --> 01:57:35,700
nothing you can do that

1845
01:57:35,710 --> 01:57:37,960
on the other hand in hindsight

1846
01:57:38,070 --> 01:57:40,920
the best trout in hindsight is

1847
01:57:40,940 --> 01:57:45,480
if i were to flip a coin t times because had this entails is that

1848
01:57:45,480 --> 01:57:49,820
if the expected value of the minimum of the number of heads number tails because

1849
01:57:49,820 --> 01:57:53,460
i for the pointy time you know i expect over two had expected which details

1850
01:57:53,680 --> 01:57:58,080
but the expected minimum because it's going to be this variation expected minimum ends up

1851
01:57:58,080 --> 01:57:59,860
being killed over two minus

1852
01:58:02,390 --> 01:58:05,830
requisitioned for twenty times you know it's going to be this binomial like this in

1853
01:58:05,880 --> 01:58:08,970
binomial kind of first order approximation it's like

1854
01:58:09,190 --> 01:58:11,900
here two plus and minus square kind of sort

1855
01:58:13,220 --> 01:58:15,200
right so you can be

1856
01:58:15,250 --> 01:58:16,820
if you if you flip a coin

1857
01:58:16,830 --> 01:58:21,380
he target year two had customized as per the and expected that the minimum is

1858
01:58:21,380 --> 01:58:22,410
going to be

1859
01:58:22,470 --> 01:58:24,050
that's gritty

1860
01:58:24,060 --> 01:58:25,380
after puberty

1861
01:58:25,420 --> 01:58:28,880
and so you divide by t per day

1862
01:58:28,880 --> 01:58:32,180
if the world axis way there's no way you can help her data is going

1863
01:58:32,180 --> 01:58:35,210
to be one of the things we can get that

1864
01:58:35,340 --> 01:58:37,580
so OK so that's just explain why

