1
00:00:00,000 --> 00:00:00,970
a prior distribution

2
00:00:01,890 --> 00:00:03,200
the likelihood and the posterior

3
00:00:03,770 --> 00:00:05,080
and and these are simply

4
00:00:08,890 --> 00:00:10,560
defined as conditional probabilities

5
00:00:11,020 --> 00:00:12,850
and so we have data random variable

6
00:00:13,480 --> 00:00:13,890
we have

7
00:00:14,720 --> 00:00:19,040
a parameter random variable and then we have the distribution of this of this parameter

8
00:00:19,040 --> 00:00:22,970
random variable we have the conditional distribution of the data given the parameter the condition

9
00:00:22,970 --> 00:00:24,470
is a fusion of the primary given the data

10
00:00:25,270 --> 00:00:26,770
these are the components of the basic model

11
00:00:27,180 --> 00:00:28,750
and so they are defined has

12
00:00:29,410 --> 00:00:31,850
has the conditional probabilities of these quantities

13
00:00:32,850 --> 00:00:34,580
now what the bayes equation says is

14
00:00:35,120 --> 00:00:39,660
under certain conditions you can relate these quantities together in this nice way

15
00:00:40,970 --> 00:00:42,270
that we all know is that that equation

16
00:00:43,700 --> 00:00:46,430
but they are not defined in these terms is just abstract definitions

17
00:00:48,290 --> 00:00:51,560
so on the outside that means we can define this resulted in the creation on

18
00:00:51,560 --> 00:00:53,660
the downside means abstract means we can't handle

19
00:00:54,100 --> 00:00:55,270
after find some way to handle

20
00:00:56,950 --> 00:00:58,140
now the basic theorem says

21
00:01:02,350 --> 00:01:03,180
you know usually the base

22
00:01:04,720 --> 00:01:08,040
the basic creation is written as a as a racial right to get the posterior

23
00:01:08,040 --> 00:01:10,220
on one side the posterior density on one side

24
00:01:10,850 --> 00:01:12,060
and then on the other side you have

25
00:01:12,540 --> 00:01:13,410
the prior density

26
00:01:13,930 --> 00:01:16,310
times like density divided by the evidence density

27
00:01:16,660 --> 00:01:17,330
right that that's

28
00:01:18,250 --> 00:01:18,790
a quotient

29
00:01:20,870 --> 00:01:23,430
if you don't get that from a slightly more abstract point of view

30
00:01:25,540 --> 00:01:27,040
the bayes equation is actually

31
00:01:28,910 --> 00:01:29,890
in the following sense

32
00:01:30,410 --> 00:01:33,970
so here we have the posterior probability and this is a conditional probability

33
00:01:35,270 --> 00:01:36,220
and then we have the prior

34
00:01:37,750 --> 00:01:41,270
which is also a conditional prior which which is which is the conditional probability

35
00:01:42,750 --> 00:01:43,700
base theorem in

36
00:01:44,390 --> 00:01:45,290
and the way it's used in

37
00:01:45,660 --> 00:01:46,830
mathematical statistics

38
00:01:48,450 --> 00:01:49,160
the density

39
00:01:49,560 --> 00:01:50,500
of the posterior

40
00:01:51,040 --> 00:01:52,140
with respect to the prior

41
00:01:54,480 --> 00:01:54,930
remember on

42
00:01:55,720 --> 00:01:58,430
what i just said before in the last slide density is

43
00:01:59,100 --> 00:02:02,120
a rule that tells us how to take one probability distribution

44
00:02:02,560 --> 00:02:03,390
and reweighted

45
00:02:03,790 --> 00:02:05,750
to turn it into another probability distribution

46
00:02:06,830 --> 00:02:08,060
now with this he says is

47
00:02:08,660 --> 00:02:09,500
we have a density

48
00:02:09,970 --> 00:02:12,870
that is parameterized by the data we have observed

49
00:02:13,390 --> 00:02:18,350
and which tells us how we have to reweight given the data how we have to reweight our prior distribution

50
00:02:19,160 --> 00:02:20,100
to give us a posterior

51
00:02:20,580 --> 00:02:21,180
given the state

52
00:02:24,790 --> 00:02:25,180
now that

53
00:02:25,750 --> 00:02:28,370
that's that's simply what we would like to compute

54
00:02:29,020 --> 00:02:30,330
and bayes theorem tells us

55
00:02:30,870 --> 00:02:32,830
this density has a specific form

56
00:02:33,640 --> 00:02:34,600
and that is given by

57
00:02:35,620 --> 00:02:36,290
the likelihood

58
00:02:37,660 --> 00:02:38,970
the density of the likelihood

59
00:02:39,750 --> 00:02:40,390
with respect to

60
00:02:41,850 --> 00:02:45,330
to the evidence and so that's the school she and that we get in bayes theorem

61
00:02:46,660 --> 00:02:51,000
so basically what theorem says is we have this absurd quantities we have no idea

62
00:02:51,850 --> 00:02:52,640
we can handle them

63
00:02:53,220 --> 00:02:54,600
but under certain conditions

64
00:02:55,180 --> 00:03:01,310
which holds in parametric statistics under certain conditions we can nicely express the relation between this

65
00:03:01,890 --> 00:03:02,310
and this

66
00:03:02,970 --> 00:03:04,310
as a quotient of densities

67
00:03:07,750 --> 00:03:07,980
all right

68
00:03:08,450 --> 00:03:12,000
and that provides us with a concrete connection between the prior and the posterior

69
00:03:13,470 --> 00:03:17,620
that's not the same property okay so when i say this doesn't always exists that

70
00:03:17,620 --> 00:03:20,220
is not the same problem has posterior tractability

71
00:03:20,870 --> 00:03:22,720
yeah i mean you might heard that that

72
00:03:22,720 --> 00:03:28,280
and the m step we maximize the lower bound with respect to the parameters holding

73
00:03:28,280 --> 00:03:31,830
the distribution over hidden variables fixed

74
00:03:34,490 --> 00:03:39,600
the intuition is as follows we're trying to maximize the log likelihood we have a

75
00:03:39,600 --> 00:03:45,340
lower bound on the log likelihood in the east that we maximize the lower bound

76
00:03:45,340 --> 00:03:50,200
with respect to distributions over hidden variable so we find the best distribution over hidden

77
00:03:50,200 --> 00:03:52,910
variables in a sense

78
00:03:52,920 --> 00:03:58,360
and that best distribution over hidden variables is in fact is the posterior distribution over

79
00:03:58,360 --> 00:04:00,250
the hidden variables

80
00:04:00,610 --> 00:04:06,070
given the observed variables and the previous settings of parameters

81
00:04:06,260 --> 00:04:07,920
then in the m step

82
00:04:08,700 --> 00:04:14,040
take that lower bound and we maximize it with respect to the parameters

83
00:04:14,060 --> 00:04:16,440
OK holding this distribution six

84
00:04:16,450 --> 00:04:20,120
and i won't go through it but it's very easy to show

85
00:04:21,640 --> 00:04:26,670
doing e step and m step is guaranteed to

86
00:04:26,680 --> 00:04:33,340
either increase the log likelihood or keep fixed and basically what that does is

87
00:04:33,360 --> 00:04:37,620
when it keeps the log likelihood fixed you've just converge to a local optimum of

88
00:04:37,620 --> 00:04:44,040
the log likelihood otherwise each e and m steps will be guaranteed to increase the

89
00:04:44,040 --> 00:04:47,140
log likelihood so you're optimizing the log likelihood

90
00:04:47,150 --> 00:04:48,730
in this procedure

91
00:04:53,530 --> 00:04:56,990
the key thing to think about in the context of

92
00:04:57,010 --> 00:05:03,520
directed graphical models is that the e step requires solving the inference problem that is

93
00:05:03,580 --> 00:05:07,000
finding the distribution over the hidden variables

94
00:05:07,010 --> 00:05:08,950
this p of x even y

95
00:05:08,960 --> 00:05:10,140
and theta

96
00:05:10,150 --> 00:05:13,940
given the current model parameters theta t

97
00:05:15,650 --> 00:05:19,080
if you're graph is singly connected

98
00:05:19,170 --> 00:05:26,920
in the e step you simply run the belief propagation algorithm to find that distribution

99
00:05:27,420 --> 00:05:31,090
if it's mostly connected if you want to do an exact piece that you would

100
00:05:31,090 --> 00:05:32,090
just run

101
00:05:32,800 --> 00:05:35,430
the junction tree algorithm for example

102
00:05:35,430 --> 00:05:39,620
so this is called a subroutine of the EM algorithm

103
00:05:39,650 --> 00:05:41,210
in this case

104
00:05:41,250 --> 00:05:43,590
is that clear

105
00:05:45,650 --> 00:05:49,230
and as you iterate these two steps the index

106
00:05:50,400 --> 00:05:53,330
is the index for iteration

107
00:05:53,340 --> 00:05:57,090
then the parameters data will converge to

108
00:05:57,100 --> 00:06:00,090
a local optimum of the log likelihood

109
00:06:00,100 --> 00:06:02,690
whereas in the complete data case

110
00:06:02,690 --> 00:06:07,240
we can find the global optimum of the log likelihood here

111
00:06:07,290 --> 00:06:11,870
because we have hidden variables in our log likelihood will in general have multiple local

112
00:06:11,870 --> 00:06:16,530
optima and this procedure is only guaranteed to find you a local optimum of the

113
00:06:16,530 --> 00:06:18,000
log likelihood

114
00:06:20,550 --> 00:06:32,960
it's generally fast people have studied the convergence properties of this method and they've analyse

115
00:06:32,980 --> 00:06:35,200
it in terms of concepts

116
00:06:35,210 --> 00:06:36,480
known as

117
00:06:38,150 --> 00:06:45,080
proportion of missing information so the more missing information you have the slower your algorithm

118
00:06:45,080 --> 00:06:46,540
will be to converge

119
00:06:46,560 --> 00:06:50,170
but generally you know this is what one would recommend

120
00:06:56,310 --> 00:07:06,950
here there was no sorry OK i'm i'm maybe it's a little confusing the question

121
00:07:06,950 --> 00:07:12,390
was we're trying to maximum likelihood but are inside the procedure we using the posterior

122
00:07:12,390 --> 00:07:14,200
distribution of the parameters

123
00:07:14,230 --> 00:07:15,770
and here you know

124
00:07:15,800 --> 00:07:21,780
we are using a point estimate of the parameters at every point in time

125
00:07:21,790 --> 00:07:26,740
the only thing we're doing posterior distributions over are the

126
00:07:26,940 --> 00:07:32,490
hidden variables the nodes in the graph which we haven't observed

127
00:07:32,520 --> 00:07:38,870
so it's like classically and we're doing full probabilistic inference over the hidden variables but

128
00:07:38,870 --> 00:07:42,680
we're doing point estimation of the parameters

129
00:07:44,540 --> 00:07:49,240
in polynomial time

130
00:07:49,280 --> 00:07:54,160
i i don't think there is such a guarantee

131
00:07:54,250 --> 00:07:58,850
and in each step increases the likelihood of a little bit so it might depend

132
00:07:58,850 --> 00:08:03,530
on your notion of convergence and i think there could be pathological cases

133
00:08:03,550 --> 00:08:07,580
so i you know i don't think there's such

134
00:08:14,430 --> 00:08:16,060
yes patients

135
00:08:17,100 --> 00:08:20,680
the question was is there a way to do it in a full bayesian manner

136
00:08:20,720 --> 00:08:23,870
OK so i'm working up incrementally to that

137
00:08:26,220 --> 00:08:28,030
here's just the summary

138
00:08:28,030 --> 00:08:31,610
drawback is basically that a lot of noise can be treated as arguments in this

139
00:08:31,610 --> 00:08:33,430
case that's me

140
00:08:33,450 --> 00:08:37,290
lead sometimes to very poor performance

141
00:08:37,330 --> 00:08:41,060
this is another approach from columbia university

142
00:08:41,110 --> 00:08:44,310
and they used to fix with clustering

143
00:08:44,310 --> 00:08:47,560
two classes of data for this very simple approach

144
00:08:47,600 --> 00:08:53,160
there are basically two points conceded the same clusters if their distance is less than

145
00:08:53,160 --> 00:08:55,100
some pre specified threshold

146
00:08:55,130 --> 00:08:56,920
a user specified threshold

147
00:08:57,050 --> 00:09:01,830
and they're are doing this in section in the sequence of all the interaction so

148
00:09:01,830 --> 00:09:06,710
basically after they do this all data across the belong to some of

149
00:09:06,770 --> 00:09:10,170
large clusters that correspond to became

150
00:09:10,180 --> 00:09:15,060
points that are a lot of they correspond to some small classes i consider normally

151
00:09:18,600 --> 00:09:23,340
there's an approach called cluster based local outlier factor

152
00:09:23,390 --> 00:09:31,080
and it's mixing the BSO for quality factor and regular clustering approaches

153
00:09:31,140 --> 00:09:36,010
and it using it using squeezer clustering algorithm to perform clustering first so let's assume

154
00:09:36,010 --> 00:09:36,840
you get

155
00:09:36,860 --> 00:09:38,330
this kind of clusters

156
00:09:38,390 --> 00:09:40,700
and then it

157
00:09:40,720 --> 00:09:43,900
computes the CBLOF

158
00:09:43,930 --> 00:09:48,690
vector for each data record the data records is in the small clusters

159
00:09:48,690 --> 00:09:53,780
then the CBO CBLOF is computed as the size of cluster times

160
00:09:53,800 --> 00:09:57,580
the distance between the days that specific data instances

161
00:09:57,630 --> 00:10:02,030
and the the closest larger cluster because CBLOF is computed for every

162
00:10:02,050 --> 00:10:03,110
data records

163
00:10:03,160 --> 00:10:03,990
if d

164
00:10:04,020 --> 00:10:07,380
object is within the larger clusters

165
00:10:08,060 --> 00:10:12,400
CBLOF is computed again as the product of size of cluster ten distance between the

166
00:10:12,400 --> 00:10:14,030
data instance

167
00:10:14,040 --> 00:10:17,790
and the centroid of the cluster it belongs to

168
00:10:19,060 --> 00:10:20,830
i'm not going into

169
00:10:20,950 --> 00:10:25,730
discussion this is good not obviously has many

170
00:10:25,760 --> 00:10:30,240
drawbacks especially if there is point far from everything else the

171
00:10:30,650 --> 00:10:34,180
size of class can be one of the difference can be huge and be quite

172
00:10:34,190 --> 00:10:38,350
similar to everything else so that can not be useful for detecting

173
00:10:39,360 --> 00:10:43,140
so these are some of the clustering based approach is most of them

174
00:10:43,150 --> 00:10:47,050
rely on similar ideas

175
00:10:47,330 --> 00:10:50,170
i'll briefly go to this statistical

176
00:10:52,200 --> 00:10:57,810
in in statistical based techniques who have the key assumption that are

177
00:10:57,860 --> 00:11:01,330
normal data records occur in high probability regions

178
00:11:01,340 --> 00:11:02,880
all the physical distribution

179
00:11:02,890 --> 00:11:09,160
while anomalies usually occur in the low probability regions of that's pretty of the statistical

180
00:11:10,210 --> 00:11:14,530
and the general approach and the statistical

181
00:11:14,540 --> 00:11:19,740
based anomaly detection techniques is to estimate the statistical distribution using given

182
00:11:20,940 --> 00:11:24,910
and then apply statistical inference test to determine if a test instance belongs to this

183
00:11:24,910 --> 00:11:29,290
distribution or not

184
00:11:29,300 --> 00:11:33,640
people are using a very simple approach is false and for centuries not for centuries

185
00:11:33,640 --> 00:11:36,850
but not since nineteen forty nineteen fifty basically

186
00:11:36,860 --> 00:11:41,030
everything that is more than three standard deviations can be treated as anomaly

187
00:11:42,600 --> 00:11:48,350
people also using the this the square statistics which is basically very similar to mahalanobis

188
00:11:48,360 --> 00:11:49,420
this does

189
00:11:49,470 --> 00:11:52,080
you have a specific data record

190
00:11:52,090 --> 00:11:54,340
x but this is actually the mean

191
00:11:54,400 --> 00:11:56,300
this is covariance matrix

192
00:11:56,360 --> 00:12:02,070
and this is just to draw a actually mahalanobis distances without this term so basically

193
00:12:02,070 --> 00:12:04,990
you're just adding this two

194
00:12:05,010 --> 00:12:06,020
but we this

195
00:12:06,430 --> 00:12:09,680
more robust larger value of the square

196
00:12:09,690 --> 00:12:11,320
would correspond to

197
00:12:11,330 --> 00:12:15,560
later because it and data may be anomalies in the small medical should correspond to

198
00:12:16,660 --> 00:12:19,810
data across

199
00:12:19,820 --> 00:12:25,170
obvious obvious advantages of statistical basic things because they can utilize already

200
00:12:25,180 --> 00:12:26,570
existing as

201
00:12:26,610 --> 00:12:32,430
statistical modeling techniques to estimate the different types of distributions

202
00:12:32,480 --> 00:12:36,050
and they provide statistically justifiable

203
00:12:36,220 --> 00:12:38,510
solutions to detect anomalies

204
00:12:38,560 --> 00:12:41,650
some drawbacks including

205
00:12:41,660 --> 00:12:42,960
the fact that

206
00:12:43,020 --> 00:12:48,680
with high dimensions it's extremely difficult to estimate the parameters and construct the hypothesis tests

207
00:12:48,690 --> 00:12:49,850
also does

208
00:12:49,860 --> 00:12:53,920
automatic assumptions about the real life datasets may not

209
00:12:53,920 --> 00:12:55,890
the true

210
00:12:55,960 --> 00:13:00,190
and this is really hard sometimes to estimate its parameters

211
00:13:00,190 --> 00:13:02,180
of and of course

212
00:13:02,350 --> 00:13:07,890
most statistical approaches work for a relatively small data sets and if you are going

213
00:13:08,000 --> 00:13:10,090
to protection from very large datasets

214
00:13:10,160 --> 00:13:15,430
can this technique can be computationally expensive

215
00:13:16,800 --> 00:13:21,240
as know two types of statistical techniques parametric and nonparametric

216
00:13:21,260 --> 00:13:25,130
in dramatic techniques you assume that normal

217
00:13:25,130 --> 00:13:29,440
data generating from the underlying formatting distribution

218
00:13:29,540 --> 00:13:31,180
and of course you may as

219
00:13:31,180 --> 00:13:33,570
you may have similar assumption for

220
00:13:33,570 --> 00:13:35,900
we will call this an eight

221
00:13:37,000 --> 00:13:40,690
fifty polarized they don't exist it's only in your head

222
00:13:40,740 --> 00:13:42,900
and the fifty refers to the fact

223
00:13:43,560 --> 00:13:46,610
fifty percent get through polarized

224
00:13:46,650 --> 00:13:52,380
in the optics gets that we hand out today we will need throughout this course

225
00:13:52,440 --> 00:13:56,430
you don't have AIDS and fifty polarized they don't exist i don't quite know what

226
00:13:56,430 --> 00:13:58,310
uses idea measure it

227
00:13:58,320 --> 00:14:00,810
he was maybe in h and twenty five

228
00:14:00,810 --> 00:14:02,950
well maybe in h and thirteen

229
00:14:03,000 --> 00:14:04,340
which would then mean

230
00:14:04,360 --> 00:14:05,950
that is i zero

231
00:14:06,000 --> 00:14:11,500
strengths of an unpolarized light of beam wouldn't be happy i zero but maybe only

232
00:14:11,500 --> 00:14:12,760
point two five

233
00:14:12,830 --> 00:14:17,330
four point three but in any case the lights that will come through your

234
00:14:17,360 --> 00:14:23,540
linear polarized this will be very closely two hundred percent polarized

235
00:14:23,580 --> 00:14:27,040
so what i will do now i will take unpolarized light

236
00:14:27,120 --> 00:14:28,940
and i will

237
00:14:28,940 --> 00:14:33,170
this light coming straight out of the blackboard perpendicular to u

238
00:14:33,210 --> 00:14:35,290
strengths i zero

239
00:14:35,300 --> 00:14:37,880
and here is one of my polarizes

240
00:14:37,930 --> 00:14:40,360
and the light that comes through

241
00:14:40,370 --> 00:14:43,010
is linearly polarized in this direction

242
00:14:43,020 --> 00:14:44,610
so we already know

243
00:14:44,620 --> 00:14:49,150
that one half i zero will come through it is an ideal polarizing and it

244
00:14:49,150 --> 00:14:51,480
is polarized in this direction

245
00:14:51,550 --> 00:14:54,570
i take a second sheet an identical one

246
00:14:54,580 --> 00:14:56,630
i put it also in the plane of the

247
00:14:56,640 --> 00:15:00,130
blackboard but rotated over an angle theta

248
00:15:00,130 --> 00:15:03,380
there is no second she

249
00:15:04,310 --> 00:15:06,760
as the preferred directions of polarization

250
00:15:06,770 --> 00:15:08,170
in this direction

251
00:15:08,210 --> 00:15:10,630
and the angle

252
00:15:10,640 --> 00:15:12,130
rotate it

253
00:15:12,140 --> 00:15:14,920
over an angle theta

254
00:15:14,960 --> 00:15:16,240
so between this one

255
00:15:16,250 --> 00:15:18,610
and this one is an angle theta

256
00:15:18,770 --> 00:15:21,360
so you can now immediately tell

257
00:15:21,380 --> 00:15:25,680
what the intensity of the light is that for the second polarizer

258
00:15:25,750 --> 00:15:28,580
it must of course be polarized in this direction

259
00:15:28,590 --> 00:15:29,760
because that is the

260
00:15:29,790 --> 00:15:33,130
allow direction organisation forty second street

261
00:15:33,180 --> 00:15:36,920
and intensity must now be one-half i zero

262
00:15:37,010 --> 00:15:38,640
that's what comes in

263
00:15:38,690 --> 00:15:40,390
and then i have to multiply

264
00:15:40,400 --> 00:15:43,140
by the cosine square of fate

265
00:15:43,150 --> 00:15:47,680
i don't have to ever is now over all angles because there's only one

266
00:15:47,690 --> 00:15:51,500
the value of between the sheets and this is so this is now the new

267
00:15:52,620 --> 00:15:55,250
and it's all polarized in this direction

268
00:15:55,310 --> 00:15:56,900
and this law

269
00:15:56,940 --> 00:16:01,590
by the light intensity is reduced by the vector cosine square say that

270
00:16:01,620 --> 00:16:02,570
is known

271
00:16:02,580 --> 00:16:04,460
as miles long

272
00:16:12,880 --> 00:16:15,570
if a there were thirty degrees

273
00:16:15,670 --> 00:16:22,540
the light intensity he would be one half i zero

274
00:16:22,580 --> 00:16:27,790
times the cosine square of thirty degrees which is o point seven five

275
00:16:27,870 --> 00:16:32,430
if there was zero degrees

276
00:16:32,500 --> 00:16:34,380
that means that the sheet

277
00:16:34,380 --> 00:16:36,480
in the same direction is this one

278
00:16:36,490 --> 00:16:38,840
if everything were ideal

279
00:16:38,860 --> 00:16:41,920
one half i zero will get through to second she

280
00:16:41,960 --> 00:16:44,520
if a is ninety degrees

281
00:16:45,590 --> 00:16:47,210
nothing will get through

282
00:16:47,240 --> 00:16:48,480
because the cosine

283
00:16:48,610 --> 00:16:52,060
ninety degrees is zero we call that cross polarizes

284
00:16:52,090 --> 00:16:53,440
across them like this

285
00:16:53,480 --> 00:16:54,960
no light will

286
00:16:54,980 --> 00:16:56,960
get through

287
00:16:56,990 --> 00:16:58,790
now before i demonstrate this

288
00:16:58,800 --> 00:17:00,510
i have to be honest with you

289
00:17:00,560 --> 00:17:04,330
because the idea of reducing the energy of individual photons

290
00:17:04,450 --> 00:17:07,560
by reducing the electric field strength

291
00:17:07,570 --> 00:17:08,840
as i did

292
00:17:08,890 --> 00:17:10,960
is achieved

293
00:17:11,070 --> 00:17:13,990
the light photon has well defined energy

294
00:17:14,040 --> 00:17:16,390
which depends only weakly on the frequency

295
00:17:16,440 --> 00:17:17,750
of the lights

296
00:17:17,830 --> 00:17:20,960
blue light has a higher frequency than red light

297
00:17:21,000 --> 00:17:24,260
so blue light has a high energy than red

298
00:17:24,370 --> 00:17:29,560
when you send blue light through a polarizer the way i did here

299
00:17:29,610 --> 00:17:32,270
control doesn't come through

300
00:17:32,320 --> 00:17:36,290
but if it does come through it is still blue light there is no such

301
00:17:36,290 --> 00:17:38,750
thing as a reduction in energy

302
00:17:38,800 --> 00:17:40,460
whereas this reduction

303
00:17:40,480 --> 00:17:43,940
by cosine theta would imply that the energy

304
00:17:43,950 --> 00:17:47,380
goes down and that we would imply then that there will be a call it

305
00:17:47,380 --> 00:17:49,440
seems that it would no longer be blue

306
00:17:49,500 --> 00:17:51,660
and that's not the case

307
00:17:51,670 --> 00:17:54,210
if you want to treat this properly you have to do it in the quantum

308
00:17:54,210 --> 00:17:55,690
mechanical way

309
00:17:55,730 --> 00:18:00,180
the interesting thing is that if you use quantum mechanics you find exactly the same

310
00:18:00,180 --> 00:18:02,560
what you find also miles law

311
00:18:02,570 --> 00:18:03,960
so the law is OK

312
00:18:03,970 --> 00:18:07,580
even though to the patient is not kosher

313
00:18:07,620 --> 00:18:10,150
now i want you to get out of the envelope

314
00:18:10,190 --> 00:18:12,460
one of your green plates

315
00:18:12,520 --> 00:18:15,600
which is a linear polarizer

316
00:18:15,690 --> 00:18:18,170
this is the kind of place that you have you have three and their only

317
00:18:18,170 --> 00:18:27,940
take one out

318
00:18:28,030 --> 00:18:30,220
these two lights shining on

319
00:18:30,230 --> 00:18:32,070
unpolarized light

320
00:18:32,180 --> 00:18:35,980
the lights that comes to you now is unpolarized

321
00:18:36,040 --> 00:18:40,150
i'm not going to hold in front of my face this polarized

322
00:18:40,160 --> 00:18:43,970
so the light that comes through is linearly polarized in this direction

323
00:18:44,030 --> 00:18:47,970
and you are going to play the role of the second polarizer close one i

324
00:18:47,970 --> 00:18:52,200
put polarisation from the u i and rotate it

325
00:18:52,210 --> 00:18:57,700
you'll see a huge difference in light intensity

326
00:18:57,720 --> 00:19:00,120
if you crossed polarized with me

327
00:19:00,180 --> 00:19:01,400
you can see me

328
00:19:01,570 --> 00:19:03,900
making very happy

329
00:19:03,990 --> 00:19:05,940
keep in mind

330
00:19:06,070 --> 00:19:07,720
if you can see me

331
00:19:07,730 --> 00:19:09,570
i can see you either

332
00:19:09,610 --> 00:19:12,240
so around and convince yourself

333
00:19:12,260 --> 00:19:16,940
is light that reaches you is now in the linearly polarized

334
00:19:16,980 --> 00:19:19,570
and when you rotate around your parameters

335
00:19:19,590 --> 00:19:22,410
we call the we call them polarizes

336
00:19:22,430 --> 00:19:24,370
you can see me either

337
00:19:24,390 --> 00:19:30,700
or you cannot see me at all there's anything and everything in between

338
00:19:30,710 --> 00:19:36,720
very well

339
00:19:36,770 --> 00:19:38,450
there's a second way

340
00:19:38,460 --> 00:19:39,820
that we can produce

341
00:19:39,830 --> 00:19:43,940
one hundred percent linearly polarized light

342
00:19:44,050 --> 00:19:46,560
we can do that by reflecting

343
00:19:46,570 --> 00:19:48,100
unpolarized light

344
00:19:48,110 --> 00:19:50,810
of the dielectric for instance water

345
00:19:53,060 --> 00:19:55,330
none of this follows from snails law

346
00:19:55,350 --> 00:19:58,260
snails almost two hundred fifty years before maxwell

347
00:19:58,290 --> 00:20:00,570
polarization wasn't even known in today's

348
00:20:00,570 --> 00:20:01,680
o of snow

349
00:20:01,770 --> 00:20:04,050
with maxwell's equations allow u

350
00:20:04,060 --> 00:20:05,490
to properly deal

351
00:20:05,520 --> 00:20:06,820
was refraction

352
00:20:07,680 --> 00:20:11,570
reflection including the polarization

353
00:20:11,570 --> 00:20:14,860
and i will make no attempt to derive this for you in detail

354
00:20:14,890 --> 00:20:17,470
it's really part of eight three if you ever take it

355
00:20:17,520 --> 00:20:19,940
but i will present you with some results

356
00:20:19,980 --> 00:20:22,440
so that you can at least appreciate

357
00:20:22,490 --> 00:20:24,520
the far reaching consequences

358
00:20:24,530 --> 00:20:25,940
of the the

359
00:20:25,960 --> 00:20:29,110
reflection in which we can produce on the percent

360
00:20:29,120 --> 00:20:32,110
polarized light

361
00:20:32,120 --> 00:20:34,440
suppose f unpolarized light

362
00:20:34,450 --> 00:20:36,690
coming in

363
00:20:37,870 --> 00:20:39,370
media one index of

364
00:20:39,390 --> 00:20:41,010
reflection and one

365
00:20:41,240 --> 00:20:44,180
two index of refraction and two

366
00:20:44,220 --> 00:20:45,310
it's coming in

367
00:20:45,310 --> 00:20:50,750
stochastic sometimes some cells go through he one in ten hours some cells go through

368
00:20:50,770 --> 00:20:53,640
cells in eleven hours some through an eight hours

369
00:20:53,670 --> 00:20:56,060
and and as a consequence

370
00:20:56,110 --> 00:21:01,120
as they moved further and further around the cell cycle the population of cells becomes

371
00:21:01,120 --> 00:21:02,960
progressively more

372
00:21:02,960 --> 00:21:07,300
asynchronous and they were really enter into the next phase

373
00:21:07,360 --> 00:21:09,140
totally synchronously

374
00:21:09,160 --> 00:21:13,030
but now we can ask the following question how long is g one

375
00:21:13,170 --> 00:21:16,950
and the way we can do that is to do another kind of experiment where

376
00:21:16,950 --> 00:21:17,440
what we do

377
00:21:17,800 --> 00:21:19,500
is we release cells

378
00:21:19,500 --> 00:21:24,360
the population of cells from an faced by removing the coal cement

379
00:21:24,410 --> 00:21:26,060
for example

380
00:21:26,090 --> 00:21:26,810
and now

381
00:21:27,000 --> 00:21:33,610
we treat cells we treat our quotes of the synchronously advancing population each hour we

382
00:21:33,610 --> 00:21:37,910
pluck out some of these cells and we expose them for let's say the next

383
00:21:37,910 --> 00:21:39,300
hour thereafter

384
00:21:39,310 --> 00:21:42,110
to some it radiated climate

385
00:21:42,200 --> 00:21:49,520
treaty is signed as you may recall h three five minutes it sometimes denoted

386
00:21:49,540 --> 00:21:53,760
it is obviously a radio labelled precursor of DNA

387
00:21:53,760 --> 00:21:56,240
and it's going to get incorporated into the DNA

388
00:21:56,250 --> 00:21:59,510
and in no other macromolecules in the cell

389
00:21:59,530 --> 00:22:01,390
and so the question is

390
00:22:01,440 --> 00:22:04,540
if we expose ourselves from our here

391
00:22:04,600 --> 00:22:07,500
two pretty finally how much

392
00:22:07,500 --> 00:22:11,190
pretty this time is going to be incorporated into their DNA

393
00:22:11,240 --> 00:22:16,780
how do we know what incorporated and how much tritium thirty five remains unincorporated we

394
00:22:16,780 --> 00:22:21,730
extracted DNA from the cells and we precipitated in an acid

395
00:22:21,770 --> 00:22:23,290
and when that happens

396
00:22:23,310 --> 00:22:27,980
the macromer more molecules of DNA go to the bottom they precipitate

397
00:22:28,020 --> 00:22:33,690
and the unincorporated pretty finding which is still soluble has not yet been polymerize into

398
00:22:33,700 --> 00:22:35,170
DNA molecules

399
00:22:35,210 --> 00:22:36,210
it remains

400
00:22:36,220 --> 00:22:40,540
in the acid solution so we just how much as imperceptible

401
00:22:40,600 --> 00:22:44,920
finally counts are there here how many in the neck are made in the next

402
00:22:44,920 --> 00:22:48,790
hour with another eloquent of cells harmony in the next hour and so forth and

403
00:22:48,790 --> 00:22:53,640
when we do such an experiment we find that we get a a curve which

404
00:22:53,640 --> 00:22:55,500
looks like this

405
00:22:55,530 --> 00:22:58,690
all of a sudden

406
00:22:58,690 --> 00:23:03,640
here we got no cells labeling and the second on third fourth fifth sixth and

407
00:23:03,640 --> 00:23:06,260
the seventh our there's some cells which make DNA

408
00:23:06,270 --> 00:23:10,410
in the eighth our the some cells even more cells making DNA and by the

409
00:23:10,410 --> 00:23:13,190
time the in the ninth and tenth our

410
00:23:13,200 --> 00:23:16,030
then we find a high rate of DNA synthesis

411
00:23:16,090 --> 00:23:19,550
now in principle we could continue this experiment if we wanted to if we had

412
00:23:19,550 --> 00:23:24,380
enough money for trade finding which one does because it's very cheap and what we

413
00:23:24,380 --> 00:23:28,600
would find is if we started labeling over here with for another hour

414
00:23:29,680 --> 00:23:32,790
when i say in our poles i mean we just put it really is time

415
00:23:32,790 --> 00:23:37,250
meeting in at the beginning of this one hour period way over here and an

416
00:23:37,250 --> 00:23:40,980
hour later we take out the cells and extracted DNA and measure how much DNA

417
00:23:40,980 --> 00:23:42,840
has been incorporated in that interval

418
00:23:43,010 --> 00:23:46,830
and if we did a pretty impulse over here we begin to see that the

419
00:23:46,830 --> 00:23:51,550
rate of DNA synthesis was actually lower and eventually it would go down but that

420
00:23:51,550 --> 00:23:52,510
down to this

421
00:23:52,520 --> 00:23:55,940
as the cells were moving asynchronously the culture

422
00:23:55,940 --> 00:23:59,570
what does that mean well this is the time obviously these cells are in this

423
00:24:00,410 --> 00:24:03,640
these cells were in g one

424
00:24:03,700 --> 00:24:07,590
and these cells have now emerged from phase into g two and there are no

425
00:24:07,590 --> 00:24:09,870
longer making DNA

426
00:24:09,890 --> 00:24:13,950
and on that basis we can actually calculate we can determine how many hours it

427
00:24:13,950 --> 00:24:16,020
takes for cells to move through g one

428
00:24:16,030 --> 00:24:21,320
we can do is similar kind of experiment to figure out how long

429
00:24:21,340 --> 00:24:26,430
g two is the gap two g two phases g to recall is the time

430
00:24:26,430 --> 00:24:31,740
between the ending of DNA replication and the beginning of mitosis and how do we

431
00:24:31,740 --> 00:24:37,690
do that we can do is inhibitor of DNA synthesis so let's say here is

432
00:24:37,750 --> 00:24:39,660
going to redraw the cell cycle

433
00:24:39,710 --> 00:24:41,560
here's s phase

434
00:24:41,580 --> 00:24:45,510
she one drawing it again here's an

435
00:24:45,520 --> 00:24:49,970
and here's a g one

436
00:24:49,980 --> 00:24:55,680
and now what we can do is will and DNA synthesis inhibitor inhibitor of DNA

437
00:24:55,680 --> 00:24:59,010
synthesis no effect as you can imagine on m phase

438
00:24:59,020 --> 00:25:02,510
this is one of those things it's called hydroxy urea

439
00:25:02,520 --> 00:25:10,980
it actually blocks the biosynthesis of the precursors the arcs around side triphosphate precursors

440
00:25:10,990 --> 00:25:15,030
of DNA and therefore you have hydroxy to cells

441
00:25:15,060 --> 00:25:20,730
and they have their their DNA replication grinds to a halt

442
00:25:20,740 --> 00:25:23,530
OK so let's do that

443
00:25:23,630 --> 00:25:25,660
and i will

444
00:25:25,700 --> 00:25:31,020
will will hydroxy urea cells for twenty four hours what's going to happen

445
00:25:31,040 --> 00:25:35,280
what what the distribution of cells be afterwards well in fact some of the cells

446
00:25:35,280 --> 00:25:39,870
when we first and the hydroxy urea the cells are scattered all around

447
00:25:39,880 --> 00:25:45,250
they were asynchronous will and hydroxy urea and what will find is the following the

448
00:25:45,250 --> 00:25:49,600
cells that were in the middle of s phase when we have the hydroxyl we

449
00:25:49,600 --> 00:25:54,030
will be stuck dead in the water they will be able to move anymore so

450
00:25:54,030 --> 00:25:57,260
these cells that were in s phase and we have the drugs you will be

451
00:25:57,260 --> 00:26:00,100
trapped right here they can't make any more DNA

452
00:26:00,110 --> 00:26:05,860
and therefore they'll will be frozen many point times point in time in space

453
00:26:05,870 --> 00:26:10,290
the cells that are outside of this ways they can continue to advance all the

454
00:26:10,290 --> 00:26:11,760
way around the cell cycle

455
00:26:11,760 --> 00:26:13,450
and accordingly

456
00:26:13,520 --> 00:26:17,380
the be lots of cells over the next twenty four hours a just pile up

457
00:26:17,380 --> 00:26:20,340
at the g one s transition

458
00:26:21,170 --> 00:26:24,590
well the hydroxy really has no effect at all on these other points of the

459
00:26:24,590 --> 00:26:28,750
cell cycle and when these cells try to go go from g one into s

460
00:26:28,790 --> 00:26:32,700
they can get in this phase because they can make any DNA so the trapped

461
00:26:32,700 --> 00:26:34,020
right over there

462
00:26:34,020 --> 00:26:35,390
and now

463
00:26:35,420 --> 00:26:39,920
it will take away the hydroxy urea which is obviously is to use the notation

464
00:26:39,920 --> 00:26:42,990
used before and inhibitor of DNA synthesis

465
00:26:43,010 --> 00:26:50,310
and what we can do now is the following

466
00:26:50,310 --> 00:26:53,350
say oh well i stopped caring about this row the

467
00:26:53,370 --> 00:26:55,830
from node d simon stopped using this

468
00:26:55,870 --> 00:27:00,770
and instead spend some time later is going to hear about this new rowdy and

469
00:27:00,770 --> 00:27:04,770
it's going to take that into stable so this process where is this sort of

470
00:27:04,770 --> 00:27:06,870
process whereby notes continually

471
00:27:09,140 --> 00:27:12,870
in a great drought and we talked about this

472
00:27:12,960 --> 00:27:16,100
and there's this this sort of interesting thing which happened which is the nodes forget

473
00:27:16,100 --> 00:27:20,080
about routs that they have heard about for a while when they went missing

474
00:27:20,120 --> 00:27:23,410
so this forgetting about droughts

475
00:27:23,420 --> 00:27:31,040
is an important principle that is often employed in networking called soft state

476
00:27:31,060 --> 00:27:33,310
the idea is the idea with soft state

477
00:27:33,370 --> 00:27:34,640
you should only

478
00:27:34,650 --> 00:27:40,000
keep state keep information when that information gets refreshed so each all the information that

479
00:27:40,000 --> 00:27:44,290
you have some time limit on it and when you have heard that information refreshed

480
00:27:44,290 --> 00:27:46,860
after some time when you throw it out so that's all we're doing with the

481
00:27:46,860 --> 00:27:50,890
around here and were sort of we only so we only keep droughts that we've

482
00:27:50,910 --> 00:27:53,770
heard advertiser recently basically

483
00:27:53,770 --> 00:27:57,840
i mean that has the nice property that allows us to adapt to falls within

484
00:27:57,840 --> 00:28:02,750
the price we saw failure happened we saw that sometime after that failure this off

485
00:28:02,750 --> 00:28:07,520
the property would cause the information about the link between the DNA disappear from the

486
00:28:07,520 --> 00:28:13,920
network and then those would rediscover their new new links that allow them to connect

487
00:28:13,960 --> 00:28:16,870
OK so

488
00:28:16,960 --> 00:28:20,440
what i what i want to do now so this is sort of the basic

489
00:28:20,440 --> 00:28:24,790
process empathic arriving is fairly similar to the way that routing in the internet actually

490
00:28:25,580 --> 00:28:28,580
next time you guys are in for next time in recitation going to talk about

491
00:28:28,580 --> 00:28:30,850
a particle called the border gateway protocol

492
00:28:30,850 --> 00:28:35,040
you actually study in much more detail how internet routing works but this is a

493
00:28:35,040 --> 00:28:40,040
nice simple model of how routing in a small network my my cat

494
00:28:41,390 --> 00:28:44,650
the problem with what we discussed so far

495
00:28:45,750 --> 00:28:50,350
is that

496
00:28:50,370 --> 00:28:52,850
if this is a very large networks this

497
00:28:52,870 --> 00:28:56,000
number of rows is going to have to hear about is really huge right suppose

498
00:28:56,000 --> 00:28:59,120
that this instead of being applied no network was million or

499
00:28:59,140 --> 00:29:03,080
well now i mean have a million advertise every node is to have a table

500
00:29:03,080 --> 00:29:06,210
that million million entries all right that's going to be really big is going to

501
00:29:06,210 --> 00:29:10,780
happen here million advertisments and every time there's a failure well that's going to be

502
00:29:10,780 --> 00:29:13,390
a pain because we're going to have to wait for that information about their failure

503
00:29:13,390 --> 00:29:16,140
to propagate through the whole network so

504
00:29:16,140 --> 00:29:20,690
in some sense this simple path vector routing protocol we have doesn't really meet the

505
00:29:20,690 --> 00:29:24,640
scalability gold we want to scale the network up to very large size

506
00:29:26,600 --> 00:29:28,920
we have an issue with routing

507
00:29:28,940 --> 00:29:33,440
and it scale

508
00:29:33,540 --> 00:29:35,060
so the solution

509
00:29:35,100 --> 00:29:40,060
is a solution that is often used the scalability problem in computer system

510
00:29:40,080 --> 00:29:41,620
it's hierarchy

511
00:29:41,810 --> 00:29:47,150
so let's see what i mean by

512
00:29:47,230 --> 00:29:50,620
so remember when i when i was showing pictures of the internet

513
00:29:50,640 --> 00:29:52,870
there are these different

514
00:29:52,890 --> 00:29:55,270
kind of sub networks there were

515
00:29:55,290 --> 00:29:58,910
forming over time on the west coast and east coast there was there are that

516
00:29:59,140 --> 00:30:03,080
in itself net and the building that that would be sort of different networks the

517
00:30:03,120 --> 00:30:06,540
all part of the internet as a whole but they were these sort of different

518
00:30:06,540 --> 00:30:12,940
regions that were separately administered and often correspond to a specific organizational like the for

519
00:30:12,940 --> 00:30:15,190
like the military

520
00:30:17,100 --> 00:30:21,870
these oftentimes these regions so it's very common when you look at any network to

521
00:30:21,870 --> 00:30:25,520
see that you have these kinds of regions and so for example you know there's

522
00:30:25,520 --> 00:30:29,940
clearly there's is a network that is MIT's network right and there's harvard for example

523
00:30:29,940 --> 00:30:34,210
has the network that's harvard's right and those two things are sort of logical

524
00:30:34,210 --> 00:30:38,060
regions the define what's the define

525
00:30:38,120 --> 00:30:40,910
different parts are different groups within the network

526
00:30:40,920 --> 00:30:44,480
so if you look at any network you would see it sort of almost any

527
00:30:44,480 --> 00:30:48,230
large network is organised in this way into these regions

528
00:30:48,250 --> 00:30:50,750
the paper next time we're going to call these regions

529
00:30:58,440 --> 00:30:59,750
or a

530
00:30:59,850 --> 00:31:01,500
so autonomous in

531
00:31:01,520 --> 00:31:07,000
operating on its own operating without the the without being dependent on the other person

532
00:31:07,000 --> 00:31:11,520
system so for example if MIT's internet connection went down connection to the outside world

533
00:31:11,520 --> 00:31:16,420
went out that wouldn't stop you from being able to connect to machines within MIT

534
00:31:16,520 --> 00:31:19,350
right so i might is autonomous in the sense that it continues to operate in

535
00:31:19,350 --> 00:31:22,620
the absence of a connection to the rest of

536
00:31:24,910 --> 00:31:28,540
let's let's look at a simple example of the network has this hierarchy property and

537
00:31:28,540 --> 00:31:32,230
see how we would modify the routing algorithm that just talked about

538
00:31:32,250 --> 00:31:36,290
so i suppose i have and i suppose i have two small networks each

539
00:31:36,290 --> 00:31:37,710
with three nodes in them

540
00:31:37,730 --> 00:31:39,270
it's called the ABC

541
00:31:39,390 --> 00:31:40,960
and the e

542
00:31:46,270 --> 00:31:50,270
so these are each to be autonomous systems which are drawn by drawing a circle

543
00:31:50,270 --> 00:31:51,710
around them

544
00:31:51,710 --> 00:31:57,900
the structural information to become a kind of data and you can do real might

545
00:31:57,900 --> 00:32:01,640
want to you can extract patterns due to

546
00:32:01,660 --> 00:32:06,750
in data mining when you reach

547
00:32:06,890 --> 00:32:11,650
information extraction

548
00:32:11,670 --> 00:32:13,580
in other words in the title

549
00:32:13,590 --> 00:32:23,920
we define the identification and consequent or concurrent classification and structuring into semantic classes

550
00:32:23,950 --> 00:32:27,500
and of specific information found in b

551
00:32:28,460 --> 00:32:33,360
such as natural language but it could also be which is on the one hand

552
00:32:33,360 --> 00:32:43,260
which might extract the information provided in addition to that integrates the unstructured data by

553
00:32:45,770 --> 00:32:56,200
so i want to classify the semantic search information graphics in structured sources so because

554
00:32:56,200 --> 00:32:58,000
sources with the computer

555
00:32:58,020 --> 00:32:59,730
it's not

556
00:32:59,750 --> 00:33:05,400
please computer what it means that if sources are

557
00:33:05,460 --> 00:33:08,750
and we have information extraction text

558
00:33:08,760 --> 00:33:12,720
mean on semantic

559
00:33:12,770 --> 00:33:18,280
on the information in the sources so that we can use this for searching for

560
00:33:18,290 --> 00:33:20,910
data mining for

561
00:33:22,970 --> 00:33:25,960
so much

562
00:33:25,980 --> 00:33:30,790
now in the information extraction from text

563
00:33:31,840 --> 00:33:33,270
most common

564
00:33:33,270 --> 00:33:35,890
it's point the fact that

565
00:33:35,900 --> 00:33:38,810
so you extract

566
00:33:40,640 --> 00:33:44,570
actually look for in terms of a system

567
00:33:44,600 --> 00:33:48,460
first of three or four

568
00:33:48,480 --> 00:33:50,530
but quite effective for

569
00:33:50,540 --> 00:33:53,470
that small to extract

570
00:33:53,520 --> 00:33:58,830
now what the four five and what

571
00:34:00,960 --> 00:34:02,120
i would like to have

572
00:34:02,140 --> 00:34:03,100
the image

573
00:34:03,130 --> 00:34:07,350
taking of fact extraction from

574
00:34:07,370 --> 00:34:10,770
but on the other hand i think we should

575
00:34:10,830 --> 00:34:13,390
go beyond fact extraction

576
00:34:15,520 --> 00:34:23,810
information extraction and text mining so i'm like going go to war and information extraction

577
00:34:23,820 --> 00:34:27,200
there are really implemented with

578
00:34:27,280 --> 00:34:32,830
part of our culture that we're going to but they it gives you idea of

579
00:34:32,830 --> 00:34:35,760
what we heard in search

580
00:34:35,770 --> 00:34:40,210
we've demonstrated also with current application

581
00:34:40,270 --> 00:34:44,790
and of course the summer school in information retrieval

582
00:34:44,820 --> 00:34:46,600
so i would like to know

583
00:34:46,620 --> 00:34:54,830
however chapter that integrate with information to the model

584
00:34:54,830 --> 00:35:02,140
so that you how we can use the semantic meaning information extraction in real

585
00:35:03,220 --> 00:35:05,640
so if i j

586
00:35:05,700 --> 00:35:10,960
and the first one is the way we have this introduction

587
00:35:11,280 --> 00:35:15,590
and then i go all the time

588
00:35:15,600 --> 00:35:18,730
use symbolic handcrafted knowledge

589
00:35:18,780 --> 00:35:22,220
we information extraction and text mining

590
00:35:22,270 --> 00:35:24,780
then the rest

591
00:35:26,290 --> 00:35:30,750
they are about machine learning

592
00:35:31,070 --> 00:35:34,890
the second part focused more on learning

593
00:35:35,140 --> 00:35:36,870
structures objects

594
00:35:38,460 --> 00:35:40,160
in text

595
00:35:40,210 --> 00:35:48,290
the force on application i'll go a lot of education maybe that technically

596
00:35:49,330 --> 00:35:56,050
like for many many applications for example and then finally the

597
00:35:57,050 --> 00:36:00,600
we integrate to

598
00:36:04,140 --> 00:36:05,770
i will start with

599
00:36:05,810 --> 00:36:07,030
all you

600
00:36:07,040 --> 00:36:14,860
the first part i start becoming what are we capable of to information

601
00:36:14,870 --> 00:36:21,040
we look at the role of natural language processing

602
00:36:21,040 --> 00:36:21,790
green and the red

603
00:36:22,460 --> 00:36:25,250
apart from some artifacts here which i think are artifacts

604
00:36:25,790 --> 00:36:27,750
and it's got the blue all separate as well

605
00:36:28,300 --> 00:36:28,740
very nice

606
00:36:29,540 --> 00:36:32,950
so that's generative models are working you can do this but we had to use

607
00:36:32,970 --> 00:36:37,130
four hundred samples the more samples you use the computational complexity goes up

608
00:36:38,910 --> 00:36:40,930
okay so deuterium is the same idea

609
00:36:43,120 --> 00:36:43,700
the difference in

610
00:36:44,150 --> 00:36:44,810
the time

611
00:36:45,320 --> 00:36:45,970
and were

612
00:36:46,030 --> 00:36:49,100
two differences i would say the there one difference when

613
00:36:50,880 --> 00:36:54,790
when the car i did his density networks he proposed using a neural network

614
00:36:55,400 --> 00:36:56,870
and duty and propose using

615
00:36:57,290 --> 00:37:01,420
and obvious network right so this is because it's the days the neural networks that

616
00:37:01,420 --> 00:37:05,020
was a big difference ones and i'll be fewer arguments about which these things better

617
00:37:05,220 --> 00:37:07,740
i think today we will defeat those two things as mappings

618
00:37:08,500 --> 00:37:12,710
is nonlinear mappings and in fact you could do these approaches with kernel methods

619
00:37:13,380 --> 00:37:16,710
and indeed i have done their duty and with kernel methods and in

620
00:37:17,130 --> 00:37:21,760
starting on the road is what led to the last model all talk about which is something i developed

621
00:37:24,890 --> 00:37:28,310
two differences one mapping is now an obvious network as a poster

622
00:37:28,750 --> 00:37:33,120
this mapping is i neural network right it doesn't matter the both mappings

623
00:37:33,680 --> 00:37:37,800
that's not really difference and today we will probably be using a kernel mapping unless

624
00:37:37,810 --> 00:37:40,440
we have some particular structural reason for using a neural network

625
00:37:41,920 --> 00:37:44,620
but what your second difference ladies points out on a grid

626
00:37:47,280 --> 00:37:49,300
and then map the grid so sample

627
00:37:50,580 --> 00:37:52,170
lay them out on a grid and then

628
00:37:55,360 --> 00:37:58,510
this is an example that this is a mixture of gaussians

629
00:37:58,960 --> 00:38:03,380
and what's going on is that these is a mixture of gaussians in three dimensional

630
00:38:03,380 --> 00:38:06,060
space where the centers of the galcians

631
00:38:08,470 --> 00:38:12,390
determined by the duty mapping from here to here so these crosses are mapped

632
00:38:12,770 --> 00:38:14,790
without any uncertainty just directly

633
00:38:16,060 --> 00:38:19,880
become the means over mixture of gaussians in the data space yeah

634
00:38:20,580 --> 00:38:21,240
as i make sense

635
00:38:21,910 --> 00:38:27,420
so this is like a constrained mixture gaussians the means are constrained to living on a two dimensional space

636
00:38:28,250 --> 00:38:30,380
an infinite mixture of gaussians by ian

637
00:38:32,070 --> 00:38:36,580
now that's exactly the same algorithm as the importance sampling algorithm i was talking about before

638
00:38:37,110 --> 00:38:39,700
fundamentally when you write it down interpretation is different

639
00:38:40,380 --> 00:38:44,210
but the algorithm is exactly the same mass comes out exactly the same

640
00:38:44,920 --> 00:38:48,100
one flaw in the which people don't really mention if this

641
00:38:49,030 --> 00:38:53,240
what you want to do is you want to the visualization cited mention at uh

642
00:38:53,260 --> 00:38:56,260
for the density networks you want to visualize in this space

643
00:38:56,890 --> 00:38:59,570
i the posterior distribution of x given why

644
00:39:00,010 --> 00:39:01,370
yeah the problem posterior

645
00:39:01,870 --> 00:39:06,550
probability x given what ants in density networks that's been done by

646
00:39:07,050 --> 00:39:10,370
importance sampling you compute the posterior by importance sampling

647
00:39:11,730 --> 00:39:12,640
in the duty em

648
00:39:14,420 --> 00:39:16,680
you can compute the posterior exactly

649
00:39:17,780 --> 00:39:19,560
you can compute what

650
00:39:22,350 --> 00:39:25,410
i mean you can compute using which means

651
00:39:25,820 --> 00:39:27,710
which calcium this cross must come from

652
00:39:29,070 --> 00:39:33,400
you compute posterior that says uh with fifty percent came from here with percent came

653
00:39:33,400 --> 00:39:35,160
from here with twenty percent came from here

654
00:39:36,730 --> 00:39:38,190
they then visualized at

655
00:39:40,380 --> 00:39:43,800
o point five times for this last point three times the plus point two times

656
00:39:43,800 --> 00:39:47,380
at any other point that's how the visualizer but that's actually nonsensical

657
00:39:47,800 --> 00:39:51,660
because in truth it must have come from you that's that's all that's

658
00:39:52,640 --> 00:39:57,250
it can't come from in between because there was no mean being mapped remembered three

659
00:39:57,750 --> 00:40:02,620
in density networks is a real subtlety in density networks use a sample based approach

660
00:40:02,740 --> 00:40:06,760
where you're saying is starting with a continuous space mapping to a continuous space so

661
00:40:06,760 --> 00:40:10,410
we need the posterior it makes sense that you can visualize things in a continuous

662
00:40:11,020 --> 00:40:15,200
teaching them is starting with the discrete space and mapping to a continuous space so

663
00:40:15,200 --> 00:40:19,510
that the theory is really continuous but actually you can visualize exactly as you do

664
00:40:20,520 --> 00:40:24,440
in density networks and you can make the same interpretation of the model is density

665
00:40:24,440 --> 00:40:29,200
networks but that's cleared up in the papers but these two models are really almost

666
00:40:29,200 --> 00:40:33,260
exactly the same thing only this one's laid out on a grid of points whereas

667
00:40:33,260 --> 00:40:36,910
the other one sample from a guassian well show the results over

668
00:40:37,290 --> 00:40:38,390
o thing i described a lot

669
00:40:39,370 --> 00:40:43,960
this is a visualization the oil data with the ten by ten grid so hundred points just like the first

670
00:40:45,020 --> 00:40:47,450
you can really see the grading of bacteria look

671
00:40:48,120 --> 00:40:51,450
the effect always talking about here you can see how they line up along the

672
00:40:51,450 --> 00:40:53,020
grid this is the spacing and the grid

673
00:40:53,440 --> 00:40:54,870
and you just seeing the effect there

674
00:40:55,520 --> 00:40:57,740
if you user four hundred points

675
00:40:58,630 --> 00:40:59,320
looks a bit better

676
00:41:00,190 --> 00:41:01,650
you can still see the effect the grid

677
00:41:04,160 --> 00:41:06,160
nine hundred points looks better again

678
00:41:07,400 --> 00:41:08,690
at i didn't go beyond nine hundred

679
00:41:10,300 --> 00:41:12,970
but you can still see the effect the great on talk a little bit about

680
00:41:13,280 --> 00:41:15,290
how the why effect occurs in a moment

681
00:41:17,300 --> 00:41:18,080
but but but but

682
00:41:19,780 --> 00:41:22,280
they also something else i wanted say i

683
00:41:22,380 --> 00:41:26,970
okay well there was something else but i can't remember this is that's sick man running stick man

684
00:41:30,630 --> 00:41:32,990
you can see the circles the data there but you can also see

685
00:41:33,490 --> 00:41:36,030
how everything's landing on these grid points here

686
00:41:37,170 --> 00:41:41,350
the percent what's happening is the posterior mean the gaussier the posterior

687
00:41:42,430 --> 00:41:47,280
component the calcium is basically saying i'm associated with at mean i'm associated with at

688
00:41:47,280 --> 00:41:51,460
me no one saying that jointly associated with multiple means and i'll explain what happens

689
00:41:51,460 --> 00:41:51,960
in a moment

690
00:41:52,540 --> 00:41:55,930
well next actually as we go to nine hundred points you start seeing

691
00:41:56,710 --> 00:41:57,970
a nice shape yeah

692
00:42:00,050 --> 00:42:02,210
okay so i call at affect the bubble wrap effect

693
00:42:04,600 --> 00:42:06,290
it seems very nice what's going on

694
00:42:06,690 --> 00:42:08,790
is easy to think if you've got these means in

695
00:42:09,450 --> 00:42:10,950
this is the bubble wrap factor

696
00:42:12,370 --> 00:42:13,520
means in latent space

697
00:42:14,890 --> 00:42:16,030
on this grid layout

698
00:42:18,890 --> 00:42:21,710
and that's it is almost like a density but it's like a bubble wrap density

699
00:42:21,710 --> 00:42:23,930
but if you put them in close enough together then the

700
00:42:24,500 --> 00:42:28,140
still overlap and it's like a pseudoinverse moved out things you see what i mean

701
00:42:28,270 --> 00:42:32,280
being a like a bubble wrap is the galcians sent the crosses in latent space

702
00:42:33,210 --> 00:42:35,940
but if i'm at this today displays

703
00:42:39,060 --> 00:42:39,460
in a sort

704
00:42:39,940 --> 00:42:41,400
only sort of nonlinear way

705
00:42:42,920 --> 00:42:48,640
these bubbles still don't matter go through the mapping actually but there exist in latent space then existing data space

706
00:42:50,030 --> 00:42:52,040
they can be a long way apart from each other

707
00:42:53,670 --> 00:42:59,050
um and that's is what gives you the effect on the posterior distributions

708
00:43:02,110 --> 00:43:04,490
if these are two galcians close together

709
00:43:05,000 --> 00:43:06,950
then this is the joint distribution the purple

710
00:43:08,150 --> 00:43:10,480
so these are like to these are the two gaussians in this case

711
00:43:11,210 --> 00:43:15,130
and if you want if you've got a data point along here you want to know its posterior being

712
00:43:15,940 --> 00:43:18,010
in fact distribution it looks like this

713
00:43:19,110 --> 00:43:20,260
nice smooth yeah

714
00:43:20,650 --> 00:43:23,310
so any they quite long hair could be even these guys

715
00:43:23,940 --> 00:43:26,280
but actually move these gaskins apart

716
00:43:29,220 --> 00:43:33,490
when you in this region here which is really unlikely the posterior distribution is the

717
00:43:33,500 --> 00:43:38,220
joint distribution is the posterior starts to become really abrupt and basically the a transition

718
00:43:38,480 --> 00:43:42,350
so any point or living in the region between these two

719
00:43:42,770 --> 00:43:44,940
will even be allocated so that's all backed

720
00:43:48,160 --> 00:43:52,210
and that means that when you look at the visualize their posterior component value

721
00:43:52,660 --> 00:43:56,190
they end up being associated with the that's all that's not in between

722
00:43:56,190 --> 00:44:00,370
well again that is again by mean one i think these actions they get these

723
00:44:00,370 --> 00:44:06,570
values but is to go OK you can see the inductive reasoning going on here

724
00:44:06,580 --> 00:44:11,860
and exactly what should appear so the quality of taking states

725
00:44:12,110 --> 00:44:15,700
taking action a in state i at time t that means to teases is to

726
00:44:17,050 --> 00:44:18,610
it's just recursively

727
00:44:18,620 --> 00:44:21,940
reward i get show how to point

728
00:44:21,990 --> 00:44:24,290
is point

729
00:44:27,160 --> 00:44:30,090
i don't think they would actually one

730
00:44:30,150 --> 00:44:34,200
would be stolen

731
00:44:37,290 --> 00:44:41,110
so i'm looking at that at at the top of the equation

732
00:44:41,930 --> 00:44:46,350
the quality of taking action a in state s it is it to go is

733
00:44:46,350 --> 00:44:50,830
the immediate reward i get past the expectation over

734
00:44:50,870 --> 00:44:53,580
this debate and i would get two in the next steps

735
00:44:53,680 --> 00:44:56,190
and values i know i get this

736
00:44:56,810 --> 00:45:01,150
they start with the one you get a keyword search so you so these are

737
00:45:01,150 --> 00:45:02,870
the ones

738
00:45:02,890 --> 00:45:05,740
let's the same be one

739
00:45:05,760 --> 00:45:07,950
this we know that the TV

740
00:45:08,010 --> 00:45:09,330
really a key one

741
00:45:09,340 --> 00:45:11,100
so i q two

742
00:45:11,110 --> 00:45:13,280
and from q two we can get

743
00:45:13,290 --> 00:45:14,520
the two

744
00:45:14,530 --> 00:45:17,100
OK so just for all t

745
00:45:17,160 --> 00:45:19,290
now if you discount gamma

746
00:45:20,530 --> 00:45:22,240
less than one

747
00:45:22,300 --> 00:45:25,650
then you can guarantee that this will converge

748
00:45:25,710 --> 00:45:27,070
the linear range

749
00:45:27,070 --> 00:45:28,190
to the star

750
00:45:28,230 --> 00:45:32,570
intuitively is very obvious because

751
00:45:32,620 --> 00:45:36,040
say gamma is point nine

752
00:45:36,180 --> 00:45:37,360
and reasoning

753
00:45:39,160 --> 00:45:41,650
one hundred decision stages

754
00:45:41,660 --> 00:45:43,730
would only gamma gamma hundreds

755
00:45:43,740 --> 00:45:45,110
times that value

756
00:45:45,180 --> 00:45:48,930
so at some point this becomes so small become less than

757
00:45:48,970 --> 00:45:50,690
you know when he negative

758
00:45:50,730 --> 00:45:52,070
sixteen or

759
00:45:52,070 --> 00:45:57,290
new or numerical precision on a computer which is i really care about

760
00:45:57,310 --> 00:45:59,490
so you only need to find

761
00:45:59,510 --> 00:46:01,570
you're going to go so he steps back

762
00:46:01,580 --> 00:46:04,590
so you don't really care about the reward you get beyond that the number of

763
00:46:05,360 --> 00:46:09,080
and that's the that's very intuitive obvious proof and why

764
00:46:09,140 --> 00:46:10,650
with discounting

765
00:46:10,760 --> 00:46:13,170
this converges to the optimal value function

766
00:46:18,100 --> 00:46:24,590
and these can also one

767
00:46:24,670 --> 00:46:28,190
in game less than one

768
00:46:28,410 --> 00:46:32,240
the to what participated

769
00:46:42,570 --> 00:46:50,650
well get rid thing is we don't know

770
00:46:54,030 --> 00:46:58,310
because in the beginning the markovian assumption tells us that that's all we care about

771
00:47:00,510 --> 00:47:04,470
w this is exactly what we need to become an assumption

772
00:47:04,520 --> 00:47:07,060
it breaks down the reasoning into

773
00:47:07,580 --> 00:47:09,150
single step

774
00:47:12,100 --> 00:47:16,850
but the nice thing is it's a mechanism has had one that you can always

775
00:47:16,850 --> 00:47:19,650
put information state to make state

776
00:47:19,700 --> 00:47:22,570
markovian you transition looks markovian

777
00:47:22,620 --> 00:47:25,380
the only depend on the list

778
00:47:27,740 --> 00:47:29,830
so this value iteration

779
00:47:29,890 --> 00:47:32,970
this will give you the starring if you get the star

780
00:47:33,000 --> 00:47:35,220
then you can determine the policy

781
00:47:35,430 --> 00:47:39,100
how to act to achieve their full value

782
00:47:39,150 --> 00:47:41,210
and you're done

783
00:47:41,210 --> 00:47:45,780
OK for the model based case

784
00:47:45,790 --> 00:47:49,270
OK now there's two have and the risk of today

785
00:47:49,320 --> 00:48:01,820
one value iteration

786
00:48:02,410 --> 00:48:07,070
and it's very easy to derive first principle because of inductive reasoning

787
00:48:07,110 --> 00:48:10,040
the other out of policy iteration

788
00:48:10,120 --> 00:48:12,140
but it's

789
00:48:12,140 --> 00:48:17,510
by shimon more and he did this in the sixties and seventies he actually recognise

790
00:48:17,510 --> 00:48:22,520
and of course many people recognised that this jellyfish was arrested and isolated the actual

791
00:48:22,520 --> 00:48:28,600
protein and determined to prove that this protein was sufficient to cause the fluorescence so

792
00:48:28,600 --> 00:48:32,730
in terms of thinking about applications of y so exciting that you have this fluorescent

793
00:48:32,730 --> 00:48:36,040
protein other than it's always really fun to look at things that are for us

794
00:48:36,050 --> 00:48:41,370
that we can think about in terms of biology why so exciting actually tag this

795
00:48:41,370 --> 00:48:45,790
protein to any other protein that you're studying and i have a visual handle on

796
00:48:45,790 --> 00:48:47,750
what's going on so for example

797
00:48:47,800 --> 00:48:52,200
if you're interested in some protein involved in cancer you can tag it with GFP

798
00:48:52,230 --> 00:48:57,120
you could watch were localised in the cell therefore as you can use to determine

799
00:48:57,120 --> 00:49:02,130
what other proteins it interacts with you can see for example when it's expressed in

800
00:49:02,130 --> 00:49:05,180
the cancer cell and where it expressed there are all sorts of things you can

801
00:49:05,180 --> 00:49:08,160
do once you can visualize something with reference

802
00:49:08,230 --> 00:49:13,620
the reason is so exciting that it's protein the protein this is the structure here

803
00:49:13,620 --> 00:49:16,910
is the ribbon structures you can kind of see what it looks like it's made

804
00:49:16,910 --> 00:49:21,410
up of all natural amino acids so this means we can code protein DNA you

805
00:49:21,410 --> 00:49:24,040
don't have to worry how my going to get into the cell

806
00:49:24,100 --> 00:49:28,210
all you have to do is to mutate the DNA which is very straightforward to

807
00:49:28,210 --> 00:49:30,210
do in in molecular biology

808
00:49:30,230 --> 00:49:34,520
and now you can tag absolutely any protein that you're interested in in so as

809
00:49:34,520 --> 00:49:39,060
i said this was first discovered and isolated from the jellyfish this was done by

810
00:49:39,060 --> 00:49:43,010
shimon more that was kind of the first step in this process of having become

811
00:49:43,020 --> 00:49:44,510
such a useful tool

812
00:49:44,620 --> 00:49:48,070
and then many years later and until nineteen ninety four

813
00:49:48,080 --> 00:49:50,120
dean martin chelsea at columbia

814
00:49:50,130 --> 00:49:53,940
show that yes i can in fact take the DNA and put it into a

815
00:49:53,940 --> 00:49:57,480
different organism and he put it into equal bacteria

816
00:49:57,490 --> 00:49:58,750
and what he can show

817
00:49:58,810 --> 00:50:02,430
was that it could be expressed as the picture from his nineteen ninety four science

818
00:50:02,430 --> 00:50:06,120
paper in that life and it is going to florence green

819
00:50:06,160 --> 00:50:10,760
now the first application can not be quite exciting as for example the other organisms

820
00:50:10,760 --> 00:50:15,180
people have put it the sense that you can have flies using transgenic mice that

821
00:50:15,180 --> 00:50:20,830
are glowing green with this GFP of course that's not be useful application for its

822
00:50:20,830 --> 00:50:23,660
more proof of principle but it does show you that you can put it in

823
00:50:23,680 --> 00:50:26,290
the first study any organisms

824
00:50:26,300 --> 00:50:31,520
and the field was really pushed forward by the discoveries of robert chance at UC

825
00:50:31,520 --> 00:50:32,660
san diego

826
00:50:32,670 --> 00:50:37,550
what he did was he actually figured out how it was that this protein four

827
00:50:37,560 --> 00:50:42,850
asked what caused the actual for instance and once he did that both he and

828
00:50:42,850 --> 00:50:47,940
many other scientists could then once they understood what caused the fluorescence make little changes

829
00:50:47,960 --> 00:50:53,000
to the actual protein and to what the properties of that the protein were so

830
00:50:53,000 --> 00:50:57,250
now for example there are a whole range just the whole rainbow fluorescent proteins that

831
00:50:57,250 --> 00:51:01,250
can be used and i'm sure you can imagine that if you want to label

832
00:51:01,250 --> 00:51:04,880
one protein green and one red one yellow now you can start looking at really

833
00:51:04,880 --> 00:51:07,370
complex biological processes

834
00:51:07,380 --> 00:51:12,810
so it's pretty rare that chemistry makes everything so hopefully all look in the normal

835
00:51:12,810 --> 00:51:17,200
papers not just the scientific journal get to read something about chemistry so it's fun

836
00:51:17,200 --> 00:51:21,570
to see how it's described in the new york times in the boston globe and

837
00:51:21,570 --> 00:51:25,190
the other thing i wanted to mention i'm not sure if exhibit still there but

838
00:51:25,190 --> 00:51:28,400
there was an exhibit of of jellyfish i know at least until last year at

839
00:51:28,400 --> 00:51:32,360
the boston museum of science in all of you guys can get in free

840
00:51:32,410 --> 00:51:37,040
need to see the glowing jellyfish and think about the fluorescent protein that in them

841
00:51:37,050 --> 00:51:40,140
so i encourage you to do that the next time you have some free time

842
00:51:40,140 --> 00:51:42,460
on your hands maybe i i p

843
00:51:42,470 --> 00:51:44,930
or something like that

844
00:51:45,070 --> 00:51:49,560
right so i like to move into today's know today we're talking about molecular orbital

845
00:51:50,490 --> 00:51:54,460
this is a show this is a new topic that we're starting

846
00:51:54,480 --> 00:51:59,840
so far we have exclusively been using lewis structures anytime we've tried to describe bonding

847
00:51:59,840 --> 00:52:01,130
within molecules

848
00:52:01,180 --> 00:52:05,530
lewis structures are really useful we use them all the time in chemistry and they

849
00:52:05,530 --> 00:52:09,410
are useful because first of all there are easy to depict the easy to draw

850
00:52:09,510 --> 00:52:14,140
relatively easy once we got get all the rules down and also the accurate over

851
00:52:14,140 --> 00:52:15,810
ninety percent of the time

852
00:52:15,820 --> 00:52:20,600
but they are not accurate all the time in predicting bonding between and within molecules

853
00:52:20,630 --> 00:52:24,160
and the reason for this is because lewis structures are not in fact based on

854
00:52:24,160 --> 00:52:25,630
quantum mechanics

855
00:52:25,730 --> 00:52:30,700
so molecular orbital theory on the other hand is based on quantum mechanics

856
00:52:30,710 --> 00:52:34,980
and specifically ML theory is a quantum mechanical description

857
00:52:34,990 --> 00:52:37,710
wave functions within molecules

858
00:52:37,770 --> 00:52:42,490
the saying way functions within molecules might sound a little confusing but remember we spend

859
00:52:42,510 --> 00:52:45,810
a lot of time talking about wave functions within atoms

860
00:52:45,830 --> 00:52:49,850
and we know how to describe that we know the wavefunction just means an atomic

861
00:52:49,850 --> 00:52:55,340
orbital it's the same thing with molecules and molecular function just means a molecular orbital

862
00:52:55,350 --> 00:52:58,980
so we'll start talking about the two kinds of of molecular orbitals we can talk

863
00:52:58,980 --> 00:53:01,560
about bonding or antibonding orbitals

864
00:53:01,570 --> 00:53:06,110
then we're going to actually use ML theory to describe bonding with in these molecules

865
00:53:06,200 --> 00:53:11,280
and we'll start with former nuclear diatomic molecules diatomic means that

866
00:53:11,320 --> 00:53:15,850
diatomic made up of two atoms and homer nuclear means that those two are the

867
00:53:15,850 --> 00:53:16,940
same pattern

868
00:53:17,040 --> 00:53:21,960
then the and looking a at an example of the heteronuclear diatomic molecules so again

869
00:53:21,960 --> 00:53:24,440
the same thing but now two different atoms

870
00:53:24,480 --> 00:53:28,710
so i will point out in terms of ML theory because it rigorously does take

871
00:53:28,710 --> 00:53:34,730
into account quantum mechanics it starts to become complicated once we go beyond diatomic molecules

872
00:53:34,730 --> 00:53:37,720
so we're going to limit our discussion in five eleven one

873
00:53:37,740 --> 00:53:42,510
four molecular orbital theory to diatomic molecules however on friday we will use a different

874
00:53:42,510 --> 00:53:46,320
approach that we can talk about bonding with the atoms that have more than two

875
00:53:46,390 --> 00:53:50,940
more than two atoms molecules with more than two atoms

876
00:53:50,980 --> 00:53:54,280
right so one thing that i first want to point out about theory that is

877
00:53:54,280 --> 00:54:00,930
the big difference from lewis structures that theory valence electrons are d localized over the

878
00:54:00,930 --> 00:54:02,610
entire molecule

879
00:54:02,660 --> 00:54:08,280
so we talked about lewis structures we actually assigned electrons to individual atoms or to

880
00:54:08,280 --> 00:54:14,210
individual bonds whereas molecular orbital theory what i'm telling you is instead we understand that

881
00:54:14,210 --> 00:54:18,790
the electrons are spread all over the molecule they're not just associated with a single

882
00:54:18,790 --> 00:54:19,560
i view

883
00:54:20,060 --> 00:54:22,000
original set s

884
00:54:22,000 --> 00:54:24,140
and the top could be estes

885
00:54:24,160 --> 00:54:26,470
and in between you have all the

886
00:54:26,470 --> 00:54:30,040
other parties although all the subsets of s

887
00:54:30,260 --> 00:54:33,770
so now

888
00:54:33,780 --> 00:54:34,910
he will

889
00:54:34,930 --> 00:54:38,370
so there is another big big big two and says you

890
00:54:38,390 --> 00:54:41,240
take a structure which is completely

891
00:54:41,250 --> 00:54:44,220
they make function

892
00:54:44,280 --> 00:54:47,110
there is always this

893
00:54:47,140 --> 00:54:50,540
for the is written can be very nice to read it

894
00:54:50,550 --> 00:54:51,750
when you have ten

895
00:54:51,760 --> 00:54:52,780
it's just

896
00:54:52,830 --> 00:54:54,020
very easy

897
00:54:54,020 --> 00:54:56,950
to show that the fixed point exists on how do you define it

898
00:54:56,960 --> 00:54:59,350
you define its way

899
00:54:59,380 --> 00:55:00,740
we're actually

900
00:55:00,750 --> 00:55:02,510
you take

901
00:55:02,530 --> 00:55:04,720
the least

902
00:55:04,860 --> 00:55:06,410
so the

903
00:55:08,220 --> 00:55:09,940
lower bound

904
00:55:16,200 --> 00:55:19,110
course fixed point

905
00:55:19,850 --> 00:55:26,530
a particular case of the otherwise is what the quality here

906
00:55:28,010 --> 00:55:29,770
picture which may

907
00:55:29,790 --> 00:55:31,550
help your understanding

908
00:55:31,710 --> 00:55:33,600
so it is and

909
00:55:33,620 --> 00:55:35,120
looks like this

910
00:55:35,130 --> 00:55:37,090
he's telling

911
00:55:37,140 --> 00:55:42,960
we just another so it's sort of the we very often draw the belly

912
00:55:42,970 --> 00:55:47,310
because you may have a point here which is not and there all the here

913
00:55:47,330 --> 00:55:50,400
because it's a partial order cannot compare them

914
00:55:50,410 --> 00:55:56,860
sure this one as this one greater than this one so

915
00:55:56,880 --> 00:55:59,950
so you could say

916
00:56:02,470 --> 00:56:06,440
islands where y

917
00:56:10,240 --> 00:56:11,820
greater than its

918
00:56:13,130 --> 00:56:18,330
a typical example is the were many elements of the latest it cannot be smaller

919
00:56:18,330 --> 00:56:20,550
than its

920
00:56:20,640 --> 00:56:24,120
its image

921
00:56:24,140 --> 00:56:25,340
and here

922
00:56:25,380 --> 00:56:26,330
if you take

923
00:56:26,340 --> 00:56:28,540
police one here

924
00:56:28,540 --> 00:56:31,630
this is a particular element

925
00:56:31,650 --> 00:56:34,790
which displays this

926
00:56:34,790 --> 00:56:38,500
as the list such that

927
00:56:40,570 --> 00:56:43,640
and actually it's very nice to have a full so

928
00:56:43,690 --> 00:56:50,490
i mean some dual way to think here we defined those such that z

929
00:56:50,500 --> 00:56:53,250
is less than half of that

930
00:56:53,260 --> 00:56:59,500
so those are called was fixed point is called reference points

931
00:56:59,510 --> 00:57:02,120
and the fighting in the middle here

932
00:57:02,140 --> 00:57:04,770
those are all the fixed points

933
00:57:06,550 --> 00:57:09,790
fixed points

934
00:57:10,060 --> 00:57:17,020
the list is this is you to also exist

935
00:57:21,310 --> 00:57:23,220
the latest thing here

936
00:57:23,220 --> 00:57:27,710
itself forms the latest

937
00:57:27,720 --> 00:57:29,070
general culture

938
00:57:29,130 --> 00:57:33,840
so what you ought to have

939
00:57:33,870 --> 00:57:38,100
is that you have another way to characterize

940
00:57:38,120 --> 00:57:40,260
well i would say

941
00:57:40,380 --> 00:57:41,750
way to compute the

942
00:57:45,830 --> 00:57:51,090
so the least fixpoint are now this one

943
00:57:51,290 --> 00:57:56,040
that's the least fix and this is the least fix point of

944
00:57:56,050 --> 00:57:58,570
the function f

945
00:57:58,600 --> 00:58:04,790
so this element here

946
00:58:04,980 --> 00:58:10,590
that's the way of writing when you have everything you need commuters use the lib

947
00:58:10,770 --> 00:58:15,250
f gives you the function

948
00:58:15,250 --> 00:58:16,300
so this one

949
00:58:16,320 --> 00:58:21,870
you can either see it as

950
00:58:22,040 --> 00:58:24,370
is the element

951
00:58:24,400 --> 00:58:27,790
of this set

952
00:58:27,800 --> 00:58:30,000
or you can also see it

953
00:58:30,010 --> 00:58:32,200
as something you capture

954
00:58:32,210 --> 00:58:34,260
by iterating

955
00:58:34,310 --> 00:58:36,480
the function f

956
00:58:36,510 --> 00:58:39,190
starting from here

957
00:58:39,210 --> 00:58:42,450
i'm going up

958
00:58:42,490 --> 00:58:44,270
so how do you go

959
00:58:44,290 --> 00:58:49,430
well the list element so if you take its image

960
00:58:50,390 --> 00:58:51,870
the show i mean

961
00:58:52,760 --> 00:58:55,390
the bottom is the list

962
00:58:56,600 --> 00:59:00,580
but because if is monotonic if i play after this

963
00:59:00,600 --> 00:59:02,050
in equation

964
00:59:02,050 --> 00:59:07,680
i get that at all but is less than ever played twice

965
00:59:12,280 --> 00:59:13,160
and so on

966
00:59:13,160 --> 00:59:22,890
so with cardinality argument things like this if you keep iterating f

967
00:59:22,890 --> 00:59:24,640
a lot

968
00:59:24,660 --> 00:59:26,740
with any luck

969
00:59:26,740 --> 00:59:28,580
so that means

970
00:59:28,620 --> 00:59:31,700
if i take the cardinal of my daughter of l

971
00:59:31,890 --> 00:59:33,910
two iterate f

972
00:59:33,950 --> 00:59:35,810
at these

973
00:59:37,950 --> 00:59:44,560
stabilisers i mean this i expected to stabilise somewhere stabiliser mean if i if i

974
00:59:44,560 --> 00:59:49,890
come to a point where the director so far for amount of time

975
00:59:49,890 --> 00:59:53,280
and if you happen to have something like this

976
00:59:53,370 --> 00:59:58,140
that i have a fixed point because f is monotone

977
00:59:59,510 --> 01:00:01,390
you don't know

978
01:00:01,430 --> 01:00:04,350
so what can be alpha

979
01:00:04,370 --> 01:00:06,530
one has to be at least

980
01:00:06,560 --> 01:00:09,390
as big as the cardinal

981
01:00:09,430 --> 01:00:10,240
o my

982
01:00:10,260 --> 01:00:12,600
of my set

983
01:00:13,910 --> 01:00:15,310
so you

984
01:00:15,350 --> 01:00:17,930
just give you an answer more

985
01:00:20,620 --> 01:00:23,760
increased number things but if you want to know more

986
01:00:23,790 --> 01:00:27,330
now wonderful books only fixed point

987
01:00:27,330 --> 01:00:30,950
so that's the way to compute it

988
01:00:30,970 --> 01:00:33,580
because what i do here a little

989
01:00:33,680 --> 01:00:39,660
that's the right here simply bottom bottom to about three

990
01:00:39,680 --> 01:00:40,740
go up

991
01:00:42,490 --> 01:00:46,370
and you can also and the

992
01:00:46,390 --> 01:00:47,870
greatest fixpoint

993
01:00:49,990 --> 01:00:52,290
as you can imagine

994
01:00:52,310 --> 01:00:55,390
you would catch it starting from the top

995
01:00:55,410 --> 01:00:59,720
and it's rating as because obviously

996
01:00:59,810 --> 01:01:04,160
the greatest element is necessarily as big as its image

997
01:01:04,180 --> 01:01:06,390
so another ten go down

998
01:01:07,280 --> 01:01:09,200
so but inclusion because

999
01:01:09,280 --> 01:01:15,180
now we are in the policy it's likely to come morning to

1000
01:01:15,180 --> 01:01:18,890
the particle eighties i told you about

1001
01:01:18,930 --> 01:01:22,120
subsets of nodes of the tree and so on

1002
01:01:22,140 --> 01:01:23,550
put them in

1003
01:01:23,580 --> 01:01:24,870
should put

1004
01:01:24,890 --> 01:01:29,330
less well as they did here so that they can take people

1005
01:01:29,350 --> 01:01:30,890
but not too

1006
01:01:30,910 --> 01:01:32,060
two hundred

1007
01:01:32,120 --> 01:01:34,740
right OK

1008
01:01:34,740 --> 01:01:37,700
you can present text URL

1009
01:01:37,720 --> 01:01:42,560
and the semantics of you using sports

1010
01:01:42,560 --> 01:01:45,830
so what do i have

1011
01:01:49,850 --> 01:01:51,430
so alphabet

1012
01:01:51,490 --> 01:01:55,140
she's going remember to integrate

1013
01:01:55,160 --> 01:01:58,060
information they used to decorate the no

1014
01:01:58,160 --> 01:01:59,620
my ministry

1015
01:01:59,680 --> 01:02:04,560
so in my life can will have proposition this is what i introduced already for

1016
01:02:04,810 --> 01:02:06,870
i would have for each such

1017
01:02:08,260 --> 01:02:10,600
the proposition that says

1018
01:02:10,770 --> 01:02:12,490
in this mode

1019
01:02:12,510 --> 01:02:16,350
well this node is labeled by a

1020
01:02:16,370 --> 01:02:19,060
so i have propositions p a

1021
01:02:19,120 --> 01:02:20,910
if you don't like p a

1022
01:02:21,910 --> 01:02:24,580
i mean locusts bee is fine

1023
01:02:24,600 --> 01:02:26,830
it's just proposition

1024
01:02:26,850 --> 01:02:28,680
you also have viable

1025
01:02:28,700 --> 01:02:30,580
this will be important because

1026
01:02:30,580 --> 01:02:32,200
as you will

1027
01:02:32,200 --> 01:02:36,260
each class and we noticed that the mall on top of one another

1028
01:02:39,180 --> 01:02:41,490
so we just need to find out maximum

1029
01:02:41,570 --> 01:02:43,470
and of course

1030
01:02:43,510 --> 01:02:45,240
and the covariance

1031
01:02:45,260 --> 01:02:47,930
it is just related to the cover charge

1032
01:02:47,930 --> 01:02:52,720
all the posterior of the joint likelihood in this case and again

1033
01:02:52,740 --> 01:02:58,510
it just gets a bit more technical but that's all it is the principle is

1034
01:02:58,510 --> 01:02:58,930
the same

1035
01:02:59,530 --> 01:03:01,030
we're no

1036
01:03:01,090 --> 01:03:05,640
the covariance is matrix is is not going to be an n k by n

1037
01:03:05,640 --> 01:03:07,010
k dimensional

1038
01:03:07,680 --> 01:03:11,070
matrix composed of this thing here

1039
01:03:12,780 --> 01:03:18,280
is basically component related to the prior the gaussian process prior to these are the

1040
01:03:19,430 --> 01:03:25,360
functions of the covariance matrices for each about individual classes these are in by n

1041
01:03:27,030 --> 01:03:28,200
and and then

1042
01:03:29,030 --> 01:03:36,030
matrix here if you remember will correspond to the elements are associated with our likelihood

1043
01:03:36,030 --> 01:03:37,930
so these are the second derivatives

1044
01:03:39,120 --> 01:03:40,660
with respect to

1045
01:03:40,680 --> 01:03:43,780
the gaussian process function values

1046
01:03:43,800 --> 01:03:46,410
of our our likelihood

1047
01:03:46,470 --> 01:03:49,390
and these are multinomial forum so

1048
01:03:49,430 --> 01:03:53,360
i won't bore you with the details but

1049
01:03:53,410 --> 01:03:56,840
it's just details

1050
01:03:59,320 --> 01:04:02,530
that's as we basically be done we we have

1051
01:04:02,550 --> 01:04:06,550
approximation for our our posterior it's goes in

1052
01:04:07,200 --> 01:04:08,780
and then we can do

1053
01:04:09,570 --> 01:04:14,120
we can make predictions and so forth

1054
01:04:14,180 --> 01:04:17,360
OK so there is

1055
01:04:17,370 --> 01:04:23,050
a small fly in the ointment as far as the laplace approximation is concerned

1056
01:04:23,070 --> 01:04:27,470
focusing processes it it's a particular weakness

1057
01:04:27,490 --> 01:04:29,600
and what's that well

1058
01:04:29,620 --> 01:04:33,090
the lord of the high dimensional goes in

1059
01:04:33,160 --> 01:04:34,870
may not necessarily

1060
01:04:34,910 --> 01:04:37,030
represent the actual mass

1061
01:04:37,050 --> 01:04:38,140
by the actual

1062
01:04:38,160 --> 01:04:40,300
probability mass

1063
01:04:40,320 --> 01:04:45,360
and we're not talking about very high dimensional go since one of the examples

1064
01:04:45,780 --> 01:04:51,200
you'll see we have something like three hundred data points

1065
01:04:51,220 --> 01:04:53,590
twenty six classes

1066
01:04:53,640 --> 01:04:59,670
so we know operating in new twenty six by three hundred dimensional space so we're

1067
01:04:59,670 --> 01:05:01,370
trying to define

1068
01:05:01,390 --> 01:05:05,200
goes in in that sort of dimensionality

1069
01:05:05,240 --> 01:05:07,470
so trying to find the more

1070
01:05:07,530 --> 01:05:13,800
it's actually quite difficult to know about finding what is is is one thing but

1071
01:05:13,800 --> 01:05:20,510
the more as i said will not capture not represent the actual probability mass

1072
01:05:20,510 --> 01:05:24,490
and you see from the experiments that this is indeed the case

1073
01:05:24,800 --> 01:05:30,720
the class approximation is good in the large sample limit

1074
01:05:30,720 --> 01:05:32,910
but we have very small numbers of samples

1075
01:05:32,930 --> 01:05:35,070
so this is not

1076
01:05:35,090 --> 01:05:38,410
a great approximation

1077
01:05:40,490 --> 01:05:44,870
we haven't about variational methods

1078
01:05:45,970 --> 01:05:49,280
during the lectures on on graphical models

1079
01:05:52,390 --> 01:05:54,990
what we're going to have to look at

1080
01:05:55,160 --> 01:05:57,740
variational approximations

1081
01:05:57,740 --> 01:06:01,740
in the variational approximations of course are

1082
01:06:01,820 --> 01:06:06,860
very general methods which can be used for any class of probabilistic modeling

1083
01:06:06,870 --> 01:06:07,870
i went to

1084
01:06:07,870 --> 01:06:09,760
focus really on the use of

1085
01:06:09,870 --> 01:06:14,700
variational methods for gaussian process classification

1086
01:06:14,720 --> 01:06:18,490
so what's the problem

1087
01:06:18,510 --> 01:06:25,200
well the problem is this posterior distribution we can get an analytic form for the

1088
01:06:26,300 --> 01:06:28,140
can we approximate

1089
01:06:28,200 --> 01:06:29,740
the posterior

1090
01:06:29,760 --> 01:06:32,510
it was a nice analytic forum well that's what we do

1091
01:06:32,620 --> 01:06:36,490
with the class approximation

1092
01:06:36,490 --> 01:06:41,200
plus approximation we say well we don't really know what the posterior looks like

1093
01:06:41,220 --> 01:06:46,970
and so let's approximate it was something that we understand understand the goes

1094
01:06:47,430 --> 01:06:48,360
but the the

1095
01:06:48,370 --> 01:06:52,140
problem is that we don't know how good this approximation is

1096
01:06:52,680 --> 01:06:55,660
where is it good when

1097
01:06:55,680 --> 01:06:57,340
so it would be

1098
01:06:57,390 --> 01:07:03,370
desirable if could have an approximation where we knew how good it was and some

1099
01:07:03,370 --> 01:07:06,470
sensible optimality

1100
01:07:09,120 --> 01:07:10,640
the way that we start

1101
01:07:10,660 --> 01:07:12,120
as we say OK

1102
01:07:12,120 --> 01:07:14,010
i've got a set of

1103
01:07:14,030 --> 01:07:20,220
random variables theta one to see to and what i want is this posterior

1104
01:07:20,240 --> 01:07:21,740
right it given

1105
01:07:21,760 --> 01:07:23,450
my observations

1106
01:07:23,470 --> 01:07:26,320
well i want an approximation to this snow

1107
01:07:26,370 --> 01:07:31,600
and one way that i can do this as i can see let me

1108
01:07:32,570 --> 01:07:39,280
probability distribution q

1109
01:07:39,370 --> 01:07:43,660
and i represent

1110
01:07:43,680 --> 01:07:45,240
my approximation

1111
01:07:45,240 --> 01:07:48,430
of the posterior

1112
01:07:48,450 --> 01:07:52,760
and product four

1113
01:07:55,120 --> 01:08:01,800
i've done two things i've simplified the problem by representing it in our product four

1114
01:08:01,860 --> 01:08:05,410
i still have the issue of what is this value of q

1115
01:08:05,430 --> 01:08:10,680
or what what is the representation of q

1116
01:08:13,120 --> 01:08:14,910
what we can do

1117
01:08:14,950 --> 01:08:17,550
as we can say look

1118
01:08:17,680 --> 01:08:19,470
i can represent

1119
01:08:19,550 --> 01:08:20,950
the logarithm

1120
01:08:20,970 --> 01:08:23,430
of my marginal likelihood

1121
01:08:24,340 --> 01:08:26,700
my doing

1122
01:08:26,740 --> 01:08:29,510
remember that i've got

1123
01:08:32,910 --> 01:08:36,620
given x and theta

1124
01:08:36,620 --> 01:08:38,700
p of

1125
01:08:38,840 --> 01:08:40,800
integrate over all scenes

1126
01:08:40,820 --> 01:08:46,070
so this gives me my marginal likelihood

1127
01:08:46,100 --> 01:08:52,070
so i can represent the log

1128
01:08:52,220 --> 01:08:54,490
marginal likelihood

1129
01:08:56,550 --> 01:09:04,620
the logarithm of the centre group

1130
01:09:04,640 --> 01:09:14,720
and what else to do

1131
01:09:14,910 --> 01:09:18,430
the result is to introduce this

1132
01:09:19,490 --> 01:09:24,450
ten here so q is some arbitrary distribution

1133
01:09:26,300 --> 01:09:27,910
so i haven't changed anything

1134
01:09:29,120 --> 01:09:33,950
let's just look at this a little bit more

1135
01:09:33,990 --> 01:09:35,970
if i write it

1136
01:09:35,990 --> 01:09:37,590
like this

1137
01:09:45,760 --> 01:09:52,910
does anyone

1138
01:09:52,910 --> 01:09:56,450
does this look familiar to anybody but this would be

1139
01:09:56,490 --> 01:10:00,280
this is an expectation of this is an average

1140
01:10:00,340 --> 01:10:03,760
all this function here

1141
01:10:05,530 --> 01:10:07,870
to this distribution

1142
01:10:07,910 --> 01:10:11,760
so i can i consider my marginal likelihood

1143
01:10:11,780 --> 01:10:17,620
the log my marginal likelihood is equal to the log of the expectations

1144
01:10:17,620 --> 01:10:19,090
with respect to

1145
01:10:19,120 --> 01:10:20,140
q theta

1146
01:10:20,160 --> 01:10:21,620
whatever qs

1147
01:10:21,640 --> 01:10:25,450
i'm seeing here is going to be some sort of product four

1148
01:10:25,470 --> 01:10:27,740
we have defined it so don't panic

1149
01:10:27,780 --> 01:10:30,050
and it's the expectation

1150
01:10:31,590 --> 01:10:32,860
p of t

1151
01:10:32,870 --> 01:10:35,240
given x

1152
01:10:35,430 --> 01:10:38,200
given seats

1153
01:10:39,140 --> 01:10:40,910
p of to

1154
01:10:40,970 --> 01:10:44,120
over q theta

1155
01:10:49,570 --> 01:10:54,050
the logarithm of course is a convex function

1156
01:10:54,050 --> 01:10:55,390
and so

1157
01:10:55,410 --> 01:10:57,860
we can define

1158
01:10:57,860 --> 01:11:00,930
a lower bound

1159
01:11:00,990 --> 01:11:05,370
on this logarithm of an expectation by writing

1160
01:11:05,510 --> 01:11:10,320
the expectation

1161
01:11:10,370 --> 01:11:12,430
of the logarithm

1162
01:11:12,450 --> 01:11:13,970
of this function

1163
01:11:13,990 --> 01:11:15,490
which is not just going to be

1164
01:11:15,490 --> 01:11:19,690
successive application rules two and three was rule one gone well actually it turns out

1165
01:11:19,690 --> 01:11:23,320
in that case it's and its it you can derive from the two or three

1166
01:11:23,320 --> 01:11:24,440
of them

1167
01:11:24,510 --> 01:11:28,590
and there are very explicit algorithms very complex

1168
01:11:28,650 --> 01:11:33,030
but they will in principle you know set up in the computer run and it

1169
01:11:33,030 --> 01:11:37,300
will it will tell you a is it possible or is it not possible to

1170
01:11:37,300 --> 01:11:41,670
derive this cause and effect in terms of the observation data and it's possible they

1171
01:11:41,670 --> 01:11:44,440
will tell you exactly what form

1172
01:11:44,490 --> 01:11:46,920
which is wonderful

1173
01:11:52,190 --> 01:11:56,900
the structure of the graph is yes

1174
01:11:56,920 --> 01:12:00,690
it's been around for so history

1175
01:12:02,440 --> 01:12:06,650
so there this

1176
01:12:06,670 --> 01:12:13,610
so so so doing asks or to clarify the structure of the

1177
01:12:13,670 --> 01:12:17,880
graph is given absolutely so the structure of the graph is not under discussion

1178
01:12:17,880 --> 01:12:21,440
we've we've come from somewhere we told the story we believe this is the structure

1179
01:12:21,440 --> 01:12:22,950
of the graph

1180
01:12:22,970 --> 01:12:26,010
on the basis of that we say can we now

1181
01:12:26,030 --> 01:12:31,300
use things we can observe the there the joint distribution of the observed the observable

1182
01:12:31,300 --> 01:12:37,220
variables in the observation regime can we get some formula putting the pieces together which

1183
01:12:37,220 --> 01:12:38,920
will finally come out with

1184
01:12:39,010 --> 01:12:45,740
the distribution of of some effect in some intervention regime which then has caused many

1185
01:12:45,740 --> 01:12:49,190
and we made enough assumptions you might or might not be able to do it

1186
01:12:49,340 --> 01:12:52,590
this will tell you what you can you can't and if you can it will

1187
01:12:52,590 --> 01:12:53,900
tell you

1188
01:12:53,920 --> 01:12:59,530
what is the

1189
01:12:59,550 --> 01:13:03,710
all right

1190
01:13:07,340 --> 01:13:09,320
so here

1191
01:13:09,320 --> 01:13:15,300
so the question the question is do they have to represent the unobserved variables in

1192
01:13:15,300 --> 01:13:18,570
the graph and the answer is definitely you have to understand the structure of the

1193
01:13:18,570 --> 01:13:22,070
whole system unobserved variables no

1194
01:13:22,110 --> 01:13:26,360
in order because that will determine the the

1195
01:13:26,400 --> 01:13:30,380
what you really need out of this is the various

1196
01:13:30,440 --> 01:13:34,260
extended conditional independence relationships involving

1197
01:13:34,280 --> 01:13:37,530
the observable variables and the intervention

1198
01:13:39,550 --> 01:13:44,400
but those what those depend on the structure of the whole graph including where the

1199
01:13:44,400 --> 01:13:48,550
unobservable variables are and how they joined by arrows to other things so i can

1200
01:13:48,550 --> 01:13:50,090
interrogate the graph

1201
01:13:50,110 --> 01:13:52,260
and get various things

1202
01:13:52,260 --> 01:13:55,840
out and i need you is here in these things and i need to use

1203
01:13:55,840 --> 01:14:00,970
that i couldn't just collapse over it because the whole thing would collapse to complete

1204
01:14:00,990 --> 01:14:03,590
unstructured mess

1205
01:14:03,610 --> 01:14:08,130
you need to know more structured ideally this is this you know you can you

1206
01:14:08,130 --> 01:14:10,740
can say what you even though you can observe that you can say the kind

1207
01:14:10,740 --> 01:14:13,400
of thing is you believe it is there

1208
01:14:13,420 --> 01:14:19,170
she returned to the number of

1209
01:14:19,190 --> 01:14:25,070
that means that you have to remember

1210
01:14:25,630 --> 01:14:29,420
what really

1211
01:14:29,440 --> 01:14:32,380
very good question so if you just got a lot of numbers and you have

1212
01:14:32,490 --> 01:14:37,530
really tried to model the to use the year that trouble you really need to

1213
01:14:37,530 --> 01:14:41,940
understand the relationship not only between the variables you got the between potential variables you

1214
01:14:41,940 --> 01:14:42,920
haven't got

1215
01:14:42,920 --> 01:14:46,610
in order to get anywhere and that's what we're coming to an XML move on

1216
01:14:48,300 --> 01:14:52,720
causal discovery how do you get these models in the first place well

1217
01:14:52,740 --> 01:14:58,300
i said one you can just make sensible arguments about coin tossing and mendelian randomisation

1218
01:14:58,470 --> 01:15:02,840
and justify conditional independencies justify these parts of these models

1219
01:15:02,880 --> 01:15:06,030
or you can look at data to try and

1220
01:15:06,030 --> 01:15:07,210
but that is

1221
01:15:07,220 --> 01:15:09,900
quite problematic but it's a big industry

1222
01:15:09,900 --> 01:15:13,480
log of the cardinality restrictions of functions in f

1223
01:15:13,500 --> 01:15:15,570
it should be iconographic if

1224
01:15:16,320 --> 01:15:19,550
the sequence x one to xn

1225
01:15:20,270 --> 01:15:22,940
and then we take expectation of that that's what

1226
01:15:22,940 --> 01:15:25,270
well with this appears so

1227
01:15:25,320 --> 01:15:28,800
you know there's this inequality between the conditional expectation

1228
01:15:28,840 --> 01:15:31,750
given the x one direct so averaging out the

1229
01:15:31,770 --> 01:15:36,480
the rademacher random variables plus minus ones that's no more than something like squared of

1230
01:15:37,050 --> 01:15:40,380
cardinality restrictions divided by n

1231
01:15:40,400 --> 01:15:44,980
OK and we can take expectation inside the this query log

1232
01:15:45,000 --> 01:15:46,170
OK this is

1233
01:15:46,190 --> 01:15:48,670
you know using using convexity

1234
01:15:49,170 --> 01:15:51,610
and yet instance inequality

1235
01:15:51,630 --> 01:15:54,550
and so we get a result that looks like this

1236
01:15:54,630 --> 01:15:59,110
rademacher averages in terms of expectation of this thing

1237
01:15:59,380 --> 01:16:02,650
the cardinality of these restrictions

1238
01:16:02,730 --> 01:16:06,730
OK so this is an expectation under the random choice of the

1239
01:16:06,750 --> 01:16:09,040
the x one to xn

1240
01:16:09,050 --> 01:16:10,320
right now

1241
01:16:10,340 --> 01:16:13,520
the growth function is the maximum cardinality

1242
01:16:13,550 --> 01:16:15,570
for all sequences of size n

1243
01:16:15,570 --> 01:16:18,770
so no matter what distribution we have the growth function will give us an upper

1244
01:16:18,770 --> 01:16:20,840
bound on this thing

1245
01:16:21,800 --> 01:16:24,570
so we can replace this with the growth function

1246
01:16:24,590 --> 01:16:26,170
and get

1247
01:16:26,230 --> 01:16:27,320
and get

1248
01:16:27,380 --> 01:16:30,400
a result in you know this is just throwing cells liver and the right the

1249
01:16:31,880 --> 01:16:36,920
here there is no more than the growth function which is you know the d

1250
01:16:36,920 --> 01:16:38,250
to the d

1251
01:16:38,340 --> 01:16:40,130
as long as n is

1252
01:16:40,150 --> 01:16:42,150
sufficiently large

1253
01:16:42,170 --> 01:16:48,460
right and then take the log of that we get this quantity delorean over d

1254
01:16:48,480 --> 01:16:53,610
all right so that's how we arrive at this result only question mark is why

1255
01:16:53,670 --> 01:16:57,400
is the expectation of this supreme

1256
01:16:57,400 --> 01:16:58,880
no bigger than

1257
01:16:58,960 --> 01:17:02,650
square root of the lower cardinality over and

1258
01:17:03,710 --> 01:17:05,000
well this uses

1259
01:17:05,020 --> 01:17:08,130
the hosting kind of ideas again

1260
01:17:08,130 --> 01:17:09,780
OK so this inequality

1261
01:17:09,800 --> 01:17:14,340
because the slide the question why is this true

1262
01:17:15,670 --> 01:17:17,190
well this is the hoeffding

1263
01:17:17,210 --> 01:17:19,440
the have kind of

1264
01:17:20,570 --> 01:17:22,440
by dr

1265
01:17:23,320 --> 01:17:30,170
let's think he is the result that implies that earlier result if we have some

1266
01:17:32,000 --> 01:17:35,900
of our and so bunch of vectors of length n

1267
01:17:35,920 --> 01:17:39,070
and all of them have euclidean norm

1268
01:17:39,070 --> 01:17:41,550
no more than i

1269
01:17:43,340 --> 01:17:47,710
then when we take the expectation of the supreme over all these vectors

1270
01:17:47,730 --> 01:17:50,940
of sigma i times i i

1271
01:17:50,960 --> 01:17:55,900
OK so this is an inner product between vector in a and this plus minus

1272
01:17:55,900 --> 01:17:57,300
one guy

1273
01:17:57,380 --> 01:17:59,070
plus minus one vector

1274
01:17:59,090 --> 01:18:04,320
OK the expectation that supreme is no more than the radius

1275
01:18:05,480 --> 01:18:09,520
right of all that is in a times square root of twice the log cardinality

1276
01:18:09,520 --> 01:18:11,440
of a

1277
01:18:11,460 --> 01:18:15,480
OK so in our case when we apply it to the set of vectors a

1278
01:18:15,590 --> 01:18:16,630
is just

1279
01:18:16,670 --> 01:18:18,340
the functions

1280
01:18:18,380 --> 01:18:23,610
f restricted to x one through x right et cetera makes one through xn

1281
01:18:23,630 --> 01:18:27,190
that is about that form as f ranges over class

1282
01:18:27,210 --> 01:18:29,050
OK so what's the

1283
01:18:29,360 --> 01:18:31,300
in our case

1284
01:18:31,360 --> 01:18:35,570
right the euclidean norm of these things is zero plus minus one vectors right so

1285
01:18:35,570 --> 01:18:38,860
the euclidean norm is the square root of n

1286
01:18:38,880 --> 01:18:42,420
all of them have the same non actually lie on the ball of radius squared

1287
01:18:46,630 --> 01:18:52,480
and the cardinality of a is the cardinality of the restrictions of functions in f

1288
01:18:52,480 --> 01:18:56,380
two x one xn so this this is precisely the result that we take one

1289
01:18:56,380 --> 01:18:58,630
over n times the

1290
01:18:59,360 --> 01:19:02,150
the euclidean norm which is square then that gives us one of the square of

1291
01:19:04,860 --> 01:19:07,400
times square to the log the cardinality of

1292
01:19:07,400 --> 01:19:11,270
a or as set of restrictions

1293
01:19:11,300 --> 01:19:13,070
OK so this is

1294
01:19:13,110 --> 01:19:15,590
this is an

1295
01:19:15,630 --> 01:19:21,360
a result this tells us about actually the rademacher averages of a finite class

1296
01:19:24,210 --> 01:19:27,170
and the proof here this is due to

1297
01:19:27,190 --> 01:19:28,780
pascal massage

1298
01:19:28,840 --> 01:19:32,250
the nice observation that that you can

1299
01:19:32,270 --> 01:19:33,570
get this kind of

1300
01:19:33,590 --> 01:19:38,150
upper bounds on rademacher averages for finite class using the

1301
01:19:38,170 --> 01:19:42,170
the ideas of postings inequality

1302
01:19:42,190 --> 01:19:44,750
so what we're doing here is

1303
01:19:44,780 --> 01:19:46,880
is considering

1304
01:19:49,940 --> 01:19:53,940
over a

1305
01:19:53,960 --> 01:19:55,020
right of

1306
01:19:55,040 --> 01:19:56,730
this summer

1307
01:19:57,650 --> 01:19:59,590
OK so let's take

1308
01:19:59,610 --> 01:20:03,400
e to the s times the expectation of this thing is the thing we're interested

1309
01:20:03,400 --> 01:20:08,800
in right the expectation of this is the maximum over set of the sum of

1310
01:20:08,800 --> 01:20:11,130
sigma i times i i

1311
01:20:11,190 --> 01:20:13,780
this this inner product

1312
01:20:13,800 --> 01:20:16,040
OK let's take either s times that

1313
01:20:16,050 --> 01:20:17,150
and then use

1314
01:20:18,110 --> 01:20:22,250
and yes inequality is to pull the expectation outside

1315
01:20:22,340 --> 01:20:25,360
right so this is no more than the expectation of the the s times that

1316
01:20:26,630 --> 01:20:30,980
and we can pull the supremum outside as well right that he the expectation of

1317
01:20:30,980 --> 01:20:32,710
the maximum over

1318
01:20:32,770 --> 01:20:34,020
vectors of

1319
01:20:34,040 --> 01:20:35,300
e the s

1320
01:20:35,320 --> 01:20:36,940
z so by

1321
01:20:37,610 --> 01:20:39,150
all this now

1322
01:20:39,150 --> 01:20:42,480
it is the moment generating function for this random variable

1323
01:20:43,000 --> 01:20:44,070
OK so

1324
01:20:44,090 --> 01:20:48,150
you know with this matrix of of taking an exponential either s times the quantity

1325
01:20:48,150 --> 01:20:54,250
were interested in and using instance inequality we have again a moment generating function

1326
01:20:54,280 --> 01:20:58,340
well not quite i guess story supreme in there we can use the

1327
01:20:58,360 --> 01:21:03,280
the fact that the supremum is of positive things so it's no more than a

1328
01:21:04,150 --> 01:21:08,730
of exponentials right so with bounding the supremum with the some

1329
01:21:08,750 --> 01:21:10,250
and this is not a bad

1330
01:21:10,300 --> 01:21:12,960
approximation here

1331
01:21:12,960 --> 01:21:15,250
right if the if the

1332
01:21:15,270 --> 01:21:17,500
maximum is dominated by one

1333
01:21:17,520 --> 01:21:19,050
one element

1334
01:21:19,050 --> 01:21:21,320
well that's it

1335
01:21:22,190 --> 01:21:25,570
if the sum is dominated by one element

1336
01:21:25,570 --> 01:21:27,400
and so of course

1337
01:21:27,420 --> 01:21:31,920
then we're in this situation of looking at the moment generating function for such a

1338
01:21:31,920 --> 01:21:33,170
random variable

1339
01:21:33,230 --> 01:21:35,230
OK and we use hoeffding inequality

1340
01:21:35,280 --> 01:21:36,380
to tell us

1341
01:21:36,400 --> 01:21:40,840
the that thing is no bigger than either squared times the two norm squared

1342
01:21:40,860 --> 01:21:42,750
over two right we saw this

1343
01:21:42,770 --> 01:21:45,150
we saw this in the last

1344
01:21:45,150 --> 01:21:47,400
in the last hour

1345
01:21:48,070 --> 01:21:51,090
you know i i just quoted there and i'm not going to

1346
01:21:51,150 --> 01:21:53,800
i'm not going to prove it here either

1347
01:21:53,860 --> 01:21:57,190
right but then that's that's no more than cardinality over

1348
01:21:57,230 --> 01:22:02,300
a of either a squared r-squared over two because our you could norms are always

1349
01:22:02,320 --> 01:22:05,040
no bigger than are

1350
01:22:05,090 --> 01:22:07,460
right and then taking logs

1351
01:22:07,480 --> 01:22:10,650
of both sides and while minimizing over s

1352
01:22:10,670 --> 01:22:12,840
and taking logs we get

1353
01:22:13,170 --> 01:22:15,340
so we get this result

1354
01:22:15,360 --> 01:22:16,800
right to this thing in here

1355
01:22:17,380 --> 01:22:22,020
there is no bigger than times the square root of

1356
01:22:22,040 --> 01:22:24,610
two log a cardinality

1357
01:22:24,710 --> 01:22:27,130
all rights is in a

1358
01:22:27,150 --> 01:22:29,110
no view of

1359
01:22:29,110 --> 01:22:33,090
and a trick to get back to this situation is getting nice pounds on on

1360
01:22:33,090 --> 01:22:36,940
the generating functions for random variables

1361
01:22:36,940 --> 01:22:40,180
slow but i don't have any

1362
01:22:40,190 --> 01:22:47,060
the reasons of this so as some researches should be done to analyse why decomposition

1363
01:22:47,080 --> 01:22:49,940
is it so bad

1364
01:22:49,970 --> 01:22:54,690
that you know

1365
01:22:56,710 --> 01:22:59,850
well before nineteen years in their

1366
01:22:59,870 --> 01:23:02,210
the force is large

1367
01:23:02,230 --> 01:23:04,520
the problem is so serious

1368
01:23:05,430 --> 01:23:10,730
with full knowledge of all you have in use composition has then this is large

1369
01:23:10,980 --> 01:23:16,960
then y is always more difficult so this effect but the company and not the

1370
01:23:16,960 --> 01:23:22,210
union then you clearly see is a big difference is that the coverage is just

1371
01:23:22,270 --> 01:23:24,830
very very slow

1372
01:23:25,210 --> 01:23:27,710
it may also be related to the starving

1373
01:23:27,770 --> 01:23:29,480
criterion or something

1374
01:23:29,870 --> 01:23:32,210
well but i

1375
01:23:32,250 --> 01:23:34,120
i don't know yet

1376
01:23:34,790 --> 01:23:40,640
but fortunately we don't need to use large c so what is sees a penalty

1377
01:23:41,390 --> 01:23:47,680
these we will overfit to fit the training data but in practice we can over

1378
01:23:47,730 --> 01:23:48,540
peter change

1379
01:23:48,540 --> 01:23:53,370
they have to we don't need very large city that is

1380
01:23:53,390 --> 01:23:55,810
the result saying that

1381
01:23:56,000 --> 01:24:01,930
there's no way using the years yet so they may not be linearly separable so

1382
01:24:03,270 --> 01:24:08,120
so for any c for any given to see if the use of dual problem

1383
01:24:08,120 --> 01:24:14,830
some props and they are only classifier that actually knows they that correspond to the

1384
01:24:14,830 --> 01:24:17,330
situation of right to PC

1385
01:24:17,330 --> 01:24:18,410
after about

1386
01:24:18,440 --> 01:24:23,230
it is not very precise but rather you for other bounded

1387
01:24:23,250 --> 01:24:27,890
four other bounding components they they are related to training error

1388
01:24:27,910 --> 01:24:33,660
we can actually prove that if this is large the

1389
01:24:33,680 --> 01:24:35,000
primal solution

1390
01:24:35,020 --> 01:24:40,160
the primal after about these issues and so there's no need to have to search

1391
01:24:40,160 --> 01:24:45,620
for you know you don't c into two to solve the problem

1392
01:24:45,660 --> 01:24:50,270
because if w is this and this is the same sort and the performance is

1393
01:24:50,270 --> 01:24:51,980
exactly the same

1394
01:24:52,060 --> 01:24:55,290
so you don't need to try to use very large c

1395
01:24:55,310 --> 01:24:57,160
then we can use another

1396
01:24:57,180 --> 01:25:02,370
another property that kind of situation that if the the number of

1397
01:25:02,390 --> 01:25:06,460
features is very it is not a large

1398
01:25:06,480 --> 01:25:12,770
so remember that in the primal dual relationship this time without is a linear combination

1399
01:25:12,790 --> 01:25:13,980
of training instances

1400
01:25:14,390 --> 01:25:19,790
so we have to delete these linear combinations of these studies and is the number

1401
01:25:19,790 --> 01:25:20,730
of features

1402
01:25:25,700 --> 01:25:28,180
somehow we can where we can prove

1403
01:25:28,210 --> 01:25:32,350
six property like these that the number of other five

1404
01:25:32,430 --> 01:25:36,790
as you can see is a very small than number of features

1405
01:25:36,790 --> 01:25:41,580
plus one so right

1406
01:25:41,580 --> 01:25:43,930
it is no more than number of features

1407
01:25:43,940 --> 01:25:48,910
yes mean even the number of features is very small

1408
01:25:48,940 --> 01:25:52,910
is much smaller than the number of training number training

1409
01:25:52,910 --> 01:25:54,290
they then

1410
01:25:54,870 --> 01:25:57,290
as you change the

1411
01:25:57,480 --> 01:26:02,120
probably to see OK so try to solve the dual problem a different see you

1412
01:26:02,120 --> 01:26:07,910
you have to have the share many elements it is zero and c

1413
01:26:07,910 --> 01:26:10,270
there's is because now

1414
01:26:10,410 --> 01:26:17,180
there is a huge difference in means of chinese senses and you only the twenty

1415
01:26:17,200 --> 01:26:18,430
twenty features

1416
01:26:18,520 --> 01:26:24,370
and you know that in my for any c you can see that after the

1417
01:26:24,370 --> 01:26:30,830
number of conformal about one is strictly but things you see is less than twenty

1418
01:26:31,390 --> 01:26:33,020
minutes and i use so some

1419
01:26:33,410 --> 01:26:40,040
distance between different seat after which they share the same many elements zero and also

1420
01:26:41,040 --> 01:26:45,080
other policy c so you can use very well

1421
01:26:45,210 --> 01:26:51,440
the kind of technique to speed becomes this

1422
01:26:51,500 --> 01:26:54,830
that is

1423
01:26:59,200 --> 01:27:00,560
to is what

1424
01:27:03,000 --> 01:27:09,350
OK so that's related to parameter selection what about that state so

1425
01:27:09,500 --> 01:27:15,230
this is usually we we do is certain kind of evaluation procedure so so point

1426
01:27:15,310 --> 01:27:17,000
you start increasing c

1427
01:27:19,230 --> 01:27:23,430
the school that later

1428
01:27:24,640 --> 01:27:30,560
so so we can use the kind of will start procedures these start to see

1429
01:27:30,560 --> 01:27:36,040
that in a situation you get face to convergence they they get up solution

1430
01:27:36,060 --> 01:27:39,200
then use that as an initial solution

1431
01:27:40,310 --> 01:27:47,390
someone you used information as an initial solution of ecstasy so

1432
01:27:47,390 --> 01:27:52,140
so you gradually increasing from small so of

1433
01:27:52,140 --> 01:27:57,200
you see because what you that we did that we get so for the same

1434
01:27:57,200 --> 01:28:00,370
problem is we can see

1435
01:28:00,390 --> 01:28:06,230
if we don't use decomposition methods to solve the situation sequence found it takes so

1436
01:28:06,230 --> 01:28:10,480
many iterations strongest sixty sold

1437
01:28:10,500 --> 01:28:12,540
but we start from

1438
01:28:12,540 --> 01:28:19,250
france equals one then w is country to reach five hundred that although we only

1439
01:28:19,250 --> 01:28:20,790
need ten so

1440
01:28:20,790 --> 01:28:26,080
so is useful for the number of features is small and so in that sense

1441
01:28:26,080 --> 01:28:32,460
we consider composition is it can also handle rather large the new support vector machines

1442
01:28:32,480 --> 01:28:37,480
in the number of features is small the number of features is large with you

1443
01:28:37,500 --> 01:28:40,850
also problem

1444
01:28:40,870 --> 01:28:47,120
so those are wrong but i i want to say about solving linear support vector

1445
01:28:48,040 --> 01:28:57,210
so to handle is the difficulty of building large kernel matrix approximation is of course

1446
01:28:57,580 --> 01:29:01,100
very very important technique

1447
01:29:01,100 --> 01:29:03,200
which is key

1448
01:29:03,220 --> 01:29:07,070
because when you use more clever proposals regions when this thing was going to work

1449
01:29:07,070 --> 01:29:10,740
and then there's many ways of computing is acceptance

1450
01:29:10,800 --> 01:29:13,130
this is just one possible way

1451
01:29:13,150 --> 01:29:16,020
in seventy three peskin

1452
01:29:16,020 --> 01:29:17,300
prove that

1453
01:29:17,350 --> 01:29:21,130
this particular way happens to we have very low

1454
01:29:21,140 --> 01:29:23,020
variance asymptotically

1455
01:29:23,070 --> 01:29:26,440
so we can write the strong law of large numbers for the

1456
01:29:26,460 --> 01:29:30,610
and we can get the central limit theorem we can characterize the various

1457
01:29:30,720 --> 01:29:34,720
the algorithm and you look at different schemes and you choose the one

1458
01:29:34,770 --> 01:29:36,780
these less variance

1459
01:29:36,790 --> 01:29:38,290
it has the better right

1460
01:29:38,300 --> 01:29:44,460
and you pick this

1461
01:29:45,660 --> 01:29:48,420
simulated annealing to answer your question

1462
01:29:48,430 --> 01:29:50,720
and simulated annealing what you do is

1463
01:29:50,760 --> 01:29:52,970
you take the distribution and use

1464
01:29:53,030 --> 01:29:56,610
and you raise the took part one of its

1465
01:29:57,510 --> 01:29:58,670
and then what you do

1466
01:29:59,550 --> 01:30:01,450
and you have a staff

1467
01:30:02,060 --> 01:30:03,820
you decrease

1468
01:30:03,870 --> 01:30:07,840
decrease the temperature decreases so this is sort of like

1469
01:30:07,890 --> 01:30:14,050
this minute annealing because it's based on this idea of people you know

1470
01:30:14,100 --> 01:30:18,130
you blacksmithing by the hierarchy

1471
01:30:18,300 --> 01:30:23,740
the important thing is is the creasing in other words the power of p is

1472
01:30:23,740 --> 01:30:28,700
increasing so you think a function power what happens the function gets compressed

1473
01:30:28,760 --> 01:30:31,940
so you have a problem is reaching its peak it can be

1474
01:30:31,990 --> 01:30:37,170
simulated annealing is often referred to as the algorithm where q is a symmetric random

1475
01:30:38,580 --> 01:30:43,650
most papers on that talk about we compared to simulated paper tells you we compared

1476
01:30:43,660 --> 01:30:45,350
against simulated annealing

1477
01:30:45,370 --> 01:30:46,870
and there were so well

1478
01:30:46,920 --> 01:30:50,630
they never tell you what the proposal is so a specification of we use simulated

1479
01:30:50,630 --> 01:30:56,220
annealing without telling what the proposal distribution was pretty much rubbish has hasn't carries no

1480
01:30:56,270 --> 01:31:00,660
simulated annealing is classifier is not one

1481
01:31:00,710 --> 01:31:07,430
so you can't compare against simulated you can compare against dealing with a specific recipe

1482
01:31:07,450 --> 01:31:11,330
most often people use the random walk with is very stupid idea

1483
01:31:11,370 --> 01:31:13,830
and that's what the compared to other methods

1484
01:31:16,060 --> 01:31:18,910
not surprisingly this but

1485
01:31:18,970 --> 01:31:22,260
so this is what happens when using nearly

1486
01:31:22,300 --> 01:31:25,360
the samples concentrate on the peaks

1487
01:31:25,560 --> 01:31:30,610
and this thing again this can be proved to converge to the peaks of the

1488
01:31:30,610 --> 01:31:32,150
schedule is low enough

1489
01:31:32,240 --> 01:31:34,650
the convergence proof is desirable

1490
01:31:34,660 --> 01:31:36,090
but it's not enough

1491
01:31:36,130 --> 01:31:40,520
because too slow

1492
01:31:40,530 --> 01:31:43,970
we did graphical models yes second skip this

1493
01:31:46,350 --> 01:31:50,180
one of the graphical model one of the areas where this is applied to a

1494
01:31:50,180 --> 01:31:52,680
lot of physics these models where

1495
01:31:52,700 --> 01:31:54,320
you have lattices

1496
01:31:54,320 --> 01:31:56,360
the discrete observations

1497
01:31:56,360 --> 01:31:57,410
and you have

1498
01:31:57,430 --> 01:32:00,410
pairwise relations between the variables

1499
01:32:00,450 --> 01:32:05,630
pairwise relations between the variables and observations and it the london used to be used

1500
01:32:05,630 --> 01:32:07,530
a lot in image analysis

1501
01:32:07,550 --> 01:32:10,340
so you can write the distribution of this model

1502
01:32:10,390 --> 01:32:15,640
and then you're goal is given the y given the pixel observations reconstruct the axis

1503
01:32:16,680 --> 01:32:22,130
these potentials basically what they do is they want x for for example here

1504
01:32:22,140 --> 01:32:25,510
the single for export to be like x two

1505
01:32:25,530 --> 01:32:28,760
the introducing smoothness constraint the image

1506
01:32:28,820 --> 01:32:32,860
and if i will say that you want to extract the y two

1507
01:32:32,860 --> 01:32:37,340
so you have to forces here you have likely model the ones

1508
01:32:37,390 --> 01:32:42,550
x to be like y but you also have the primal the spatial prior

1509
01:32:42,570 --> 01:32:46,530
once all the observations to be the same so the sort of thing that alex

1510
01:32:46,530 --> 01:32:49,990
was talking about history when he was talking about here so

1511
01:32:50,030 --> 01:32:51,450
and so

1512
01:32:51,450 --> 01:32:56,530
and that's the reconstruction of that image so you you get rid of the noise

1513
01:32:56,550 --> 01:32:58,260
so this is your life

1514
01:32:58,280 --> 01:33:01,740
computer vision graphics

1515
01:33:01,740 --> 01:33:05,680
the algorithm that is used there is the version of metropolis hastings

1516
01:33:05,700 --> 01:33:07,220
where the proposal

1517
01:33:12,180 --> 01:33:15,300
and note in this so this graph will have many nodes

1518
01:33:15,320 --> 01:33:17,550
if you take a note

1519
01:33:17,610 --> 01:33:18,870
and your new

1520
01:33:18,890 --> 01:33:23,470
by hand you can write the probability of the node given its neighbors

1521
01:33:23,490 --> 01:33:28,180
and if you were to sample from the node given its neighbors

1522
01:33:28,220 --> 01:33:30,760
and i'm going to leave that as an exercise as well

1523
01:33:30,780 --> 01:33:34,740
just substitute this particular proposal into the metropolis hastings

1524
01:33:34,780 --> 01:33:39,030
you'll find out that the acceptance is one

1525
01:33:39,070 --> 01:33:42,180
i can take over the exercise

1526
01:33:42,370 --> 01:33:46,660
so the algorithm then it's very some metropolis hastings algorithm that goes like this

1527
01:33:46,720 --> 01:33:47,890
you sample

1528
01:33:47,890 --> 01:33:51,640
x one variable x one in the graph so if you have a graphical model

1529
01:33:51,640 --> 01:33:55,260
is simple one variable given its neighbors then you move to the next there is

1530
01:33:55,260 --> 01:33:59,570
simply given its neighbors to move to next variables sample given expected and usually you

1531
01:33:59,570 --> 01:34:02,070
can derive expressions for all of these

1532
01:34:02,090 --> 01:34:06,800
graphical models it is this software package called bugs

1533
01:34:06,860 --> 01:34:09,430
you can if i wanted to go gibbs sampling

1534
01:34:09,430 --> 01:34:12,640
use bugs

1535
01:34:12,720 --> 01:34:16,030
bikes is a really really nice package

1536
01:34:16,030 --> 01:34:17,870
but is based in the

1537
01:34:20,240 --> 01:34:25,180
it's distributed at medical research

1538
01:34:25,220 --> 01:34:29,360
anyone from cambridge here

1539
01:34:29,360 --> 01:34:31,530
mit MRF or

1540
01:34:33,320 --> 01:34:35,140
immersed in cambridge

1541
01:34:35,140 --> 01:34:37,590
medical whatever search

1542
01:34:37,610 --> 01:34:39,490
council cambridge

1543
01:34:39,510 --> 01:34:45,030
addenbrookes hospital

1544
01:34:45,030 --> 01:34:48,890
of the AI but i'll just give you a little

1545
01:34:48,920 --> 01:34:54,220
i a little sample of this game actually i have really tried it if you

1546
01:34:54,220 --> 01:34:58,890
get the sound here now we have to see if it works

1547
01:35:09,560 --> 01:35:18,230
signature songs you

1548
01:35:18,240 --> 01:35:23,690
i'm afraid we don't have solved with this but well just watch the graphics and

1549
01:35:23,690 --> 01:35:24,630
you'll see

1550
01:35:24,710 --> 01:35:29,770
what to do that

1551
01:35:29,780 --> 01:35:33,480
what wanna give it

1552
01:35:33,490 --> 01:35:37,810
that that's actually

1553
01:35:37,830 --> 01:35:39,610
good idea

1554
01:35:39,740 --> 01:36:17,310
you have to have

1555
01:36:17,320 --> 01:36:25,290
she so this last

1556
01:36:25,310 --> 01:36:29,440
his last phase he saw as the g-man and his kind of pulling all the

1557
01:36:29,440 --> 01:36:32,130
strings in the story

1558
01:36:32,150 --> 01:36:36,480
so just point out a few things that are of interest here you saw that

1559
01:36:36,490 --> 01:36:41,760
there there was a lot of storytelling in the sense that you know you saw

1560
01:36:41,760 --> 01:36:46,930
a lot of people going around and he sees the hardest merges with movies you

1561
01:36:46,930 --> 01:36:51,240
see the story involving people talk to each other but you can actually walk around

1562
01:36:51,590 --> 01:36:56,200
and see them from the back or interact with them while the stories involving then

1563
01:36:56,200 --> 01:37:00,370
you have this thing at some point he grabbed a grenade was thrown at him

1564
01:37:00,370 --> 01:37:02,510
with a core of the

1565
01:37:03,670 --> 01:37:09,430
gravity grab things and and actually could go back to the opponent before it exploded

1566
01:37:09,430 --> 01:37:12,540
and if he hadn't done that it would have explored on his side and taking

1567
01:37:12,620 --> 01:37:17,230
taking life from him but so it could hurt the opponent when he's shooting there's

1568
01:37:17,940 --> 01:37:21,360
impact on on the environment things

1569
01:37:21,390 --> 01:37:26,500
breakdown because the bullets hit hit so there's there's a lot of stuff to find

1570
01:37:26,500 --> 01:37:29,040
the things this g-man guy

1571
01:37:29,040 --> 01:37:35,920
the big the big guy and for him they developed an entire facial model with

1572
01:37:36,460 --> 01:37:42,780
all kinds of actuators artificial actuators and then learn how to control this model in

1573
01:37:42,790 --> 01:37:47,780
ways that would fit what he's saying and so he other entire scenes in the

1574
01:37:47,780 --> 01:37:53,330
game where he's talking and he's he's really the evil guy in way but he's

1575
01:37:53,330 --> 01:37:55,650
i think quite quite convincing

1576
01:37:55,730 --> 01:38:00,780
however a lot of the story of course is told

1577
01:38:02,590 --> 01:38:03,960
in this

1578
01:38:03,960 --> 01:38:08,150
can you fix scripted way although you can kind of walk around things see them

1579
01:38:08,150 --> 01:38:14,350
from various perspectives what really happens is that these characters just follow six built in

1580
01:38:14,350 --> 01:38:19,980
story and that refer to as scripting and that i mean for example if you

1581
01:38:19,980 --> 01:38:24,330
play the second time course they do exactly the same thing and that's that's not

1582
01:38:24,330 --> 01:38:26,140
so great

1583
01:38:26,170 --> 01:38:31,110
OK so i just wanted to show you this in order to give you an

1584
01:38:31,110 --> 01:38:36,610
impression what a state-of-the-art computer game is like and where its strengths lie in words

1585
01:38:36,610 --> 01:38:41,530
weaknesses lie and now i would like to move on and

1586
01:38:41,540 --> 01:38:47,570
tell you a little bit about where i see potential for for using machine learning

1587
01:38:47,570 --> 01:38:51,920
in games and this list will essentially also be

1588
01:38:51,930 --> 01:38:56,680
the the structure of the for the remainder of this space lecture

1589
01:38:57,520 --> 01:39:00,270
maybe the main point of interest is

1590
01:39:00,280 --> 01:39:03,450
adaptive and learning game AI

1591
01:39:03,460 --> 01:39:09,180
and here we have different things that we can do we have this idea of

1592
01:39:09,180 --> 01:39:14,900
avatars or in in driving games to make all the driver terrace which essentially try

1593
01:39:14,950 --> 01:39:20,960
to copy or or learn the behavior that was shown to them by a human

1594
01:39:21,480 --> 01:39:27,020
and then emulate that behaviour so that's one one way machine learning can be used

1595
01:39:27,040 --> 01:39:31,370
another one is of course to not to just copy the behavior of the player

1596
01:39:31,370 --> 01:39:33,340
about to be the opposition

1597
01:39:33,440 --> 01:39:39,170
and that's of particular importance in front in these first-person shooters because if you're not

1598
01:39:39,170 --> 01:39:43,610
playing online in which case human people are your position which is very nice

1599
01:39:43,620 --> 01:39:48,570
o thing to have if you play just with the computer you want intelligent opposition

1600
01:39:48,570 --> 01:39:52,570
because otherwise playing it's no fun what's usually happening and that's

1601
01:39:52,580 --> 01:39:57,970
still happening in pretty good games is that since they can create really good artificial

1602
01:39:57,970 --> 01:40:04,830
intelligence vol one opponent they throw met very many stupid opponents at you for that

1603
01:40:04,830 --> 01:40:10,000
that can be done right but it's in the long-term it's really not very satisfying

1604
01:40:10,000 --> 01:40:16,650
because this is due one against one is usually more interesting

1605
01:40:16,670 --> 01:40:21,540
so the third thing of course is that you might want to have teammates in

1606
01:40:21,540 --> 01:40:26,340
the game so in the game of soccer which we see as an example you

1607
01:40:26,340 --> 01:40:31,140
want people you can only really control one character right so but you need the

1608
01:40:31,150 --> 01:40:36,350
team and they need to go to reasonable positions and and play with the right

1609
01:40:36,450 --> 01:40:42,480
so that's something that's of interest and then there's something called ambient AI and that's

1610
01:40:44,040 --> 01:40:48,470
the idea that you go through the world and you may not view the people

1611
01:40:48,470 --> 01:40:53,230
that you meet as enemy stories friends but you would like to interact with them

1612
01:40:53,230 --> 01:40:57,640
in some way and that you want that interaction to be believable and that's also

1613
01:40:57,640 --> 01:41:03,030
quite difficult although it may be a little easier than than opposition teammates which involves

1614
01:41:03,030 --> 01:41:07,510
the competitive element but they really have to be good at something well and i

1615
01:41:07,510 --> 01:41:09,680
just essentially has to be convincing

1616
01:41:09,700 --> 01:41:12,580
so a second area

1617
01:41:12,610 --> 01:41:19,110
that's very interesting is believing it is been usable physical movement so i showed you

1618
01:41:19,110 --> 01:41:25,260
in this little half-life to do more that their vehicles and are nicely simulated the

1619
01:41:25,330 --> 01:41:31,110
objects you should behave in good ways but what really doesn't work yet is

1620
01:41:31,120 --> 01:41:33,530
animals or

1621
01:41:33,530 --> 01:41:36,080
robots or

1622
01:41:36,100 --> 01:41:42,000
humans anything that is internal actuators and displaced movement that doesn't work very well in

1623
01:41:42,000 --> 01:41:47,840
computer games there was you saw several examples most of them were scripted so we're

1624
01:41:47,840 --> 01:41:53,940
using motion capture and so if someone runs along then you are always only runs

1625
01:41:53,940 --> 01:41:58,540
along exactly this particular route and

1626
01:41:58,540 --> 01:42:05,670
the other own character is very is similarly done in a very script way

1627
01:42:05,710 --> 01:42:11,740
and only micro movements are ever simulated if anything so

1628
01:42:11,750 --> 01:42:14,540
there are two nice piece of work that i would like to

1629
01:42:14,780 --> 01:42:20,090
o point two one is inverse kinematics for dimensionality reduction which

1630
01:42:20,110 --> 01:42:24,500
tries to detect this problem in more visually satisfying way rather than in a physical

1631
01:42:24,500 --> 01:42:29,510
way and there are some interesting work on learning how to walk

1632
01:42:29,510 --> 01:42:34,200
and the benefit of that would of course be that he would have opponents that

1633
01:42:34,200 --> 01:42:38,830
directly interact with the physical world and that would much look much more believable than

1634
01:42:38,830 --> 01:42:43,780
people who run for example again the one example if someone wanting to find that

1635
01:42:43,780 --> 01:42:47,730
may be the case because he's matter right but please don't talk about the same

1636
01:42:47,730 --> 01:42:53,500
time run structural will to flipping over the floor we would then want to see

1637
01:42:53,500 --> 01:42:56,720
maybe this junk on the site

1638
01:42:59,070 --> 01:43:00,740
the negative stuff

1639
01:43:01,640 --> 01:43:04,410
the positive stuff

1640
01:43:04,450 --> 01:43:05,770
all right

1641
01:43:05,800 --> 01:43:09,610
let's think of that instead of thinking it is of it does clause let's think

1642
01:43:09,610 --> 01:43:11,810
of it as a sequent

1643
01:43:11,860 --> 01:43:13,890
this write it that way

1644
01:43:13,900 --> 01:43:18,990
OK the clauses are tautology if and only if the sequence valid which it is

1645
01:43:19,000 --> 01:43:31,530
if and only if there is some atom that occurs on both sides

1646
01:43:39,050 --> 01:43:41,480
forget they

1647
01:43:41,480 --> 01:43:46,080
side stuff again familiar using familiar notation

1648
01:43:46,230 --> 01:43:49,310
i'm gonna write that is gamma delta

1649
01:43:49,330 --> 01:43:53,450
so that it looks like stuff we had this morning this morning we had sequence

1650
01:43:53,450 --> 01:43:57,510
that only had a single formula on the right

1651
01:43:57,520 --> 01:44:01,970
but it's not that big a generalisation to allow several formulae on the right

1652
01:44:02,010 --> 01:44:04,770
it makes something nice and symmetric

1653
01:44:07,370 --> 01:44:10,350
so we can now build up the calculus zero sequence

1654
01:44:11,010 --> 01:44:14,560
this a slightly more general than the sequence this morning

1655
01:44:14,570 --> 01:44:16,260
the reading of this is

1656
01:44:16,260 --> 01:44:18,460
if all of these things hold

1657
01:44:18,460 --> 01:44:21,970
that at least one of these things holes

1658
01:44:23,700 --> 01:44:29,480
there's a nice sort of duality estimate left-right symmetry about all this

1659
01:44:29,500 --> 01:44:33,230
and that's our axioms

1660
01:44:33,270 --> 01:44:35,270
that's our axioms for this

1661
01:44:35,280 --> 01:44:37,330
proof system

1662
01:44:37,350 --> 01:44:38,950
so is there any

1663
01:44:38,990 --> 01:44:43,860
such a sequence is valid if the left and right sides are not disjoint

1664
01:44:43,890 --> 01:44:45,660
on some matters

1665
01:44:45,710 --> 01:44:48,250
the because but

1666
01:44:48,260 --> 01:44:52,760
and then

1667
01:44:52,770 --> 01:44:57,800
we have rules that correspond exactly to the simplification principles

1668
01:44:57,810 --> 01:44:59,640
so suppose we have

1669
01:44:59,680 --> 01:45:01,970
and negative formula

1670
01:45:02,050 --> 01:45:05,090
occurring on one side or the other negation

1671
01:45:05,860 --> 01:45:06,990
not a

1672
01:45:11,040 --> 01:45:12,620
that occurs

1673
01:45:12,670 --> 01:45:16,750
the in what conditions will that happen well exactly

1674
01:45:21,170 --> 01:45:24,970
and negated formula because on the other side so we can read that as an

1675
01:45:24,970 --> 01:45:27,660
inference rule going in this direction

1676
01:45:27,710 --> 01:45:30,160
this simplification rule going this way

1677
01:45:30,170 --> 01:45:33,070
rule of inference going this way

1678
01:45:33,090 --> 01:45:35,510
that's how you can introduce

1679
01:45:35,560 --> 01:45:36,950
negative formula

1680
01:45:36,960 --> 01:45:38,740
on the left of the sequent

1681
01:45:38,750 --> 01:45:43,970
you get it from the same thing with the negative formula on the right

1682
01:45:43,980 --> 01:45:47,400
and the dual role for the other way around

1683
01:45:58,510 --> 01:46:00,260
we can

1684
01:46:00,280 --> 01:46:03,420
making deductions we can introduce

1685
01:46:03,420 --> 01:46:06,170
the negation on the right

1686
01:46:06,210 --> 01:46:10,610
provided we have the simplest sequent with us on the left

1687
01:46:10,670 --> 01:46:13,210
they mean

1688
01:46:15,860 --> 01:46:18,670
if we are asked to prove something that's got the negation on the right the

1689
01:46:18,670 --> 01:46:20,210
way we prove it

1690
01:46:20,220 --> 01:46:21,430
is by

1691
01:46:21,440 --> 01:46:22,570
to proving this

1692
01:46:22,670 --> 01:46:25,120
the this is simpler

1693
01:46:25,610 --> 01:46:28,650
it's got there on the left is simply because it's not good many connectives in

1694
01:46:32,260 --> 01:46:36,550
so our first system is going to tell us what to do

1695
01:46:36,600 --> 01:46:40,760
same thing with other connectives

1696
01:46:56,210 --> 01:46:57,680
if we have

1697
01:46:57,720 --> 01:47:00,490
if we want to prove something with that

1698
01:47:00,550 --> 01:47:02,980
a conjunction

1699
01:47:03,020 --> 01:47:05,290
on the left

1700
01:47:05,330 --> 01:47:07,850
well we can prove it

1701
01:47:07,890 --> 01:47:11,220
i mean what it says if all this stuff follows and this something in here

1702
01:47:12,330 --> 01:47:17,010
well for all this stuff to hold is just to hold beta holland

1703
01:47:17,030 --> 01:47:21,500
so we can simplify this way or we can make an inference going the other

1704
01:47:23,150 --> 01:47:33,330
and for conjunction on the right

1705
01:47:33,500 --> 01:47:38,490
we can

1706
01:47:41,790 --> 01:47:44,980
we can

1707
01:47:45,020 --> 01:47:48,180
to derive something that have to derive the conjunction

1708
01:47:48,220 --> 01:47:50,480
this may be parametric stuff

1709
01:47:50,530 --> 01:47:54,760
you derive the two conjure separate

1710
01:48:01,410 --> 01:48:03,140
the disjunction is going to be dual

1711
01:48:04,300 --> 01:48:08,850
so disjunction on the left is just like conjunction on the right

1712
01:48:08,910 --> 01:48:12,250
you know because you think about it in this these terms

1713
01:48:13,210 --> 01:48:14,730
conversion to

1714
01:48:14,740 --> 01:48:17,870
two conjunctive normal form these terms

1715
01:48:17,870 --> 01:48:20,420
base if you use like the hinge loss function

1716
01:48:20,750 --> 01:48:24,020
now we have an hours lots of them have long

1717
01:48:24,150 --> 01:48:27,840
and to generalize well to have a very small norm on your weight vector

1718
01:48:28,360 --> 01:48:32,830
this use are convex optimization permits perfect because we just like

1719
01:48:33,060 --> 01:48:34,760
like close all from

1720
01:48:35,010 --> 01:48:38,300
data can meters solution use quite fast

1721
01:48:39,610 --> 01:48:44,150
hallway you can see like yeah this is a picture shows like

1722
01:48:44,330 --> 01:48:45,800
the bound this is to allow

1723
01:48:46,920 --> 01:48:49,370
in regression problems need the data examples

1724
01:48:49,630 --> 01:48:51,230
yeah some outliers

1725
01:48:51,390 --> 01:48:53,220
which are both in the actual bound

1726
01:48:53,490 --> 01:48:58,420
so you have to use that for i was c and day ok where through the

1727
01:48:59,360 --> 01:49:04,530
let's say itself high homemaker parameter which should be assumed to get optimal forms

1728
01:49:05,230 --> 01:49:08,120
get these other space recursive problem

1729
01:49:08,300 --> 01:49:12,650
so the function should know be much bigger than the outputs func

1730
01:49:12,780 --> 01:49:15,160
and the data example should also not like marts

1731
01:49:16,170 --> 01:49:17,190
a vector function

1732
01:49:18,810 --> 01:49:21,810
so difficult to the dual space of basic parts

1733
01:49:21,940 --> 01:49:26,150
yeah each alpha necessarily think yourself kernel reformulations

1734
01:49:26,630 --> 01:49:28,340
basically the idea is to work

1735
01:49:28,540 --> 01:49:33,180
to use conch multiplies support vector coefficients alpha

1736
01:49:33,370 --> 01:49:35,870
and then you come up with the following formulation

1737
01:49:36,220 --> 01:49:39,350
when years some workers based on value it's

1738
01:49:39,470 --> 01:49:41,600
because we want to maximise his w

1739
01:49:41,850 --> 01:49:43,470
objective the dual objective

1740
01:49:44,180 --> 01:49:46,850
tool all the all all star alpha

1741
01:49:47,100 --> 01:49:50,530
here means that large we lies

1742
01:49:50,770 --> 01:49:53,550
the means to correlate well with outputs

1743
01:49:53,710 --> 01:49:55,110
and you means that

1744
01:49:55,270 --> 01:49:58,560
based on similar you which is very similar

1745
01:49:58,890 --> 01:50:00,720
basically you want to set

1746
01:50:00,990 --> 01:50:03,720
some of his office cereal because otherwise

1747
01:50:03,880 --> 01:50:05,140
would minimize this

1748
01:50:06,570 --> 01:50:10,960
function and some on on and this is the bias constraint

1749
01:50:11,190 --> 01:50:13,190
which we also it years later

1750
01:50:13,600 --> 01:50:15,200
so finally if you'll

1751
01:50:15,400 --> 01:50:17,160
get estimated all of those

1752
01:50:17,410 --> 01:50:19,100
who can i have a function which is

1753
01:50:19,260 --> 01:50:21,770
basically the linear case can transform this just to

1754
01:50:23,190 --> 01:50:24,760
tool or normal weight

1755
01:50:26,320 --> 01:50:29,310
in the boolean functions each threated here in this way

1756
01:50:29,500 --> 01:50:31,510
and this is sites vector x function

1757
01:50:31,660 --> 01:50:33,440
but only have to keep my

1758
01:50:33,740 --> 01:50:37,490
balance which are the differences between alpha alpha-channel zero

1759
01:50:39,280 --> 01:50:45,120
so optimization algorithms on many like many people use svm-light svm

1760
01:50:45,310 --> 01:50:48,400
based on s xml that programming

1761
01:50:48,650 --> 01:50:53,010
but to make it much simpler for us code developed this deep support vector machine

1762
01:50:53,150 --> 01:50:56,290
and also because we don't want to minimize objective

1763
01:50:56,520 --> 01:50:58,490
you find that is to acylated

1764
01:50:59,090 --> 01:51:01,770
we are developed like the gradient ascent method

1765
01:51:02,320 --> 01:51:07,060
basically you just have if you have to do you need to compute the derivative

1766
01:51:07,280 --> 01:51:09,650
but the give to each alpha coefficient and

1767
01:51:10,100 --> 01:51:11,000
you get right

1768
01:51:11,210 --> 01:51:14,280
learning basic this this is very simple to program

1769
01:51:14,510 --> 01:51:18,400
although of course is not open more like time complexity

1770
01:51:18,700 --> 01:51:22,030
all work needs to be to be down to make spots faster

1771
01:51:22,330 --> 01:51:27,140
but it's a very simple way to yeah just developed all support vector machine pulpit

1772
01:51:27,360 --> 01:51:28,560
instead of relying

1773
01:51:28,820 --> 01:51:31,350
on all known tool this case for the change them

1774
01:51:34,480 --> 01:51:37,630
so some relations i have multiple kernel learning

1775
01:51:38,060 --> 01:51:43,350
so but also the previous talk francesco dinuzzo we saw like multiple kernel learning

1776
01:51:44,760 --> 01:51:48,390
i even like multiple layer kernel learning that's related but

1777
01:51:48,840 --> 01:51:51,410
there are some differences in multiple kernel learning

1778
01:51:51,710 --> 01:51:55,870
like for example the kernel learning you want maybe who wait so

1779
01:51:56,020 --> 01:51:56,950
all of

1780
01:51:57,340 --> 01:52:00,380
the ancient to have an rbf kernel in this way

1781
01:52:00,750 --> 01:52:03,510
result based can be weighted more heavily than others

1782
01:52:03,780 --> 01:52:07,830
which is normal things to do although always normalize the data still makes sense

1783
01:52:08,290 --> 01:52:12,390
and then you get some kind both the min-max problem in the dual objective

1784
01:52:12,800 --> 01:52:14,530
if ronald bicycle ok

1785
01:52:18,340 --> 01:52:23,990
the dsvm is kind of different in this frame because basically yeah if you want

1786
01:52:24,010 --> 01:52:30,660
to use kernel you can just like support vectors ok it gets the inputs and goods one feature

1787
01:52:31,040 --> 01:52:34,500
so you stack like kind of all those support vector machines

1788
01:52:34,650 --> 01:52:37,200
the more you get a big feature

1789
01:52:37,460 --> 01:52:38,950
the feature presentation

1790
01:52:39,260 --> 01:52:41,170
then this the main

1791
01:52:41,170 --> 01:52:45,920
so the topic today is dynamic programming

1792
01:52:58,070 --> 01:52:59,910
programming in this

1793
01:52:59,920 --> 01:53:05,150
in in the name of this

1794
01:53:05,160 --> 01:53:08,530
i doesn't refer to computer programming

1795
01:53:08,550 --> 01:53:13,450
programming is an old word means in tabular method

1796
01:53:13,500 --> 01:53:15,310
for accomplishing something

1797
01:53:15,320 --> 01:53:18,250
so you'll hear about linear programming

1798
01:53:18,800 --> 01:53:21,270
dynamic programming those you know

1799
01:53:21,320 --> 01:53:22,620
we now

1800
01:53:22,630 --> 01:53:26,150
incorporate those algorithms in computer programs

1801
01:53:26,160 --> 01:53:28,370
originally computer programming

1802
01:53:28,420 --> 01:53:34,320
in a data sheet and you put one line per line of

1803
01:53:34,340 --> 01:53:38,980
as the tabular method for telling giving the machine instructions as to what to do

1804
01:53:39,000 --> 01:53:44,530
OK so the term programming is older course now conventionally we say programming

1805
01:53:44,540 --> 01:53:49,920
software computer program but that wasn't always the case and these terms are continue in

1806
01:53:49,920 --> 01:53:50,870
the literature

1807
01:53:52,760 --> 01:53:56,780
so dynamic programming is a design technique

1808
01:54:03,350 --> 01:54:07,660
i x with scenes such as divide and conquer

1809
01:54:07,670 --> 01:54:11,640
OK so it's a way of solving a class of problems

1810
01:54:11,690 --> 01:54:13,060
rather than

1811
01:54:13,070 --> 01:54:16,490
particular algorithm with some

1812
01:54:16,540 --> 01:54:22,530
so now work for this for example

1813
01:54:22,550 --> 01:54:26,050
the so-called longest

1814
01:54:34,890 --> 01:54:37,740
so called lcs

1815
01:54:38,850 --> 01:54:41,920
which is the problem that comes up in a variety of contexts and it is

1816
01:54:41,930 --> 01:54:45,010
particularly important in computational biology

1817
01:54:45,020 --> 01:54:49,150
we have warned DNA strings and you're trying to find commonalities

1818
01:54:49,200 --> 01:54:51,010
between two strings

1819
01:54:51,060 --> 01:54:52,380
one which may be

1820
01:54:52,390 --> 01:54:54,900
do you know that

1821
01:54:54,910 --> 01:54:57,660
and one may be various

1822
01:54:57,720 --> 01:55:05,450
when people do is that thing called with a evolutionary comparisons the

1823
01:55:05,500 --> 01:55:08,120
evolutionary trees here but there's

1824
01:55:08,170 --> 01:55:10,480
right right exactly what is it

1825
01:55:10,530 --> 01:55:14,290
phylogenetic trees they go OK for which good

1826
01:55:14,300 --> 01:55:17,690
so here's the problem

1827
01:55:17,780 --> 01:55:20,360
given two sequences

1828
01:55:20,410 --> 01:55:24,690
x going from one to n

1829
01:55:26,960 --> 01:55:29,160
running from one end

1830
01:55:29,260 --> 01:55:33,880
we're fine

1831
01:55:33,930 --> 01:55:36,970
the longest

1832
01:55:39,540 --> 01:55:43,940
comment remove

1833
01:55:43,990 --> 01:55:47,610
here i say

1834
01:55:56,290 --> 01:56:00,070
one of the longest common subsequence

1835
01:56:02,030 --> 01:56:07,850
longest common subsequence isn't unique several different subsequence the high

1836
01:56:07,900 --> 01:56:09,160
for that

1837
01:56:09,210 --> 01:56:14,090
however you know people tend to want the sloppiness is that people will say the

1838
01:56:14,090 --> 01:56:16,920
i'll try to say

1839
01:56:16,960 --> 01:56:18,350
most is unique

1840
01:56:19,430 --> 01:56:23,030
and they slip as well because just such a common thing to

1841
01:56:23,160 --> 01:56:25,950
just talk about the even even though it's

1842
01:56:26,000 --> 01:56:28,300
now there might be multiple

1843
01:56:28,340 --> 01:56:35,210
so here's an example suppose x is the sequence

1844
01:56:35,280 --> 01:56:44,610
why is the sequence

