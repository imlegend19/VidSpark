1
00:00:00,000 --> 00:00:02,830
however here here's the problem

2
00:00:02,830 --> 00:00:06,500
individuals do not consciously maximize anything

3
00:00:06,500 --> 00:00:13,220
anybody who associates consciousness conscious behavior with brain behavior is making very dense state

4
00:00:13,770 --> 00:00:17,500
consciousness is very small icebergs it

5
00:00:17,540 --> 00:00:21,890
of the brain and behavior most decision making by humans is not very consciously to

6
00:00:21,890 --> 00:00:23,120
me i can't

7
00:00:23,200 --> 00:00:25,620
this may well unconscious

8
00:00:25,680 --> 00:00:30,250
on maximizing is something done by council of maximise some

9
00:00:30,290 --> 00:00:35,250
completely obvious i still hear sociologist saying that people don't maximum when you choose to

10
00:00:36,120 --> 00:00:37,660
he maximize

11
00:00:37,700 --> 00:00:39,350
what maximizing

12
00:00:41,350 --> 00:00:44,270
you know to consciously to maximize

13
00:00:44,270 --> 00:00:50,970
individual choices need not be welfare enhancing people often do things that hurt them

14
00:00:51,020 --> 00:00:55,370
there's nothing that says trends preferences as to maximize your lifespan

15
00:00:55,500 --> 00:01:02,140
lead to personal happiness or something like that it may or may not

16
00:01:02,220 --> 00:01:06,600
also these are more technical point preferences are function of the current state

17
00:01:06,640 --> 00:01:09,580
so if you look at the traditional rational actor model

18
00:01:09,600 --> 00:01:12,270
it doesn't have the current state status quo

19
00:01:12,290 --> 00:01:15,810
one thing continent to first get out taught us

20
00:01:15,870 --> 00:01:18,730
you must but the current state of the organism

21
00:01:18,770 --> 00:01:23,540
if you to explain behavior is behavior is always the differential movement from the present

22
00:01:24,370 --> 00:01:26,620
to another state

23
00:01:27,700 --> 00:01:28,890
i believe beliefs

24
00:01:28,890 --> 00:01:36,250
economists beliefs as individual properties like preferences you know i like asparagus you know it

25
00:01:36,250 --> 00:01:39,950
i believe in the big mind you believe the big problem

26
00:01:39,970 --> 00:01:41,600
OK we don't care

27
00:01:41,600 --> 00:01:44,600
however to do game theory

28
00:01:44,640 --> 00:01:48,620
and to coordinate behavior in aligned group of agents

29
00:01:48,660 --> 00:01:51,250
individual beliefs have to be coordinated

30
00:01:51,270 --> 00:01:55,350
the content of different lines have to be similar

31
00:01:55,430 --> 00:01:56,870
individual beliefs

32
00:01:56,870 --> 00:02:01,600
are part of the net social network of interdependent beliefs one things human heart

33
00:02:01,640 --> 00:02:03,520
humans have very

34
00:02:03,540 --> 00:02:05,810
very weird if you study

35
00:02:05,830 --> 00:02:10,970
you behaviour primates like tomasello paul van allen et al work with

36
00:02:11,000 --> 00:02:12,250
learning in

37
00:02:12,270 --> 00:02:14,830
chimpanzees baboons et cetera

38
00:02:14,850 --> 00:02:19,620
they don't they don't have a theory of mind way we may different approach

39
00:02:20,000 --> 00:02:23,750
the notion of symmetric reasoning if i'm thinking something

40
00:02:23,790 --> 00:02:26,370
and i know that you thinking that same thing

41
00:02:26,390 --> 00:02:31,520
i can infer what you believe from what i believe or from your actions

42
00:02:32,890 --> 00:02:37,410
this is very important and it must be added certainly game theory and rational actor

43
00:02:37,410 --> 00:02:39,680
model to get anything like

44
00:02:39,700 --> 00:02:42,120
model of human behavior

45
00:02:42,160 --> 00:02:44,660
well i've already talked about game theory

46
00:02:44,680 --> 00:02:46,330
the game theory

47
00:02:46,410 --> 00:02:52,600
from across evolutionary game theory and time for complex systems i mentioned

48
00:02:55,790 --> 00:02:58,870
but i guess and because you have to quote

49
00:02:58,910 --> 00:03:01,600
dead time this is great

50
00:03:01,640 --> 00:03:05,950
social life comes from the double source the likeness of consciousness is

51
00:03:05,970 --> 00:03:07,950
and the division of labor

52
00:03:07,970 --> 00:03:09,850
the beauty of

53
00:03:10,990 --> 00:03:12,470
that's more

54
00:03:12,640 --> 00:03:13,580
is not

55
00:03:13,680 --> 00:03:15,680
division of labour

56
00:03:15,680 --> 00:03:17,540
cemented together

57
00:03:19,890 --> 00:03:24,850
that's theory social workers and by the way i only been talking about social work

58
00:03:24,870 --> 00:03:27,080
i'm not talking about social disorder that

59
00:03:27,100 --> 00:03:29,180
the disequilibrium fly

60
00:03:29,870 --> 00:03:32,290
you know the the next step

61
00:03:32,310 --> 00:03:37,060
and he recognised the division of labour supported by the like this consciousness

62
00:03:37,120 --> 00:03:40,020
economists have worked on the division of labor

63
00:03:40,040 --> 00:03:43,180
don't mention the like this country

64
00:03:43,230 --> 00:03:47,270
no theory of y two lines would have the same content

65
00:03:47,310 --> 00:03:49,930
and then you get the great economics

66
00:03:50,020 --> 00:03:54,080
statement there's no such thing as society here only in the there are individual men

67
00:03:54,080 --> 00:03:55,080
and women

68
00:03:55,120 --> 00:03:58,180
because she throws and their families

69
00:03:58,180 --> 00:04:01,680
so i was i was just asked to do something that i was trying to

70
00:04:01,680 --> 00:04:06,970
do anyway to put into context where we are and to motivate where we came

71
00:04:06,970 --> 00:04:11,250
from and maybe to repeat little bit what we did at the end yesterday chose

72
00:04:11,250 --> 00:04:12,500
to say

73
00:04:12,530 --> 00:04:15,970
we should interrupted any time i'm not sure if it is the first time i

74
00:04:15,970 --> 00:04:17,520
didn't happen but please

75
00:04:17,540 --> 00:04:21,500
you do if you want to discuss anything please do

76
00:04:21,520 --> 00:04:23,630
i got some of the questions

77
00:04:23,630 --> 00:04:26,770
was a question

78
00:04:27,590 --> 00:04:28,640
you can't hear me

79
00:04:31,530 --> 00:04:34,760
the cat idaho michigan and

80
00:04:34,770 --> 00:04:37,380
i don't know why it works for the other speakers and not for

81
00:04:37,410 --> 00:04:38,630
most of my life

82
00:04:38,630 --> 00:04:41,090
i put it up

83
00:04:41,100 --> 00:04:43,230
she tried

84
00:04:43,270 --> 00:04:45,550
why is that

85
00:04:45,590 --> 00:04:46,980
very large

86
00:04:47,130 --> 00:04:49,190
a belong

87
00:04:49,330 --> 00:04:57,060
OK i'll try to speak a bit louder and that doesn't work then i'll hold

88
00:04:57,130 --> 00:05:00,480
here so so you just give me signal don't worry i've i've always had this

89
00:05:00,480 --> 00:05:05,300
problem in lot of my i remember when i was a student phd advisor threatened

90
00:05:05,300 --> 00:05:08,060
that he would me micron like that for the group meetings

91
00:05:08,180 --> 00:05:13,320
events OK

92
00:05:13,370 --> 00:05:16,820
so i also got a couple of questions yesterday

93
00:05:16,840 --> 00:05:20,070
that might be able to address during the talk today

94
00:05:20,710 --> 00:05:22,540
and so

95
00:05:22,560 --> 00:05:28,200
let's see somewhere where do we come from we started with well we started that

96
00:05:28,210 --> 00:05:32,460
last part of the talk with the definition of a positive definite kernels and in

97
00:05:32,460 --> 00:05:35,070
a way we could have started the whole

98
00:05:35,070 --> 00:05:40,130
actually that we could have just dropped all this stuff about what we think a

99
00:05:40,130 --> 00:05:44,960
similarity measure studied informally with the similarity measure we could just say OK this is

100
00:05:44,960 --> 00:05:51,460
a similarity measure and that's what we consider similarity measures

101
00:05:51,480 --> 00:05:55,710
a positive definite kernel function which means a function of two arguments which is symmetric

102
00:05:55,710 --> 00:06:00,960
and which has this property that if we compute the gram matrix by substituting all

103
00:06:00,960 --> 00:06:05,880
pairs of training examples in this ground matrix will be positive definite

104
00:06:05,900 --> 00:06:10,560
which means that this equality holds true

105
00:06:10,620 --> 00:06:16,120
so we could have something like that and then we show to certain properties for

106
00:06:16,120 --> 00:06:20,060
these kinds of kernels and we showed how to construct the feature space and as

107
00:06:20,430 --> 00:06:24,830
such nice i would like to do that because a lot of people work with

108
00:06:24,830 --> 00:06:29,430
support vector machines and kernels take it for granted and i think it's something magic

109
00:06:29,430 --> 00:06:33,900
this feature space and you need some complicated theorems in functional analysis but in fact

110
00:06:33,900 --> 00:06:36,330
it's quite elementary we're done

111
00:06:36,340 --> 00:06:40,620
all the steps basically together how to construct this feature space so

112
00:06:40,640 --> 00:06:46,030
i think if only if if it's only four losing your respect and feeling more

113
00:06:46,030 --> 00:06:48,830
comfortable with it then that's already

114
00:06:48,840 --> 00:06:55,920
the purpose for doing this construction so so let's quickly think about this again so

115
00:06:55,930 --> 00:06:59,340
are the idea was we take

116
00:06:59,390 --> 00:07:03,470
so we want to map our input points which come from some set into a

117
00:07:03,470 --> 00:07:08,190
function space so this is space of functions mapping that's it into real numbers so

118
00:07:08,190 --> 00:07:11,990
we have we start with points let's say is o input space you points here

119
00:07:11,990 --> 00:07:17,330
are functions on that space each point is mapped into a function

120
00:07:17,370 --> 00:07:20,490
and the point will be mapped into the function that we get if we substitute

121
00:07:20,520 --> 00:07:24,440
point into one of the arguments of the kernel function and we keep the other

122
00:07:24,440 --> 00:07:28,220
argument so this is the a function of that first opened argument

123
00:07:28,240 --> 00:07:32,020
so that's our representations and

124
00:07:32,030 --> 00:07:36,340
note that actually in this case it could well be an infinite dimensional space function

125
00:07:36,340 --> 00:07:38,140
spaces of infinite dimensional

126
00:07:38,590 --> 00:07:44,800
is the space of all continuous functions on the interval is infinite dimensional so we

127
00:07:44,800 --> 00:07:48,560
map of points into this very high dimensional space

128
00:07:49,080 --> 00:07:51,640
with is nonlinear mapping

129
00:07:51,650 --> 00:07:58,080
and now we have to construct a dot product in that space to construct the

130
00:07:58,080 --> 00:08:01,730
product we first have to turn it into a vector space we do that in

131
00:08:01,730 --> 00:08:06,590
the trivial way we just by claiming that linear combinations of such points are also

132
00:08:06,770 --> 00:08:12,280
space so we just to the linear completion of the space within construct dot product

133
00:08:12,280 --> 00:08:14,620
and we want the dot product to satisfy

134
00:08:14,620 --> 00:08:19,460
when zero press has a given information on one of another defamation

135
00:08:19,470 --> 00:08:22,290
and if we compare theory c

136
00:08:22,310 --> 00:08:24,930
so this is experiments this is very

137
00:08:24,930 --> 00:08:27,070
we see that the first here

138
00:08:27,100 --> 00:08:29,740
these simulated to

139
00:08:29,770 --> 00:08:31,090
this one

140
00:08:31,150 --> 00:08:34,230
is well reproduced

141
00:08:34,240 --> 00:08:35,560
and this one you see

142
00:08:35,580 --> 00:08:38,560
we can say that it's quite axial vibration

143
00:08:38,580 --> 00:08:43,780
so that's why it's very important to make progress posted description

144
00:08:43,790 --> 00:08:49,160
of nuclear structure and making new experiments

145
00:08:49,970 --> 00:08:54,960
when we consider that it's oblate and prolate it's move the weight function so these

146
00:08:54,960 --> 00:08:57,280
are the weight function for the first time

147
00:08:57,310 --> 00:09:01,470
so zero plus two plus four to six ago then here

148
00:09:01,480 --> 00:09:02,510
so a

149
00:09:02,520 --> 00:09:06,450
if you've got this is the probability of present so consider this line

150
00:09:06,450 --> 00:09:09,480
we present pool at the slack overlay

151
00:09:09,500 --> 00:09:12,200
so here is located

152
00:09:12,220 --> 00:09:14,580
and here you have stayed overlay

153
00:09:14,610 --> 00:09:18,720
that's one way to identify in their only top quality

154
00:09:18,730 --> 00:09:23,430
in the media it's try access to pre-ex then is the degree of freedom

155
00:09:23,450 --> 00:09:30,800
that allows us to go continuously from april eight two

156
00:09:30,820 --> 00:09:35,280
so now i'm going to talk about a new structure

157
00:09:35,870 --> 00:09:41,050
as we have in this exotic nuclei a lot of neutron excess

158
00:09:41,070 --> 00:09:44,070
do we expect new things so

159
00:09:44,080 --> 00:09:48,270
this is an example that presented last time concerning the term given by then you

160
00:09:48,310 --> 00:09:51,430
the sand which have been discovered so

161
00:09:51,430 --> 00:09:57,730
the radius of this guy is very huge and that's been interpreted as kind meetings

162
00:09:57,750 --> 00:10:01,550
so how means you have the car here of the night

163
00:10:01,580 --> 00:10:04,570
and you have two external neutron

164
00:10:04,590 --> 00:10:08,910
it was very huge special extension

165
00:10:08,920 --> 00:10:13,150
one spending with this is that it's like goldman system so means history but since

166
00:10:13,150 --> 00:10:16,260
then you really need these three bodies with body forces

167
00:10:16,270 --> 00:10:20,270
to have something but if you take away one of the elements so it's no

168
00:10:20,270 --> 00:10:21,450
more about

169
00:10:21,460 --> 00:10:24,430
for example you need to ten is not

170
00:10:24,440 --> 00:10:26,910
so there are now local search

171
00:10:27,580 --> 00:10:34,770
other so not only these light nuclei but also made as nuclear

172
00:10:34,790 --> 00:10:40,320
or to look at the present time and for example

173
00:10:41,260 --> 00:10:42,970
something also

174
00:10:42,990 --> 00:10:49,790
any it's serious is these impact factors are more than calculations

175
00:10:49,810 --> 00:10:54,670
so these are the results of calculations with these models so this has been done

176
00:10:54,670 --> 00:10:56,670
in oxygen sixteen

177
00:10:56,790 --> 00:10:59,840
to obtain m forty six

178
00:10:59,860 --> 00:11:01,930
and you can see here

179
00:11:01,930 --> 00:11:04,340
these are alpha particles

180
00:11:04,360 --> 00:11:11,040
so i'm particles out they rearrange themselves to have these structure exact another structure so

181
00:11:11,040 --> 00:11:13,560
these shapes are predicted

182
00:11:13,580 --> 00:11:16,750
so i'm cluster is predicted by theory

183
00:11:16,760 --> 00:11:18,530
so that means that

184
00:11:18,550 --> 00:11:21,610
in some you know you shouldn't you don't have

185
00:11:22,060 --> 00:11:27,930
proton neutron but you have some structure and for structure inside this

186
00:11:30,280 --> 00:11:31,880
this is a very famous

187
00:11:32,360 --> 00:11:37,570
it that picture from the contest and so you amassed member excitation energy

188
00:11:37,600 --> 00:11:42,200
so first that are different configurations which are predicted

189
00:11:42,210 --> 00:11:44,770
here magnesium flares and fun

190
00:11:45,090 --> 00:11:47,300
oxygen first three and five

191
00:11:47,320 --> 00:11:49,370
and here only outside

192
00:11:49,440 --> 00:11:52,410
the same for magnesium they also can

193
00:11:52,420 --> 00:11:56,660
so this has been predicted by different approaches

194
00:11:58,210 --> 00:12:04,970
what about experiments so there some experiments are performed for example this is an experiment

195
00:12:05,200 --> 00:12:07,070
beryllium ten

196
00:12:08,360 --> 00:12:11,460
from that these people so these are

197
00:12:11,470 --> 00:12:13,320
citation energy

198
00:12:13,350 --> 00:12:14,920
and what you see here

199
00:12:15,150 --> 00:12:17,510
that you have some state

200
00:12:17,530 --> 00:12:20,510
which looked like a really really passionate about and so

201
00:12:20,530 --> 00:12:23,340
this behavior spacing that

202
00:12:23,350 --> 00:12:26,950
the spacing between the labels looks like fashion brands

203
00:12:26,950 --> 00:12:31,430
and in fact if you compare this speculation is associated to the shape

204
00:12:31,450 --> 00:12:36,900
where you have two fires and two neutrons something very well different

205
00:12:39,380 --> 00:12:45,160
this is and the most famous examples in the old state with

206
00:12:45,910 --> 00:12:48,480
in the past only was i think

207
00:12:52,830 --> 00:12:57,330
that it should exist as you have stated that seventy and leaving you was looking

208
00:12:57,330 --> 00:13:00,780
at the amount of carbon in the cosmos and he said because of

209
00:13:00,830 --> 00:13:05,000
i capture had you get it should be you should have stayed here

210
00:13:05,030 --> 00:13:09,800
gallant twenty so these that has been discovered rather good energy

211
00:13:09,830 --> 00:13:15,630
and you can not reproduce the states if you consider usual shell model calculations with

212
00:13:15,630 --> 00:13:17,940
three particle proton and neutron

213
00:13:17,940 --> 00:13:19,060
the only way

214
00:13:19,120 --> 00:13:23,440
you have to represent states to consider and outside clusters

215
00:13:23,440 --> 00:13:28,270
so this date is considered as three and five cluster here

216
00:13:28,300 --> 00:13:32,430
and even more of considered and was also the can even at the same states

217
00:13:32,430 --> 00:13:33,490
so this is

218
00:13:33,490 --> 00:13:36,200
the way we describe things

219
00:13:36,220 --> 00:13:40,690
zero placed second of the states

220
00:13:41,560 --> 00:13:42,900
people are

221
00:13:44,630 --> 00:13:49,090
looking at these new structure and now they are not only looking at the scene

222
00:13:49,090 --> 00:13:51,000
elements is the number

223
00:13:51,000 --> 00:13:54,630
of nucleons proportional to alpha particles

224
00:13:54,650 --> 00:14:02,070
but also the predicted that you can have different first and different neutron configurations

225
00:14:02,070 --> 00:14:03,730
so that's why this

226
00:14:03,750 --> 00:14:07,960
if you the minimax optimal universal code

227
00:14:07,960 --> 00:14:11,670
so now i can find a fine MDL

228
00:14:11,670 --> 00:14:12,860
at least

229
00:14:12,880 --> 00:14:17,440
one version for model selection and that worked well we just have to model

230
00:14:17,460 --> 00:14:18,790
and one and two

231
00:14:18,810 --> 00:14:22,340
so think about this for example is the first order and second order markov model

232
00:14:22,340 --> 00:14:24,360
for data

233
00:14:24,520 --> 00:14:27,980
now according to this modern version of MDL if you want to select between those

234
00:14:27,980 --> 00:14:29,810
two models for the same data

235
00:14:29,830 --> 00:14:31,770
you should take

236
00:14:31,790 --> 00:14:36,790
the model for which the associated optimal universal model so that's the normalized maximum likelihood

237
00:14:36,790 --> 00:14:40,360
model assigns the largest probability to the data

238
00:14:40,380 --> 00:14:45,310
or equivalently you pick the model which gives the smallest coupling to the data

239
00:14:45,330 --> 00:14:50,880
if you use the minimax optimal universal code relative to the model

240
00:14:52,340 --> 00:14:54,730
i only did this for models but

241
00:14:54,750 --> 00:14:57,960
exactly the same thing can be done for infinite models in a moment we'll see

242
00:14:57,960 --> 00:15:01,500
how and then it becomes actually practically useful

243
00:15:01,500 --> 00:15:05,190
o thing to do

244
00:15:06,110 --> 00:15:07,170
let's look at this

245
00:15:07,190 --> 00:15:11,540
again so we have two models we select the one which minimizes

246
00:15:11,560 --> 00:15:14,460
the code length according to these universal called

247
00:15:17,420 --> 00:15:19,790
if we recall that this is the fraction

248
00:15:19,810 --> 00:15:25,060
of the probability according to the maximum likelihood divided by the sum of over all

249
00:15:25,060 --> 00:15:29,440
sequences of the probability according to the next like for that sequence

250
00:15:29,440 --> 00:15:34,170
so the mines like fraction is equal to minus log of the numerator plus lack

251
00:15:34,190 --> 00:15:35,500
of the denominator

252
00:15:35,520 --> 00:15:39,250
so we select the model from which this summer smallest

253
00:15:39,270 --> 00:15:43,360
and you can think of this as the trade which is familiar in other model

254
00:15:44,190 --> 00:15:46,170
selection methods

255
00:15:46,770 --> 00:15:49,750
like AIC and BIC trade-off between goodness of fit

256
00:15:49,770 --> 00:15:50,960
and complexity

257
00:15:51,090 --> 00:15:56,000
so if the best fitting distribution in your model fits the data better than this

258
00:15:56,000 --> 00:15:59,400
will be larger so minus local this will be small

259
00:15:59,400 --> 00:16:02,900
on the other hand if the complexity of your model

260
00:16:02,920 --> 00:16:08,170
is larger complexity here means that can fit more patterns well

261
00:16:08,190 --> 00:16:12,630
that's as we will see roughly corresponds to how many parameters to model has

262
00:16:12,670 --> 00:16:16,750
and this term will get larger so this term the richard model is the more

263
00:16:16,750 --> 00:16:20,770
data patterns that can fit in there for the more risk of overfitting

264
00:16:20,790 --> 00:16:24,110
the largest this term will be

265
00:16:24,610 --> 00:16:29,000
so hard large this term for finite models well

266
00:16:29,020 --> 00:16:32,210
we've already seen if you ever find models and elements

267
00:16:32,230 --> 00:16:35,860
this term must be bounded by the log of the number of elements because you've

268
00:16:35,860 --> 00:16:39,130
already seen it can always called the data with the two part code

269
00:16:39,150 --> 00:16:41,920
where the overhead for each

270
00:16:43,110 --> 00:16:44,690
is log

271
00:16:44,710 --> 00:16:46,690
right space simply encoding

272
00:16:46,730 --> 00:16:47,940
the index

273
00:16:47,960 --> 00:16:51,590
of the maximum likelihood distribution theta had

274
00:16:51,590 --> 00:16:54,250
by a uniform called which takes look and bits

275
00:16:55,420 --> 00:16:59,560
here we have the same overhead for every particular sequence because we know there's a

276
00:16:59,560 --> 00:17:04,380
code of overhead mark this particular over it must be smaller than log on the

277
00:17:04,380 --> 00:17:07,400
model how much smaller

278
00:17:07,400 --> 00:17:12,210
so the question is of course doesn't make any sense nor is using the normalized

279
00:17:12,210 --> 00:17:15,670
maximum likelihood distribution to compare models

280
00:17:15,670 --> 00:17:18,310
and to try to convince you that it does

281
00:17:18,360 --> 00:17:21,420
i'll give four interpretations of it

282
00:17:21,440 --> 00:17:25,980
and actually the first interpretation i already gave

283
00:17:26,020 --> 00:17:30,860
basically what you do is if you believe as an axiom the MDL idea that

284
00:17:30,860 --> 00:17:31,880
it's good

285
00:17:31,900 --> 00:17:35,460
to select the model which allows for the shortest

286
00:17:35,480 --> 00:17:37,750
code length of the data

287
00:17:37,770 --> 00:17:40,820
then i already have a i mean but then you have to believe that but

288
00:17:40,820 --> 00:17:45,400
if you believe that there is a clear motivation already because what we do here

289
00:17:45,400 --> 00:17:48,840
is exactly that select the model which allows to for the most compression of the

290
00:17:50,500 --> 00:17:54,900
and to formalize the idea we associate each model set of distributions with the coat

291
00:17:55,060 --> 00:17:58,590
and we do that in such a way that all distributions within the model are

292
00:17:58,590 --> 00:18:00,790
treated on an equal footing

293
00:18:00,840 --> 00:18:01,940
of course we can

294
00:18:01,960 --> 00:18:05,500
associate a code to encode data for the model in an arbitrary way what we

295
00:18:05,500 --> 00:18:06,570
did it in a way

296
00:18:06,730 --> 00:18:11,770
the city overhead we have we have two whatever distribution in the model

297
00:18:11,790 --> 00:18:14,900
is a good code for the data is as small as possible

298
00:18:14,940 --> 00:18:19,190
and note and this makes it really different from any other approach to machine learning

299
00:18:19,190 --> 00:18:21,610
and statistics that i know of

300
00:18:22,590 --> 00:18:28,520
this done in worst-case setting not case over distributions that might generate data but worst

301
00:18:28,520 --> 00:18:29,840
case over data

302
00:18:29,860 --> 00:18:33,920
so it's really important to realize that i haven't talked for one

303
00:18:33,980 --> 00:18:40,040
second about things which generate data i have taken no expectations they don't exist here

304
00:18:40,040 --> 00:18:42,690
we use distributions only

305
00:18:43,360 --> 00:18:44,860
encode data

306
00:18:44,880 --> 00:18:48,400
and not something from which data might come here

307
00:18:48,420 --> 00:18:52,610
and we then look at how good these distributions are in the worst case overall

308
00:18:52,610 --> 00:18:56,860
data sequences so we make no assumptions about the data comes from at all

309
00:18:56,860 --> 00:19:01,170
at least when defining our model later we were defining our methods

310
00:19:01,210 --> 00:19:04,560
later we're going to prove that the method works well in some cases then will

311
00:19:04,560 --> 00:19:08,540
make such assumptions but for now we have made those assumptions at all

312
00:19:08,590 --> 00:19:13,340
we want to use codes which work well no matter what data we get

313
00:19:14,540 --> 00:19:17,000
so now the the second interpretation

314
00:19:17,020 --> 00:19:20,000
has something to do with this complexity term

315
00:19:20,020 --> 00:19:24,980
and it turns out you can give that kind of counting geometric interpretation

316
00:19:25,730 --> 00:19:29,460
it turns out you can think of this as something like total fit your model

317
00:19:29,460 --> 00:19:31,310
assigns to the data

318
00:19:32,670 --> 00:19:35,880
equivalent either lock not of the number of elements in your model

319
00:19:35,900 --> 00:19:39,360
the model is finite it would be this before ready it's more equal than a

320
00:19:39,360 --> 00:19:40,480
lot of the number of elements

321
00:19:40,940 --> 00:19:45,520
but something like the effective number of distributions so what is that

322
00:19:46,270 --> 00:19:48,560
if i look now there's a lot of this

323
00:19:48,570 --> 00:19:51,500
complexity term but the term itself

324
00:19:51,520 --> 00:19:55,840
i can rewrite it the sum in two separate terrorist can take the sum over

325
00:19:55,840 --> 00:19:57,340
all theta as

326
00:19:57,380 --> 00:20:01,480
in my model and the sum over all data from which that particular seat

327
00:20:01,500 --> 00:20:03,110
fits the data best

328
00:20:03,130 --> 00:20:07,170
the probability of the data according to see this and this is simply the same

329
00:20:07,190 --> 00:20:11,830
so i partition the set of all sequences into those sequences which have the same

330
00:20:11,830 --> 00:20:14,070
maximum likelihood estimator

331
00:20:14,090 --> 00:20:17,940
so now i can rewrite this of course is the sum over all theta my

332
00:20:17,940 --> 00:20:20,170
model of the probability

333
00:20:20,190 --> 00:20:23,170
according to see to that i get a sequence for which

334
00:20:23,170 --> 00:20:27,230
peter is actually the maximum likelihood estimator

335
00:20:27,250 --> 00:20:31,290
and this is of course the sum of all theta of one minus the probability

336
00:20:31,290 --> 00:20:32,460
that i get

337
00:20:32,480 --> 00:20:38,590
a sequence for which he is not the maximum likelihood estimator

338
00:20:38,610 --> 00:20:40,230
according to peter

339
00:20:40,250 --> 00:20:43,940
so this is the probability according to see to the data look as if they

340
00:20:43,940 --> 00:20:45,560
do not come from theta

341
00:20:45,610 --> 00:20:50,310
i think of very simple model that's a bernoulli model with just four

342
00:20:50,330 --> 00:20:54,650
elements the probability of one zero point twos are point fours or point six point

343
00:20:55,250 --> 00:20:59,920
if the frequency of one is approximately zero point four

344
00:21:02,460 --> 00:21:06,790
you look at the terminus some with zero point two this is the probability that

345
00:21:06,790 --> 00:21:07,730
you get

346
00:21:07,750 --> 00:21:12,960
some sequence for example with the frequency of zero point four or even larger such

347
00:21:12,960 --> 00:21:16,980
that the theta had is not equal to zero point two

348
00:21:19,060 --> 00:21:22,020
this you can pull this one out of the sum and if this is the

349
00:21:22,020 --> 00:21:26,670
final this is simply the number of distributions and you get strictly negative terms

350
00:21:26,670 --> 00:21:30,230
which you might call the amount of confusion in your model

351
00:21:30,290 --> 00:21:32,330
so this is the probability

352
00:21:32,340 --> 00:21:36,360
according to see to that the maximum likelihood estimator is not the time the probability

353
00:21:36,360 --> 00:21:38,860
according to see that you get an atypical

354
00:21:38,880 --> 00:21:44,210
sequence with that which doesn't look like sums over all theta

355
00:21:44,210 --> 00:21:51,770
here here here here

356
00:22:06,220 --> 00:22:23,580
we are really

357
00:22:41,940 --> 00:22:44,920
we had our

358
00:22:56,640 --> 00:23:01,810
observation of

359
00:23:22,590 --> 00:23:26,980
now what

360
00:24:16,840 --> 00:24:20,210
in this case

361
00:24:24,450 --> 00:24:35,890
now the right now

362
00:25:32,790 --> 00:25:38,590
in the

363
00:25:38,640 --> 00:25:48,850
the you

364
00:25:48,860 --> 00:25:51,610
and here

365
00:26:13,990 --> 00:26:17,520
all right

366
00:26:37,980 --> 00:26:43,220
it was longer

367
00:28:30,960 --> 00:28:35,540
the rest of the

368
00:28:38,940 --> 00:28:42,430
you are

369
00:28:52,690 --> 00:28:54,920
it be

370
00:28:54,940 --> 00:29:01,970
o nine

371
00:29:29,600 --> 00:29:35,300
a lot of

372
00:29:37,940 --> 00:29:41,380
is it

373
00:29:56,970 --> 00:30:00,190
because they

374
00:30:03,280 --> 00:30:09,350
you are

375
00:31:06,050 --> 00:31:06,740
which is

376
00:31:08,910 --> 00:31:11,180
it is not

377
00:31:11,180 --> 00:31:14,180
and what happens to the well known shell structure

378
00:31:14,190 --> 00:31:18,630
but we know very well in the stable nuclei what happens when we move away

379
00:31:18,630 --> 00:31:21,530
from the stable nucleus

380
00:31:22,170 --> 00:31:28,050
concerning the limit of existence you have here is an important member neutron number

381
00:31:28,060 --> 00:31:31,380
what we are looking at for example you consider given

382
00:31:32,910 --> 00:31:37,260
given the number of full-time fifteen for example and then you add neutral neutral neutral

383
00:31:37,260 --> 00:31:39,150
neutral and then

384
00:31:39,170 --> 00:31:42,970
you can no more and one neutron the system is no more about some of

385
00:31:42,970 --> 00:31:45,300
these neutron is no more about

386
00:31:45,320 --> 00:31:48,490
so it means that the limit of existence it that point

387
00:31:48,540 --> 00:31:52,410
so when you have all these points for all the value and for all n

388
00:31:53,360 --> 00:31:57,500
you can define this new mode of existence what a neutral what we call the

389
00:31:59,790 --> 00:32:03,660
so this has been defined by with the different

390
00:32:08,130 --> 00:32:12,290
what are the main features of these three lines are the fact that

391
00:32:12,370 --> 00:32:16,560
this report only playing here is not so far from stability

392
00:32:16,620 --> 00:32:19,080
so that's why we can reach now

393
00:32:19,100 --> 00:32:19,850
so far

394
00:32:19,870 --> 00:32:23,080
a small number of nuclei we can reach these three planes farm

395
00:32:23,340 --> 00:32:27,450
for the in the proton rich side so that we can we can show the

396
00:32:28,980 --> 00:32:31,400
these three plants is very far away

397
00:32:31,410 --> 00:32:35,080
from the stability this neutron drip line

398
00:32:35,100 --> 00:32:40,780
so that's why when we talk about exotic nuclei we talk about neutron rich nuclei

399
00:32:40,790 --> 00:32:44,820
and neutron rich nuclei if really neutron rich you can

400
00:32:45,060 --> 00:32:48,020
mimic what happens in the neutron stars so

401
00:32:48,030 --> 00:32:52,300
that's why we are mostly interested in neutral matter

402
00:32:52,340 --> 00:32:57,850
it's really a challenge to locate these drip lines from an experimental point of view

403
00:32:57,850 --> 00:32:59,450
it's to challenge because the

404
00:32:59,460 --> 00:33:05,530
counting rate and prediction is very low so it's very difficult concerning theory

405
00:33:05,550 --> 00:33:09,670
what i want to show you that you need a very big accuracy

406
00:33:11,030 --> 00:33:14,730
when you look at the by means you have to to consider infinity on this

407
00:33:14,730 --> 00:33:18,780
bound of not so you have to calculate the what we call s and what

408
00:33:18,780 --> 00:33:20,330
i presented yesterday

409
00:33:20,850 --> 00:33:26,040
neutron separation energy this neutron separation energy the energy that you need to put into

410
00:33:26,040 --> 00:33:27,130
the system

411
00:33:28,180 --> 00:33:31,910
take away one nucleon so it's the difference between the mass

412
00:33:31,930 --> 00:33:34,230
four so many energy of the system

413
00:33:34,320 --> 00:33:37,300
minus the binding energy of the daughter system

414
00:33:37,310 --> 00:33:39,800
so you need to know very precisely

415
00:33:39,810 --> 00:33:43,070
the masses of the constituent so you need to have

416
00:33:43,120 --> 00:33:48,640
the population valid for all the different isotopes and i suppose

417
00:33:48,680 --> 00:33:53,550
and a few calculations have been performed to check the validity of the present approaches

418
00:33:53,550 --> 00:33:55,660
concerning the mass predictions

419
00:33:55,670 --> 00:34:00,300
so this is the well known d one s force that has been used since

420
00:34:00,300 --> 00:34:06,180
the nineteen eighties and it has been fitted into that into only a few nuclei

421
00:34:06,200 --> 00:34:10,450
that we're well known at that time these stable magic nuclei

422
00:34:10,470 --> 00:34:16,410
but since then many many new nuclei have been discovered experimentally so that's why people

423
00:34:16,410 --> 00:34:20,540
have tried to do OK what are the prediction problem as all these

424
00:34:20,570 --> 00:34:25,920
for these four what is plotted here is the difference between theoretical and experimental

425
00:34:25,930 --> 00:34:31,510
binding energy for the different nuclei depending on n so the number of neutrons

426
00:34:31,520 --> 00:34:35,440
so you have having color different isotopes chance

427
00:34:35,450 --> 00:34:42,000
and you see that you have average zero but you have some brief here today

428
00:34:42,050 --> 00:34:44,630
so for example for these chain here

429
00:34:44,710 --> 00:34:48,750
so this is an act and i you can have a to ten amy

430
00:34:49,750 --> 00:34:55,230
maybe it's not so much if you consider the binding energy which is two thousand

431
00:34:55,250 --> 00:34:56,520
two thousand amy

432
00:34:56,530 --> 00:34:58,960
so it's o point five per cent

433
00:34:58,980 --> 00:35:02,220
so it's not so bad but when you calculate the

434
00:35:02,280 --> 00:35:04,680
it's and so the separation energy

435
00:35:04,700 --> 00:35:08,630
you need to do be difference between this point and this point here

436
00:35:08,640 --> 00:35:10,560
and this is to a

437
00:35:10,570 --> 00:35:13,540
so that's it's really bad to calculate

438
00:35:13,550 --> 00:35:16,410
the limit of existence of these nuclei

439
00:35:16,450 --> 00:35:19,270
so that's why you can not know more

440
00:35:19,310 --> 00:35:21,800
of calculation like that

441
00:35:21,820 --> 00:35:24,460
so people have tried to react just

442
00:35:25,630 --> 00:35:28,190
with these force here the one m

443
00:35:28,250 --> 00:35:29,520
m four masses

444
00:35:29,540 --> 00:35:32,130
in order to better reproduce the results

445
00:35:32,170 --> 00:35:34,310
so first they have tried

446
00:35:35,150 --> 00:35:38,720
to be better with these three so they have something flat here

447
00:35:38,800 --> 00:35:41,530
so it's better now to get this in

448
00:35:41,560 --> 00:35:43,700
progress has been made recently

449
00:35:43,720 --> 00:35:47,720
in order to introduce more correlation and having something

450
00:35:47,750 --> 00:35:50,380
almost flat here

451
00:35:50,390 --> 00:35:55,920
the root mean square west three MTV for the initial for you when is sort

452
00:35:55,920 --> 00:36:00,090
of three in a b and now it's less than eight hundred kv

453
00:36:00,210 --> 00:36:02,170
so it's better

454
00:36:02,830 --> 00:36:09,570
be careful for these nucleosome person for astrophysical calculations are really need very precise

455
00:36:09,590 --> 00:36:12,770
values for the masses what they would like

456
00:36:12,780 --> 00:36:14,410
it would be something with

457
00:36:14,420 --> 00:36:17,420
and accuracy on the order of tens of kv

458
00:36:17,430 --> 00:36:20,750
so we are off by one order of magnitude

459
00:36:20,760 --> 00:36:23,460
so that's why in astrophysics

460
00:36:23,480 --> 00:36:28,420
cold calculation they used evaluated data or database and experiment

461
00:36:28,430 --> 00:36:30,280
but then we try to

462
00:36:30,300 --> 00:36:33,760
to make progress in order to to to give them some

463
00:36:33,800 --> 00:36:38,130
microscopic calculations that are needed for these processes

464
00:36:38,140 --> 00:36:41,770
so i think that in a few years a lot of progress will be made

465
00:36:41,770 --> 00:36:45,570
in that direction in order to really improve

466
00:36:45,590 --> 00:36:48,650
the calculations and the first

467
00:36:48,660 --> 00:36:50,480
but it's indirect so you

468
00:36:50,500 --> 00:36:53,270
we prove the first but then you don't

469
00:36:53,280 --> 00:36:58,190
feet the experimental data directly you do all your complicated calculation and then you have

470
00:36:58,190 --> 00:37:00,390
an expectation value which is the mass

471
00:37:00,420 --> 00:37:01,810
and then you feel that

472
00:37:01,830 --> 00:37:06,620
so it's difficult to re-educate these parameters

473
00:37:06,630 --> 00:37:11,390
and we try also to derive that first from first principles in order not to

474
00:37:11,390 --> 00:37:14,420
have so many parameters

475
00:37:14,430 --> 00:37:19,180
so what about new magic numbers in exotic nuclei

476
00:37:19,230 --> 00:37:23,550
so i told you yesterday that there is something that we well know

477
00:37:23,550 --> 00:37:28,890
are these magic numbers so these magic numbers that are due to che to gaps

478
00:37:28,890 --> 00:37:31,300
in the single particle levels here

479
00:37:31,340 --> 00:37:35,300
and we know that these nuclei are especially stable

480
00:37:35,310 --> 00:37:37,870
but what about this magic nuclei

481
00:37:37,880 --> 00:37:42,890
when you move away away from the stability when you're dealing with exotic nuclei and

482
00:37:42,890 --> 00:37:46,460
this is really a current issue that is study here for example

483
00:37:46,520 --> 00:37:50,670
as did in many other laboratories

484
00:37:52,100 --> 00:37:56,970
this is the the region that is of interest nowadays and it was twenty eight

485
00:37:56,970 --> 00:38:00,930
so this should be magic numbers and this is the magic number

486
00:38:00,930 --> 00:38:04,870
very important so here's some just some examples from

487
00:38:04,900 --> 00:38:10,850
perspective or relative estimation sizes so most of you will think the table longer than

488
00:38:10,850 --> 00:38:11,750
this one

489
00:38:11,990 --> 00:38:16,930
and the this monster is smaller than the than the one behind it whereas in

490
00:38:16,930 --> 00:38:21,280
fact if you just rotate the image you see the effect types exactly the same

491
00:38:21,280 --> 00:38:25,540
size and what's this exactly the same size so what's happening is you've seen your

492
00:38:25,540 --> 00:38:29,870
eyes taking in the whole scene interpreting it and then giving us size judgement based

493
00:38:29,870 --> 00:38:33,350
on that based on the links the table things like this all the

494
00:38:33,360 --> 00:38:38,580
the size the place in in the corridor for monsters and that judgement is completely

495
00:38:39,480 --> 00:38:41,430
but it's also very useful

496
00:38:41,440 --> 00:38:44,780
because it tends to real seems to give you the right the right and the

497
00:38:44,780 --> 00:38:49,280
right judgement even though visually in terms of the actual vision it's wrong

498
00:38:49,420 --> 00:38:54,940
OK if you just a few final points on human object recognition so there's a

499
00:38:54,940 --> 00:39:00,100
thing called pop out if you see a real object on basically that background if

500
00:39:00,100 --> 00:39:00,800
you see

501
00:39:00,800 --> 00:39:02,150
horizontal age

502
00:39:02,180 --> 00:39:06,150
basically i feel the vertical niches things pop out immediately you don't need to do

503
00:39:06,150 --> 00:39:11,060
any visual search they appear immediately you're i can count immediately to the right place

504
00:39:11,060 --> 00:39:16,180
so you able to process that followed out very quickly and secondly you can recognise

505
00:39:16,180 --> 00:39:20,290
objects well if you happen to have been trained a lot and that human faces

506
00:39:20,290 --> 00:39:24,500
case in point humans are very good at setting human faces

507
00:39:24,510 --> 00:39:29,660
we're actually getting quite good at it faces as well in terms of computer vision

508
00:39:29,670 --> 00:39:33,270
and humans are also very good at recognizing family faces which were not quite so

509
00:39:33,270 --> 00:39:34,270
good at yet

510
00:39:34,320 --> 00:39:39,980
just this just because there's have a lot of experience with faces standing faces for

511
00:39:40,100 --> 00:39:41,410
a very early age

512
00:39:41,430 --> 00:39:48,320
for other kinds of objects which are not kind of expert objects which is not

513
00:39:48,330 --> 00:39:52,870
about it seems that you have to do some kind of process analogous to aligning

514
00:39:52,870 --> 00:39:58,050
the object was some kind of reference image sure templated you have

515
00:39:58,070 --> 00:39:59,870
the that object class

516
00:39:59,890 --> 00:40:04,560
and the alignment for example gets slower if you have to move the thing for

517
00:40:04,560 --> 00:40:07,130
them and if you recognition gets slower

518
00:40:07,300 --> 00:40:12,360
look all kinds of psychophysics experiments on these kinds of things so it seems that

519
00:40:12,370 --> 00:40:16,350
humans do use representations which are probably mainly to d

520
00:40:16,350 --> 00:40:19,620
mainly kind of template based for the object recognition

521
00:40:20,000 --> 00:40:25,060
these very difficult experiments to do so nothing is certain but that seems to be

522
00:40:25,060 --> 00:40:27,860
what the current experiments show

523
00:40:27,870 --> 00:40:31,010
OK just a few things on the visual

524
00:40:31,010 --> 00:40:32,710
one is asian individual

525
00:40:33,760 --> 00:40:39,400
so i optical nerve goes back to the natural genetic nucleus which is a small

526
00:40:39,400 --> 00:40:42,430
kind of switch box in the center of your brain and it gets mapped onto

527
00:40:42,450 --> 00:40:48,220
the visual cortex v one region we start doing visual processing left and right halves

528
00:40:48,220 --> 00:40:51,880
of the size of both eyes separated and move to the right and left half

529
00:40:52,420 --> 00:40:57,110
the one and the stereo processing happens

530
00:40:57,380 --> 00:40:59,550
in and the one region mainly

531
00:41:00,230 --> 00:41:02,130
the LGN

532
00:41:02,150 --> 00:41:05,640
what is actually a single blow but he returns to

533
00:41:05,690 --> 00:41:09,900
does a lot of contrast normalisation and things like that as well kind of roots

534
00:41:09,900 --> 00:41:15,640
things it helps contrast normalizes does all sorts of strange things but not

535
00:41:15,660 --> 00:41:20,160
things that you count as being very high-level processing just kind of a regulator box

536
00:41:20,160 --> 00:41:21,300
in the center

537
00:41:22,500 --> 00:41:24,320
so i said

538
00:41:24,340 --> 00:41:28,880
these two things then you go through what called the one simple cells which basically

539
00:41:28,880 --> 00:41:30,190
linear filters

540
00:41:30,210 --> 00:41:32,680
and through the one complex cells

541
00:41:33,210 --> 00:41:34,300
which are kind of

542
00:41:34,630 --> 00:41:39,610
there's the rectification states of this kind like an energy or half wave rectification going

543
00:41:40,190 --> 00:41:43,370
followed by some kind of integration

544
00:41:43,440 --> 00:41:48,350
responses over local region which is what the the one complex cells to each stage

545
00:41:48,350 --> 00:41:53,190
of this process the strong game controllers natural division from nearby cells and other things

546
00:41:53,190 --> 00:41:55,340
that are going on

547
00:41:55,340 --> 00:42:00,020
the reason is already dance image enhancement is sorted out of course a lot of

548
00:42:00,050 --> 00:42:04,270
for camera what will be attitude control control of the united states and things

549
00:42:04,350 --> 00:42:11,140
audience signal routing some kind of contrast control and support cells are multiscale decomposition

550
00:42:11,170 --> 00:42:13,520
complex cells disintegration of that

551
00:42:13,530 --> 00:42:16,060
so let's just see some examples of this

552
00:42:16,980 --> 00:42:21,530
so response and the origins a center surround the response

553
00:42:21,540 --> 00:42:22,460
the center

554
00:42:22,480 --> 00:42:24,560
some signal coming in in in

555
00:42:24,570 --> 00:42:28,590
the signals around the suppressed things so that the matter

556
00:42:28,670 --> 00:42:31,890
natural brain with that kind of response

557
00:42:33,360 --> 00:42:35,140
a single cell and all

558
00:42:35,170 --> 00:42:37,910
so you can see that these kinds responses simple cells

559
00:42:37,920 --> 00:42:46,010
basically by filters or each filters various different types and complex cells integrate normalized responses

560
00:42:46,640 --> 00:42:50,570
a number of different simple cells so they tend have rather lot like

561
00:42:52,140 --> 00:42:54,840
regions with negative and positive

562
00:43:02,880 --> 00:43:07,370
one thing the simple cells you can actually model and pretty well with what are

563
00:43:07,370 --> 00:43:12,780
called global fields filters so will come back to that later but basically complex exponentials

564
00:43:12,780 --> 00:43:19,080
of gaussians so nice to wave waves type things you can model the fitting nowhere

565
00:43:19,080 --> 00:43:22,900
so we models of these things listed that level but it's not doesn't take is

566
00:43:22,900 --> 00:43:24,550
very far

567
00:43:26,020 --> 00:43:31,620
some statistics of tuning distributions may be is not important but basically this is a

568
00:43:34,350 --> 00:43:36,300
o bandwidth so

569
00:43:36,350 --> 00:43:46,830
of the cells the spread over angular special

570
00:43:46,870 --> 00:43:51,500
the figures you typically have them with the one half octaves and angular bandwidth about

571
00:43:51,500 --> 00:43:56,250
forty degrees and that means you can build a model these cells may be used

572
00:43:56,250 --> 00:44:03,640
for vision later if you

573
00:44:06,350 --> 00:44:09,700
a nice model makes use signal independent

574
00:44:09,720 --> 00:44:13,280
you get numbers which are very much like human numbers

575
00:44:13,320 --> 00:44:16,970
OK so it seems that there no the

576
00:44:17,000 --> 00:44:21,800
the figures are coherent with nature in some sense

577
00:44:21,850 --> 00:44:26,110
so i said that this energy model something important to know if you look into

578
00:44:26,120 --> 00:44:33,690
for example neural types of treatments of vision simple cells get halfway rectified in then

579
00:44:33,690 --> 00:44:36,150
integrated into a complex cell

580
00:44:36,150 --> 00:44:37,580
walter lewin

581
00:44:40,520 --> 00:44:44,720
in its own direction of increasing x

582
00:44:44,780 --> 00:44:46,530
if i call this point

583
00:44:46,610 --> 00:44:48,780
eight x equals zero

584
00:44:48,830 --> 00:44:52,130
and i call this point b x equals x

585
00:44:52,210 --> 00:44:55,320
and i can calculate the work that i have to do to bring it from

586
00:44:55,320 --> 00:44:56,350
a to b

587
00:44:58,610 --> 00:45:00,590
walter lewin has to do

588
00:45:00,640 --> 00:45:02,500
bring it from a to b

589
00:45:02,530 --> 00:45:03,790
if the integrals

590
00:45:03,820 --> 00:45:05,790
in going from a to b

591
00:45:05,800 --> 00:45:07,590
one of my force

592
00:45:07,650 --> 00:45:11,630
the x to dot product

593
00:45:11,640 --> 00:45:13,980
but since the angle between the two

594
00:45:15,770 --> 00:45:18,830
zero degrees the cosine of the angle

595
00:45:18,840 --> 00:45:21,920
is one so i can forget about the fact that is the dot product i

596
00:45:21,920 --> 00:45:23,620
moved in this direction

597
00:45:23,700 --> 00:45:27,200
so that becomes the integral in going from zero

598
00:45:27,220 --> 00:45:29,650
two a position x

599
00:45:30,540 --> 00:45:31,790
plus x

600
00:45:31,830 --> 00:45:33,690
plus kx

601
00:45:33,720 --> 00:45:36,640
the x and that is one half

602
00:45:37,880 --> 00:45:39,500
x quick

603
00:45:39,510 --> 00:45:42,380
and this is what we call the potential energy

604
00:45:44,680 --> 00:45:48,650
this is potential and it means that we

605
00:45:48,730 --> 00:45:50,730
x equals zero

606
00:45:50,780 --> 00:45:54,550
we define potential energy to be zero you don't have to do that but it

607
00:45:54,550 --> 00:45:58,360
would be ridiculous to do it any other way

608
00:45:58,400 --> 00:46:00,740
so in the case that we have

609
00:46:00,780 --> 00:46:03,400
the new or situation of gravity

610
00:46:03,440 --> 00:46:05,900
we had a choice where we put our zero

611
00:46:05,910 --> 00:46:07,080
potential energy

612
00:46:07,100 --> 00:46:09,810
in case that we deal with very large distances

613
00:46:09,830 --> 00:46:12,000
we do not have the choice anymore

614
00:46:12,010 --> 00:46:13,860
we defined it in such a way

615
00:46:13,870 --> 00:46:18,570
that the potential energy at infinity is zero

616
00:46:18,630 --> 00:46:19,860
as a result of that

617
00:46:19,870 --> 00:46:21,660
all the potential entities

618
00:46:21,670 --> 00:46:22,860
and negative

619
00:46:22,870 --> 00:46:24,420
and here was the spring

620
00:46:24,480 --> 00:46:26,970
you don't have a choice either you choose

621
00:46:29,120 --> 00:46:30,430
x equals zero

622
00:46:30,440 --> 00:46:33,180
you choose potential energy to be zero

623
00:46:33,650 --> 00:46:35,030
so if you know

624
00:46:35,040 --> 00:46:36,410
make a plot

625
00:46:36,420 --> 00:46:40,530
of the potential energy as a function of x

626
00:46:40,540 --> 00:46:43,110
you get the parabola

627
00:46:43,150 --> 00:46:45,770
and if you are here

628
00:46:45,810 --> 00:46:47,650
if the object is here

629
00:46:47,740 --> 00:46:51,180
and the force is always in the direction

630
00:46:51,190 --> 00:46:52,950
opposing increasing

631
00:46:53,000 --> 00:46:56,470
potential energy if you go this way potential energy increases

632
00:46:56,490 --> 00:46:57,870
so it's clear

633
00:46:57,880 --> 00:46:59,130
that the force

634
00:46:59,170 --> 00:47:00,200
it's going to be

635
00:47:00,220 --> 00:47:01,480
in this direction

636
00:47:01,520 --> 00:47:02,720
if you're here

637
00:47:03,180 --> 00:47:07,780
the force is always in the direction opposing increasing potential energy

638
00:47:07,820 --> 00:47:10,300
increase in potential energy is in this direction

639
00:47:10,310 --> 00:47:14,320
so the force is in this election see restoring force

640
00:47:15,360 --> 00:47:18,760
always in the direction opposite to increasing

641
00:47:18,770 --> 00:47:19,990
potential energy

642
00:47:20,010 --> 00:47:24,640
if you release an object here at zero speed it will therefore go

643
00:47:24,650 --> 00:47:27,690
so it's lower potential energy force

644
00:47:27,790 --> 00:47:32,620
will drive it to lower potential energy

645
00:47:32,720 --> 00:47:34,680
if we know the force

646
00:47:34,690 --> 00:47:36,640
in this case the spring force

647
00:47:36,650 --> 00:47:42,750
or in those cases the gravitational force we were able to calculate the potential energy

648
00:47:42,760 --> 00:47:45,980
now the question is can we also go back

649
00:47:47,260 --> 00:47:50,270
we knew the potential energy can we don't find

650
00:47:50,280 --> 00:47:52,320
the force again and the answer is

651
00:47:52,330 --> 00:47:54,050
yes we can

652
00:47:54,060 --> 00:47:57,780
let's take the situation of the spring first

653
00:47:57,820 --> 00:47:59,480
we have that the

654
00:47:59,530 --> 00:48:01,070
potential energy u

655
00:48:01,080 --> 00:48:02,650
of the spring

656
00:48:02,670 --> 00:48:04,120
equals plus

657
00:48:04,130 --> 00:48:05,860
one half k

658
00:48:05,870 --> 00:48:07,980
x squared

659
00:48:07,990 --> 00:48:10,170
and if i take the derivative

660
00:48:10,230 --> 00:48:11,790
well that was x

661
00:48:12,290 --> 00:48:14,890
then i get

662
00:48:14,920 --> 00:48:16,360
plus kx

663
00:48:17,230 --> 00:48:19,310
but the force

664
00:48:19,320 --> 00:48:22,790
the spring force itself is minus kx

665
00:48:22,850 --> 00:48:26,100
so this equals minus

666
00:48:26,820 --> 00:48:28,320
the spring force

667
00:48:28,330 --> 00:48:29,940
so we have that

668
00:48:31,510 --> 00:48:35,150
was minus f and i put px there

669
00:48:35,230 --> 00:48:37,170
because it's the one dimensional problem

670
00:48:37,180 --> 00:48:38,370
it is only in the

671
00:48:41,020 --> 00:48:42,410
the minus sign

672
00:48:42,420 --> 00:48:43,650
he's telling you

673
00:48:43,690 --> 00:48:46,450
that the force is always pointing

674
00:48:46,510 --> 00:48:48,650
in the direction which is opposite

675
00:48:48,680 --> 00:48:52,020
two increasing values of the potential energy

676
00:48:52,020 --> 00:48:55,150
that is what the minus sign is that which darren you in the face what

677
00:48:55,150 --> 00:48:58,680
i've been telling you for the past five minutes

678
00:48:58,730 --> 00:49:02,390
we have a three-dimensional situation that we know the potential energy

679
00:49:02,400 --> 00:49:04,620
as a function of x y and z

680
00:49:04,660 --> 00:49:07,150
then we can

681
00:49:07,150 --> 00:49:11,480
go back and find the forces as a function of x y and zee doesn't

682
00:49:11,480 --> 00:49:15,150
matter where these are spring to what is gravity or but the electric forces or

683
00:49:15,150 --> 00:49:16,540
nuclear forces

684
00:49:16,600 --> 00:49:18,040
you then find

685
00:49:18,780 --> 00:49:20,650
the UTX

686
00:49:20,720 --> 00:49:22,250
it was miners

687
00:49:22,280 --> 00:49:24,310
f of x

688
00:49:24,370 --> 00:49:25,900
the UT y

689
00:49:25,910 --> 00:49:29,440
because minus

690
00:49:30,790 --> 00:49:32,650
and the UTC

691
00:49:32,680 --> 00:49:34,740
people minus

692
00:49:34,750 --> 00:49:36,500
after what does this mean

693
00:49:36,510 --> 00:49:37,690
it means

694
00:49:37,730 --> 00:49:40,610
that if you're in three-dimensional space

695
00:49:40,620 --> 00:49:44,040
if you move only in the x direction you keep

696
00:49:44,080 --> 00:49:46,390
y and z constant

697
00:49:46,390 --> 00:49:53,070
example positive if it's positive example negative if it's a negative example and so then

698
00:49:53,070 --> 00:49:58,130
you can as usual apply the the trick for evaluating a test point in which

699
00:49:58,130 --> 00:50:04,510
you have some alpha rhi kappa X I X where the alpha rhies are these counts

700
00:50:04,510 --> 00:50:10,930
now one nice property of this algorithm is we're starting to see sparsity because

701
00:50:10,930 --> 00:50:16,090
one of the perceptron convergence algorithm bounds the number of updates in terms of the

702
00:50:16,090 --> 00:50:20,770
margin in fact it's R squared over gamma squared where gamma is the margin of the of

703
00:50:20,770 --> 00:50:28,390
the best classifier on the training data and so that means that if you

704
00:50:28,390 --> 00:50:35,050
have a reasonable margin you won't have made very many updates and you'll have made significantly

705
00:50:35,050 --> 00:50:39,530
fewer updates than there are training examples and if you haven't updated on a training

706
00:50:39,550 --> 00:50:44,510
example than its alpha rhi is still zero because it's never got incremented beyond zero

707
00:50:44,510 --> 00:50:48,710
so when you're evaluating you actually only need to actually evaluate the inner product so

708
00:50:48,750 --> 00:50:55,790
those guys that you've actually updated on so in a way it's a little counterintuitive that your

709
00:50:55,800 --> 00:51:00,930
classification depends on the tough examples and not on the easy examples you'd expect the

710
00:51:00,930 --> 00:51:03,770
kind of easy examples to be nice sort of ones that show you the right way

711
00:51:03,770 --> 00:51:10,330
but actually you're using the ones that cost you grief in the training phase as

712
00:51:10,390 --> 00:51:14,810
to guide your classification of a test example but this is means that this is

713
00:51:14,810 --> 00:51:21,470
actually a very efficient evaluation and as I said this was actually published in nineteen sixty four by

714
00:51:21,470 --> 00:51:28,430
Aizerman et al in the USSR despite actually a bound that you can prove on

715
00:51:28,430 --> 00:51:33,650
the performance of this thing which is very tight  it does tend to overfit a bit

716
00:51:33,650 --> 00:51:41,010
and they abandoned this method and didn't pursue it um but I would contend that if

717
00:51:41,010 --> 00:51:45,850
you actually  replace the update rule by this margin version which is a bit

718
00:51:45,850 --> 00:51:53,590
more aggressive it updates  more readily in other words the problem with this previous

719
00:51:53,590 --> 00:51:58,050
thing is that when it finishes some of the training data could be only just

720
00:51:58,050 --> 00:52:03,270
correctly classified you know they as soon as it gets this classification right it stops

721
00:52:03,270 --> 00:52:09,930
no  point in doing anymore it's sort of super lazy and that sort of leaves the

722
00:52:09,930 --> 00:52:17,150
the decision boundary potentially quite close to training data and that can lead to some poor

723
00:52:17,150 --> 00:52:23,390
performance but if you add in this  parameter tau and which is set bigger

724
00:52:23,390 --> 00:52:28,410
than zero then it's being a bit more aggressive and it's saying no I'm not happy

725
00:52:28,410 --> 00:52:33,410
just to be correctly classified I wanna get a bit of a margin and for instance you

726
00:52:33,480 --> 00:52:39,050
can just set tau equals one and you're  guaranteed to get half of the optimal margin so

727
00:52:39,050 --> 00:52:46,450
it's a very easy optimization to do and I would contend it's pretty competitive

728
00:52:46,450 --> 00:52:51,470
with the support vector machine there's not much between this and sort vector machines so

729
00:52:51,680 --> 00:52:56,570
sort of nice to think that you are one parameter away from a from a kernel classification

730
00:52:56,570 --> 00:53:04,670
algorithm able to generalize in high-dimensions in nineteen sixty four but okay I'm  using site historical fact

731
00:53:05,070 --> 00:53:09,790
okay so let's move to support vector machines  so what what are we trying

732
00:53:09,790 --> 00:53:14,230
to do in support vector machines it's the usual thing we're really familiar now is just

733
00:53:14,230 --> 00:53:17,210
a linear function in a kernel defined feature space

734
00:53:17,390 --> 00:53:24,710
the only difference is that we're going to optimize relative to a slightly different performance

735
00:53:24,710 --> 00:53:29,990
measure and the  measure that we're gonna optimize is this idea of a

736
00:53:29,990 --> 00:53:38,450
margin  on the training data and the reason that that is chosen is there's a bound

737
00:53:38,540 --> 00:53:45,990
that shows you that the margin translates into good performance on test data so

738
00:53:45,990 --> 00:53:51,890
it's the same idea as the the  ridge regression where we would keep in control of

739
00:53:51,890 --> 00:53:56,510
the weight vector in some sense the margin keeps control of the complexity and in

740
00:53:56,510 --> 00:54:00,650
a very explicit sense actually you can see it is reducing the norm of the weight vector so

741
00:54:00,650 --> 00:54:07,270
it's a very  explicit link there but I will just mention this is is you

742
00:54:07,270 --> 00:54:15,380
identify super exponential faster exponential growth of prize which are characteristic of this positive feedback

743
00:54:15,410 --> 00:54:20,740
the best textbook example that i always had to my students to explain this positive

744
00:54:20,740 --> 00:54:24,770
feedback either behaviour of human population in the last two

745
00:54:24,770 --> 00:54:25,810
thousand years

746
00:54:25,820 --> 00:54:28,650
where the log of the population now one

747
00:54:28,730 --> 00:54:33,560
billion seven billion people here of two hundred two thousand years

748
00:54:33,600 --> 00:54:34,550
should be

749
00:54:34,560 --> 00:54:37,890
exponential these matters was right

750
00:54:37,930 --> 00:54:39,030
the constant

751
00:54:39,030 --> 00:54:42,800
the rate of growth you should have constant slope here

752
00:54:42,820 --> 00:54:47,910
what you see is this super exponential acceleration were constantly the rate of growth of

753
00:54:47,910 --> 00:54:52,500
the population has increased and we understand why the carrying capacity of the earth has

754
00:54:52,500 --> 00:54:55,700
increased by positive feedback of human populations

755
00:54:55,990 --> 00:54:57,150
in captivity

756
00:54:57,170 --> 00:54:58,870
expression of new niches

757
00:54:59,200 --> 00:55:06,160
industrial revolution agricultural revolution and so forth pushing this in accidentally whether the positive feedback

758
00:55:06,190 --> 00:55:07,640
in finance many

759
00:55:08,770 --> 00:55:10,580
i don't have time to go through

760
00:55:10,590 --> 00:55:13,750
the behaviour of very important imitation

761
00:55:15,070 --> 00:55:19,570
it is a fundamental aspect i believe of markets it is actually not the stupid

762
00:55:20,660 --> 00:55:22,810
that often using cartoons

763
00:55:22,810 --> 00:55:24,830
it is about actually

764
00:55:24,850 --> 00:55:29,910
being rational to imitate because we don't know everything and when is that if a

765
00:55:29,910 --> 00:55:31,080
look in the

766
00:55:31,120 --> 00:55:35,340
and i was not preparing you you would all major to look at the floor

767
00:55:36,100 --> 00:55:40,410
it might be danger it is rational to imitate the thing we learn it is

768
00:55:40,420 --> 00:55:44,700
put in the software artificially intelligent

769
00:55:46,000 --> 00:55:49,550
there are even actually even journal of finance the show that

770
00:55:49,600 --> 00:55:54,880
hedge funds mutual funds immediately show their are geographic and so on and so forth

771
00:55:54,910 --> 00:55:59,080
so this gives rise when you take all this into account positive feedback by imitation

772
00:55:59,080 --> 00:56:00,880
by many of the processes

773
00:56:00,900 --> 00:56:03,960
in pp core level behaviour

774
00:56:05,990 --> 00:56:10,330
and that's very important about doesn't come out of the blue comes when something rather

775
00:56:10,330 --> 00:56:15,300
positive emerges out of the preceding phase which was small

776
00:56:15,320 --> 00:56:19,180
it's a mistake to say so there is a good sign always start nuclear with

777
00:56:19,900 --> 00:56:25,670
new outlook maybe some displacement of technology like the ITC bubble or like the real

778
00:56:25,670 --> 00:56:29,350
estate bubble in eighteen forty in the in the UK

779
00:56:29,370 --> 00:56:33,810
and then you have pretty creation before you can state and evolution

780
00:56:33,810 --> 00:56:40,000
and when you collect data you see these really commonality when you collect these these

781
00:56:40,340 --> 00:56:43,420
i would say a butterfly these bubbles

782
00:56:43,440 --> 00:56:50,100
that led to the crash using this characteristic has resurgence of this super express tracks

783
00:56:51,090 --> 00:56:53,010
which we can model

784
00:56:53,070 --> 00:56:58,320
brother precisely just want to mention that we can also play with

785
00:56:58,370 --> 00:57:02,750
george erasmus student here in the room with agent based model that takes into account

786
00:57:02,750 --> 00:57:08,180
imitation learning process and you find a that is just wide new white noise news

787
00:57:08,230 --> 00:57:09,990
can create crashes

788
00:57:10,000 --> 00:57:13,300
here i want to advertise the presentation by

789
00:57:13,810 --> 00:57:17,790
one of my students visiting chinese leader in

790
00:57:17,840 --> 00:57:23,430
well on wednesdays going actually to explain much more the technicalities of the type of

791
00:57:23,460 --> 00:57:29,120
techniques that we use to identify bubbles in particle of these very interesting innovations

792
00:57:29,170 --> 00:57:30,090
which is the

793
00:57:30,100 --> 00:57:32,370
first methodology that we have

794
00:57:32,390 --> 00:57:37,590
found which may consist in the evolution of the price itself rather than return that's

795
00:57:37,590 --> 00:57:42,210
open to the public questions how do is is to make the most prize which

796
00:57:42,210 --> 00:57:43,630
is not stationary

797
00:57:43,680 --> 00:57:47,840
the big problems in econometric estimation versus the return but that's

798
00:57:47,950 --> 00:57:53,070
so what know which cover now having said that i'm using this technology

799
00:57:53,070 --> 00:57:58,460
and i'm just showing my textbook example of a series of about of crashes the

800
00:57:59,260 --> 00:58:02,330
of the price on carbon which is the free market

801
00:58:02,340 --> 00:58:03,480
the full of time

802
00:58:03,480 --> 00:58:07,130
you have an average exponential growth but the market is always

803
00:58:07,160 --> 00:58:15,330
bubbling below port accelerating crashing accelerating pressure activating crushing accelerating in the super exponential with

804
00:58:15,360 --> 00:58:19,690
the row growth rate of the price is growing itself as a function of time

805
00:58:19,690 --> 00:58:25,600
and then it's not just neighbouring crashes and the systematic precocious of this event as

806
00:58:25,600 --> 00:58:27,290
very clearly are very clear

807
00:58:28,120 --> 00:58:30,700
with distorted we can go back to

808
00:58:32,850 --> 00:58:38,280
the story and the show indeed the first evidence of about when for example the

809
00:58:38,280 --> 00:58:41,190
intense stop with were multiplied by a factor

810
00:58:41,210 --> 00:58:45,910
fourteen as opposed to the non intense stock just move be within the range of

811
00:58:45,910 --> 00:58:46,840
ten percent

812
00:58:46,890 --> 00:58:48,660
i mean that is the scarcity

813
00:58:48,670 --> 00:58:53,880
so that automatically new at this super exponential acceleration and that the value huge capital

814
00:58:53,880 --> 00:59:00,120
inflow in the US poor in actually to buy the internet technology has been documented

815
00:59:00,150 --> 00:59:02,230
like emerging market

816
00:59:02,300 --> 00:59:06,670
that is young nineteen ninety four nineteen ninety seven this is you as being actually

817
00:59:06,670 --> 00:59:10,530
floated by cash to force the market

818
00:59:10,580 --> 00:59:15,990
the response of the fed was these growth of the money minor money supply

819
00:59:16,040 --> 00:59:17,420
very interestingly

820
00:59:18,890 --> 00:59:22,100
city with the money money supply growth and decay of the

821
00:59:22,760 --> 00:59:28,110
fed interest rate which begin became negative just in the aftermath of the crash of

822
00:59:28,110 --> 00:59:32,310
these ideas about what you see in real meaning you adjust for inflation

823
00:59:32,310 --> 00:59:37,310
if you think about it a real rate which is negative so you you just

824
00:59:37,310 --> 00:59:39,870
want to spend that's what happened before

825
00:59:39,890 --> 00:59:45,100
and this occurred at the time when the estimation productivity with the highest

826
00:59:45,100 --> 00:59:48,950
and the economy there will be the you that this is a recipe for disaster

827
00:59:48,960 --> 00:59:53,780
there's one productivity is accelerating this when you actually we should increase interest rate two

828
00:59:53,780 --> 00:59:57,780
slow down with the engine that exactly the opposite what was done by my great

829
00:59:59,150 --> 01:00:03,190
and what happened this is the data where you can actually see

830
01:00:03,200 --> 01:00:06,370
contrary to the market we acting

831
01:00:07,210 --> 01:00:08,600
the set policing

832
01:00:08,600 --> 01:00:12,300
we show that the policy was actually lagging behind the

833
01:00:12,370 --> 01:00:18,380
successive drafts of the market what was showing here is nasdaq index SP five hundred

834
01:00:18,380 --> 01:00:21,900
index from the time in the aftermath of the

835
01:00:21,920 --> 01:00:27,580
crash and then the subsequent two years of dismal performance of the market and direction

836
01:00:27,580 --> 01:00:29,000
of the fed here

837
01:00:29,010 --> 01:00:32,890
in the red showing how flagged by two months

838
01:00:34,000 --> 01:00:40,350
two months trying to somehow stop these descent on the market very interesting now the

839
01:00:40,350 --> 01:00:45,260
question is can we say that this paper titles so suggests that there was a

840
01:00:45,260 --> 01:00:50,060
causal slaving of the fed to market this requires much more investigation

841
01:00:50,080 --> 01:00:52,150
now this of course bush

842
01:00:52,380 --> 01:00:55,180
real estate bubble tremendous height

843
01:00:55,200 --> 01:00:58,830
and with this technology was just telling you we were able to actually published in

844
01:00:58,830 --> 01:01:00,100
advance when you're in

845
01:01:00,150 --> 01:01:01,250
in advance

846
01:01:01,250 --> 01:01:03,120
that the peak of the UK bubble

847
01:01:03,210 --> 01:01:04,760
all the US bubble

848
01:01:04,760 --> 01:01:08,580
it would be respectively in mid two thousand four on the two thousand six in

849
01:01:08,580 --> 01:01:12,050
the case of the US just to impress new

850
01:01:12,060 --> 01:01:16,930
that this crisis of this stick this picks may not be so much

851
01:01:16,950 --> 01:01:23,920
so much and predictable or as is often claimed that clear evidence of consistent regime

852
01:01:23,920 --> 01:01:29,120
region so when the human is being asked what do they think is that the

853
01:01:29,120 --> 01:01:33,580
threshold they just pick up right where they've been seeing the machine present samples so

854
01:01:33,580 --> 01:01:37,890
there really humanism doing in this particular case maybe there would be a better way

855
01:01:37,890 --> 01:01:42,960
to construct it in the yellow situations it's more interesting information out there but really

856
01:01:42,960 --> 01:01:47,330
humans just reporting where has machine been looking at x

857
01:01:47,360 --> 01:01:50,260
and it's not the person is doing very much

858
01:01:50,280 --> 01:01:54,230
of anything in that case there's that it's your question

859
01:01:57,330 --> 01:02:03,390
the in the

860
01:02:07,010 --> 01:02:09,330
that's an interesting question so

861
01:02:09,410 --> 01:02:12,830
and that gets to work so the question is

862
01:02:12,840 --> 01:02:17,620
if a person were yoked with a learn a better way to solve this

863
01:02:17,620 --> 01:02:22,950
the issue of where is the threshold maybe people

864
01:02:23,130 --> 01:02:26,990
if you haven't done this before they just kind of do some random picking of

865
01:02:27,320 --> 01:02:30,750
but once they see what the machine is doing i can do this by section

866
01:02:31,120 --> 01:02:35,360
maybe people start to do bisection and i guess there have two

867
01:02:35,360 --> 01:02:40,360
so that's a good point and that might be right we didn't evaluate whether they

868
01:02:40,360 --> 01:02:44,280
learn how to learn better by being yoked to the machine and they perform better

869
01:02:44,280 --> 01:02:48,970
in future cases that's interesting and i guess we can use for cooking use summarizer

870
01:02:48,970 --> 01:02:53,220
are two kinds of flavours of strategies that people tended to follow in the active

871
01:02:53,220 --> 01:02:54,850
mode so

872
01:02:54,860 --> 01:02:57,860
one of the strategies people do

873
01:02:57,870 --> 01:03:02,100
would be to do just by bisection and then they come an error in this

874
01:03:02,100 --> 01:03:04,970
estimate the asked that same question again

875
01:03:04,980 --> 01:03:06,390
and for low

876
01:03:06,400 --> 01:03:10,820
uncertainty that i the procedure actually works relatively well

877
01:03:10,860 --> 01:03:13,780
so that was one strategy

878
01:03:13,790 --> 01:03:17,590
and the other was are more gradient type of approach where they

879
01:03:17,620 --> 01:03:21,160
searches to sort start off at one point in kind of explore from there and

880
01:03:21,160 --> 01:03:27,100
follow their knows something like that like then this idea just doing binary search and

881
01:03:27,100 --> 01:03:31,710
repeating queries in this case asking for the same egg to hatch many times until

882
01:03:31,710 --> 01:03:36,530
you sure whether you think it's more probably snicker or bird is one way of

883
01:03:36,530 --> 01:03:38,610
making a noise tolerant

884
01:03:38,620 --> 01:03:43,000
active learning algorithm you start off with our algorithm is designed to work well without

885
01:03:43,000 --> 01:03:46,700
noise present and then you just follow the same algorithm but each time you take

886
01:03:46,700 --> 01:03:51,790
a sample you repeatedly query that's one approach it turns out to be suboptimal

887
01:03:51,820 --> 01:03:57,950
it in terms of sample complexity those procedures are repeated querying don't work as well

888
01:03:57,950 --> 01:04:04,250
as something like the BBC method this probabilistic bisection that's so this question takes any

889
01:04:04,250 --> 01:04:08,110
other questions and human active learning stuff

890
01:04:08,120 --> 01:04:12,660
OK so let me just go quickly snatches century when fixing so one of the

891
01:04:12,660 --> 01:04:14,980
reasons people don't do very well

892
01:04:14,990 --> 01:04:16,770
in the very noisy setting

893
01:04:17,730 --> 01:04:19,990
you might have to do with memory because

894
01:04:20,010 --> 01:04:24,990
it requires you to remember what you've seen before you to do an informed decision

895
01:04:26,540 --> 01:04:29,040
if there is not much uncertainty uncertainty

896
01:04:29,060 --> 01:04:34,730
just remembering what happened the night last four five x suffices but with more uncertainty

897
01:04:34,760 --> 01:04:36,330
want to go

898
01:04:36,350 --> 01:04:40,560
so far away into the past so that might be one reason people are also

899
01:04:40,560 --> 01:04:43,860
not very wrong answer

900
01:04:43,980 --> 01:04:47,920
right just to follow up on that one thing we've sort of back to your

901
01:04:47,920 --> 01:04:53,340
question is maybe what people would need in certain scenarios is helpful

902
01:04:53,350 --> 01:04:58,720
way of summarizing past experience graphically or some other visual way that would help them

903
01:04:58,720 --> 01:05:01,610
make more informed choices

904
01:05:01,640 --> 01:05:06,910
as they are learning so there could be ways to help augment human memory limitations

905
01:05:06,910 --> 01:05:10,390
that are suggested by this as well so there are a lot of different directions

906
01:05:10,390 --> 01:05:16,950
it might be my goal with this question

907
01:05:17,980 --> 01:05:25,380
what we know that the

908
01:05:25,400 --> 01:05:28,120
they have a certain number of queries i'm not sure if he even knew the

909
01:05:28,120 --> 01:05:32,620
number of queries specific for each subject task was after

910
01:05:32,630 --> 01:05:35,340
after you're done just give us

911
01:05:35,340 --> 01:05:38,150
give give our best bet that

912
01:05:38,170 --> 01:05:42,270
they they probably would

913
01:05:42,280 --> 01:05:44,930
that's what i like in the twenty questions and that's what

914
01:05:44,960 --> 01:05:49,250
people do like kids like playing games like OK only have

915
01:05:49,250 --> 01:05:52,160
there's only a few policies on the board

916
01:05:52,180 --> 01:05:54,390
what should i do try to

917
01:05:54,400 --> 01:05:58,630
i guess these are just give certainty there so the other person as well as

918
01:05:58,630 --> 01:06:00,320
regression so on

919
01:06:00,330 --> 01:06:02,560
but in this case the number of business

920
01:06:02,590 --> 01:06:07,330
six and they didn't even knew it they had to report what was there as

921
01:06:07,330 --> 01:06:14,960
what we have is that the covariance between t and t j could be some

922
01:06:16,910 --> 01:06:20,170
x i and x j

923
01:06:21,750 --> 01:06:25,640
it could be some function of x i and x j

924
01:06:27,200 --> 01:06:30,460
usually what we have is that some function

925
01:06:30,470 --> 01:06:33,280
where the covariance decreases

926
01:06:33,300 --> 01:06:36,500
with distance between x i and x j

927
01:06:36,510 --> 01:06:40,210
so nearby points in input space tend to have similar

928
01:06:40,220 --> 01:06:45,490
target value output values and far points and have uncorrelated

929
01:06:45,560 --> 01:06:48,410
output values

930
01:06:48,420 --> 01:06:55,920
i wish i had to wait for the end of the war

931
01:06:56,750 --> 01:06:59,730
here's a summary of what the gas process actually is

932
01:07:00,490 --> 01:07:05,780
so guess in process places the prior directly on the space of functions

933
01:07:05,800 --> 01:07:07,210
such that

934
01:07:07,240 --> 01:07:10,800
any finite selection of points x one through x and

935
01:07:10,850 --> 01:07:16,830
in this input space the corresponding function values he one thirty and have a multivariate

936
01:07:16,830 --> 01:07:18,900
gaussians distribution

937
01:07:23,770 --> 01:07:25,700
for example these are

938
01:07:25,700 --> 01:07:28,990
four samples from guassian process

939
01:07:29,570 --> 01:07:34,390
where does the input space and this is the output space

940
01:07:37,080 --> 01:07:40,310
these are samples from the prior

941
01:07:40,350 --> 01:07:44,440
if we observe some data would say we observe these three data points these to

942
01:07:44,460 --> 01:07:47,200
be samples from the posterior distribution

943
01:07:47,200 --> 01:07:52,130
of this casting process

944
01:07:52,950 --> 01:07:58,280
you know the the functions go through the data i have higher probability

945
01:07:58,560 --> 01:08:01,940
once we observed the data and the functions they don't go through the data

946
01:08:02,150 --> 01:08:07,990
so the covariance between two function values ti and tj

947
01:08:08,040 --> 01:08:11,100
under this prize given by the covariance function

948
01:08:11,110 --> 01:08:13,530
some function of x i and x j

949
01:08:13,540 --> 01:08:17,540
which typically decays monotonically with the distance between these two

950
01:08:17,550 --> 01:08:21,530
and that encodes the fact that you believe the function to be sort of smooth

951
01:08:21,580 --> 01:08:26,150
but you can put all sorts of information here you can build in periodic function

952
01:08:26,220 --> 01:08:30,360
is you can have a rough functions if you want get functions with different length

953
01:08:30,360 --> 01:08:34,350
scales you functions with jumps in the etcetera etcetera

954
01:08:34,490 --> 01:08:37,920
and you do all that by controlling this covariance function

955
01:08:37,940 --> 01:08:40,790
and the covariance function is exactly

956
01:08:40,810 --> 01:08:44,840
why people sometimes call the kernel function

957
01:08:45,590 --> 01:08:47,850
so the kernel

958
01:08:47,890 --> 01:08:49,630
is essentially

959
01:08:50,530 --> 01:08:56,700
it has to satisfy certain positive definiteness constraints and those of the same constraints that

960
01:08:56,700 --> 01:09:00,420
the covariance function has to satisfy

961
01:09:00,430 --> 01:09:02,190
and the kernel is also

962
01:09:02,200 --> 01:09:07,530
often derived in terms of the dot product in some feature space and here exactly

963
01:09:07,530 --> 01:09:09,970
i define this covariance function

964
01:09:09,980 --> 01:09:15,380
in terms of the dot product in feature spaces dot products some over d

965
01:09:15,390 --> 01:09:16,710
of these

966
01:09:16,920 --> 01:09:20,780
elements in d could actually be infinite dimensional by one two

967
01:09:20,840 --> 01:09:22,960
OK so this is finite

968
01:09:22,960 --> 01:09:25,750
covariance you know this is the covariance function

969
01:09:25,770 --> 01:09:30,690
the comes from a finite number of basis functions but many useful covariance functions correspond

970
01:09:30,690 --> 01:09:33,790
to infinitely many basis functions just like many

971
01:09:33,840 --> 01:09:37,600
the kernel corresponds to a mapping into two-dimensional

972
01:09:37,960 --> 01:09:41,630
feature spaces

973
01:09:42,560 --> 01:09:44,860
what we want to think of this

974
01:09:45,880 --> 01:09:49,560
it is if we have a kernel or covariance function and then either we could

975
01:09:49,560 --> 01:09:50,920
do something like

976
01:09:52,010 --> 01:09:52,820
you know

977
01:09:52,830 --> 01:09:58,510
support vector regression using the kernel or we can do something like calcium process regression

978
01:09:58,560 --> 01:10:03,390
well we treat that kernel as a covariance function casting process and then we get

979
01:10:03,440 --> 01:10:08,300
things like error bars on all predictions we can compute marginal likelihood so we can

980
01:10:08,300 --> 01:10:11,850
compare between different kernels and do model selection

981
01:10:11,900 --> 01:10:16,150
etcetera etcetera

982
01:10:16,160 --> 01:10:19,470
that's the regression case and i can tell you about classification but let me take

983
01:10:19,470 --> 01:10:21,530
some questions

984
01:10:22,250 --> 01:10:27,200
speech recognition problem which

985
01:10:29,110 --> 01:10:34,390
support vector machines basically

986
01:10:36,600 --> 01:10:39,080
finding the mean of the gas in process

987
01:10:40,700 --> 01:10:43,930
and if i the mean of gas process and as far as i understand it

988
01:10:43,930 --> 01:10:45,290
so basically

989
01:10:45,960 --> 01:10:49,900
you have

990
01:10:52,230 --> 01:10:54,510
essentially you don't get the sparsity

991
01:10:54,560 --> 01:10:59,750
right in the quadratic support vector machines support vector regression if you do support vector

992
01:11:01,070 --> 01:11:01,990
now it

993
01:11:02,230 --> 01:11:04,100
the quadratic term

994
01:11:04,380 --> 01:11:09,610
and then you don't get the error bars either that you get in in this

995
01:11:09,820 --> 01:11:13,770
procedure so you don't have uncertainty in the values of x you don't get the

996
01:11:13,770 --> 01:11:15,860
model selection procedures and so on

997
01:11:17,500 --> 01:11:18,660
yeah i

998
01:11:18,660 --> 01:11:22,230
i'm not sure why we would want to use those

999
01:11:22,240 --> 01:11:28,790
any other questions about this

1000
01:11:28,840 --> 01:11:43,320
what you see here is a covariance function

1001
01:11:43,350 --> 01:11:46,940
which says that the covariance between x i and x j

1002
01:11:46,940 --> 01:11:49,660
the cases e to the minus

1003
01:11:49,760 --> 01:11:52,310
the norm of excitement six

1004
01:11:52,330 --> 01:11:54,720
OK you know minus the norm squared

1005
01:11:55,250 --> 01:11:59,540
so the covariance function decays with distance between science today

1006
01:11:59,540 --> 01:12:01,270
go for it

1007
01:12:01,280 --> 01:12:04,620
four modules of graphical models

1008
01:12:12,790 --> 01:12:19,630
now it seems like you

1009
01:12:21,900 --> 01:12:26,290
i mean field another type of graphical models like vision

1010
01:12:27,130 --> 01:12:32,520
it's just not my field will see that there there will be no

1011
01:12:32,590 --> 01:12:35,870
are but just connect nodes

1012
01:12:38,200 --> 01:12:40,950
let's start

1013
01:12:41,490 --> 01:12:43,680
if you remember you saw

1014
01:12:43,740 --> 01:12:45,480
the networks

1015
01:12:45,490 --> 01:12:46,810
we basically

1016
01:12:46,840 --> 01:12:49,090
o thing in bayesian networks

1017
01:12:49,160 --> 01:12:51,200
by starting from

1018
01:12:51,270 --> 01:12:54,100
this object which is the probability

1019
01:12:54,170 --> 01:12:56,300
of the five

1020
01:12:56,450 --> 01:12:58,160
five will be

1021
01:12:58,170 --> 01:12:59,880
index smaller

1022
01:13:00,560 --> 01:13:01,210
o thing

1023
01:13:01,240 --> 01:13:04,040
so there is no right

1024
01:13:04,100 --> 01:13:09,370
we were working on these options essentially the assumption was that we were going to

1025
01:13:10,370 --> 01:13:12,210
a few edges

1026
01:13:12,320 --> 01:13:14,170
that connect

1027
01:13:15,790 --> 01:13:20,820
and then we obtain the conditional independence assumption

1028
01:13:22,320 --> 01:13:24,960
and this also that when we should use

1029
01:13:25,710 --> 01:13:30,790
conditional independence assumptions of the conditional independence assumption

1030
01:13:31,680 --> 01:13:35,590
p as well

1031
01:13:37,170 --> 01:13:41,430
there are certain types of conditional independence

1032
01:13:41,510 --> 01:13:43,790
statements that cannot

1033
01:13:43,840 --> 01:13:47,790
the set five exactly what you want to do

1034
01:13:47,840 --> 01:13:50,180
make it a little bit more precise

1035
01:13:53,150 --> 01:13:54,760
state is

1036
01:13:54,810 --> 01:13:56,120
very very

1037
01:13:58,120 --> 01:14:00,510
in other words there are certain classes

1038
01:14:00,530 --> 01:14:04,200
all interesting probability distribution

1039
01:14:04,210 --> 01:14:07,520
that vision is more

1040
01:14:07,530 --> 01:14:09,930
appropriate appropriate

1041
01:14:09,970 --> 01:14:15,030
model to represent

1042
01:14:15,080 --> 01:14:19,390
so if you like freedom also represent clusters of

1043
01:14:19,590 --> 01:14:20,690
it is

1044
01:14:20,710 --> 01:14:26,400
so there is another class of graphical model which you come here

1045
01:14:26,410 --> 01:14:33,660
it is widely used in many of the party

1046
01:14:34,470 --> 01:14:37,060
o thing about michael feels that they

1047
01:14:37,080 --> 01:14:40,870
look for the specification of a different class

1048
01:14:40,890 --> 01:14:44,560
of conditional independence day

1049
01:14:44,620 --> 01:14:46,650
when compared the

1050
01:14:46,690 --> 01:14:50,660
so in some sense microbial invasion

1051
01:14:51,960 --> 01:14:56,680
not exactly those of her relationship with

1052
01:14:56,680 --> 01:14:58,430
class of

1053
01:14:58,440 --> 01:15:01,640
condition and status for light field

1054
01:15:02,360 --> 01:15:03,090
can be

1055
01:15:03,110 --> 01:15:07,840
we find easily in terms of graph we know that we know all directed edges

1056
01:15:07,840 --> 01:15:11,270
with undirected

1057
01:15:11,370 --> 01:15:16,190
and those things that formal separation in the case of the americans in this case

1058
01:15:16,280 --> 01:15:18,300
will become much easier

1059
01:15:18,370 --> 01:15:21,590
because there will be no all directions in the

1060
01:15:21,610 --> 01:15:23,680
edges and things

1061
01:15:23,960 --> 01:15:28,000
is it understand what

1062
01:15:28,060 --> 01:15:30,180
in same cases we had

1063
01:15:30,190 --> 01:15:32,780
the concept of the separation

1064
01:15:32,870 --> 01:15:36,190
in vision that had the concept of

1065
01:15:37,470 --> 01:15:38,910
in the case of the

1066
01:15:38,930 --> 01:15:40,250
or just think

1067
01:15:41,890 --> 01:15:44,530
this is exactly this case for example

1068
01:15:45,280 --> 01:15:47,930
a set a is separated

1069
01:15:47,940 --> 01:15:51,900
from the set b by the set

1070
01:15:51,900 --> 01:15:54,330
in this case

1071
01:15:54,340 --> 01:15:56,240
we simply say that

1072
01:15:56,340 --> 01:15:58,030
this is true if

1073
01:15:58,080 --> 01:16:02,310
every path that goes from here to here

1074
01:16:02,370 --> 01:16:04,430
passes through

1075
01:16:04,470 --> 01:16:05,810
one element

1076
01:16:08,140 --> 01:16:12,060
there's basically no from a to b

1077
01:16:12,110 --> 01:16:14,760
that doesn't go through c

1078
01:16:16,000 --> 01:16:20,440
more technically if you have an undirected graph g being a b and c disjoint

1079
01:16:20,440 --> 01:16:22,440
subsets of nodes

1080
01:16:22,480 --> 01:16:27,220
if every path from a to b includes at least one of them c

1081
01:16:27,270 --> 01:16:31,160
then c is said to separate a from b

1082
01:16:31,220 --> 01:16:33,160
in december

1083
01:16:33,210 --> 01:16:33,960
this is the

1084
01:16:33,980 --> 01:16:38,170
this case definition very simple because it is about a bold action

1085
01:16:38,270 --> 01:16:40,530
this definition of graphs by

1086
01:16:40,540 --> 01:16:42,040
the definition

1087
01:16:42,300 --> 01:16:46,550
in the same way as we have a definition for these words

1088
01:16:46,600 --> 01:16:53,400
which was much more tricky

1089
01:16:53,400 --> 01:16:56,340
the feature from mark

1090
01:16:56,350 --> 01:16:58,230
it is

1091
01:16:58,250 --> 01:17:03,860
in the same way as patient networks work class or but distribution that could be

1092
01:17:03,880 --> 01:17:05,920
prior look forward to

1093
01:17:05,940 --> 01:17:07,170
the graph

1094
01:17:07,260 --> 01:17:12,710
in the microrna is set of probability distributions

1095
01:17:12,710 --> 01:17:17,810
that that that basically explores what is a best way to

1096
01:17:17,810 --> 01:17:24,450
estimate the slope of a power-law dis distribution and how to test for

1097
01:17:24,450 --> 01:17:29,630
various hypothesis in like because we can have a we can have a power-low

1098
01:17:29,630 --> 01:17:33,010
you can have a log normal and so on so there are many different kinds of

1099
01:17:33,010 --> 01:17:37,770
distributions that are all heavy tailed and then you'd like to distinguish between them and this is

1100
01:17:37,770 --> 01:17:43,450
the paper that deals with that so if you'd like more on that topic

1101
01:17:43,450 --> 01:18:04,610
check out this paper okay so I can change

1102
01:18:04,610 --> 01:18:09,210
so this is what I want to talk about next so now we saw how

1103
01:18:09,480 --> 01:18:12,830
how how the networks are what I want to talk now is how do

1104
01:18:12,830 --> 01:18:16,820
things propagate in the network right so this will be like part two then we'll do a

1105
01:18:16,830 --> 01:18:20,910
break and then I'll I'll show this because I think it's very interesting and

1106
01:18:20,910 --> 01:18:28,630
entertaining so this is this is the outline of what I'll talk in this part right so

1107
01:18:31,130 --> 01:18:38,630
I want first I want to go over some basic mathematical models of wireless propagation and then

1108
01:18:38,630 --> 01:18:44,610
diffusion and also see what kind of algorithmic consequences do these models

1109
01:18:44,610 --> 01:18:50,740
give us then I want to show some empirical studies of of diffusion on large on

1110
01:18:50,740 --> 01:18:56,110
on large networks namely the the viral marketing how people recommend products and the

1111
01:18:56,110 --> 01:19:02,610
how information propagates and then show some more things so I I showed this

1112
01:19:02,610 --> 01:19:05,910
slide slide already right I asked the question what do we know about the

1113
01:19:05,910 --> 01:19:11,090
networks and in the in the in the past so in the previous before

1114
01:19:11,090 --> 01:19:15,310
lunch what we saw was was about the structure so now we want to know

1115
01:19:15,320 --> 01:19:21,200
about processes and dynamics and what you should think off is again the same slide as I

1116
01:19:21,200 --> 01:19:26,550
had right for example this shows how the DVD recommendations propagate right this would be people

1117
01:19:26,810 --> 01:19:32,150
that made a that buy a DVD and then they make recommendations and you see nobody really

1118
01:19:32,150 --> 01:19:36,950
follows them maybe this person followed and then made some more recommendations and I think this

1119
01:19:36,960 --> 01:19:42,490
is this is the propagation graph of some of some disease how it started

1120
01:19:42,490 --> 01:19:49,110
and how it spread and just visually the graphs have this similar structure of having these

1121
01:19:49,110 --> 01:19:56,910
like clouds of points so what should you think of diffusion or what is diffusion

1122
01:19:56,910 --> 01:20:02,290
right it's like a behavior that cascades from node to node in an epidemic

1123
01:20:02,290 --> 01:20:06,390
fashion okay and there are numerous examples of these for example we can have

1124
01:20:06,390 --> 01:20:12,450
like news opinions rumors fads that propagate through the social network we can have this

1125
01:20:12,450 --> 01:20:18,410
word of mouth effects in like marketing like how new websites free web based services how

1126
01:20:18,410 --> 01:20:21,460
do  we get to know out of them right usually some friend tells us and

1127
01:20:21,460 --> 01:20:26,770
we are happy and we go tell we go tell this on then a also very classical example are

1128
01:20:26,770 --> 01:20:31,530
viruses virus propagation you have to be in with someone so if I have flu I

1129
01:20:31,530 --> 01:20:35,350
have to be in contact with someone else so that they get the flu right so it also

1130
01:20:35,350 --> 01:20:39,930
propagates over over the network then similarly you could say for like smoking

1131
01:20:39,950 --> 01:20:46,420
recycling so this like changes in social priorities or behavior then same for like news coverage

1132
01:20:46,440 --> 01:20:52,430
how the particular topic diffuses through the network I'll talk this about this more towards the end

1133
01:20:52,430 --> 01:20:56,010
and so on sorry

1134
01:20:56,390 --> 01:21:02,310
okay so this is what I want and now I want to briefly review some

1135
01:21:02,310 --> 01:21:09,470
of the classical studies of diffusion and it all started here in nineteen forty-three where

1136
01:21:09,470 --> 01:21:14,850
Ryan and Gross were studying the the spread of new agricultural practices right so they

1137
01:21:14,850 --> 01:21:18,890
went to Iowa and they they talked for a few years with these

1138
01:21:18,900 --> 01:21:24,230
like two hundred sixty farmers and in that time so I think at the end of

1139
01:21:24,230 --> 01:21:30,270
nineteen twenties there was a new corn seed this seed if you planted

1140
01:21:30,270 --> 01:21:35,410
this new corn seed instead of traditional corn seed then you would harvest much more much

1141
01:21:35,410 --> 01:21:38,630
more corn but the problem with this new one is that every year you have

1142
01:21:38,630 --> 01:21:42,550
to buy it again right so you can't save some of the seed from the previous years and

1143
01:21:42,550 --> 01:21:45,850
plant that seed again and get more and more corn so every year you

1144
01:21:45,860 --> 01:21:50,010
have to buy this new corn so there is some trade off for the farmer so

1145
01:21:50,010 --> 01:21:53,150
they have to buy this seed every year which is a disadvantage for them but they

1146
01:21:53,150 --> 01:21:57,170
could harvest more and more corn so they were studying how how this

1147
01:21:57,190 --> 01:22:04,870
corn adoption spread between these armers and what they found was that these

1148
01:22:04,910 --> 01:22:11,870
interpersonal network played the crucial role in adoption of these new hybrid corn seed so

1149
01:22:11,870 --> 01:22:17,230
what they basically established is that diffusion is a social process right how new new

1150
01:22:17,230 --> 01:22:23,050
corn seed got adopted is a is a is a social process then so

1151
01:22:23,050 --> 01:22:26,810
this is the first study the second study that came a bit later

1152
01:22:26,810 --> 01:22:32,430
was the spread of a new medical practices so here these people from

1153
01:22:32,430 --> 01:22:36,590
Columbia University they went and they studied adoption of a new drug between the doctors

1154
01:22:36,590 --> 01:22:40,190
Illinois so they they went and they talked to the doctors at that time

1155
01:22:40,190 --> 01:22:46,070
there was some new new antibiotic that just came out and for doctors

1156
01:22:46,070 --> 01:22:51,360
they they see how the adoption of this new antibiotic that was very good spread

1157
01:22:51,370 --> 01:22:57,070
and again they saw for example that the clinical studies and like scientific evaluations

1158
01:22:57,070 --> 01:23:01,410
they did not play much role in doctors like convincing doctors

1159
01:23:01,410 --> 01:23:06,570
to use this antibiotic but what played the role was like social power of peers for

1160
01:23:06,570 --> 01:23:11,130
example the one of the most important features was that people who were at the

1161
01:23:11,130 --> 01:23:17,070
same hospital were more likely to adopt this new behavior and so on so again it's

1162
01:23:17,080 --> 01:23:21,090
the it's the social network the social network that leads to adoption and not necessarily some

1163
01:23:21,090 --> 01:23:29,390
kind of objective studies that this antibiotic really works okay so these so these are two

1164
01:23:29,400 --> 01:23:36,020
most well known ones for example people also tried to study another classical study would

1165
01:23:36,020 --> 01:23:40,890
be the adoption of the of the Dvorzak keyboard so the querty keyboard we are

1166
01:23:40,890 --> 01:23:45,370
are using today is like very slow for typing right so there is this new keyboard

1167
01:23:45,390 --> 01:23:48,870
layout that is much better so you could think that that could also

1168
01:23:48,870 --> 01:23:54,090
spread virally right if you use it you you are happy with it you tell it to your friends but that doesn't

1169
01:23:54,090 --> 01:23:59,780
spread nobody is using that keyboard so it seems I don't know the threshold to enter to to high or

1170
01:24:00,310 --> 01:24:04,050
there was another study where they went to Chile and they were they went

1171
01:24:04,050 --> 01:24:08,870
to some village some tribe and they were trying to promote that

1172
01:24:08,870 --> 01:24:13,810
theladies at home should cook the water before using it and again what they

1173
01:24:13,810 --> 01:24:18,130
found was that they did not influence the right people at start so that the whole village

1174
01:24:18,130 --> 01:24:24,370
would start pre-cooking the water before they are using it sort of prevent the diseases so

1175
01:24:24,370 --> 01:24:30,390
there are successful examples of I don't know diffusion and there are also many not successful

1176
01:24:30,390 --> 01:24:37,790
examples of diffusion the diffusion or cascades can can have a very very very

1177
01:24:37,800 --> 01:24:42,910
interesting flavors for example there was just a recent study where they were studied

1178
01:24:42,910 --> 01:24:48,830
they were studying how being overweight spreads virally right so for example they

1179
01:24:48,830 --> 01:24:53,310
found they in the paper they have this claim that if you have a overweight friend then

1180
01:24:53,310 --> 01:24:58,240
introduce a binary random variable say that marks the relevance of the document d

1181
01:24:58,240 --> 01:25:02,710
and we have a query q and what we'd like to know is whether given

1182
01:25:02,710 --> 01:25:04,690
a particular query document

1183
01:25:04,780 --> 01:25:08,850
d is relevant not and you know one way to look at this is simply

1184
01:25:08,860 --> 01:25:11,550
to apply this rule here

1185
01:25:11,560 --> 01:25:14,870
and so we will have to contributions here

1186
01:25:14,900 --> 01:25:19,570
the prior probability of relevance for document well that can is usually based on things

1187
01:25:19,570 --> 01:25:24,950
like popularity of the document or it's just a constant on the context

1188
01:25:25,040 --> 01:25:28,330
and then something written here which is

1189
01:25:28,390 --> 01:25:32,500
you know in in plain language you can understand or you could express is the

1190
01:25:32,500 --> 01:25:36,150
probability of generating a particular query q

1191
01:25:36,340 --> 01:25:40,450
to ask for relevant d so given that we assume that the document is relevant

1192
01:25:40,450 --> 01:25:42,750
what's the probability to ask

1193
01:25:42,800 --> 01:25:45,380
for the document with a particular query

1194
01:25:46,640 --> 01:25:49,990
and that's basically

1195
01:25:50,460 --> 01:25:52,340
OK so

1196
01:25:52,760 --> 01:25:57,800
the standard approach to that is

1197
01:25:57,800 --> 01:26:01,710
to basically treat the query terms as a sample

1198
01:26:02,130 --> 01:26:05,420
drawn from relevant documents basically

1199
01:26:05,450 --> 01:26:11,560
i mean their models of different complexity here but basically what we'd like to have

1200
01:26:11,570 --> 01:26:12,920
is a

1201
01:26:12,940 --> 01:26:16,460
a model for each document that tells us how probable so the words are and

1202
01:26:16,460 --> 01:26:23,260
then we just assume that queries the sample generated from these were

1203
01:26:23,270 --> 01:26:25,190
OK so this is

1204
01:26:25,230 --> 01:26:30,960
this is the language modeling paradigm again to make clear conceptually so the idea is

1205
01:26:30,960 --> 01:26:32,770
the user has some

1206
01:26:32,780 --> 01:26:36,750
document in mind that is kind of irrelevant relevant doc perhaps or

1207
01:26:36,770 --> 01:26:38,400
a relevant document

1208
01:26:38,460 --> 01:26:40,490
and from that

1209
01:26:40,590 --> 01:26:42,100
use it generates

1210
01:26:42,130 --> 01:26:43,800
a particular query

1211
01:26:43,820 --> 01:26:46,490
and then we'd like to reverse

1212
01:26:51,130 --> 01:26:53,240
so what is no need approach

1213
01:26:53,780 --> 01:26:58,590
for doing that where we just say well we just use you know the maximum

1214
01:26:58,590 --> 01:27:03,070
likelihood estimate here i mean we're calling it there is basically no we could just

1215
01:27:03,070 --> 01:27:04,760
use the

1216
01:27:04,770 --> 01:27:06,800
just the the

1217
01:27:06,810 --> 01:27:08,330
count statistics

1218
01:27:08,350 --> 01:27:12,360
right normalize and that's the probability to see word in the document so that just

1219
01:27:14,070 --> 01:27:18,720
it assigns relative frequencies words that actually occur but then of course we have this

1220
01:27:19,870 --> 01:27:24,080
that the terms did not occur gets zero probability and then you know a query

1221
01:27:24,080 --> 01:27:27,840
that contains the term that contained in the document will just get a score of

1222
01:27:30,120 --> 01:27:34,690
the real question here is in the estimation problem is how can we bring in

1223
01:27:34,690 --> 01:27:39,850
the other documents in the collection is a very practical point of view

1224
01:27:39,860 --> 01:27:43,840
the idea is not just to treat document in isolation but rather the whole collection

1225
01:27:43,840 --> 01:27:48,230
which is also encoded in this term document matrix that encodes the whole

1226
01:27:48,270 --> 01:27:50,190
the document collection

1227
01:27:51,390 --> 01:27:56,100
and how can we bring that into improved to come up with better estimates

1228
01:27:56,110 --> 01:27:57,500
all of these

1229
01:27:57,500 --> 01:28:02,310
document specific language model the probability of the word given doc

1230
01:28:03,970 --> 01:28:06,360
and so this

1231
01:28:06,450 --> 01:28:08,390
the approach that

1232
01:28:08,570 --> 01:28:13,760
we've been developing under the name probabilistic latent semantic analysis works like this OK we

1233
01:28:13,760 --> 01:28:15,780
have again document in terms

1234
01:28:15,790 --> 01:28:18,740
and we introduce an intermediate

1235
01:28:18,750 --> 01:28:22,980
layer like bottleneck layer of so called latent concepts

1236
01:28:23,040 --> 01:28:24,750
and what we

1237
01:28:24,760 --> 01:28:28,110
try to do now instead of you know learning directly

1238
01:28:28,130 --> 01:28:33,300
probabilities of terms and documents we learn the probability for each document two

1239
01:28:33,710 --> 01:28:36,800
if you like participate in particular con cept

1240
01:28:37,810 --> 01:28:39,940
and then the probability

1241
01:28:40,050 --> 01:28:45,330
for each concept that that concept is expressed as concept is expressed by a particular

1242
01:28:45,330 --> 01:28:49,240
term OK so the idea is we might have a document here

1243
01:28:49,290 --> 01:28:55,630
dealing with issues related to trade hopefully our models we feed our model talk about

1244
01:28:55,630 --> 01:28:56,730
the second

1245
01:28:56,780 --> 01:29:00,500
you will find a concept that might correspond to something like trade and that might

1246
01:29:00,500 --> 01:29:04,710
have very high probability to be expressed by terms like coming

1247
01:29:04,720 --> 01:29:09,650
in trade

1248
01:29:12,980 --> 01:29:17,590
notice that these a concept expression probabilities are estimated based on all documents that are

1249
01:29:17,590 --> 01:29:18,990
dealing with a concept

1250
01:29:19,010 --> 01:29:22,440
we use in the whole collection at that point try to

1251
01:29:24,630 --> 01:29:26,770
and what we're doing here anyway

1252
01:29:26,770 --> 01:29:31,960
is we'll see the next slide is basically a mixture model where we assume that

1253
01:29:32,040 --> 01:29:34,320
each document is composed of simple

1254
01:29:34,340 --> 01:29:37,790
a mixture of topics or concepts

1255
01:29:37,830 --> 01:29:42,390
and that's what we're trying to uncover

1256
01:29:42,620 --> 01:29:45,870
the nice thing about this is is completely unsupervised

1257
01:29:47,670 --> 01:29:51,260
just based on the co occurrences of words

1258
01:29:51,270 --> 01:29:55,140
OK and here's what i just described the picture

1259
01:29:55,200 --> 01:29:58,840
it is also very simple

1260
01:30:00,030 --> 01:30:02,010
as an algebraic expression

1261
01:30:02,030 --> 01:30:04,540
how this model looks like so

1262
01:30:04,580 --> 01:30:09,960
probability of word given document is a simple mixture here because these services are document

1263
01:30:09,960 --> 01:30:11,300
language model

1264
01:30:11,300 --> 01:30:13,330
we have these latent concept

1265
01:30:13,390 --> 01:30:14,250
and then

1266
01:30:14,260 --> 01:30:21,380
we have this concept expression probabilities and this document specific mixture

1267
01:30:21,400 --> 01:30:26,400
another way to look at this is to say that it's a convex combination of

1268
01:30:26,400 --> 01:30:29,830
these second expression for every document

1269
01:30:29,840 --> 01:30:32,980
and these add up to one negative

1270
01:30:32,980 --> 01:30:37,200
right forms a convex combination everything appeared w given c is a vector from the

1271
01:30:37,200 --> 01:30:39,900
convex combination of the

1272
01:30:39,960 --> 01:30:44,310
OK so we're trying to find kind of the basis or set of

1273
01:30:44,380 --> 01:30:45,990
vectors here

1274
01:30:46,000 --> 01:30:47,330
so that by

1275
01:30:47,330 --> 01:30:53,590
so the final decision will be on treaty restricted boltzmann machines using approximations like gradient

1276
01:30:53,590 --> 01:30:56,120
i teach monty them in

1277
01:30:56,150 --> 01:31:03,070
this thank you can you hear me

1278
01:31:03,440 --> 01:31:07,550
so i am

1279
01:31:08,010 --> 01:31:10,440
what is the same group rocks

1280
01:31:10,480 --> 01:31:15,510
and i'm going to talk about how to train these restricted boltzmann machines and related

1281
01:31:16,820 --> 01:31:18,460
we have seen

1282
01:31:18,480 --> 01:31:24,200
many most famous using contrastive divergence but the ways

1283
01:31:24,210 --> 01:31:32,320
so let's talk about about markov random fields mrfs also known as undirected graphical models

1284
01:31:32,320 --> 01:31:36,850
with without hidden unit

1285
01:31:36,890 --> 01:31:41,900
these these things in the popular through machines an example but also

1286
01:31:41,920 --> 01:31:44,250
from the physical ones

1287
01:31:44,700 --> 01:31:48,070
with connections between the visible units

1288
01:31:50,150 --> 01:31:51,830
this can be used for

1289
01:31:51,880 --> 01:31:55,440
from the training dataset that this

1290
01:31:55,460 --> 01:32:01,390
unsupervised learning nothing conditionally here

1291
01:32:01,610 --> 01:32:07,080
the problem which is the thing you might want to do with this is intractable

1292
01:32:07,250 --> 01:32:15,830
unless of course you specify very restrictive collectivity like true collectivity or collectivity

1293
01:32:16,520 --> 01:32:18,850
and you know you throw

1294
01:32:18,910 --> 01:32:21,140
the potential of these models

1295
01:32:21,270 --> 01:32:22,970
i fully connected

1296
01:32:22,990 --> 01:32:24,520
there are many things because

1297
01:32:27,610 --> 01:32:29,770
so some work and

1298
01:32:29,800 --> 01:32:32,850
and i mentioned

1299
01:32:32,890 --> 01:32:35,570
this various data collected

1300
01:32:37,270 --> 01:32:44,960
gradient approximators suggest contrastive divergence pseudo likelihood and so must be a fairly inaccurate

1301
01:32:44,970 --> 01:32:46,990
as the rest has shown you

1302
01:32:47,020 --> 01:32:53,220
using CV one contrastive divergence with one step pretty bad models

1303
01:32:53,300 --> 01:32:57,270
and the very popular ground is this one

1304
01:32:57,300 --> 01:33:02,110
just do something else using direct way

1305
01:33:04,610 --> 01:33:06,110
that's good news

1306
01:33:06,180 --> 01:33:11,710
there is is a solution that works a lot better all of these

1307
01:33:11,710 --> 01:33:16,740
well let's have a look at what exactly is the issue here is that the

1308
01:33:16,900 --> 01:33:20,680
markov random fields models

1309
01:33:20,740 --> 01:33:23,710
which means that the

1310
01:33:23,730 --> 01:33:25,510
they defined

1311
01:33:27,140 --> 01:33:28,560
for that you cannot immediately

1312
01:33:28,670 --> 01:33:34,730
see probability because it has to be divided by the sum of these normalized probabilities

1313
01:33:34,770 --> 01:33:40,200
and that is what makes so many things and is intractable

1314
01:33:40,450 --> 01:33:45,640
the training procedure for these models for these

1315
01:33:45,710 --> 01:33:48,420
levels consist of two parts

1316
01:33:48,460 --> 01:33:52,210
first we must increase the probability

1317
01:33:52,230 --> 01:33:54,610
of the training data

1318
01:33:54,620 --> 01:33:56,300
second last

1319
01:33:56,300 --> 01:33:58,980
reduce the probability

1320
01:34:00,650 --> 01:34:05,330
states of configurations that do not appear in the training data

1321
01:34:05,340 --> 01:34:13,300
so one of the best papers by christina now called this that the probability of

1322
01:34:13,370 --> 01:34:15,430
from some area

1323
01:34:15,990 --> 01:34:20,420
and that's what we have to do

1324
01:34:25,140 --> 01:34:29,240
yes and it may have

1325
01:34:29,290 --> 01:34:34,520
the large probability to training data but it sounds like

1326
01:34:34,580 --> 01:34:38,920
as probability elsewhere that's the model

1327
01:34:39,010 --> 01:34:43,140
so must to reduce amount probability elsewhere

1328
01:34:43,400 --> 01:34:44,860
using itself

1329
01:34:44,900 --> 01:34:46,330
it's not to difficult

1330
01:34:46,360 --> 01:34:49,240
the problem is to find the area

1331
01:34:49,540 --> 01:34:53,080
i would be if we could draw samples from but that's one of the things

1332
01:34:53,080 --> 01:34:55,050
that's intractable in these models

1333
01:34:55,210 --> 01:34:58,360
so has the difficulty

1334
01:34:58,370 --> 01:35:06,840
to get samples what want to get approximate samples what one usually does this sort

1335
01:35:06,840 --> 01:35:08,760
of the markov chain

1336
01:35:09,090 --> 01:35:13,610
with gibbs transition operators transition operators

1337
01:35:13,620 --> 01:35:15,700
initialize some state

1338
01:35:15,730 --> 01:35:17,990
for many updates and

1339
01:35:18,000 --> 01:35:19,820
after long time say

1340
01:35:19,840 --> 01:35:23,990
the statement about approximate sample he's going to do the

1341
01:35:24,090 --> 01:35:28,860
the other thing going to reduce the possibility

1342
01:35:28,920 --> 01:35:31,490
that's a lot of time if you want to do that

1343
01:35:31,610 --> 01:35:38,090
and when they said well takes infinite time that's not available

1344
01:35:38,670 --> 01:35:43,650
some things do this the approximately

1345
01:35:43,730 --> 01:35:45,550
there was

1346
01:35:45,550 --> 01:35:46,940
so samples

1347
01:35:46,950 --> 01:35:48,630
not really examples but the

1348
01:35:48,650 --> 01:35:54,570
blood samples that it's close to the training data so

1349
01:35:58,370 --> 01:36:03,550
contrastive divergence mentioned and pseudo likelihood of the best examples of these

1350
01:36:03,550 --> 01:36:07,490
in pseudolikelihood also explains

1351
01:36:07,590 --> 01:36:12,540
you take one you take the training data points and change one of the values

1352
01:36:12,540 --> 01:36:13,530
in that

1353
01:36:13,600 --> 01:36:16,100
and you decide that what you get that

1354
01:36:17,990 --> 01:36:22,850
use that sort example to do something that of course is very close to the

1355
01:36:22,850 --> 01:36:24,340
training data

1356
01:36:24,380 --> 01:36:29,260
so was contested edits especially contrastive divergence with one step which is the most popular

1357
01:36:29,260 --> 01:36:31,610
approach because it's the fastest

1358
01:36:35,300 --> 01:36:38,530
markov chain at some training data

1359
01:36:38,530 --> 01:36:43,680
but we should have a lot of questions like what is this denomination c

1360
01:36:43,720 --> 01:36:50,430
and more than what are these functions fj which are functions of x and y

1361
01:36:50,580 --> 01:36:55,940
and so maybe i'm so i'll give the quick answer to the question was the

1362
01:36:55,940 --> 01:36:57,440
nominator first

1363
01:36:57,450 --> 01:37:01,580
so this is sort of the to just the formal answer

1364
01:37:01,630 --> 01:37:04,320
e of x and w

1365
01:37:04,340 --> 01:37:05,900
is the sum

1366
01:37:05,930 --> 01:37:09,080
over all y prime

1367
01:37:11,950 --> 01:37:14,490
some of the j

1368
01:37:15,800 --> 01:37:16,830
fj j

1369
01:37:18,130 --> 01:37:19,940
why prime

1370
01:37:23,580 --> 01:37:24,770
here we have

1371
01:37:24,780 --> 01:37:29,340
the numerator which refers to x and y x and y come from here

1372
01:37:29,430 --> 01:37:32,750
and here we have a sum over all y prime

1373
01:37:32,780 --> 01:37:34,330
of the numerator

1374
01:37:34,340 --> 01:37:40,060
x and y prime so i plugged in why prime et cetera why can can

1375
01:37:41,160 --> 01:37:45,040
and i said the only thing that we need to assume

1376
01:37:45,060 --> 01:37:46,480
about the

1377
01:37:46,490 --> 01:37:52,950
this about the labels that y is that is finite and the reason why

1378
01:37:52,960 --> 01:37:57,450
the reason i need the labels so widely finite is you here i'm something of

1379
01:37:57,470 --> 01:38:01,330
everything in the label set so that only well-defined if i'm something of a finite

1380
01:38:02,420 --> 01:38:04,970
but can somebody explain

1381
01:38:06,220 --> 01:38:11,480
this has to be the formal definition of c

1382
01:38:13,400 --> 01:38:17,930
without this having really any particular intuitive meaning it still has mathematically has to be

1383
01:38:17,930 --> 01:38:20,520
this why

1384
01:38:23,160 --> 01:38:27,990
normalisation so can you explain a bit more

1385
01:38:32,500 --> 01:38:34,700
for any x

1386
01:38:34,750 --> 01:38:37,810
one otherwise is going to be the label

1387
01:38:38,560 --> 01:38:41,170
when i sum of all the wise

1388
01:38:41,190 --> 01:38:44,400
the sum of the probabilities has to be one

1389
01:38:46,340 --> 01:38:47,680
the so

1390
01:38:48,390 --> 01:38:55,830
and then i get the probabilities by having different numerator by some the numerator is

1391
01:38:55,840 --> 01:38:57,470
over all y

1392
01:38:57,590 --> 01:39:03,340
and then divide into the numerator denominator the result has to equal one

1393
01:39:03,350 --> 01:39:06,900
this is just you have the most basic principle of probabilities that

1394
01:39:06,910 --> 01:39:09,770
if you have a set of mutually exclusive events

1395
01:39:09,790 --> 01:39:11,220
then the sum of their

1396
01:39:11,230 --> 01:39:16,410
an exhaustive and the sum of the probabilities one so just from a formal point

1397
01:39:16,410 --> 01:39:19,480
of view the denominator has taken to some of the numerator

1398
01:39:20,380 --> 01:39:22,710
the nominator

1399
01:39:22,730 --> 01:39:27,450
he calls some of new readers

1400
01:39:27,550 --> 01:39:29,950
and that's the busy

1401
01:39:31,490 --> 01:39:35,380
i'll come back to this but on that

1402
01:39:35,440 --> 01:39:39,310
the first level explanation of why xe is the way it is

1403
01:39:42,070 --> 01:39:43,910
these are much

1404
01:39:56,320 --> 01:40:00,290
h j

1405
01:40:00,370 --> 01:40:02,920
of x and y

1406
01:40:02,960 --> 01:40:04,710
is called

1407
01:40:04,780 --> 01:40:10,910
a feature function

1408
01:40:12,770 --> 01:40:17,950
it plays the role of the feature in the just aggression i had wj times

1409
01:40:21,730 --> 01:40:24,730
here my my ex is not necessary vectors

1410
01:40:24,740 --> 01:40:30,590
so i can't i could multiply wj by xj xj may not even exist

1411
01:40:32,190 --> 01:40:37,520
but this function f plays the role of the features and functions i call the

1412
01:40:37,530 --> 01:40:38,900
feature function

1413
01:40:50,530 --> 01:40:52,850
it is

1414
01:40:52,960 --> 01:40:56,910
so function from tax crosswise

1415
01:40:56,920 --> 01:40:58,250
to the wheels

1416
01:41:01,700 --> 01:41:10,170
there's a arm

1417
01:41:22,830 --> 01:41:29,440
so notice so

1418
01:41:31,320 --> 01:41:33,300
f j if x and y

1419
01:41:33,320 --> 01:41:35,440
is greater than zero

1420
01:41:37,830 --> 01:41:41,820
and wj is greater than zero

1421
01:41:46,570 --> 01:41:50,410
p of y given x w

1422
01:41:57,700 --> 01:42:04,830
compared to

1423
01:42:05,050 --> 01:42:08,020
j of x and y

1424
01:42:08,090 --> 01:42:10,240
equals zero

1425
01:42:12,470 --> 01:42:19,540
the intuitive meaning of the feature function

1426
01:42:19,550 --> 01:42:24,120
if it has positive weight

1427
01:42:24,130 --> 01:42:25,100
is that

1428
01:42:25,110 --> 01:42:27,750
the bigger the value of the feature function

1429
01:42:27,770 --> 01:42:28,920
the more

1430
01:42:28,930 --> 01:42:30,030
this why

1431
01:42:30,070 --> 01:42:32,990
is compatible with this tax

1432
01:42:34,130 --> 01:42:35,380
of course

1433
01:42:35,400 --> 01:42:36,780
i think so

1434
01:42:39,280 --> 01:42:41,710
x y equals

1435
01:42:43,630 --> 01:42:49,060
of compatibility

1436
01:42:49,100 --> 01:42:52,900
between x and y

1437
01:42:58,960 --> 01:43:01,280
if you have

1438
01:43:01,360 --> 01:43:05,250
if for a particular accent particular why

1439
01:43:05,310 --> 01:43:07,450
the feature function is has a big

1440
01:43:08,340 --> 01:43:10,580
and they have a positive wj

1441
01:43:10,600 --> 01:43:14,480
then that y is going to have a high probability

1442
01:43:18,850 --> 01:43:21,040
OK if the feature

1443
01:43:21,060 --> 01:43:21,820
it is

1444
01:43:21,830 --> 01:43:23,740
negatively correlated

1445
01:43:25,430 --> 01:43:27,430
so if we had

1446
01:43:27,470 --> 01:43:30,330
if if if wj was positive

1447
01:43:30,350 --> 01:43:33,180
then f j had a negative value

1448
01:43:33,190 --> 01:43:35,910
then the bigger the negative value

1449
01:43:35,910 --> 01:43:37,400
insularity that

1450
01:43:37,420 --> 01:43:39,510
companies that

1451
01:43:39,520 --> 01:43:41,270
a second optically

1452
01:43:41,290 --> 01:43:44,920
optical behaviour

1453
01:43:44,930 --> 01:43:51,150
well the transparent to visible light again the high

1454
01:43:52,610 --> 01:43:56,760
covalent bonds transparent to visible light

1455
01:43:56,770 --> 01:44:00,130
and in the case of

1456
01:44:00,170 --> 01:44:03,230
amorphous amorphous there

1457
01:44:05,940 --> 01:44:08,010
amorphous are clear

1458
01:44:08,020 --> 01:44:11,130
and when we have the semi crystalline

1459
01:44:11,190 --> 01:44:15,240
semi crystal and set up

1460
01:44:15,470 --> 01:44:18,490
in regions of higher

1461
01:44:18,550 --> 01:44:23,990
adam density which translate into regions of higher refractive index

1462
01:44:24,040 --> 01:44:29,730
and since the refractive indices are not match between the amorphous and the

1463
01:44:29,760 --> 01:44:35,880
semi crystalline regime this acts effectively as an interface that can scatter light so the

1464
01:44:35,880 --> 01:44:41,210
semi crystal and although there is formally the transparent to visible light these in fact

1465
01:44:41,210 --> 01:44:46,560
proved to be opaque as you can see from the appearance of that node you

1466
01:44:46,720 --> 01:44:53,170
now question to ask is could you generate semi crystalline zones

1467
01:44:53,210 --> 01:44:54,960
that were

1468
01:44:55,060 --> 01:44:57,210
either index matched or

1469
01:44:57,260 --> 01:45:01,380
on the length scales shorter than that of visible light we saw that in the

1470
01:45:01,570 --> 01:45:07,810
inorganic case with the difference between higher serum glass envisions the only difference was that

1471
01:45:07,810 --> 01:45:09,890
the crystallites in visions

1472
01:45:09,930 --> 01:45:13,810
were much much smaller and as a result we were able to get the strengthening

1473
01:45:13,810 --> 01:45:18,700
effect without incurring a penalty of opacity

1474
01:45:18,710 --> 01:45:21,470
third one is chemically inert

1475
01:45:21,490 --> 01:45:24,460
colors are chemically inert

1476
01:45:24,470 --> 01:45:30,450
again the strong covalent bonds

1477
01:45:30,460 --> 01:45:37,600
octet stability same same reasons and this leads to very extensive used in packaging

1478
01:45:37,690 --> 01:45:41,600
packaging especially of food

1479
01:45:41,630 --> 01:45:48,070
and beverages and foods and beverages exploit this

1480
01:45:48,080 --> 01:45:49,700
this property

1481
01:45:49,720 --> 01:45:53,890
and lastly there is solid at room temperature

1482
01:45:53,930 --> 01:46:02,310
solid room temperature long molecules even though there are only weak van der waals bonds

1483
01:46:02,310 --> 01:46:08,400
operative between molecules remember the strong covalent bonds are within the molecule if you want

1484
01:46:08,400 --> 01:46:12,290
to ask the question is something solid liquid or gas you have to ask how

1485
01:46:12,290 --> 01:46:18,530
does one molecule bonded to another one of identical composition and for that we've got

1486
01:46:18,530 --> 01:46:23,330
not strong covalent bonds but we've got weak van der waals bonds

1487
01:46:23,340 --> 01:46:28,670
we then evolved bonds

1488
01:46:28,690 --> 01:46:38,550
but there operating over such large contact areas these molecules are very very long so

1489
01:46:38,550 --> 01:46:39,430
the long

1490
01:46:39,440 --> 01:46:45,440
molecule presents a high surface to volume ratio

1491
01:46:45,460 --> 01:46:47,070
very high

1492
01:46:47,090 --> 01:46:50,540
very high surface to volume ratio

1493
01:46:54,380 --> 01:46:55,690
and that leads to

1494
01:47:00,690 --> 01:47:04,060
strong bonding and ultimately it leads to

1495
01:47:07,020 --> 01:47:13,960
so we have entanglement we have a weak van der waals bonds over

1496
01:47:14,130 --> 01:47:17,610
high surface to volume ratio and that leads to

1497
01:47:18,400 --> 01:47:20,570
solidity at room temperature

1498
01:47:20,580 --> 01:47:24,430
and we recognise last day that absent crosslinking

1499
01:47:24,440 --> 01:47:26,930
absolutely crosslinking

1500
01:47:26,930 --> 01:47:30,640
can process these by

1501
01:47:34,590 --> 01:47:39,790
this gives us plastics can be reprocessed

1502
01:47:39,810 --> 01:47:44,780
and even in the case of hydrogen bonding although the nylons have to go to

1503
01:47:44,780 --> 01:47:48,480
slightly higher temperatures we are able to

1504
01:47:48,510 --> 01:47:54,550
reprocess the nylons in the case of crosslinking of course we have to go to

1505
01:47:54,560 --> 01:48:00,400
other measures because heating to very very high temperatures capable of breaking the cross links

1506
01:48:00,400 --> 01:48:07,600
will also breaks backbone so these things become not very easily recyclable

1507
01:48:07,690 --> 01:48:13,930
and in fact i've got here is that the recycle codes which you see on

1508
01:48:13,930 --> 01:48:15,380
products today

1509
01:48:15,390 --> 01:48:16,230
here's the

1510
01:48:16,280 --> 01:48:22,010
first one is the polyethylene terephthalate which we just saw the reaction for using these

1511
01:48:22,880 --> 01:48:28,330
plastics for many of the things such as in this case is dishwasher detergent the

1512
01:48:28,330 --> 01:48:31,320
two-litre soda bottles and so on are are the

1513
01:48:31,330 --> 01:48:34,420
the high density polyethylene

1514
01:48:34,480 --> 01:48:36,010
it's this one here

1515
01:48:36,030 --> 01:48:39,820
the next one v is the reaction i show to the beginning of the lecture

1516
01:48:40,200 --> 01:48:46,220
polyvinyl chloride fourth one is low density polyethylene which is the stretch and c and

1517
01:48:46,220 --> 01:48:50,700
so on and i want to note that all of these have names associated with

1518
01:48:51,430 --> 01:48:56,780
these are inventions all of these materials i'm showing you are inventions of the modern

1519
01:48:57,970 --> 01:48:59,190
they all came

1520
01:48:59,190 --> 01:49:00,930
as a result of

1521
01:49:02,630 --> 01:49:08,390
patenting et cetera so you see this one nineteen forty nineteen fifty one these are

1522
01:49:08,390 --> 01:49:12,580
very very recent materials

1523
01:49:12,590 --> 01:49:21,720
polypropylene nineteen fifty one polystyrene technically was and was isolated by a german apothecary in

1524
01:49:21,720 --> 01:49:26,680
eighteen thirty nine who didn't realize what he had had some material already got out

1525
01:49:26,680 --> 01:49:31,160
of this resonant was around nineteen twenty two when herman starting here

1526
01:49:31,210 --> 01:49:36,250
the reason that in fact this was the long chain of styrene and polystyrene he

1527
01:49:36,250 --> 01:49:42,450
wrote extensively on and eventually won the nobel prize for it but technically simon isolated

1528
01:49:44,480 --> 01:49:50,150
eighteen thirty nine and n seven is everything else so if you see seven at

1529
01:49:50,150 --> 01:49:51,860
the bottom it's

1530
01:49:52,930 --> 01:49:57,330
so that's the that's the change

1531
01:49:57,330 --> 01:49:58,750
look at

1532
01:49:58,760 --> 01:50:02,560
now the one love plus distribution so this may be something that

1533
01:50:02,610 --> 01:50:06,110
well you know quite is familiar with

1534
01:50:06,130 --> 01:50:07,740
so that one

1535
01:50:09,840 --> 01:50:10,760
for instance

1536
01:50:10,770 --> 01:50:13,690
is this quite a bit so like atomic decay

1537
01:50:13,720 --> 01:50:15,750
well the time with probability

1538
01:50:15,750 --> 01:50:18,260
theta dx atom will decay

1539
01:50:18,280 --> 01:50:22,500
and this in a certain time interval if it still exists so we can look

1540
01:50:22,500 --> 01:50:24,250
at the distribution of

1541
01:50:24,300 --> 01:50:25,390
the cave-ins

1542
01:50:25,410 --> 01:50:26,670
over time

1543
01:50:26,690 --> 01:50:29,910
and then if you consult your physics book

1544
01:50:29,920 --> 01:50:31,600
and you will see that

1545
01:50:32,440 --> 01:50:33,500
p of x

1546
01:50:33,530 --> 01:50:38,500
is given by theta times e to the c takes

1547
01:50:39,630 --> 01:50:44,500
is the positive orthant it actually turns out that it has to be in

1548
01:50:44,520 --> 01:50:49,290
so there should be a minus you otherwise it doesn't make so much sense

1549
01:50:51,280 --> 01:50:53,420
is after you want to decay

1550
01:50:53,450 --> 01:50:55,640
all the teams don't grow over time

1551
01:50:55,660 --> 01:50:59,500
so how do we

1552
01:50:59,550 --> 01:51:03,040
now get this back into an exponential families notation

1553
01:51:03,050 --> 01:51:06,330
well we just do a standard log exp trick

1554
01:51:06,380 --> 01:51:08,370
so this is it to the

1555
01:51:08,540 --> 01:51:13,330
this is the weight in the product is just the product between two numbers

1556
01:51:13,330 --> 01:51:15,220
namely a minus six

1557
01:51:15,240 --> 01:51:17,150
c is the miners again

1558
01:51:17,200 --> 01:51:19,250
and theta

1559
01:51:20,660 --> 01:51:24,100
the negative log of the

1560
01:51:24,350 --> 01:51:30,650
it's really simple sufficient statistic namely for fixes one six

1561
01:51:30,660 --> 01:51:33,340
and the normalisation is also quite simple

1562
01:51:33,340 --> 01:51:37,650
you can see that this function is clearly convex in theta

1563
01:51:39,900 --> 01:51:41,410
here's an example of one

1564
01:51:41,420 --> 01:51:46,010
OK everybody in an exponential decay before

1565
01:51:46,060 --> 01:51:49,000
but it's also exponential families distribution

1566
01:51:49,790 --> 01:51:51,470
engineer's favourite

1567
01:51:51,510 --> 01:51:54,200
normal distribution everybody some that before

1568
01:51:54,260 --> 01:51:56,300
and this is why would learn

1569
01:51:56,350 --> 01:52:02,060
if it's just the univariate one multivariate once local deformation but not so much

1570
01:52:03,840 --> 01:52:10,050
but that doesn't really look like an inner product between some for fixing something else

1571
01:52:10,090 --> 01:52:10,810
at all

1572
01:52:12,840 --> 01:52:15,920
well let's see how we can reshape this

1573
01:52:15,990 --> 01:52:17,370
o p of x

1574
01:52:17,380 --> 01:52:19,650
i can just write it out

1575
01:52:19,670 --> 01:52:22,770
so the first thing is i'm going to push this into the exponential

1576
01:52:22,780 --> 01:52:26,110
the second thing is going to expand

1577
01:52:26,130 --> 01:52:28,300
this query expression

1578
01:52:28,310 --> 01:52:31,750
so get one is one of the two sigma square enix square

1579
01:52:31,840 --> 01:52:34,000
plus mu over sigma square x

1580
01:52:34,190 --> 01:52:39,970
minus mu square to sigma squared minus one half block of two pi sigma square

1581
01:52:40,890 --> 01:52:42,810
it's good

1582
01:52:44,070 --> 01:52:48,130
you get some manufacturer from p x squared

1583
01:52:48,370 --> 01:52:51,640
i get some other linear factor from px

1584
01:52:51,650 --> 01:52:53,170
get some junk here

1585
01:52:53,180 --> 01:52:57,240
which ensures that everything integrates out to one

1586
01:52:58,950 --> 01:53:03,640
now let's write it in terms of well in approach between something in the axis

1587
01:53:03,690 --> 01:53:05,560
and the parameters theta

1588
01:53:05,590 --> 01:53:09,740
and then something else

1589
01:53:11,060 --> 01:53:12,410
so we get

1590
01:53:12,430 --> 01:53:13,610
it's easy to be

1591
01:53:13,610 --> 01:53:17,550
in approach between x and x squared on one hand and theta

1592
01:53:17,680 --> 01:53:20,820
which now captures those two parameters

1593
01:53:20,820 --> 01:53:24,630
now this integral doesn't always exist in particular doesn't exist

1594
01:53:24,660 --> 01:53:27,060
if this term is not negative

1595
01:53:27,110 --> 01:53:29,420
then the integral diverge

1596
01:53:29,470 --> 01:53:30,960
so we've seen interesting

1597
01:53:30,970 --> 01:53:35,000
phenomenon occurring that the domain of the status

1598
01:53:35,020 --> 01:53:36,040
need not be

1599
01:53:36,050 --> 01:53:37,520
all of far

1600
01:53:38,150 --> 01:53:40,160
can be a subset of far

1601
01:53:40,170 --> 01:53:43,380
in fact it's a convex subset of

1602
01:53:43,400 --> 01:53:47,510
why so because if got the convex function we want to ensure that this is

1603
01:53:47,510 --> 01:53:51,350
less equal than it it's like that it doesn't diverge

1604
01:53:51,390 --> 01:53:53,020
so basically

1605
01:53:53,060 --> 01:53:55,010
it's this is pretty

1606
01:53:55,030 --> 01:53:58,760
then that the brain will be a convex domain

1607
01:53:58,800 --> 01:54:03,590
now the last bit that you have to do is you have to rewrite this

1608
01:54:03,590 --> 01:54:06,450
in terms of theta one and theta two

1609
01:54:07,550 --> 01:54:08,620
quite tedious

1610
01:54:08,650 --> 01:54:10,500
but you can work it out

1611
01:54:10,550 --> 01:54:12,940
theta one of the mu sigma square

1612
01:54:12,940 --> 01:54:15,380
so welcome back everybody

1613
01:54:18,290 --> 01:54:21,700
what happens before was actually quite scary

1614
01:54:21,710 --> 01:54:23,630
somebody came up to me and said

1615
01:54:23,640 --> 01:54:27,850
alex i don't understand this graph while way i talked a lot of other people

1616
01:54:27,850 --> 01:54:30,760
they didn't understand is OK

1617
01:54:30,810 --> 01:54:33,580
this is scary for several reasons

1618
01:54:33,600 --> 01:54:37,850
OK first of all it means that really awful job explaining it effectively

1619
01:54:37,900 --> 01:54:40,080
and i was explaining that in the morning

1620
01:54:42,170 --> 01:54:46,570
by all means if something like this happens when you have to ask you and

1621
01:54:46,570 --> 01:54:47,850
asking enough

1622
01:54:47,860 --> 01:54:49,600
so it's your fault right

1623
01:54:49,610 --> 01:54:55,110
no but seriously to ask you something is not clear

1624
01:54:55,130 --> 01:54:57,480
don't be shy

1625
01:54:57,490 --> 01:55:00,060
i mean rather ask a stupid question

1626
01:55:00,920 --> 01:55:03,780
the point is if you ask a stupid question

1627
01:55:03,780 --> 01:55:06,800
you're not going embarrass yourself but what it will do is it will tell me

1628
01:55:07,130 --> 01:55:10,460
that i didn't explain things well

1629
01:55:10,480 --> 01:55:11,350
i mean

1630
01:55:11,360 --> 01:55:15,350
i don't expect you to ask terribly interesting questions was that at some point will

1631
01:55:15,350 --> 01:55:19,170
lead to new paper they so as in questions not to show off how right

1632
01:55:19,170 --> 01:55:20,490
you are

1633
01:55:20,500 --> 01:55:25,450
it's so that actually information is if something isn't clear please ask

1634
01:55:25,560 --> 01:55:29,240
i mean it seriously but also in the other lectures please do so

1635
01:55:29,280 --> 01:55:31,450
otherwise you will not learn this stuff

1636
01:55:31,560 --> 01:55:33,910
the lectures they know this stuff at least

1637
01:55:33,950 --> 01:55:36,670
usually we try to pretend that beta

1638
01:55:40,000 --> 01:55:41,580
you must ask otherwise

1639
01:55:41,640 --> 01:55:44,080
and just carry on now

1640
01:55:44,100 --> 01:55:45,670
back to these

1641
01:55:46,020 --> 01:55:52,380
so i do this this graph here had k and y there now

1642
01:55:53,640 --> 01:55:55,470
you probably remember from nando

1643
01:55:55,490 --> 01:55:58,300
that these are random variables right

1644
01:55:58,360 --> 01:56:00,360
so k actually would be

1645
01:56:00,380 --> 01:56:02,390
my covariance matrix

1646
01:56:07,910 --> 01:56:13,110
OK so this depends on my doctor so i have some x here

1647
01:56:13,250 --> 01:56:16,220
and this

1648
01:56:16,270 --> 01:56:17,670
determines k

1649
01:56:17,760 --> 01:56:21,830
as follows that k i j is simply k

1650
01:56:21,840 --> 01:56:24,200
of x y and xj

1651
01:56:24,240 --> 01:56:27,300
so is no statistical dependency between those two

1652
01:56:27,330 --> 01:56:29,640
it's a fully deterministic things

1653
01:56:29,640 --> 01:56:31,870
i could make this

1654
01:56:31,930 --> 01:56:36,800
stochastic by simply the also assuming that they have some parameter like signal which is

1655
01:56:36,800 --> 01:56:40,540
the kernel with which also can determine

1656
01:56:40,580 --> 01:56:44,140
they might just so happened that will have to my topic and then i draw

1657
01:56:44,150 --> 01:56:46,770
something when they get my covariance matrix

1658
01:56:48,040 --> 01:56:51,870
o thing this will not be deterministic dependency anymore but let's not make things messy

1659
01:56:51,870 --> 01:56:53,480
then there should be

1660
01:56:53,830 --> 01:56:56,830
so want my kernels

1661
01:56:56,860 --> 01:57:00,140
well i can say well i'm going to observe some teas

1662
01:57:00,270 --> 01:57:01,420
which are

1663
01:57:01,460 --> 01:57:07,150
jointly normal according to this mean you and this covariance matrix k well actually couldn't

1664
01:57:07,150 --> 01:57:10,570
actually include incorporate new directly but anyway

1665
01:57:10,670 --> 01:57:15,520
now unfortunately i'm not really observing those city's directly but i'm observing the wise which

1666
01:57:15,520 --> 01:57:18,670
are the tease was an additive noise

1667
01:57:19,550 --> 01:57:21,860
for instance the tease could be

1668
01:57:21,870 --> 01:57:26,110
well the voltages at the socket or sockets all throughout kerala

1669
01:57:27,120 --> 01:57:31,800
now what i'm observing otherwise which is what happens if i take my voltmeter plug

1670
01:57:31,800 --> 01:57:32,870
it in

1671
01:57:32,920 --> 01:57:35,080
it's going to be a bit unreliable

1672
01:57:35,120 --> 01:57:38,390
there might be some overall fluctuations of the voltage

1673
01:57:38,390 --> 01:57:40,110
throughout callow

1674
01:57:40,150 --> 01:57:45,390
simply because maybe the wiring isn't terribly good maybe at some places the ball is

1675
01:57:45,390 --> 01:57:47,590
heating up some water what even

1676
01:57:47,710 --> 01:57:52,480
so these variations will be equal will be can be dealt with by k

1677
01:57:52,520 --> 01:57:55,980
so for instance if i have some network right

1678
01:57:56,020 --> 01:58:00,210
some real wiring network so these are some parts here

1679
01:58:00,390 --> 01:58:03,510
so you've got to heart here

1680
01:58:04,990 --> 01:58:07,110
it's not visible

1681
01:58:07,570 --> 01:58:15,420
give the the argmax them

1682
01:58:15,430 --> 01:58:25,690
so let's say this is a wiring diagram of color

1683
01:58:25,700 --> 01:58:28,160
maybe the hot here

1684
01:58:28,290 --> 01:58:31,130
another one here

1685
01:58:31,200 --> 01:58:33,520
maybe he is a big one

1686
01:58:40,040 --> 01:58:41,380
so when i go

1687
01:58:41,390 --> 01:58:44,940
and i take my voltmeter and plug into the socket everywhere here

1688
01:58:45,330 --> 01:58:47,190
and then measure things

1689
01:58:47,200 --> 01:58:51,290
they would of course assume that those two voltages are quite similar

1690
01:58:51,330 --> 01:58:54,040
because they come up the same cable here

1691
01:58:54,060 --> 01:58:58,390
you could also think of that as internet throughput like it's really horrible up here

1692
01:58:58,390 --> 01:59:01,950
but maybe in the lecture theatre works and down there we are having tea is

1693
01:59:01,970 --> 01:59:03,950
not really working very well

1694
01:59:05,570 --> 01:59:11,390
this diagram can then be directly translated into this matrix k

