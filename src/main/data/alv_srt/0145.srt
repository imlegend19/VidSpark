1
00:00:00,000 --> 00:00:01,760
these are different aspects

2
00:00:01,780 --> 00:00:06,880
so the DBLP inferred from co authorships citation networks

3
00:00:06,990 --> 00:00:08,460
but for blocks

4
00:00:08,470 --> 00:00:10,870
people explicitly specify

5
00:00:10,890 --> 00:00:12,870
their social network

6
00:00:12,890 --> 00:00:14,640
or inferred from links

7
00:00:14,650 --> 00:00:17,400
comments on of these

8
00:00:17,410 --> 00:00:20,800
in the communities communities also different

9
00:00:21,690 --> 00:00:22,910
dblp the

10
00:00:22,960 --> 00:00:26,040
school site citation networks

11
00:00:26,100 --> 00:00:27,620
they are relatively

12
00:00:27,660 --> 00:00:29,180
stand a static

13
00:00:29,180 --> 00:00:31,950
OK they don't change as much

14
00:00:31,990 --> 00:00:33,890
as the block

15
00:00:33,910 --> 00:00:36,150
black communities

16
00:00:36,230 --> 00:00:36,890
you can

17
00:00:36,910 --> 00:00:39,110
tell for example the last week

18
00:00:39,130 --> 00:00:42,470
you can see a lot of traffic about olympics

19
00:00:42,610 --> 00:00:44,250
all those controversies

20
00:00:44,260 --> 00:00:48,380
OK now let graduates last night probably about

21
00:00:48,400 --> 00:00:54,320
i don't know if it's if people would be interested were probably that's the basketball

22
00:00:56,160 --> 00:01:00,840
did you watch that make it so close from europe of course you probably can

23
00:01:00,860 --> 00:01:03,750
anyone from spain

24
00:01:06,360 --> 00:01:08,780
and i am sure today

25
00:01:08,790 --> 00:01:11,940
it will be what about the democratic convention

26
00:01:12,840 --> 00:01:14,540
the jill biden

27
00:01:14,630 --> 00:01:16,460
so running mate

28
00:01:16,500 --> 00:01:22,470
and all of these things because this john mckay already had the new at about

29
00:01:22,470 --> 00:01:23,410
to this

30
00:01:23,440 --> 00:01:27,540
joe biden it's interesting how they do very fast

31
00:01:27,560 --> 00:01:30,000
so they want to reach people so then

32
00:01:30,000 --> 00:01:31,600
blast is under the

33
00:01:31,620 --> 00:01:37,380
are the change so it changes very fast next week will be republican convention

34
00:01:37,390 --> 00:01:39,130
so that they different

35
00:01:39,260 --> 00:01:41,660
they share some commonality so that's why

36
00:01:42,060 --> 00:01:44,210
when with the research

37
00:01:44,210 --> 00:01:49,640
we try to use what guess what the simpler more static easier one right the

38
00:01:50,720 --> 00:01:53,860
but when the work developed something really

39
00:01:54,010 --> 00:01:56,490
important and useful

40
00:01:56,530 --> 00:02:00,820
you have to work on the real to me OK that's what i

41
00:02:02,090 --> 00:02:04,040
i think it's time for

42
00:02:04,040 --> 00:02:08,000
me to stop to hand over to knitting and to talk about

43
00:02:08,330 --> 00:02:11,450
blogosphere research issues

44
00:02:11,490 --> 00:02:14,820
and we try to make like thirty minutes

45
00:02:14,960 --> 00:02:16,700
many sessions

46
00:02:16,700 --> 00:02:22,190
let's look at some of the current research issues in blogosphere and

47
00:02:22,210 --> 00:02:27,110
some of the representative works in blogosphere i would try to boil guys down very

48
00:02:27,110 --> 00:02:34,150
detailed data quickly go through all these research issues

49
00:02:34,180 --> 00:02:37,480
OK so let's look at the

50
00:02:37,530 --> 00:02:41,060
first decision which is like modelling the blogsphere so

51
00:02:41,080 --> 00:02:45,190
why why do we need models for the blog search for understanding the structures and

52
00:02:45,190 --> 00:02:50,210
properties in the blogosphere it also has been gaining insight into the relationships between bloggers

53
00:02:50,210 --> 00:02:54,620
readers blog post comments different blog sites in the block

54
00:02:54,640 --> 00:03:00,020
and these models also has been generating artificial data sets and you can tune the

55
00:03:00,020 --> 00:03:06,360
parameters to simulate the special scenarios and compared to study the different algorithms

56
00:03:06,370 --> 00:03:10,850
they also have been studying peculiarities in the blogosphere and inference patterns

57
00:03:10,900 --> 00:03:13,020
and structures that could

58
00:03:13,100 --> 00:03:19,460
help explain certain phenomena like influence diffusion splogs community extraction we should go through in

59
00:03:19,470 --> 00:03:24,190
the next few slides

60
00:03:24,190 --> 00:03:30,770
so there are some key differences between them and the models and lockheed martin

61
00:03:30,790 --> 00:03:34,800
would not many blocks for specific models have been developed so

62
00:03:34,820 --> 00:03:39,040
we review some of the models developed for the world wide web and how other

63
00:03:39,040 --> 00:03:42,090
works have modified and extended these models

64
00:03:42,110 --> 00:03:44,070
into the blogsphere

65
00:03:45,700 --> 00:03:49,250
models developed for their assume dense graph structures

66
00:03:49,300 --> 00:03:54,290
due to the large number of interconnecting hyperlinks within the pages which is contrary to

67
00:03:54,290 --> 00:03:57,830
blocks really because you don't have so many links because it's a very casual environment

68
00:03:58,080 --> 00:04:01,310
people tend to miss gentleman linking two

69
00:04:01,330 --> 00:04:04,070
the source their citing over the differing

70
00:04:04,090 --> 00:04:06,730
so the level of interaction

71
00:04:06,760 --> 00:04:11,150
is also different in that and blocks where you find a larger interaction blocks as

72
00:04:11,150 --> 00:04:15,150
compared to to as pointed out by one

73
00:04:16,880 --> 00:04:20,800
the highly dynamic and short lived nature of the blog post makes it different from

74
00:04:20,800 --> 00:04:24,140
the web because the models do not consider that kind of dynamicity in the web

75
00:04:25,440 --> 00:04:30,640
the market in the pages containing over time which is not with blogsphere

76
00:04:30,650 --> 00:04:34,960
because blocks with the links as soon as the topic is that nobody cares about

77
00:04:34,970 --> 00:04:37,590
the block so they don't inside that

78
00:04:37,610 --> 00:04:38,800
and then

79
00:04:38,800 --> 00:04:43,860
different things like categories and tags and by the users in the logs very different

80
00:04:43,860 --> 00:04:47,120
than that because you don't find that kind of flexibility

81
00:04:47,210 --> 00:04:53,030
and then you have descriptive filenames for the blogs as compared to the pages

82
00:04:53,080 --> 00:04:57,660
so these are some of the differences between them in the blogosphere

83
00:04:57,710 --> 00:04:59,970
i mean you go through some of the models

84
00:05:01,150 --> 00:05:05,120
for the remainder what has been adapted for the blogsphere

85
00:05:05,150 --> 00:05:09,540
so there is a preferential attachment model

86
00:05:09,730 --> 00:05:13,040
but basically say the probability of new edge to a node to be added depends

87
00:05:13,720 --> 00:05:15,120
it's degree

88
00:05:15,180 --> 00:05:17,990
that is in the creation of an edge between

89
00:05:18,000 --> 00:05:21,540
but this to be a and b g depends on the

90
00:05:21,550 --> 00:05:25,550
edges incident upon the vertex media

91
00:05:25,600 --> 00:05:31,510
and this also leads to all the distribution are you going so confiscated redistribution will

92
00:05:31,510 --> 00:05:35,610
explain it in detail in the slides

93
00:05:35,610 --> 00:05:40,630
it is a question of intervention type it's not as sociation can not

94
00:05:40,640 --> 00:05:45,360
solve this problem by collaboration

95
00:05:46,100 --> 00:05:50,290
five years ago my linear model i spent so much time in this tutorial on

96
00:05:50,290 --> 00:05:51,550
the linear model

97
00:05:51,700 --> 00:05:53,710
look again at the linear model

98
00:05:53,720 --> 00:05:58,370
you scale covariance and we tried to answer this question why

99
00:05:58,390 --> 00:06:00,820
so the regression coefficient measures

100
00:06:00,830 --> 00:06:06,410
about the size of the data measures the importance of variable exchange in terms of

101
00:06:06,410 --> 00:06:10,350
what people call association but what it measures

102
00:06:10,360 --> 00:06:13,780
the importance of this defect of a j

103
00:06:13,780 --> 00:06:16,280
on y and outcomes important stuff

104
00:06:16,300 --> 00:06:21,830
when keeping all other variables fixed this is what the regression is still

105
00:06:21,850 --> 00:06:25,840
and in this gene example nobody believes that this is the case right if you

106
00:06:25,850 --> 00:06:31,240
make an intervention in the gene or genes will change the whole stuff

107
00:06:31,240 --> 00:06:32,880
i want to quantify

108
00:06:32,910 --> 00:06:34,970
the effect of the total

109
00:06:34,980 --> 00:06:37,270
to my response of interest

110
00:06:37,290 --> 00:06:44,650
so this keeping the other fixed is really not realistic forty intervention problem

111
00:06:44,660 --> 00:06:46,250
OK i think

112
00:06:46,260 --> 00:06:54,940
keep that so you can define using pearl's do calculus what it is

113
00:06:54,960 --> 00:06:57,620
and maybe this is

114
00:06:57,650 --> 00:06:59,690
so it's defined

115
00:06:59,700 --> 00:07:04,390
i bring you more tools all of the problem is time here so we can

116
00:07:04,390 --> 00:07:09,140
define an i think some people knowing how causal effect is defined so this is

117
00:07:09,990 --> 00:07:13,830
using the framework of say per or many others

118
00:07:13,860 --> 00:07:19,970
this definition you need to go through that can make the markov assumption on the

119
00:07:19,990 --> 00:07:23,610
then you have a defined effect and what it actually these causal effect but you

120
00:07:23,610 --> 00:07:25,520
have to find

121
00:07:25,530 --> 00:07:30,180
it's just the same as if you would have run the randomized trial to see

122
00:07:30,200 --> 00:07:31,470
this effect

123
00:07:31,490 --> 00:07:33,990
so this is kind of the idea of what you want to do

124
00:07:34,000 --> 00:07:38,850
so you have a causal effect which is defined actually is the same as if

125
00:07:38,870 --> 00:07:41,970
you were to run a fully randomized trial

126
00:07:41,990 --> 00:07:45,390
this study in order to find out the fact that the goal is of course

127
00:07:45,470 --> 00:07:51,070
i want to infer this effect without doing the randomized trial

128
00:07:51,090 --> 00:07:54,860
so here is we can take that the definition of the causal effect of of

129
00:07:54,860 --> 00:07:58,330
course is been an abstract definition in this way

130
00:07:58,340 --> 00:08:03,000
but i assume if i had y x one and x all jointly gaussians make

131
00:08:03,020 --> 00:08:04,770
things easier

132
00:08:04,770 --> 00:08:10,000
and i'm interested in the derivatives of the expected value of y

133
00:08:10,040 --> 00:08:13,130
when i do an intervention at the j th variable

134
00:08:13,160 --> 00:08:16,070
and said it to evaluate lex

135
00:08:16,080 --> 00:08:21,040
the gauss in case this derivative is known to be a single number single coefficient

136
00:08:21,070 --> 00:08:22,720
called theta j

137
00:08:22,730 --> 00:08:23,820
it's like

138
00:08:23,840 --> 00:08:27,380
another classification regression regression must data j

139
00:08:27,390 --> 00:08:28,910
here theta j

140
00:08:28,930 --> 00:08:34,080
this is the causal effect this intervention effect of the j th variable on my

141
00:08:34,080 --> 00:08:36,010
response which is

142
00:08:36,030 --> 00:08:41,260
here's an interesting and important characterisation was there are several ones but this is an

143
00:08:41,260 --> 00:08:42,780
interesting one

144
00:08:42,800 --> 00:08:44,960
you can find theta j

145
00:08:44,970 --> 00:08:47,460
these regression

146
00:08:47,510 --> 00:08:50,320
use of the link graph forest illustrations

147
00:08:50,340 --> 00:08:52,850
here is my response of interest

148
00:08:52,880 --> 00:08:56,060
here is the variable there are two dimension

149
00:08:56,080 --> 00:09:00,080
and the first back door criterion is telling you

150
00:09:00,100 --> 00:09:05,910
is that for example you can find the causal effect of x two on y

151
00:09:05,990 --> 00:09:08,010
by running the regression

152
00:09:08,020 --> 00:09:09,150
of y

153
00:09:09,180 --> 00:09:10,860
on x two and now

154
00:09:10,890 --> 00:09:12,040
the parents

155
00:09:12,100 --> 00:09:13,650
fixed and

156
00:09:14,380 --> 00:09:17,330
so this is all what you have to do in the gauss in world

157
00:09:17,340 --> 00:09:19,750
just run the regression of y

158
00:09:19,750 --> 00:09:21,990
other variable very two-dimensional

159
00:09:22,010 --> 00:09:25,840
and the parents of the interaction with

160
00:09:25,860 --> 00:09:30,740
and you can run your statistical software on that and you get the confusion

161
00:09:30,750 --> 00:09:35,850
OK so the interesting thing is you only need actually local information from the graph

162
00:09:35,850 --> 00:09:39,910
you only need parental set which is a hard problem but you don't need the

163
00:09:39,910 --> 00:09:40,910
full graph

164
00:09:40,940 --> 00:09:42,270
i only need

165
00:09:42,400 --> 00:09:46,700
parental set for every variable

166
00:09:46,710 --> 00:09:50,380
OK so from a more distant point of view what's going on here

167
00:09:50,410 --> 00:09:53,230
again causal inferences only

168
00:09:53,230 --> 00:09:55,790
but it's hard but only in regression

169
00:09:55,810 --> 00:10:00,070
and conditioning on the right variables in the right variables are the parents you doing

170
00:10:00,070 --> 00:10:03,550
your intervention

171
00:10:03,560 --> 00:10:05,690
OK so so

172
00:10:05,700 --> 00:10:10,020
how can you try to estimate that stuff to make points clear i really and

173
00:10:10,020 --> 00:10:11,790
also amazed example

174
00:10:11,800 --> 00:10:13,580
i do not have to grab

175
00:10:13,600 --> 00:10:17,740
i don't know which are the parents and which are not that try to learn

176
00:10:17,740 --> 00:10:19,800
from data

177
00:10:19,820 --> 00:10:26,010
and so the experts say come on i mean this is impossible of course it's

178
00:10:26,010 --> 00:10:28,030
impossible you cannot do that

179
00:10:28,030 --> 00:10:35,400
the main problem is inferring that parents from observational data you cannot do that

180
00:10:35,470 --> 00:10:41,150
the these example take two bivariate gaussians you have infinitely many observations here and you

181
00:10:41,150 --> 00:10:45,160
can not distinguish whether it goes this way or that way

182
00:10:45,190 --> 00:10:50,010
so what kind of breaks down

183
00:10:51,290 --> 00:10:58,040
the ground truth the usual statistical principle doesn't broke the usual statistical principle if you

184
00:10:58,040 --> 00:11:00,970
have the data generating distribution p

185
00:11:00,990 --> 00:11:06,980
observational data is the observation that if you have enough observations we can find or

186
00:11:06,980 --> 00:11:12,600
identify the parameters you are interested i think it depends on the distribution p which

187
00:11:12,600 --> 00:11:16,220
generates the data here you need to p

188
00:11:16,230 --> 00:11:22,860
and the true underlying causal graph and the parameter causal effect is a function of

189
00:11:23,130 --> 00:11:28,320
the distribution on the graph and the graph itself so much harder problem and the

190
00:11:28,320 --> 00:11:35,960
problem is you cannot find the gene from p so it intrinsically involved the gene

191
00:11:36,070 --> 00:11:41,560
so it's impossible because say this by various gauss and nobody can tell you the

192
00:11:41,560 --> 00:11:43,530
direction so

193
00:11:43,570 --> 00:11:45,550
this is a slide problem here

194
00:11:45,580 --> 00:11:49,770
but what is possible and try to spend my last minutes on this

195
00:11:49,850 --> 00:11:56,210
is don't give up here i mean this is a huge identifiability problem you cannot

196
00:11:56,210 --> 00:11:58,910
get the causal effect in general but

197
00:11:58,930 --> 00:12:04,910
you can get something which actually used in example you can get lower bounds

198
00:12:04,930 --> 00:12:07,260
when your quarterly

199
00:12:07,280 --> 00:12:11,000
and just to give you a conceptual procedure what's going on here

200
00:12:11,010 --> 00:12:16,190
somebody gives you the probability distribution p the data generating distribution

201
00:12:16,200 --> 00:12:20,220
what you can do is you can find the equivalence class

202
00:12:20,230 --> 00:12:23,680
of that seventies only considering that's here

203
00:12:23,690 --> 00:12:25,620
you can find the equivalence class

204
00:12:25,640 --> 00:12:28,070
by clever algorithms

205
00:12:28,090 --> 00:12:33,740
and then conceptually what you can do is to find all members conceptually not practically

206
00:12:33,930 --> 00:12:35,140
enumerate all

207
00:12:35,150 --> 00:12:37,870
that means your equivalence class

208
00:12:38,790 --> 00:12:41,540
enumerate all of them and then you apply

209
00:12:41,570 --> 00:12:45,620
per stu intervention calculus to be that member

210
00:12:45,690 --> 00:12:51,610
so for every DAG member the g bar and every variable

211
00:12:51,660 --> 00:12:54,110
there are two and invention exchange

212
00:12:54,210 --> 00:13:01,820
i can compute by integration formula intervention effect data rj thirty intervention effect in the

213
00:13:01,820 --> 00:13:03,520
graph g or

214
00:13:03,520 --> 00:13:07,520
but i sure will do all the basis of the key posteriors everything's

215
00:13:07,570 --> 00:13:11,900
i think that makes sense but i can see why you know it's it's certainly

216
00:13:11,900 --> 00:13:15,750
not very pure way of looking at things but let me let me just argue

217
00:13:15,750 --> 00:13:18,460
for a little bit so here's the argument that

218
00:13:18,750 --> 00:13:21,940
or at least here's how it plays out

219
00:13:21,960 --> 00:13:23,690
what we'd like to do

220
00:13:23,690 --> 00:13:26,460
is handled gridworld like this gridworld

221
00:13:26,480 --> 00:13:29,980
or real world would be even better but let's just say this for today

222
00:13:30,030 --> 00:13:34,710
so here's here's how the dynamics of this gridworld work there's each cell this is

223
00:13:34,710 --> 00:13:38,590
an elaboration of the problem that you can find in the russell norvig AI textbook

224
00:13:38,590 --> 00:13:43,820
which people probably have access to the same dynamics but different grade configurations so each

225
00:13:43,820 --> 00:13:47,650
cell you've got north south east west

226
00:13:47,670 --> 00:13:52,500
each action takes you with eighty percent probability in the direction that you intend ten

227
00:13:53,710 --> 00:13:57,110
right angle from that and ten percent the other right angle from that ls is

228
00:13:57,110 --> 00:14:00,420
the wall there in which case you get by back to where you work

229
00:14:01,190 --> 00:14:03,790
in terms of local dynamics

230
00:14:04,070 --> 00:14:07,940
well all the cells are kind of the same but the different inasmuch as there

231
00:14:07,940 --> 00:14:12,570
is what different wall patterns around so for example this cell

232
00:14:12,570 --> 00:14:16,820
and this'll have very similar dynamics if you go

233
00:14:17,940 --> 00:14:21,650
then the probability that you end up over here is point eight probability and appears

234
00:14:21,650 --> 00:14:24,990
point one and probably you stay put this point one and it's also true here

235
00:14:24,990 --> 00:14:26,130
to shift it up

236
00:14:26,170 --> 00:14:29,980
OK so you can generalize from the dynamics of oneself to any other so that

237
00:14:29,980 --> 00:14:32,320
has the same book award patterns

238
00:14:32,320 --> 00:14:35,840
but they have different local wall patterns and it's a little weird right because you

239
00:14:35,840 --> 00:14:39,320
can get pinched and so you're slide actions two different riverside actions culture not to

240
00:14:39,320 --> 00:14:42,690
move so it's usable dangerous to generalize between

241
00:14:42,750 --> 00:14:44,130
a different world

242
00:14:44,210 --> 00:14:48,670
so how can you also if you knew that if you knew that

243
00:14:48,690 --> 00:14:51,690
each each of these cells out there has a particular wall pattern and all the

244
00:14:51,690 --> 00:14:55,070
ones with the same while pattern have the same local dynamics then what you could

245
00:14:55,630 --> 00:14:58,690
it is run very variation of of

246
00:14:58,690 --> 00:15:03,300
r max but takes into consideration these clusters this is the so-called ram r max

247
00:15:03,320 --> 00:15:05,880
and that would actually work really well

248
00:15:05,900 --> 00:15:09,190
this seems achieve because it's actually taking advantage of the fact that you know we

249
00:15:10,050 --> 00:15:13,670
which cells should be generalizing connected with each other so if we don't do that

250
00:15:13,690 --> 00:15:16,730
and just say they're all the same

251
00:15:16,900 --> 00:15:18,880
we're not saying that there all this what we say they're all the same that

252
00:15:18,880 --> 00:15:22,230
would be really problematic because they're not and you learn of broken model that will

253
00:15:22,230 --> 00:15:23,960
cause it to make really bad decisions

254
00:15:23,980 --> 00:15:26,100
on the other hand if you say they are all independent of each other are

255
00:15:26,100 --> 00:15:30,290
completely separate then it's then it devolved back to our maximum learns about each cell

256
00:15:30,320 --> 00:15:31,980
completely independently

257
00:15:32,000 --> 00:15:36,000
so what would be nice is if we could sort of

258
00:15:36,020 --> 00:15:39,630
so we had some kind of prior that says that so walls with similar kinds

259
00:15:39,630 --> 00:15:42,820
of wall patterns tend to have similar kinds of dynamics

260
00:15:43,710 --> 00:15:47,150
in but not necessarily right if there are different there are different but maybe the

261
00:15:47,150 --> 00:15:50,800
say well if that's the case then when

262
00:15:50,820 --> 00:15:54,610
the agents visit to sell the probably like some other cell that has really good

263
00:15:54,610 --> 00:15:58,170
guess as to what the local dynamics are there that's for sure but i think

264
00:15:59,770 --> 00:16:03,770
if it's the case that that guess is has high enough probability

265
00:16:03,840 --> 00:16:07,710
maybe it should just act on not really worried that call that's the unknown anymore

266
00:16:07,710 --> 00:16:10,500
and try experimenting and it just kind of go with it

267
00:16:10,550 --> 00:16:15,170
and in fact if we want to achieve PAC MDP tight bounds on the probability

268
00:16:15,840 --> 00:16:18,320
wrong is small enough

269
00:16:18,360 --> 00:16:19,750
then we really can

270
00:16:19,770 --> 00:16:24,300
ignore it and it won't hurt us in terms of achieving our objectives of having

271
00:16:24,300 --> 00:16:31,070
PAC MDP is because why because

272
00:16:33,000 --> 00:16:37,690
the delta probability says that this whole procedure can fail with some small probability so

273
00:16:37,690 --> 00:16:38,750
as long as we

274
00:16:38,800 --> 00:16:41,650
only ignore something if it falls into that

275
00:16:41,670 --> 00:16:45,320
underneath the delta rugby then we're going to be OK

276
00:16:45,320 --> 00:16:48,980
so that's the basic idea is that we is on keeping problem which tracker things

277
00:16:48,980 --> 00:16:52,570
things that are possible but not very likely can be swapped swept under the delta

278
00:16:52,570 --> 00:16:53,980
rock and we can still have

279
00:16:54,000 --> 00:16:55,030
pac mdp

280
00:16:55,050 --> 00:16:59,020
so for example in this case well

281
00:16:59,020 --> 00:17:03,900
the grid for ninety nine percent sure that we're in world x first worldwide

282
00:17:03,940 --> 00:17:07,880
we can still be PAC MDP by just forgetting about worldwide

283
00:17:07,880 --> 00:17:09,690
if the

284
00:17:09,710 --> 00:17:12,230
error probability is large enough

285
00:17:12,250 --> 00:17:15,880
OK so that's the sort of the basic idea we were able to generalize this

286
00:17:15,880 --> 00:17:17,590
to two

287
00:17:17,610 --> 00:17:19,730
two an algorithm

288
00:17:19,750 --> 00:17:25,590
from DP it works like this it uses optimism and optimism uncertainty not bayes optimal

289
00:17:27,230 --> 00:17:31,150
we sample model so it step in time it's keeping the posterior in some form

290
00:17:31,380 --> 00:17:35,000
it samples of a couple models from the posterior side of the world i mean

291
00:17:35,000 --> 00:17:38,380
but let me just choose a bunch of fairly likely world can now got a

292
00:17:38,380 --> 00:17:41,630
bunch worlds that i might be and what it does is it then stitches them

293
00:17:41,630 --> 00:17:45,820
together to form a kind of meta MDP so late in learning when one is

294
00:17:45,820 --> 00:17:49,820
pretty sure about which MDP etc and it does a couple draws from the posterior

295
00:17:49,820 --> 00:17:52,480
in public it's the same model each time so

296
00:17:52,480 --> 00:17:56,460
the stitching together just leave you with the true MDP but in the beginning when

297
00:17:56,460 --> 00:18:01,320
there's lots of different possibilities it states together and creates an optimistic MDP one it's

298
00:18:01,550 --> 00:18:04,250
actually better than any of the ones that really saw

299
00:18:04,270 --> 00:18:09,880
four through an act optimally with respect to that optimistic MDP

300
00:18:09,900 --> 00:18:14,270
at that point OK so solved to find the best of the best of the

301
00:18:14,270 --> 00:18:18,270
sample said that's where the name comes from and acts accordingly it acts according to

302
00:18:18,270 --> 00:18:23,020
that policy just just created until something new is learned to visit the state has

303
00:18:23,020 --> 00:18:29,110
been to before something surprising happens in in that shifts the posterior dramatically causes reset

304
00:18:29,110 --> 00:18:33,170
to the top and resamples models and then acts on the resulting models

305
00:18:33,190 --> 00:18:36,110
so what we were able to show is that if the set is big enough

306
00:18:36,130 --> 00:18:40,630
still polynomial but actually fairly large we pull budget models together stitching together then we

307
00:18:40,630 --> 00:18:43,530
can actually get this the PAC MDP

308
00:18:43,550 --> 00:18:47,980
criteria to be satisfied with near optimality with high probability

309
00:18:48,190 --> 00:18:52,070
we we gave way in this paper we gave one particular algorithm for doing it

310
00:18:52,070 --> 00:18:55,550
i think there's a whole family of things that all work as far as this

311
00:18:55,550 --> 00:18:59,460
this criteria is concerned and you know the bound that we get

312
00:18:59,520 --> 00:19:05,090
looks like that got states and actions in it and

313
00:19:05,110 --> 00:19:07,730
i think that's lines deltas in dallas so

314
00:19:07,730 --> 00:19:13,040
applying these methods too much more general data types than was apparent when we started

315
00:19:13,060 --> 00:19:16,840
and that's almost as an accident of what you know the other

316
00:19:16,880 --> 00:19:19,150
strategies that have been born

317
00:19:19,170 --> 00:19:22,130
so certainly you know this is a nice property and now

318
00:19:22,150 --> 00:19:26,400
you know talk a little bit more about that in the time i should finish

319
00:19:26,460 --> 00:19:29,230
tenth edition that's that's my

320
00:19:30,840 --> 00:19:35,730
and we've gained and this is crucial this more flexible regularisation generalisation control i think

321
00:19:35,730 --> 00:19:40,860
that's absolutely crucial to the whole strategy you know because of these kernels being able

322
00:19:40,860 --> 00:19:42,940
to throw in very high dimensional

323
00:19:42,940 --> 00:19:45,290
data very easily

324
00:19:45,290 --> 00:19:47,420
we need that lever

325
00:19:47,420 --> 00:19:52,920
to be able to control the generalization and so that's crucial sort of step

326
00:19:52,940 --> 00:19:54,860
well you know normal

327
00:19:54,880 --> 00:19:57,400
approaches to learning would require much more

328
00:19:57,420 --> 00:20:01,090
careful control of the capacity of the function class here we were able to throw

329
00:20:01,090 --> 00:20:03,630
in very high dimensional function class

330
00:20:04,090 --> 00:20:07,060
generalization and

331
00:20:07,500 --> 00:20:09,980
but control through the

332
00:20:10,000 --> 00:20:13,210
through the use of this weight

333
00:20:16,840 --> 00:20:20,400
and again this convex optimisation problem

334
00:20:20,420 --> 00:20:25,570
and so we don't have the problem of local minima

335
00:20:25,590 --> 00:20:28,330
OK so i should just mention that the

336
00:20:28,500 --> 00:20:34,710
very briefly why is that convex well it's convex because that matrix that i've described

337
00:20:37,360 --> 00:20:41,810
is positive semi definite so actually that property of the kernel required for it to

338
00:20:41,810 --> 00:20:46,770
be a kernel is actually exactly the property we need to keep convexity so in

339
00:20:46,770 --> 00:20:50,060
a sense the you know you might say well right away doesn't matter and we

340
00:20:50,060 --> 00:20:54,300
can get around perhaps but but it's also trying way something valuable which is this

341
00:20:56,330 --> 00:20:58,440
sorry of convex optimization

342
00:20:58,470 --> 00:21:01,430
so there is is connection

343
00:21:01,470 --> 00:21:04,680
OK i want to spend a little bit of time looking at the algorithm exp

344
00:21:04,680 --> 00:21:05,940
for solving this

345
00:21:06,230 --> 00:21:11,850
so to convex quadratic programme as i mentioned this you know people out there who

346
00:21:11,890 --> 00:21:17,440
spend a lifetime trying to solve these things and you can apply standard optimisation

347
00:21:18,640 --> 00:21:24,180
but they don't exploit the specific to the problem and they can be inefficient

348
00:21:24,850 --> 00:21:29,330
one thing that's important is to use chunking for large datasets chunking is where you

349
00:21:29,480 --> 00:21:32,630
look at a small set of the data to solve it on that sort of

350
00:21:32,630 --> 00:21:34,880
the data and then start to expand

351
00:21:34,880 --> 00:21:37,190
bring in more data and

352
00:21:37,220 --> 00:21:40,260
more training data and extend

353
00:21:40,290 --> 00:21:42,380
the application

354
00:21:42,390 --> 00:21:45,760
just do you know picking vanilla

355
00:21:46,970 --> 00:21:48,800
maybe for matlab

356
00:21:48,810 --> 00:21:50,890
plugging in your

357
00:21:50,900 --> 00:21:54,970
quadratic optimisation problem pressing the button it'll be a long time if it's a large

358
00:21:54,970 --> 00:21:56,460
dataset you want

359
00:21:56,470 --> 00:21:58,460
come up with a solution very quickly

360
00:21:59,000 --> 00:22:02,650
but there are packages out there you know which do this for you so it's

361
00:22:04,190 --> 00:22:07,090
not a problem but actually

362
00:22:07,100 --> 00:22:10,690
you can use very simple gradient ascent algorithms

363
00:22:10,720 --> 00:22:14,710
yourself or individual transform just going to give you know if you really wanted to

364
00:22:14,710 --> 00:22:17,340
do it yourself it's actually very very easy

365
00:22:17,930 --> 00:22:20,170
and i think i'm just going to show you

366
00:22:20,170 --> 00:22:27,090
an algorithm called kernel adatron which is extremely simple to implement and it's pretty good

367
00:22:27,090 --> 00:22:29,290
provided you include the chance

368
00:22:29,300 --> 00:22:34,280
in fact very so if we fix the threshold is zero so this was the

369
00:22:34,280 --> 00:22:38,190
point i made where we assume that b is fixed to zero

370
00:22:38,210 --> 00:22:40,800
so we don't have that extra

371
00:22:40,810 --> 00:22:43,800
some outcry y equals zero

372
00:22:44,930 --> 00:22:50,020
then the following is the gradient descent algorithm will well let's work out coordinate wise

373
00:22:50,020 --> 00:22:56,480
so you work out the gradient of the optimisation you're trying to optimise the value

374
00:22:56,500 --> 00:23:00,840
trying to optimize with respect to an individual i and you actually end up with

375
00:23:00,840 --> 00:23:02,830
one minus the

376
00:23:05,220 --> 00:23:06,630
margin of that

377
00:23:06,650 --> 00:23:10,990
examples of g is the function that the current version of the function that we

378
00:23:10,990 --> 00:23:12,860
got so it's alpha

379
00:23:12,880 --> 00:23:16,420
remember i said there was this extra wages and some alpha j y j that's

380
00:23:16,420 --> 00:23:20,140
just the sign that tells us whether there should be positive or negative and the

381
00:23:20,140 --> 00:23:24,570
inner product between the test examples and the corresponding for training example

382
00:23:24,590 --> 00:23:27,650
so this is just the margin of that example

383
00:23:27,680 --> 00:23:30,420
and of course you know our aim was to get it up to one so

384
00:23:30,420 --> 00:23:33,540
if it's less than one then we need to increase that i'll try and if

385
00:23:33,540 --> 00:23:35,960
it's bigger than one we can deduce that alpha i

386
00:23:35,970 --> 00:23:38,020
basically sort of very natural

387
00:23:38,040 --> 00:23:41,680
and this is the update then you just take the alpha rai and here is

388
00:23:41,680 --> 00:23:43,350
the gradient and you

389
00:23:43,360 --> 00:23:45,860
if you adding this extra factor here

390
00:23:45,880 --> 00:23:50,400
then you actually go straight to the sort of best value of itself right

391
00:23:50,430 --> 00:23:54,470
of course this may take alpha i out of the interval which is supposed to

392
00:23:54,470 --> 00:23:59,180
be an you remember it was the boston transit so that authorize should be within

393
00:23:59,210 --> 00:24:03,260
the interval zero to see so what happens is you do this update and because

394
00:24:03,260 --> 00:24:05,920
outside that interval you just pop it back inside

395
00:24:05,940 --> 00:24:08,920
and that's basically so you just work out these

396
00:24:08,930 --> 00:24:13,930
you know picking out work out this updated if it was outside the information back

397
00:24:13,930 --> 00:24:16,710
to the thing and you keep doing that until

398
00:24:16,730 --> 00:24:18,600
none of the authorized change

399
00:24:18,600 --> 00:24:21,540
say if you wanted to work fast you need to be a little bit more

400
00:24:21,540 --> 00:24:25,630
about the order in which we choose alpha basically choose the ones where there's lots

401
00:24:25,630 --> 00:24:26,380
of error

402
00:24:26,390 --> 00:24:27,960
you know that the

403
00:24:27,970 --> 00:24:30,830
the thing is either inside the marginal

404
00:24:31,840 --> 00:24:38,940
now SMO is just this is probably the most popular algorithm is just an adaptation

405
00:24:38,940 --> 00:24:46,480
the video and the the powerp the PDF version of the PowerPoint okay so

406
00:24:46,500 --> 00:24:50,040
this is something you are going to see twice more before the end of the

407
00:24:50,040 --> 00:24:58,130
week this is the cosmological standard model that is the framework of

408
00:24:58,140 --> 00:25:03,120
the series of lectures and we've been working our way around here and today we're

409
00:25:03,120 --> 00:25:10,260
going to reach possibly the most dominant and most important aspect of the present composition

410
00:25:10,260 --> 00:25:17,940
of the universe dark energy now in the first lecture I mentioned that the the

411
00:25:17,940 --> 00:25:26,990
only evidence that there is dark energy or a cosmological constant comes from measurements which

412
00:25:26,990 --> 00:25:39,760
tell us something about the expansion history of the universe now the ex

413
00:25:39,760 --> 00:25:54,300
maybe I should close this I guess no telling what it might have said right

414
00:25:54,300 --> 00:26:01,980
so the expansion rate of the universe is the key most fundamental quantity in

415
00:26:01,980 --> 00:26:06,600
cosmology and to be able to predict that on the basis of a theory and

416
00:26:06,600 --> 00:26:12,460
a model and to compare that with observation is so important it is a very

417
00:26:12,460 --> 00:26:20,540
fundamental quantity it comes from the excuse me the zero zero component of the Einstein

418
00:26:20,540 --> 00:26:27,360
equations the zero zero component of the Einstein equation gives the expansion rate its this is also

419
00:26:27,360 --> 00:26:32,920
known as the Friedmann equation and the Friedmann equation can be written in terms

420
00:26:32,920 --> 00:26:38,880
of the present expansion rate of the universe Hubble's constant and the contribution of various

421
00:26:38,880 --> 00:26:46,240
components which scale in different way with time scale factor or redshift and it is

422
00:26:46,240 --> 00:26:51,660
a fact that these different components scale in different way will allow us to disentangle

423
00:26:51,660 --> 00:26:58,860
their magnitude and their contribution the first term is a curvature term and the curvature

424
00:26:58,860 --> 00:27:05,140
term is proportional to one minus the present value of the total value of Omega

425
00:27:05,180 --> 00:27:12,260
and this scale says one plus the redshift squared and the coeficient of one minus

426
00:27:12,260 --> 00:27:18,120
omega total can be deduced from observations of the CMB and that in

427
00:27:18,120 --> 00:27:26,860
fact is zero according to the observations with an uncertainty of maybe one percent

428
00:27:26,860 --> 00:27:34,600
the other contribution with which is a matter contribution can be deduced from observations of

429
00:27:34,600 --> 00:27:42,740
large-scale structure and that is omega today of about point three the contribution of radiation

430
00:27:42,740 --> 00:27:47,900
today can be determined from measurements of the temperature of the CMB and

431
00:27:47,900 --> 00:27:52,680
this is about ten to the minus five but of course since its scale says one plus

432
00:27:52,680 --> 00:27:59,440
Z to the fourth if you go back to large Z eventually it becomes dominant

433
00:27:59,440 --> 00:28:07,140
so there is an additional contribution that's added in order for the observa observations to

434
00:28:07,140 --> 00:28:11,940
be accounted for in terms of a model so there are two ways to think

435
00:28:11,940 --> 00:28:16,500
of this if you measure the expansion rate as a function of redshift and it's

436
00:28:16,500 --> 00:28:22,620
not fit by these three known quantities then you can think you've discovered something that by

437
00:28:22,620 --> 00:28:28,280
the name of dark energy or you if you were a cynic you might say

438
00:28:28,280 --> 00:28:34,940
well you've added an epicycle in order to make the observations agree with the theory

439
00:28:34,940 --> 00:28:41,580
so the dark energy that's added is a coefficient that's determined by H of Z and

440
00:28:41,580 --> 00:28:48,020
scaling is one plus the redshift to the three times one plus W the simplest

441
00:28:48,020 --> 00:28:53,880
model is W equal to minus one so there is no scaling with redshift that

442
00:28:53,880 --> 00:28:59,820
is a cosmological constant but in fact people phenomenalogically have W as a

443
00:28:59,820 --> 00:29:05,820
function of A or Z so there is some function that you can add and this

444
00:29:05,820 --> 00:29:11,540
won't surprise you if you can add an arbitrary function W then you can agree wonderfully

445
00:29:11,540 --> 00:29:19,680
with the observations now the expansion rate of the universe is the key quantity

446
00:29:19,680 --> 00:29:30,380
that enters cosmological observables and most observables depend upon something like the integral of the

447
00:29:30,380 --> 00:29:39,490
redshift of one over H of Z so you weigh the different terms in H of Z now among the

448
00:29:39,500 --> 00:29:46,340
cosmological observables that we will talk about to get information about H of Z and inferred information

449
00:29:46,340 --> 00:29:54,640
about dark energy is the luminosity distance redshift relationship the luminosity distance is defined as

450
00:29:54,650 --> 00:30:00,300
the equivalent inverse square law distance if you measure the flux and you know the

451
00:30:00,300 --> 00:30:08,260
luminosity that defines the luminosity distance there is also the angular diameter distance again defined

452
00:30:08,290 --> 00:30:14,240
in terms of the newtonian result if there is some object of a known

453
00:30:14,240 --> 00:30:20,760
physical size and you measure its angular scale then it's related to the angular diameter

454
00:30:20,760 --> 00:30:23,760
for bayesian modeling got dave macon nineteen ninety

455
00:30:24,420 --> 00:30:28,810
three i guess starting to publish on bayesian approach models for neural networks so you

456
00:30:28,810 --> 00:30:32,970
really have all these theoretical stuff being absorbed by the community and the community moving

457
00:30:33,970 --> 00:30:34,630
to an extent

458
00:30:35,090 --> 00:30:37,370
such that i think in many areas we actually

459
00:30:38,460 --> 00:30:39,780
overtook um

460
00:30:40,950 --> 00:30:44,690
the fields we were competing with in terms of statistics which at one stage was

461
00:30:45,230 --> 00:30:49,670
regarded as more rigorous the machine learning but actually machine learning has really you know

462
00:30:49,670 --> 00:30:52,620
it's putting a lot back in towards statistics

463
00:30:53,580 --> 00:30:56,700
so my personal view is the machine learning benefited greatly by

464
00:30:57,300 --> 00:31:02,840
incorporating these ideas from psychology but not being afraid to incorporate rigorous theory and one

465
00:31:02,840 --> 00:31:06,740
of the things we're seeing now is this return to those type models with deep

466
00:31:06,740 --> 00:31:10,140
learning these models are difficult to run

467
00:31:10,240 --> 00:31:15,940
analyse models but they're doing amazing stuff algorithmically and actually doing some pretty incredible

468
00:31:16,930 --> 00:31:18,500
unsupervised learning with these models

469
00:31:19,840 --> 00:31:23,910
but instead of just people playing with those things in sync algorithmically his this

470
00:31:24,400 --> 00:31:29,660
you've got people like ian murray doing great analysisof trying to do the annealed importance

471
00:31:29,660 --> 00:31:34,080
sampling to estimate marginal likelihoods in these very complex models trying to understand better what's

472
00:31:34,080 --> 00:31:37,190
going on and i think it's a very powerful community as a result of

473
00:31:39,250 --> 00:31:40,870
so i think learning machine learning with u

474
00:31:41,380 --> 00:31:43,890
with a lot of scepticism by the statistics community

475
00:31:44,730 --> 00:31:45,960
but certainly for me

476
00:31:46,500 --> 00:31:50,340
i'm saying that you can take a lot of people i'm most interested in talking

477
00:31:50,340 --> 00:31:53,600
to are statisticians doing bayesian learning

478
00:31:55,240 --> 00:31:57,180
or what i think the bayesian modeling

479
00:31:57,590 --> 00:31:58,250
and this now

480
00:32:01,190 --> 00:32:03,280
interaction between machine learning and statistics

481
00:32:03,790 --> 00:32:06,620
but i think there is still a difference so when people tell you

482
00:32:07,390 --> 00:32:10,040
what's going on and they say to you i just

483
00:32:10,720 --> 00:32:13,300
just statistics done in a hacky way whatever

484
00:32:13,750 --> 00:32:16,080
i think that's not true my personal view is that the

485
00:32:16,650 --> 00:32:19,030
the philosophy the two fields are fundamentally different

486
00:32:20,220 --> 00:32:21,830
whatever you're doing at the moment

487
00:32:22,310 --> 00:32:26,590
whatever your tasks or if you want machine learning you want to replace the human

488
00:32:26,930 --> 00:32:31,420
in the processing of your dataset so there was this i had lunch next we

489
00:32:31,420 --> 00:32:33,050
were doing gaussianprocess workshop

490
00:32:33,690 --> 00:32:34,340
and zoubin

491
00:32:35,300 --> 00:32:39,460
gharamani tonio hating with any hagans leaving statistician

492
00:32:39,920 --> 00:32:45,650
who did a lot to introduce casting processes the statistics community and a very well known bayesian statistician

493
00:32:46,050 --> 00:32:50,570
anthony cities said i don't believe you can ever take a dataset and-or

494
00:32:52,450 --> 00:32:57,210
this analyzed by computer without human being involved in the loop to interpret the analysis

495
00:32:57,730 --> 00:33:01,620
at which i agree with in terms of the technology we have today

496
00:33:02,090 --> 00:33:05,760
but zoubin immediately said well but unless you believe that the humans doing anything

497
00:33:06,650 --> 00:33:09,170
magical unless you believe there's a ghost in the machine

498
00:33:10,070 --> 00:33:10,630
put it one way

499
00:33:11,180 --> 00:33:14,900
you should be able to replace whatever the human is doing as well now i

500
00:33:14,900 --> 00:33:19,500
think that statistics as a community is about summarizing data

501
00:33:20,070 --> 00:33:25,240
to the extent that humans can interpret the data so you try and say

502
00:33:25,910 --> 00:33:29,770
the mean if this data is seven so if you look if you go back to the

503
00:33:30,140 --> 00:33:35,000
when statistics became field in its own right and they are often working with social

504
00:33:35,000 --> 00:33:39,600
science so if you want to understand poverty in manchester verses poverty in london

505
00:33:40,310 --> 00:33:41,190
you could take be

506
00:33:41,640 --> 00:33:42,530
average income

507
00:33:42,950 --> 00:33:44,310
other people in manchester

508
00:33:44,630 --> 00:33:47,280
and find that it's twelve shillings and sixpence

509
00:33:48,950 --> 00:33:53,430
yeah and you can see the average income the people in london and find its

510
00:33:53,600 --> 00:33:56,010
eighteen shillings per year and then you want to know

511
00:33:56,620 --> 00:33:59,810
the difference the fact that these numbers are different doesn't mean anything

512
00:34:00,880 --> 00:34:02,120
these populations different

513
00:34:02,710 --> 00:34:08,150
so statistics itself we know what that's like baseball you say batting averages one runs

514
00:34:08,150 --> 00:34:10,260
batted in this point four orient cricket

515
00:34:10,670 --> 00:34:14,690
you know you say the batting averages thirty runs that's what statistics means when we

516
00:34:14,690 --> 00:34:20,160
say statistics we're talking mathematical statistics which is really the study of whether the fact

517
00:34:20,160 --> 00:34:25,450
that those numbers different means that the populations are different and that's what statistical set

518
00:34:25,450 --> 00:34:26,980
out to do so it's set up to be

519
00:34:27,850 --> 00:34:29,920
so understand the numbers you're feeding humans

520
00:34:30,480 --> 00:34:33,380
the machine learning set up to eliminate humans

521
00:34:38,090 --> 00:34:41,210
fundamentally we're aiming for me i singularity okay

522
00:34:42,050 --> 00:34:45,580
i mean which fortunately anyone in the field notes so far away from it we

523
00:34:45,590 --> 00:34:49,690
don't have to worry yet like this going out but there is really are that

524
00:34:49,690 --> 00:34:50,400
we don't want

525
00:34:51,090 --> 00:34:51,790
you know i'm

526
00:34:52,430 --> 00:34:53,490
i presented the

527
00:34:53,940 --> 00:34:57,560
guassian process latent variable model to the the stats group in sheffield

528
00:35:00,230 --> 00:35:02,700
and tony hayward there actually built the end of it

529
00:35:03,210 --> 00:35:07,970
one of the lectures in the group say okay that's okay that's fine but what you tell the client

530
00:35:09,190 --> 00:35:10,200
unlike who's the client

531
00:35:11,050 --> 00:35:16,680
statisticians always worrying about clients the someone comes to them with data and they're trying to say

532
00:35:17,540 --> 00:35:21,870
something about the data tell the client computer peabody i don't think we care about

533
00:35:21,870 --> 00:35:25,040
the client because in some sense which is trying to integrate it in a computer

534
00:35:25,040 --> 00:35:28,920
program which i'm trying to write a module in a computer program that has an

535
00:35:28,920 --> 00:35:33,470
end result like those faces you can animate faces right the client varies

536
00:35:34,300 --> 00:35:35,010
graphics guy

537
00:35:36,460 --> 00:35:38,550
so i don't tell him anything the computer does all that

538
00:35:38,880 --> 00:35:40,680
so i think that's the fundamental difference

539
00:35:41,110 --> 00:35:42,590
but for the moment the two here

540
00:35:42,910 --> 00:35:44,080
overlap very strongly

541
00:35:44,680 --> 00:35:46,480
why do they overlap in the moment

542
00:35:47,720 --> 00:35:52,850
because we're not capable of replacing the human so very often we need to say

543
00:35:52,850 --> 00:35:54,670
something to the human about the data

544
00:35:55,320 --> 00:35:58,800
so we are interested in computing these numbers and the sort of thing statisticians do

545
00:35:59,840 --> 00:36:00,570
i think also

546
00:36:01,770 --> 00:36:06,650
one of the main roles statistics again is a personal view is that you can disagree or agree or whatever

547
00:36:09,160 --> 00:36:12,480
a lot of statistics in terms of what the film is set up to do

548
00:36:13,410 --> 00:36:19,280
with solved quite early on so in terms of being able to infer draw conclusions about data

549
00:36:19,840 --> 00:36:24,560
about if you've got two sets crops and your fertilizing them in different ways you

550
00:36:24,560 --> 00:36:30,420
want to know which fertilizers better randomizing the samples these sort of ideas came out

551
00:36:30,480 --> 00:36:35,050
early this a model free they allow you to infer things about the data clinical

552
00:36:37,900 --> 00:36:41,730
a lot of those questions again solved and statisticians have moved into the things which

553
00:36:41,730 --> 00:36:46,560
are much more model-based which originally the field try to avoid so bayesian i think

554
00:36:46,560 --> 00:36:52,150
bayesian statistics personally is a complete oxymoron because statistics set itself up as a field

555
00:36:52,150 --> 00:36:53,710
like this

556
00:37:01,800 --> 00:37:07,660
OK some content from bristol university this is my first talk to really introduction support

557
00:37:07,660 --> 00:37:12,770
vector machines are just doing so classical story started binary classification

558
00:37:12,820 --> 00:37:16,390
the multiclass classification soft margins that sort of stuff

559
00:37:16,650 --> 00:37:20,610
my second talk after break have to sort out what the hell on the break

560
00:37:20,610 --> 00:37:25,210
is so we get to it but be on more general kernel methods in fact

561
00:37:25,210 --> 00:37:30,970
the idea come as this is can define many types of learning machines many types

562
00:37:30,970 --> 00:37:35,120
of kernels many types of things you can do with this kernel based methods so

563
00:37:35,140 --> 00:37:40,180
the second talk after breaking trying to to make the subject much more general OK

564
00:37:40,470 --> 00:37:45,820
this often known as opportunity to actually try and support vector machines now that's really

565
00:37:45,820 --> 00:37:50,220
sort of four levels of cover archibald talk when we come to it symbols levels

566
00:37:50,220 --> 00:37:54,650
and i still graphical thing done by steve gunn we click points on the screen

567
00:37:54,650 --> 00:37:59,090
and you see the actual support vector machines setting up the boundary to separate data

568
00:37:59,480 --> 00:38:02,110
and then the first other options as

569
00:38:02,230 --> 00:38:09,150
package around myself where you can use UCI repository data and download data do classification

570
00:38:09,150 --> 00:38:10,140
time problems

571
00:38:10,180 --> 00:38:17,120
finally feel with matlab you can try the quote proc programming optimization toolbox actually write

572
00:38:17,120 --> 00:38:22,860
your own SVM learner so we will come to soften them

573
00:38:22,870 --> 00:38:28,480
but this is support vector machines and my own interest i was research for a

574
00:38:28,480 --> 00:38:32,260
lot of support vector machines in the past in the second talk will be

575
00:38:32,310 --> 00:38:36,000
quite a few bits which are on my own research seems novelty detection i worked

576
00:38:36,640 --> 00:38:41,810
a few things that make mention the bayes point machine worked on the remaining interest

577
00:38:41,810 --> 00:38:47,710
in the moment probabilistic graphical methods and also work alot and applications to cancer in

578
00:38:47,710 --> 00:38:52,320
fact which is a nice article cancer formatting right at the end of using these

579
00:38:52,590 --> 00:38:58,350
support vector machines for example to decide relax versus non relapse predicting the future

580
00:38:58,370 --> 00:39:03,080
where the cancer patients will have a relapse or not and it's quite successful problem

581
00:39:03,080 --> 00:39:04,270
i did with

582
00:39:04,310 --> 00:39:09,340
staff the issue for cancer research in london so but we get onto applications at

583
00:39:09,340 --> 00:39:14,240
the end of the second talk so if you like

584
00:39:14,250 --> 00:39:15,780
this first talk i want to

585
00:39:15,800 --> 00:39:19,150
first of all just to the simple story with the support vector machine for binary

586
00:39:20,750 --> 00:39:26,440
and this topic of soft margins are come onto to that the course many real

587
00:39:26,440 --> 00:39:29,310
life applications of multi class

588
00:39:30,150 --> 00:39:34,410
then elaborate the story pun class to multiclass classification

589
00:39:34,430 --> 00:39:40,050
the schema uses finally christian is in in my department it with the john platt

590
00:39:40,060 --> 00:39:45,080
and then briefly also talk about regression so

591
00:39:45,090 --> 00:39:48,400
that's the plan for the first talk a second

592
00:39:48,500 --> 00:39:54,750
in the first talk i really use quadratic programming to learn the problem

593
00:39:55,030 --> 00:39:58,840
in the second talk i want to really show that is a big subject out

594
00:39:59,410 --> 00:40:05,430
and it's not just quadratic programming you can do kernel methods with linear programming and

595
00:40:05,430 --> 00:40:09,370
indeed sort of nonlinear optimization and many other things

596
00:40:09,410 --> 00:40:13,650
i'll be talking about how to train SVM

597
00:40:13,690 --> 00:40:20,340
model selection there is potentially one parameter you have in your SVM which is the

598
00:40:20,340 --> 00:40:21,590
kernel parameter

599
00:40:21,630 --> 00:40:25,470
how do you find it well you might find the data you so the kernel

600
00:40:25,470 --> 00:40:27,560
parameter from the data

601
00:40:27,560 --> 00:40:29,270
using cross validation

602
00:40:29,280 --> 00:40:35,030
but there are other ways of doing it from various theorems which mean established so

603
00:40:35,030 --> 00:40:38,020
this is the topic model selection come onto then

604
00:40:38,060 --> 00:40:42,550
different types of kernels come on to that and then i thought just now but

605
00:40:42,550 --> 00:40:47,560
actually using phantom practice in same medical applications

606
00:40:47,580 --> 00:40:53,210
so let's first will start what are the advantages of SVM well

607
00:40:53,240 --> 00:40:57,430
is the principal approach to machine learning in particular

608
00:40:57,430 --> 00:41:01,430
at all the various different times machine learning classification regression

609
00:41:01,440 --> 00:41:03,410
novelty detection etcetera

610
00:41:03,470 --> 00:41:08,670
svm system good generalisation in practice when i first encountered this subject we should be

611
00:41:08,670 --> 00:41:14,690
in the year two thousand ninety eight i tried out i've been doing your networks

612
00:41:14,690 --> 00:41:19,660
and point i tried out the multilayer perceptron against SVM across a whole lot of

613
00:41:19,660 --> 00:41:21,330
UCI datasets

614
00:41:21,340 --> 00:41:25,350
and my impression was for classification SVM is better

615
00:41:25,420 --> 00:41:30,590
the the multilayer perceptron though for regression they were fairly similar

616
00:41:30,690 --> 00:41:33,980
but in any case exhibits good generalisation

617
00:41:33,990 --> 00:41:35,700
two new data

618
00:41:35,710 --> 00:41:39,900
and the hypothesis has explicit dependence on the data

619
00:41:40,590 --> 00:41:43,520
these are courses support vectors which i come onto

620
00:41:43,590 --> 00:41:48,720
when i say he is devoted to take somebody neural network model its dependence on

621
00:41:48,720 --> 00:41:51,150
the data is not so clear cut

622
00:41:51,210 --> 00:41:52,890
i can interpret

623
00:41:53,910 --> 00:41:57,510
depends on the data quite simply from the model i generate

624
00:41:57,550 --> 00:42:00,080
hence it can be broadly interpreted

625
00:42:01,840 --> 00:42:09,610
various advantages when as principal approach to the subject the actual motivation for the subject

626
00:42:09,610 --> 00:42:14,670
to support vector machines really came from statistical learning theory various bounds was approved in

627
00:42:14,670 --> 00:42:17,830
that subject and so

628
00:42:17,890 --> 00:42:24,300
unlike many other approaches to in machine learning neural networks rather sort of ad hoc

629
00:42:24,320 --> 00:42:26,210
so the approach

630
00:42:26,270 --> 00:42:32,530
here you've got learning theory learning theory motivation approach an approach is successful

631
00:42:32,530 --> 00:42:34,340
relatively few if any

632
00:42:34,350 --> 00:42:39,470
parameters you have to adjust to the operator doesn't come into it you simply percent

633
00:42:39,530 --> 00:42:43,030
data and your learning machine goes and learns

634
00:42:44,960 --> 00:42:50,900
now the really big box of the subject is the optimisation task is convex

635
00:42:50,910 --> 00:42:57,520
again in contrast to a new network the new network typically the function i wish

636
00:42:57,520 --> 00:43:01,020
to optimize has false minima now has local

637
00:43:02,780 --> 00:43:06,290
i have a problem finding some sort of global minimum well actually

638
00:43:06,300 --> 00:43:10,550
the task was to do with the support vector machine is a quadratic problem

639
00:43:10,600 --> 00:43:13,190
so it's convex one solutions

640
00:43:13,230 --> 00:43:17,690
no local minima unlike neural networks is a big plus

641
00:43:17,950 --> 00:43:19,720
for this approach

642
00:43:20,280 --> 00:43:25,650
relatively few world very few parameters to adjust the tension is the soft margin and

643
00:43:25,650 --> 00:43:27,240
the kernel parameter

644
00:43:27,260 --> 00:43:31,650
if do medical data like gene expression data i use later on don't

645
00:43:31,660 --> 00:43:36,980
use anything more than in any kind i don't have a kernel parameter

646
00:43:36,990 --> 00:43:42,480
i don't have anything to adjust unlike say your network again where i have questions

647
00:43:42,480 --> 00:43:48,210
about how many hidden nodes are the how many hidden layers what to do about

648
00:43:48,210 --> 00:43:52,170
some of the parameters and updating functions etcetera OK

649
00:43:52,260 --> 00:43:56,300
also can implement a confidence measure

650
00:43:56,320 --> 00:44:00,070
very important for classifier is not just says

651
00:44:00,080 --> 00:44:03,080
relapse among relapse for my medical application

652
00:44:03,090 --> 00:44:05,510
but it should be able to assign probability

653
00:44:06,440 --> 00:44:08,590
those two categories OK

654
00:44:08,590 --> 00:44:13,450
so these or classes of this whole approach

655
00:44:13,460 --> 00:44:17,070
so one of the first of all just recount story for

656
00:44:17,090 --> 00:44:19,590
binary classification OK

657
00:44:20,570 --> 00:44:22,840
first or just something someone notation

658
00:44:22,910 --> 00:44:28,340
the inputs will be this big x OK potential vector

659
00:44:28,390 --> 00:44:35,590
the attributes have never attributes components i will label the particular sample and considering

660
00:44:35,690 --> 00:44:41,010
associated with this particular input have a corresponding output which should be denoted y

661
00:44:41,020 --> 00:44:42,480
i OK

662
00:44:42,510 --> 00:44:47,630
and i'm just going to binary classification to start with some outputs my labels i

663
00:44:47,630 --> 00:44:51,020
call them will be possible minus one OK

664
00:44:51,050 --> 00:44:53,850
medical application talk about much further on

665
00:44:53,860 --> 00:45:00,090
will be a binary classification task where possibly the patient relapse minus we may well

666
00:45:00,090 --> 00:45:00,840
be that

667
00:45:00,840 --> 00:45:03,030
now call these the labels

668
00:45:03,040 --> 00:45:05,460
you could also called the targets

669
00:45:05,490 --> 00:45:08,530
and these guys are the

670
00:45:08,590 --> 00:45:13,280
a range from one to n where n is the number of samples or

671
00:45:13,480 --> 00:45:16,830
the pairings of x and y k

672
00:45:16,880 --> 00:45:22,160
so these excited to find a space which are called input space OK you probably

673
00:45:22,160 --> 00:45:24,350
seen input spaces in the previous

674
00:45:26,870 --> 00:45:31,970
in the input space consists of labelled points label plus and minus one

675
00:45:32,010 --> 00:45:36,410
now it is at the beginning that so what's nice about subject support vector machines

676
00:45:36,410 --> 00:45:42,000
you will be used to supply milady and postal addresses in the system

677
00:45:42,080 --> 00:45:46,430
the match is not based on those million

678
00:45:46,450 --> 00:45:50,560
matches they created two different groups one is the control group one is

679
00:45:50,580 --> 00:45:52,250
treatment groups

680
00:45:52,250 --> 00:46:00,430
so is the treatment group were people who are exposed to the readers whenever they

681
00:46:00,430 --> 00:46:02,160
came to the our website

682
00:46:02,160 --> 00:46:06,910
you know they were shown these retail that treatment

683
00:46:06,950 --> 00:46:09,660
not exposed to the editor

684
00:46:09,680 --> 00:46:15,080
and then these users were followed for six weeks and the

685
00:46:15,120 --> 00:46:19,600
you know that although their online and offline purchases were tracked

686
00:46:21,410 --> 00:46:25,910
it turned out that the people who got exposed to the display advertising

687
00:46:26,220 --> 00:46:30,790
the revenue the revenue from people who do you get exposed to the

688
00:46:30,850 --> 00:46:34,430
the single was significantly

689
00:46:34,450 --> 00:46:37,870
higher than those who are not exposed

690
00:46:37,890 --> 00:46:39,700
so this clearly shows that

691
00:46:39,700 --> 00:46:42,020
brand advertising works it's not

692
00:46:42,390 --> 00:46:48,480
it's not the advertisers do get banned for the back well for doing display advertising

693
00:46:48,810 --> 00:46:50,930
and this is a controlled experiments so

694
00:46:51,180 --> 00:46:55,000
there are no confounding factors here at all it was difficult to conduct such an

695
00:46:55,000 --> 00:46:56,890
experiment was conducted

696
00:46:56,950 --> 00:47:02,040
it clearly shows the brand doesn't work so so this is good news

697
00:47:02,060 --> 00:47:06,910
especially since the advertiser takes all the risk and the brand advertising so there's really

698
00:47:06,910 --> 00:47:12,350
good news for the users and the good news for this brand of advertising

699
00:47:18,750 --> 00:47:22,100
ah yes so

700
00:47:22,140 --> 00:47:26,250
based on this data there have been several so this paper is going to come

701
00:47:26,370 --> 00:47:31,560
very soon you have done a lot of other analysis like that they analysed the

702
00:47:31,560 --> 00:47:36,100
amount of online sales were offline sales on their feet they have done a lot

703
00:47:36,100 --> 00:47:39,890
of slice and dice analysis to see that section of the population actually did respond

704
00:47:39,890 --> 00:47:42,910
to it with a lot of details on this work and

705
00:47:42,910 --> 00:47:48,850
you know i would have distributed the paper but it's not submitted but but yes

706
00:47:48,850 --> 00:47:54,310
but the goal so the main point here is it's a controlled experiment and a

707
00:47:54,520 --> 00:47:56,580
good reasonably confident that

708
00:47:56,620 --> 00:48:03,790
the results for some closure believe artificial the discourse

709
00:48:06,290 --> 00:48:07,930
so that's good

710
00:48:07,980 --> 00:48:12,930
OK so now let's go to the printer display advertising so first the two key

711
00:48:12,930 --> 00:48:17,870
players this advertising of course the advertiser like in any retail you have an advertiser

712
00:48:17,870 --> 00:48:19,270
buyer and the seller

713
00:48:19,450 --> 00:48:24,890
so advertiser here of course is the bias and they buy ad space

714
00:48:24,910 --> 00:48:26,370
from publishers

715
00:48:26,560 --> 00:48:30,730
these acts this is about the and so for instance if you're a big advertisers

716
00:48:30,730 --> 00:48:34,980
like GM i should not mention GM sorry if you have a big and would

717
00:48:35,040 --> 00:48:41,250
like that so i talked about the affair began advisory

718
00:48:41,680 --> 00:48:46,330
you might buy ad space is when you're in landslide OK i what i want

719
00:48:46,330 --> 00:48:51,340
to put about i want to put my water and i want to make sure

720
00:48:52,330 --> 00:48:54,370
do my booking right now

721
00:48:55,870 --> 00:48:59,870
that's one way of doing that i mean small advertisers are going to that because

722
00:48:59,870 --> 00:49:02,680
in few book things superior

723
00:49:02,790 --> 00:49:07,180
so just by and from the spot market as and when it is available so

724
00:49:07,180 --> 00:49:12,770
that different ways of buying are typically if you want to guarantee

725
00:49:12,790 --> 00:49:16,330
you pay more that's quite reasonable

726
00:49:16,350 --> 00:49:19,310
and then publisher who cells

727
00:49:19,350 --> 00:49:23,720
they demanded well in advance and you the guarantee that they're going to deliver that

728
00:49:23,720 --> 00:49:26,100
many number of impressions if they don't then

729
00:49:26,200 --> 00:49:27,870
there's a penalty

730
00:49:27,890 --> 00:49:33,330
since charging more to make sure that you do guarantee what you

731
00:49:33,350 --> 00:49:34,350
as promised

732
00:49:34,370 --> 00:49:38,200
and while they can't sell everything in guaranteed market

733
00:49:38,250 --> 00:49:42,830
that's what they would like to do but that's not always possible so they

734
00:49:43,230 --> 00:49:45,910
the remnant inventory as it is called

735
00:49:45,930 --> 00:49:47,660
it's sort in the spot market

736
00:49:49,410 --> 00:49:54,480
and of course there is the idea the that connects these two as the mentioning

737
00:49:54,660 --> 00:50:00,290
publishers by and then you that network to connect these two in the exchange model

738
00:50:00,290 --> 00:50:05,020
the there are several ad network so there are some small network and big networks

739
00:50:05,020 --> 00:50:09,290
and the small networks might lead to the big networks so it's a very complicated

740
00:50:09,290 --> 00:50:13,540
revenue model ecosystems pretty complicated like call it

741
00:50:13,560 --> 00:50:17,370
so you have an EM saying ad network doesn't mean to single out network it

742
00:50:17,370 --> 00:50:19,180
could be a chain of networks

743
00:50:19,200 --> 00:50:22,500
work together descartes

744
00:50:22,660 --> 00:50:28,870
and then becomes complicated but i mean this discourse simplified scenario

745
00:50:30,480 --> 00:50:34,660
OK so again as i was mentioning the traditional so the display advertising and movies

746
00:50:34,660 --> 00:50:40,620
more close to traditional advertising like television advertising which we know has existed for a

747
00:50:40,620 --> 00:50:47,290
long time and that's really targeted a particular demographic so these are some examples but

748
00:50:47,290 --> 00:50:51,330
you might want to target ads GM ads on yahoo autos

749
00:50:51,390 --> 00:50:56,160
to be shown two males above fifty five and that's one wales this is targeting

750
00:50:57,850 --> 00:50:58,750
the penang on

751
00:50:59,310 --> 00:51:07,040
no one thing so something which which happens at the beginning of justice

752
00:51:07,250 --> 00:51:09,770
something of advertisers both

753
00:51:09,830 --> 00:51:12,390
but so that's not

754
00:51:12,450 --> 00:51:16,910
you are the seller and you really want to

755
00:51:16,910 --> 00:51:22,200
show you that what but showing at like costume

756
00:51:22,220 --> 00:51:23,370
so what you might

757
00:51:23,430 --> 00:51:24,290
say well

758
00:51:24,310 --> 00:51:29,890
the white book my what they book campaign on yahoo help you know

759
00:51:29,890 --> 00:51:30,830
he said well

760
00:51:30,850 --> 00:51:36,790
the book book campaign on yahoo help but i will want to target

761
00:51:36,810 --> 00:51:40,600
users who have visited yahoo articles recently

762
00:51:40,700 --> 00:51:43,370
so instead of directly trying to book

763
00:51:43,520 --> 00:51:48,890
your campaign on yahoo autos you indirectly target users who have been exposed to auto

764
00:51:48,910 --> 00:51:54,870
or what interested in articles and at the facilities like that do allow you to

765
00:51:55,700 --> 00:51:58,730
users who are interested in a particular

766
00:51:59,680 --> 00:52:04,350
who have recently so this is again another way users to the various creative ways

767
00:52:04,350 --> 00:52:08,180
to users can actually target areas and actually read stories they want to

768
00:52:09,350 --> 00:52:13,020
the use targeting schemes if if you see netflix and on help is don't be

769
00:52:13,020 --> 00:52:17,600
surprised if they might be targeting people who probably are interested in movies for all

770
00:52:17,600 --> 00:52:18,930
you know the

771
00:52:27,350 --> 00:52:31,810
so it depends on the publisher right and yahoo provides this particular set of characteristics

772
00:52:31,810 --> 00:52:36,350
by which you can look at and that depends on the salesforce that you know

773
00:52:36,390 --> 00:52:40,230
depends on the relationship with their customers if you are in new publisher you might

774
00:52:40,230 --> 00:52:41,100
want to

775
00:52:41,120 --> 00:52:43,270
create your own set of attributes

776
00:52:43,270 --> 00:52:45,680
and so them in the open market

777
00:52:45,680 --> 00:52:49,890
and you can just go and edit these things and you know i community the

778
00:52:49,890 --> 00:52:51,860
kind of resources grow

779
00:52:51,860 --> 00:52:55,940
freebase is also has its name implies is free so anyone is able to go

780
00:52:55,940 --> 00:53:00,740
on use it and powers takes advantage of this in two ways one is we

781
00:53:00,740 --> 00:53:05,220
download all the data in a similar way to other resources like wordnet and we

782
00:53:05,220 --> 00:53:08,080
use the offline in creating our index

783
00:53:08,100 --> 00:53:13,440
in addition we were able to throw queries to this in real time by their

784
00:53:13,440 --> 00:53:17,200
API should example of that moment

785
00:53:17,820 --> 00:53:21,190
but that's all the pieces are put together from multiple

786
00:53:21,210 --> 00:53:25,860
the semantic resources and composed together in a natural language search system

787
00:53:25,860 --> 00:53:30,530
and from this it's pretty clear that the more the system is able to derive

788
00:53:30,540 --> 00:53:31,890
broad value

789
00:53:31,900 --> 00:53:33,880
even with very limited knowledge

790
00:53:33,990 --> 00:53:37,540
but the more knowledge that has the better it can be in the deeper can

791
00:53:39,090 --> 00:53:43,620
when you look at these kind of community resources the more every time somebody goes

792
00:53:43,620 --> 00:53:48,510
in and something in wordnet or every time somebody creates some new information in freebase

793
00:53:48,510 --> 00:53:54,610
that's automatically able to help make ourselves kind of natural language search better and better

794
00:53:57,050 --> 00:53:58,850
to give another example

795
00:53:58,860 --> 00:54:00,510
because it's fun

796
00:54:00,520 --> 00:54:03,490
who said something about WMD's

797
00:54:03,510 --> 00:54:08,550
WND is the abbreviation for weapons of mass destruction

798
00:54:09,360 --> 00:54:13,410
think from moment about how you would express that kind of query in in keyword

799
00:54:17,080 --> 00:54:21,850
any ideas

800
00:54:21,950 --> 00:54:27,770
it's really hard you know you want the concept of someone saying something and that

801
00:54:27,770 --> 00:54:32,870
can happen all kinds of ways and you also want to be about

802
00:54:32,880 --> 00:54:37,590
WMD's the number it's a i i can even think about how you express this

803
00:54:37,590 --> 00:54:41,950
in keyword search only for the events keyword search you really need that to basically

804
00:54:41,950 --> 00:54:47,380
be hitting a very rich semantic index essentially this is a kind of you know

805
00:54:47,570 --> 00:54:51,610
complex query over you know RDF type of structure

806
00:54:51,710 --> 00:54:57,800
so the results because powerset has done that the results are things like this

807
00:54:57,840 --> 00:55:00,620
it's not the

808
00:55:00,670 --> 00:55:04,830
these anonymous officials said that in our judgment the CIA's work on wmds has been

809
00:55:04,830 --> 00:55:07,070
set back ten years

810
00:55:07,210 --> 00:55:12,540
OK so that that's actually capturing the notion where you WMD itself is the

811
00:55:12,540 --> 00:55:14,760
objectivist saying

812
00:55:14,780 --> 00:55:19,890
o two rho say that she worked at the agency on WMD once again

813
00:55:19,940 --> 00:55:24,600
now here we have the the attack was to current was to occur in july

814
00:55:24,600 --> 00:55:28,920
two thousand six and about the twenty one year-old lebanese man identified only as youssef

815
00:55:29,840 --> 00:55:35,790
prosecutors said youssef another man left suitcases stuffed with crude propane gas bombs on the

816
00:55:36,920 --> 00:55:42,560
so that is the same event and we're we're now getting crew propane gas bombs

817
00:55:42,790 --> 00:55:46,700
mismatching there's no mention all in text about WMD's

818
00:55:47,320 --> 00:55:49,490
and the results go on

819
00:55:49,620 --> 00:55:52,400
we we have pepper spray

820
00:55:52,410 --> 00:55:56,770
we have the china stealing the report also says that china still design information about

821
00:55:56,820 --> 00:55:57,800
neutron bomb

822
00:55:57,840 --> 00:55:59,290
and and so on

823
00:55:59,340 --> 00:56:03,860
and once again how is that happening well i won't go through hell the processing

824
00:56:03,860 --> 00:56:08,860
for someone saying something about something happens but it's pretty clear as result of syntactic

825
00:56:08,860 --> 00:56:10,190
semantic analysis

826
00:56:12,650 --> 00:56:15,800
i have to say i was actually surprised when this query work because i had

827
00:56:15,810 --> 00:56:20,010
no idea that i system knew anything at all about WMD's it turns out that

828
00:56:20,010 --> 00:56:22,150
we're has a fairly rich

829
00:56:22,170 --> 00:56:26,490
the amount of information about weapons of mass destruction

830
00:56:26,500 --> 00:56:28,260
here we see bioweapons

831
00:56:28,270 --> 00:56:29,770
chemical weapons

832
00:56:29,780 --> 00:56:32,160
you know there's a poison gas and so on

833
00:56:33,580 --> 00:56:38,370
it's really interesting that you take these kind of community resources composed them with a

834
00:56:38,370 --> 00:56:42,810
rich general purpose natural language system and one of being able to do you know

835
00:56:42,810 --> 00:56:48,440
incredibly rich and powerful queries that would just not be possible otherwise

836
00:56:48,470 --> 00:56:52,450
now let's look a little bit out

837
00:56:53,330 --> 00:56:59,470
power sets some powerset semantic database that automatically created our testbed right now is wikipedia

838
00:56:59,490 --> 00:57:04,250
wikipedia is about two million articles it's a great open community resource growing all the

839
00:57:04,250 --> 00:57:09,200
time and covers many the broad topics that people care about it's a good match

840
00:57:09,200 --> 00:57:12,750
as well as people in this community center web community have clearly locked onto that

841
00:57:12,750 --> 00:57:17,210
i think the probably like maybe ten papers at this conference on using wikipedia in

842
00:57:17,210 --> 00:57:18,270
various ways

843
00:57:18,270 --> 00:57:20,940
this thing crosses at some other place

844
00:57:20,950 --> 00:57:22,620
which is this point q

845
00:57:22,630 --> 00:57:24,610
but it's not really the fact

846
00:57:24,660 --> 00:57:28,430
this the thing crosses the two place because the wind could be weakly the curve

847
00:57:28,430 --> 00:57:32,350
could be weakly and it could cost back and forth the number of times that's

848
00:57:32,350 --> 00:57:34,150
not what distinguishes

849
00:57:34,280 --> 00:57:37,070
the tangent line

850
00:57:37,120 --> 00:57:40,680
so i'm going to have to somehow grasped this in the first two it in

851
00:57:43,670 --> 00:57:45,910
and it's the following idea

852
00:57:45,990 --> 00:57:47,100
it's that

853
00:57:47,110 --> 00:57:48,920
if you take

854
00:57:49,880 --> 00:57:54,150
orange line which is called the secant line

855
00:57:54,160 --> 00:57:55,330
and you

856
00:57:55,380 --> 00:57:57,520
think of the queue

857
00:57:57,560 --> 00:58:00,780
the point q was getting closer and closer to p

858
00:58:00,820 --> 00:58:04,740
then the slope of that line will get closer and closer to the

859
00:58:04,790 --> 00:58:07,870
the slope of the red line

860
00:58:09,600 --> 00:58:12,160
if we tried close enough

861
00:58:13,150 --> 00:58:16,370
that's going to be the correct line so that's really what i did sort of

862
00:58:16,370 --> 00:58:19,230
in my brain when i drew that first line and so that's the way i'm

863
00:58:19,230 --> 00:58:24,090
going to articulated first now so the tangent line

864
00:58:27,890 --> 00:58:29,520
is equal to

865
00:58:29,530 --> 00:58:36,530
the limit of what so-called c can find

866
00:58:42,010 --> 00:58:44,070
new tends to

867
00:58:44,120 --> 00:58:47,770
and here we thinking about he is being

868
00:58:47,880 --> 00:58:51,660
and q is very

869
00:58:51,680 --> 00:58:54,910
all right so so that's the

870
00:58:54,920 --> 00:58:58,420
the g again this is still the geometric discussion

871
00:58:58,430 --> 00:58:59,330
but now

872
00:58:59,480 --> 00:59:03,950
we're going to be able to put symbols and formulas to this computation and we'll

873
00:59:03,950 --> 00:59:06,000
be able to

874
00:59:06,010 --> 00:59:07,310
to work out

875
00:59:07,360 --> 00:59:13,570
formulas in any example

876
00:59:15,830 --> 00:59:18,670
so let's do that

877
00:59:18,780 --> 00:59:20,480
so first of all

878
00:59:20,710 --> 00:59:23,010
i'm gonna write out

879
00:59:23,020 --> 00:59:25,420
these points p and q again

880
00:59:25,430 --> 00:59:28,270
so maybe we'll put he here

881
00:59:28,310 --> 00:59:30,060
q here

882
00:59:30,070 --> 00:59:34,120
and and i'm thinking of this line through them i guess it was orange so

883
00:59:34,140 --> 00:59:35,500
will leave it

884
00:59:39,680 --> 00:59:40,480
and now

885
00:59:40,540 --> 00:59:43,710
i want to compute its slope

886
00:59:43,750 --> 00:59:48,850
and so this gradually will do this in two steps and these steps will introduce

887
00:59:48,850 --> 00:59:54,510
us to the basic notations which are used throughout calculus including multivariable calculus across the

888
00:59:55,420 --> 00:59:58,720
so the first notation used is

889
00:59:58,770 --> 01:00:00,170
you imagine

890
01:00:00,220 --> 01:00:02,470
here's the x axis underneath

891
01:00:02,480 --> 01:00:05,170
here's the next year the location

892
01:00:05,210 --> 01:00:07,510
directly below the point p

893
01:00:07,520 --> 01:00:11,270
and we're traveling here the horizontal distance

894
01:00:11,320 --> 01:00:13,640
which is denoted by delta x

895
01:00:13,650 --> 01:00:16,290
so that's

896
01:00:16,310 --> 01:00:18,950
delta x the so-called

897
01:00:19,000 --> 01:00:23,150
and we could also called the change in x

898
01:00:23,230 --> 01:00:28,070
all right so that's

899
01:00:28,120 --> 01:00:31,460
one thing we want to measure in order to get the slope of this line

900
01:00:31,460 --> 01:00:34,380
p q and the other thing is the right

901
01:00:34,430 --> 01:00:41,990
so that's the distance here which we denote delta which is the change in

902
01:00:42,040 --> 01:00:43,320
and then

903
01:00:43,450 --> 01:00:45,300
the slope

904
01:00:45,440 --> 01:00:46,420
is just

905
01:00:46,430 --> 01:00:49,290
the ratio delta f delta x

906
01:00:49,300 --> 01:00:51,410
so this is the slope

907
01:00:51,520 --> 01:00:53,920
of the

908
01:00:54,080 --> 01:00:55,410
the secant

909
01:00:55,450 --> 01:01:04,360
in the process i just described over here with this limit

910
01:01:04,410 --> 01:01:08,920
applies not just to the whole line itself but also in particular to its slow

911
01:01:08,970 --> 01:01:11,460
and the way we write that in the limit

912
01:01:11,550 --> 01:01:14,050
as delta x goes to zero

913
01:01:14,060 --> 01:01:16,190
and that's going to be

914
01:01:16,200 --> 01:01:17,900
this is the slope

915
01:01:36,170 --> 01:01:37,710
this is still

916
01:01:37,730 --> 01:01:42,000
a little little general and i'm going to want to work

917
01:01:42,690 --> 01:01:44,500
more usable form

918
01:01:45,830 --> 01:01:48,520
i want to work out a better formula for this

919
01:01:48,610 --> 01:01:50,420
in in order to do that

920
01:01:50,430 --> 01:01:52,250
i'm going to write

921
01:01:52,300 --> 01:01:55,730
delta as the numerator more explicitly

922
01:01:56,410 --> 01:01:57,650
the change in

923
01:01:57,700 --> 01:01:59,610
so remember

924
01:01:59,620 --> 01:02:01,190
the point he

925
01:02:01,200 --> 01:02:03,080
is the point x zero

926
01:02:03,100 --> 01:02:06,930
after that

927
01:02:06,990 --> 01:02:11,200
all right that's what we got from formula for the point

928
01:02:12,330 --> 01:02:14,790
in order to compute these

929
01:02:14,810 --> 01:02:18,700
distances and in particular the vertical distance here and you have to get a formula

930
01:02:18,700 --> 01:02:20,820
for q as well

931
01:02:20,830 --> 01:02:23,960
so if this horizontal distance

932
01:02:24,020 --> 01:02:28,700
is delta x then this location is zero loss

933
01:02:31,170 --> 01:02:33,900
so the point above that point

934
01:02:33,920 --> 01:02:35,800
as the formula

935
01:02:35,850 --> 01:02:37,760
which is

936
01:02:37,770 --> 01:02:38,890
x zero plus

937
01:02:38,920 --> 01:02:40,750
so i don't act

938
01:02:42,380 --> 01:02:44,450
and this is the amount

939
01:02:44,540 --> 01:02:46,590
x zero

940
01:02:46,640 --> 01:02:55,020
right so there is the formula for the point q here's is the formula for

941
01:02:55,020 --> 01:02:56,310
point he

942
01:02:56,330 --> 01:02:59,310
and now i can write different

943
01:03:04,190 --> 01:03:05,710
for the derivative

944
01:03:05,720 --> 01:03:09,960
which is the following so this

945
01:03:10,010 --> 01:03:12,310
after to that zero

946
01:03:12,330 --> 01:03:14,730
which is the same as an

947
01:03:14,780 --> 01:03:17,430
is going to be the limit

948
01:03:17,480 --> 01:03:20,500
delta x goes to to zero

949
01:03:20,520 --> 01:03:23,000
of the change in as well

950
01:03:23,030 --> 01:03:26,690
the change in as is the value of f

951
01:03:26,700 --> 01:03:27,370
at the

952
01:03:27,390 --> 01:03:32,570
of point here which is zero loss don't act

953
01:03:33,420 --> 01:03:36,120
minus its value with lower point

954
01:03:36,130 --> 01:03:39,960
which is that zero

955
01:03:39,970 --> 01:03:41,070
divided by

956
01:03:41,070 --> 01:03:43,150
dimension for this is still a long way to go

957
01:03:44,300 --> 01:03:48,990
maybe i imagine other people contribute ideas and have different objective functions my work better

958
01:03:49,110 --> 01:03:50,650
than the one we have we have so far

959
01:03:51,180 --> 01:03:52,490
but i think it's the right direction

960
01:03:53,010 --> 01:03:54,320
to have robots come from

961
01:03:55,010 --> 01:04:01,380
these manufacturing violence two hour home environments and have been learnt about this particular geometry that they encountered

962
01:04:03,800 --> 01:04:08,700
now the story gets shorter and shorter and shorter three three stories i wanna touch upon here

963
01:04:09,720 --> 01:04:10,780
let's start with the first one

964
01:04:12,430 --> 01:04:13,740
here's something with a little while ago

965
01:04:14,320 --> 01:04:15,470
and we said let's look at how

966
01:04:15,930 --> 01:04:17,200
optimal control tends to work

967
01:04:17,880 --> 01:04:21,530
you try to find a control policy that we do is you specify dynamics model

968
01:04:21,700 --> 01:04:24,010
specify a cost function or reward function

969
01:04:24,900 --> 01:04:27,320
you sonoma control problem outcomes controller

970
01:04:27,720 --> 01:04:29,930
that's nice it's often a lot easier to do than

971
01:04:30,610 --> 01:04:34,240
specified here function and then a smaller specify a control policy

972
01:04:35,050 --> 01:04:37,410
still specifying both those can be difficult

973
01:04:38,150 --> 01:04:39,030
for the reward function

974
01:04:41,470 --> 01:04:44,910
if you look at the number of parameters in your function has own a lot

975
01:04:44,910 --> 01:04:46,840
less than the control policy that's nice

976
01:04:47,570 --> 01:04:49,340
and so one thing we've been looking at is

977
01:04:50,900 --> 01:04:55,010
in practice can be hard misspecified reward functions so why not learn it right but

978
01:04:55,010 --> 01:04:58,650
so you know your she cares about being in the right place for car and

979
01:04:58,650 --> 01:05:02,030
having a safe driving business we don't know how much to weight each one of

980
01:05:02,820 --> 01:05:05,700
the practical process is one of our weight in some way

981
01:05:06,180 --> 01:05:08,300
orthonormal control policies see what happened

982
01:05:08,970 --> 01:05:14,950
also or not caring about the saefong distance lightweight up you go back in and keep repeating this right

983
01:05:15,570 --> 01:05:19,900
so the question we looked at is he takes demonstration every covering the weighting between

984
01:05:19,900 --> 01:05:22,380
all the features that contribute to your reward function

985
01:05:22,990 --> 01:05:29,050
it's not a work has a long history start common back sixty four who invented control way

986
01:05:29,450 --> 01:05:32,740
and then he said well if someone gives me a control policy

987
01:05:33,280 --> 01:05:37,930
can i decide whether the game is something that's optimal with respect to some problems that are not

988
01:05:38,820 --> 01:05:39,130
and then

989
01:05:39,670 --> 01:05:44,180
stephen boyd generalized from one meter high dimensions with linear matrix inequalities

990
01:05:44,880 --> 01:05:46,950
then further down the road entering stuart russell

991
01:05:47,340 --> 01:05:49,320
can looked at it in the reinforcement learning setting

992
01:05:49,990 --> 01:05:51,150
then we started our work

993
01:05:51,610 --> 01:05:52,320
the tides

994
01:05:53,070 --> 01:05:54,880
this to address entering work was mostly

995
01:05:55,400 --> 01:06:00,260
pointing out that there's a lot of ambiguity is not a unique reward function associated with someone demonstration

996
01:06:00,800 --> 01:06:02,220
we start looking at ways of

997
01:06:03,950 --> 01:06:06,700
these putting a prior to tell you how to disambiguate

998
01:06:07,400 --> 01:06:10,110
and then there was a whole lot work different ways of putting in priors

999
01:06:10,530 --> 01:06:15,450
that is any greater weight function make a choice among the ones that are all consistent with the demonstration

1000
01:06:16,090 --> 01:06:17,760
some to some other applications

1001
01:06:19,170 --> 01:06:20,820
when the show that this one here

1002
01:06:21,610 --> 01:06:22,700
is a four legged robot

1003
01:06:24,220 --> 01:06:28,820
the problem is to get it to walk across rocky terrains system cannot climb across

1004
01:06:29,590 --> 01:06:33,820
moving the robot legs into a particular configuration was a solved problem where we worked on this

1005
01:06:34,380 --> 01:06:37,360
in the motors and with a good model for what i'll move defeat

1006
01:06:38,200 --> 01:06:42,510
the hard part was choose where to place the feet so the robot would get

1007
01:06:42,510 --> 01:06:45,680
across tipping over a sliding out of the train

1008
01:06:48,630 --> 01:06:54,530
you'd say why is it hard why we talk about reward functions slipping and sliding tumbling sounds like dynamics well

1009
01:06:55,030 --> 01:06:57,470
that's just part of practicality forty robotics

1010
01:06:58,490 --> 01:07:02,630
you have the enormous volume reward function you dynamics small of isn't good enough

1011
01:07:03,110 --> 01:07:06,700
and you might never get a good enough and we end up doing is saying

1012
01:07:06,760 --> 01:07:10,180
i'm gonna put some of my knowledge about the problem that has i don't like

1013
01:07:10,180 --> 01:07:11,010
slipping and sliding

1014
01:07:11,490 --> 01:07:16,050
i can similarly reliably quickly enough and put in the cost function and just say

1015
01:07:16,400 --> 01:07:19,070
don't get into situations where you would slip and slide

1016
01:07:19,490 --> 01:07:21,550
and now we avoid the parts and dynamics

1017
01:07:21,970 --> 01:07:23,010
they can simulate well

1018
01:07:23,450 --> 01:07:26,970
thanks for tuning a reward function but we wanted to and it will learn it

1019
01:07:27,300 --> 01:07:30,070
sort of thing that our processes that tuning it so things work

1020
01:07:31,030 --> 01:07:35,700
so here are twenty five features an example features what is the distance the centre of gravity

1021
01:07:36,180 --> 01:07:42,970
from the boundary support triangle the three feet currently on the ground ideally inside the support triangle that's when stable

1022
01:07:44,510 --> 01:07:46,780
but how much money have will affect how stable you walk

1023
01:07:48,200 --> 01:07:54,070
what is the slope and then each other defeat what's the height differential between the front feet and so forth

1024
01:07:54,780 --> 01:07:55,700
we have twenty five total

1025
01:07:56,900 --> 01:07:57,530
so we said okay

1026
01:07:57,990 --> 01:08:00,050
let's demonstrate a robot across the rocky terrain

1027
01:08:01,070 --> 01:08:04,300
let's no run are apprenticeship learning approach which learns

1028
01:08:04,300 --> 01:08:05,400
and there's a theorem that says

1029
01:08:05,800 --> 01:08:06,860
if you run this thing

1030
01:08:07,420 --> 01:08:08,190
sufficiently long

1031
01:08:09,070 --> 01:08:12,920
and then pick one i got a very sensitive ongoing

1032
01:08:13,420 --> 01:08:14,800
that getting bigger and bigger and bigger

1033
01:08:15,360 --> 01:08:16,790
many take the very last point

1034
01:08:17,690 --> 01:08:19,380
probability distribution over all right

1035
01:08:20,920 --> 01:08:24,250
from a distribution is ever closer to the distribution

1036
01:08:25,090 --> 01:08:25,800
along and this

1037
01:08:28,300 --> 01:08:29,440
so that's a beautiful property

1038
01:08:30,380 --> 01:08:34,520
it leaves you with a lot of question marks that you don't have a longer run this thing before

1039
01:08:36,170 --> 01:08:37,300
and you have to invent new

1040
01:08:38,270 --> 01:08:39,980
as well so what do

1041
01:08:40,980 --> 01:08:41,790
how long after i

1042
01:08:43,070 --> 01:08:46,110
but nevertheless theorem that says when you run it for long enough

1043
01:08:46,520 --> 01:08:47,380
you will get point

1044
01:08:49,090 --> 01:08:53,520
and indeed if you run it for longer than such points also have

1045
01:08:54,090 --> 01:08:57,650
property that they are coming be didn't come independently it

1046
01:08:58,570 --> 01:09:00,610
there doing of a random walk so

1047
01:09:00,980 --> 01:09:03,230
each point will look a bit like a little bit more

1048
01:09:04,090 --> 01:09:06,270
the probability distribution and anyone

1049
01:09:06,880 --> 01:09:08,170
if you are on the table and

1050
01:09:08,590 --> 01:09:09,570
is he

1051
01:09:10,360 --> 01:09:11,250
and that's fantastic

1052
01:09:14,650 --> 01:09:16,260
i'm not going to tell you about

1053
01:09:16,680 --> 01:09:17,360
it's something

1054
01:09:22,970 --> 01:09:24,110
and get something is

1055
01:09:28,470 --> 01:09:29,920
markov chain monte carlo methods

1056
01:09:30,510 --> 01:09:32,680
which only makes sense if we have two dimensions

1057
01:09:38,600 --> 01:09:42,430
so what i'm going to talk about now is a problem has two dimensions namely

1058
01:09:42,760 --> 01:09:48,170
the inference of the mean and the standard deviation guassian given and points from

1059
01:09:50,890 --> 01:09:52,260
so i just get something work

1060
01:09:56,860 --> 01:09:58,180
i'll be done in five minutes

1061
01:10:12,250 --> 01:10:12,970
get sampling

1062
01:10:16,260 --> 01:10:18,480
the assumption is that this is a big assumption

1063
01:10:25,710 --> 01:10:26,480
that's even though

1064
01:10:27,570 --> 01:10:29,800
he itself as the read distribution

1065
01:10:30,340 --> 01:10:31,590
we assume that we have

1066
01:10:36,350 --> 01:10:37,820
the conditional distributions

1067
01:10:44,840 --> 01:10:49,420
these dimensions conditional on all others so he is a nasty rash thing

1068
01:10:50,380 --> 01:10:51,720
some complicated

1069
01:10:54,860 --> 01:10:57,100
the assumption is that slice

1070
01:10:58,010 --> 01:10:59,250
well this think

1071
01:11:01,180 --> 01:11:02,840
x two x three x four x i

1072
01:11:04,470 --> 01:11:11,220
just like you know as one we can actually draw from this conditional distribution so we can draw from

1073
01:11:11,690 --> 01:11:13,210
you want connect to

1074
01:11:15,230 --> 01:11:15,960
yeah that's to

1075
01:11:16,390 --> 01:11:20,100
given x one indicates that problem there are two variables

1076
01:11:22,390 --> 01:11:24,310
an algorithm works like this stuff somewhere

1077
01:11:29,590 --> 01:11:30,260
the rule

1078
01:11:31,350 --> 01:11:35,060
x one new values from the conditional distribution x one

1079
01:11:35,790 --> 01:11:36,710
given to

1080
01:11:37,210 --> 01:11:38,140
at is she

1081
01:11:41,430 --> 01:11:42,890
juice plus one value

1082
01:11:43,630 --> 01:11:44,650
the conditional distribution

1083
01:11:45,260 --> 01:11:45,680
next to

1084
01:11:46,970 --> 01:11:49,100
seven x one is you about

1085
01:11:49,310 --> 01:11:49,680
so you

1086
01:11:50,030 --> 01:11:50,890
the new value

1087
01:11:52,800 --> 01:11:55,980
and then you feature which means that you to show

1088
01:11:58,850 --> 01:12:02,310
that's how you do it with just two variables you've got pay variables

1089
01:12:02,890 --> 01:12:07,310
just cycle through growing each of them from its distribution conditional on all the others

1090
01:12:09,180 --> 01:12:12,680
so this is a big assumption that you can do this but there are examples

1091
01:12:12,760 --> 01:12:16,970
where this is quite easy and the gas in distribution is a good example of

1092
01:12:18,600 --> 01:12:22,030
if i give you some data is five points and i said what the posterior

1093
01:12:22,030 --> 01:12:28,560
probability u and sigma chi you know the answer but you can enumerate hypotheses of

1094
01:12:30,070 --> 01:12:31,260
it's dependent

1095
01:12:33,570 --> 01:12:34,550
distribution where

1096
01:12:39,810 --> 01:12:41,770
she calls out concentric

1097
01:12:42,640 --> 01:12:43,390
that's what it looks like

1098
01:12:43,940 --> 01:12:46,920
but he slice through this is fantastic news and simple

1099
01:12:47,470 --> 01:12:50,510
so even though this is a slightly complicated and distribution

1100
01:12:50,930 --> 01:12:52,180
slice in this direction

1101
01:12:53,010 --> 01:12:53,680
many of these

1102
01:12:54,440 --> 01:12:54,960
a guassian

1103
01:12:55,810 --> 01:12:58,400
the slice in this direction is just a gamma distribution

1104
01:12:59,090 --> 01:12:59,890
so these are gammas

1105
01:13:01,100 --> 01:13:02,070
and these an almost

1106
01:13:03,390 --> 01:13:08,100
and the properties of the general gamma distributions you can do it says

1107
01:13:08,680 --> 01:13:13,060
just alternately to one another one and the size of the earth

1108
01:13:15,420 --> 01:13:19,050
in red i'm showing you what the true distribution actually looks like that's what the

1109
01:13:19,050 --> 01:13:23,510
blindfold man can't see all the blindfold man can do it so i'm starting somewhere

1110
01:13:23,510 --> 01:13:24,140
where there

1111
01:13:25,260 --> 01:13:26,130
so let's start

1112
01:13:26,730 --> 01:13:31,260
showing you in data space hypothesis looks like as well so this is

1113
01:13:31,800 --> 01:13:33,970
a very broad question women

1114
01:13:35,790 --> 01:13:40,180
and then you tell allocates update new by drawing from the gas in distribution

1115
01:13:43,880 --> 01:13:45,810
leaving significance that is about

1116
01:13:46,320 --> 01:13:50,810
you can roll back at it's a simple thing that you can work out in five points

1117
01:13:51,310 --> 01:13:53,100
what's the conditional distribution me

1118
01:13:53,890 --> 01:13:55,390
given sigma so he doesn't at

1119
01:13:56,220 --> 01:13:57,130
that's first set

1120
01:13:57,850 --> 01:13:59,350
and then the second step

1121
01:14:00,260 --> 01:14:01,170
is there

1122
01:14:02,090 --> 01:14:04,730
sigma y

1123
01:14:04,800 --> 01:14:06,010
draws from a gamma distribution

1124
01:14:06,600 --> 01:14:11,560
and that changes the the standard deviation from the broad value we just randomly initialize to

1125
01:14:12,110 --> 01:14:12,760
the new value

1126
01:14:13,440 --> 01:14:14,140
then we have me

1127
01:14:16,060 --> 01:14:17,850
and we have the means and sigma

1128
01:14:18,670 --> 01:14:21,810
and there is a dependence between these points is one of them is

1129
01:14:22,270 --> 01:14:24,260
created a weighted depends on the previous one

1130
01:14:25,150 --> 01:14:29,060
and every other step and highlighting in yellow when we go through

1131
01:14:29,810 --> 01:14:30,690
to be more

1132
01:14:31,610 --> 01:14:32,510
you understand

1133
01:14:33,750 --> 01:14:34,220
he was going

1134
01:14:34,350 --> 01:14:35,940
well i think of link

1135
01:14:38,380 --> 01:14:42,630
another running and you can see in yellow what was happening is that

1136
01:14:43,070 --> 01:14:45,130
on the right is a summary of what happened

1137
01:14:45,130 --> 01:14:52,620
is that if you look and can predict some uncertainty  some predicted error ellipse that

1138
01:14:52,620 --> 01:14:58,960
the area of this eg error ellipse in the W zero W A plane

1139
01:14:58,970 --> 01:15:06,640
where the central value here is the cosmological constant W zero equal to o z minus one

1140
01:15:06,660 --> 01:15:15,620
W A equal to zero  how well will observations or projects do in de determining displacements

1141
01:15:15,620 --> 01:15:24,180
from this and  we modeled various things and what I think emerged is

1142
01:15:24,180 --> 01:15:31,100
that the key to this is going to be systematics so you can say how well you can

1143
01:15:31,100 --> 01:15:37,960
ask how well we all LSST do in determining the dark energy parameters well

1144
01:15:37,960 --> 01:15:44,380
if you ignore s systematic errors this would be the error that you would get if

1145
01:15:44,380 --> 01:15:52,400
you take very optimistic sys  projections of systematic errors the very optimistic was defined as

1146
01:15:52,400 --> 01:15:56,640
taking what the project claims that we will be able to do and multiply it  by three

1147
01:15:56,640 --> 01:16:01,400
or four you would get a number like this an error ellipse like

1148
01:16:01,400 --> 01:16:05,800
this and if you would be pessimistic if you would ask the people who

1149
01:16:05,810 --> 01:16:10,400
would do other projects how badly is this are this group gonna be they would tell

1150
01:16:10,400 --> 01:16:16,400
you they would do this fairly badly systematics are  going to be the key for

1151
01:16:16,400 --> 01:16:21,960
the first ten years of this game we were limited by statistical errors we're past

1152
01:16:21,970 --> 01:16:32,580
that the important thing now is systematic errors another thing that we discovered is

1153
01:16:32,600 --> 01:16:41,200
how important it is to combine techniques so this is an example of one looking

1154
01:16:41,200 --> 01:16:46,870
at two  different techniques done individually so if you do this technique this might be

1155
01:16:46,870 --> 01:16:52,340
the projected error ellipse and if you another technique this might be the projected

1156
01:16:52,340 --> 01:16:59,820
error ellipse actually one of these are weak lensing the other is a supernova projection

1157
01:16:59,820 --> 01:17:06,720
so  you can say how well will you do in the Fisher matrix estimation technique by doing

1158
01:17:06,720 --> 01:17:11,520
combining the data of both of them and if you combine the data of both of them

1159
01:17:11,520 --> 01:17:18,460
that's how well you do now this might look suspicious and you wouldn't believe this

1160
01:17:18,470 --> 01:17:23,840
if this these were the only two parameters right so if you combine techniques how

1161
01:17:23,840 --> 01:17:30,540
can you do so much better than the intersection of those two and the answer is

1162
01:17:30,540 --> 01:17:39,160
there's really eight  dimensions in  this in these cosmological parameters space and in the Fisher matrix you can

1163
01:17:39,160 --> 01:17:48,340
have contours of of uncertainty in this eight-dimensional space and you project it down on two

1164
01:17:48,340 --> 01:17:56,620
dimensions and the different techniques are sensitive in different ways to uncertainties in other

1165
01:17:56,620 --> 01:18:04,600
cosmological parameters so with this what this represents is a projection of the intersection of

1166
01:18:04,600 --> 01:18:12,080
these eight-dimensional error contours onto the two-dimensional plane so you want to look at

1167
01:18:12,080 --> 01:18:21,260
the projection of the intersection not of the intersection and combining techniques are very powerful

1168
01:18:21,260 --> 01:18:27,720
so my personal feeling is there's not going to be any one single measurement or

1169
01:18:27,720 --> 01:18:32,640
any one single technique that's going to run the table on this but rather we'll

1170
01:18:32,640 --> 01:18:43,040
get the best information by combinings the results from different techniques and different approaches

1171
01:18:43,040 --> 01:18:57,060
they are proposed  so in this procedure we call it the White Papers

1172
01:18:57,060 --> 01:19:06,640
are various proposals and so these are actual proposals that do exist let me go up a

1173
01:19:06,640 --> 01:19:13,160
couple let me skip ahead and skip back so what's ahead in this game of dark

1174
01:19:13,160 --> 01:19:21,920
energy well again different techniques are being pursued weak lensing baryon acoustic oscillations supernova clusters

1175
01:19:21,920 --> 01:19:28,940
and CMB gives us a lot of information and in lensing there are many different

1176
01:19:28,940 --> 01:19:34,960
projects that are propose baryon acoustic oscillations some of them are being done many more

1177
01:19:34,960 --> 01:19:43,090
are proposed supernovae the same and studying clusters of galaxies now includes space missions so

1178
01:19:43,100 --> 01:19:48,570
na we  all of these won't be done but if you add-up the price tag

1179
01:19:48,580 --> 01:19:54,920
it's a couple of billion dollar  enterprise so I know a billion dollars is not

1180
01:19:54,920 --> 01:19:59,380
much in euros right but you know t  in the U.S. it's considered a lot

1181
01:19:59,380 --> 01:20:06,180
of money so it's a big expensive problem and of course at CERN people don't

1182
01:20:06,180 --> 01:20:10,640
red black trees are interesting the kinds of changes they make are very simple

1183
01:20:10,680 --> 01:20:13,410
and they also don't think very many of them

1184
01:20:13,430 --> 01:20:18,080
the competitions here's the rotation

1185
01:20:30,580 --> 01:20:37,240
this is the a way engineering drawing generic part of the tree we have seen

1186
01:20:37,240 --> 01:20:40,910
as a and b there is some subtrees hanging on the tree draws triangles we

1187
01:20:40,910 --> 01:20:42,430
don't know how big they are

1188
01:20:42,450 --> 01:20:46,740
we know the better all have the same black kite for to reflect tree

1189
01:20:46,740 --> 01:20:49,800
but in general just looks like this there some parents some rest of the tree

1190
01:20:50,000 --> 01:20:51,600
here which we don't draw

1191
01:20:51,640 --> 01:20:54,660
give the subtrees names greek names

1192
01:20:54,680 --> 01:20:57,640
beta gamma

1193
01:20:57,660 --> 01:21:01,910
and all define the operation right rotate

1194
01:21:06,910 --> 01:21:08,560
in general by i having b

1195
01:21:08,580 --> 01:21:12,400
i looked at it and i want to derive rotation look at its left child

1196
01:21:12,400 --> 01:21:15,850
and rather picture of the subtrees of the students

1197
01:21:16,930 --> 01:21:17,970
i created

1198
01:21:17,970 --> 01:21:22,260
this tree

1199
01:21:31,780 --> 01:21:33,040
so i've done this

1200
01:21:33,060 --> 01:21:35,120
turn this edge

1201
01:21:35,140 --> 01:21:36,740
ninety degrees

1202
01:21:36,760 --> 01:21:39,010
the pair was the parent to be

1203
01:21:39,010 --> 01:21:40,390
it is now part of a

1204
01:21:40,410 --> 01:21:42,240
a is now the new parent to be

1205
01:21:42,260 --> 01:21:47,600
the subtrees rearranged before the repositories of these two are subtrees of a and gamma

1206
01:21:47,600 --> 01:21:52,970
was said to be the most also three BNL still a data

1207
01:21:52,970 --> 01:21:54,870
switch to

1208
01:21:54,930 --> 01:21:57,010
being subtree of b

1209
01:21:57,260 --> 01:22:02,450
the main thing we want to check here is that this operation preserves

1210
01:22:02,510 --> 01:22:04,390
the binary search tree property

1211
01:22:05,910 --> 01:22:09,430
number the binary search tree property says that

1212
01:22:09,450 --> 01:22:14,120
all the elements in the left subtree of nodes are less than or equal to

1213
01:22:14,140 --> 01:22:17,780
the node and all the elements in the right subtree are greater than or equal

1214
01:22:19,810 --> 01:22:22,220
so in particular if we take

1215
01:22:22,220 --> 01:22:24,870
some node will a and alpha

1216
01:22:24,870 --> 01:22:27,450
we'll be in data

1217
01:22:27,470 --> 01:22:29,990
little c in gamma

1218
01:22:31,430 --> 01:22:36,450
a is lesson equal to capital a string literal little b

1219
01:22:36,510 --> 01:22:38,160
system equal to

1220
01:22:38,180 --> 01:22:40,660
capital b

1221
01:22:40,680 --> 01:22:41,870
let's recall two

1222
01:22:41,890 --> 01:22:43,720
a little seed

1223
01:22:43,740 --> 01:22:47,180
OK and that this is the condition both on the left side and on the

1224
01:22:47,180 --> 01:22:48,310
right side

1225
01:22:48,310 --> 01:22:53,470
because out is less is left everything is in between a and b

1226
01:22:53,580 --> 01:22:55,910
and gamma is after b

1227
01:22:55,930 --> 01:22:59,160
the same thing is true over here is still suppose we all the nodes that

1228
01:22:59,160 --> 01:23:00,080
come between

1229
01:23:00,120 --> 01:23:02,350
capital and capital b

1230
01:23:02,370 --> 01:23:05,030
so this is this is good

1231
01:23:05,080 --> 01:23:08,200
we can definitely do this operation still have a binary search tree and we're going

1232
01:23:08,200 --> 01:23:12,490
to use rotations particularly careful way to make sure that we maintain all of these

1233
01:23:13,540 --> 01:23:15,330
that's the hard part

1234
01:23:15,370 --> 01:23:18,830
rotations will be asking this is the right rotate operations

1235
01:23:18,850 --> 01:23:21,240
the reverse operation is left

1236
01:23:21,260 --> 01:23:26,870
this is left rotate of a

1237
01:23:26,890 --> 01:23:31,890
in general of the tunas that are involved we list the top one

1238
01:23:31,990 --> 01:23:35,810
right rotator b will give you this left able to do this

1239
01:23:35,830 --> 01:23:39,580
these are reversible operations

1240
01:23:39,600 --> 01:23:44,740
feels good the other thing is that they only take constant time

1241
01:23:44,760 --> 01:23:47,470
these are constant time

1242
01:23:47,490 --> 01:23:48,780
o operations

1243
01:23:48,780 --> 01:23:53,930
because we're only changing constant number of pointers as long as you know the b

1244
01:23:53,930 --> 01:23:55,390
that you're interested in

1245
01:23:55,410 --> 01:23:57,600
you said the the last owner b

1246
01:23:57,600 --> 01:23:58,430
so that's a sparse

1247
01:23:59,040 --> 01:23:59,940
that's a matrix

1248
01:24:01,000 --> 01:24:03,460
and you wanna treat sparsely one exploit the sparsity

1249
01:24:03,930 --> 01:24:06,910
and exporting sparsity and at the same time

1250
01:24:07,410 --> 01:24:09,330
benefiting from parallelism

1251
01:24:10,210 --> 01:24:11,410
approach computing is

1252
01:24:12,390 --> 01:24:13,390
this is difficult

1253
01:24:16,060 --> 01:24:17,500
okay let's go back to the

1254
01:24:18,000 --> 01:24:18,890
slides here

1255
01:24:22,350 --> 01:24:23,690
just had a print out here of

1256
01:24:24,830 --> 01:24:27,370
my code for solving a problem

1257
01:24:28,160 --> 01:24:30,100
this particular problem had

1258
01:24:30,730 --> 01:24:32,710
well one thousand three hundred fifty

1259
01:24:34,560 --> 01:24:35,830
one hundred forty six

1260
01:24:39,270 --> 01:24:39,910
my code is

1261
01:24:40,500 --> 01:24:42,270
general and then allows for

1262
01:24:43,020 --> 01:24:48,390
very very must be nonnegative or they can be not nonnegative that can be what we call free

1263
01:24:49,100 --> 01:24:49,480
they can be

1264
01:24:50,730 --> 01:24:51,600
positive or negative

1265
01:24:52,120 --> 01:24:54,940
or they can be bounded which means they have an upper and the lower bound

1266
01:24:56,060 --> 01:24:57,520
so these are treated a bit differently

1267
01:24:58,040 --> 01:24:58,660
and so on

1268
01:24:59,080 --> 01:25:00,140
my code reports

1269
01:25:01,140 --> 01:25:01,910
the breakout here

1270
01:25:02,480 --> 01:25:08,310
and also the constraints can be equalities inequalities or what we call range which means

1271
01:25:09,020 --> 01:25:10,940
it's a two-sided inequality

1272
01:25:11,660 --> 01:25:14,270
constraint can be above this and below or below there

1273
01:25:16,460 --> 01:25:17,210
so this problem had

1274
01:25:18,080 --> 01:25:22,830
i forty six equality constraints and thirteen hundred and fifty nine negative variables

1275
01:25:23,540 --> 01:25:25,330
but look at the aim matrix here

1276
01:25:26,020 --> 01:25:29,120
this has only five thousand two hundred eighty eight

1277
01:25:29,640 --> 01:25:31,060
nonzero entries

1278
01:25:31,660 --> 01:25:37,270
so that's per column is about four r this has four nonzeros per column so

1279
01:25:37,690 --> 01:25:39,390
they matrix is extraordinarily

1280
01:25:46,770 --> 01:25:47,170
and so

1281
01:25:47,850 --> 01:25:52,930
the code exploits that's and it took this particular problem took fifty iterations self

1282
01:25:54,390 --> 01:25:55,190
and iteration

1283
01:25:55,560 --> 01:25:57,500
ah i print out

1284
01:25:58,080 --> 01:26:01,000
these primal objective value as you currently have it

1285
01:26:01,500 --> 01:26:02,730
the dual objective value

1286
01:26:03,230 --> 01:26:05,270
as as you have it the primal

1287
01:26:05,750 --> 01:26:08,930
infeasibility so it's basically the norm of the vector rho

1288
01:26:09,810 --> 01:26:12,980
the dual invisibility is the norm of the vector sigma

1289
01:26:15,940 --> 01:26:20,540
end as we know it to be primal feasible system rustic it's small

1290
01:26:21,060 --> 01:26:24,020
we have to be dual feasible so this abrupt has to get small

1291
01:26:24,580 --> 01:26:29,230
and that's the bizarro duality gap so the difference between these two numbers has to be small

1292
01:26:29,810 --> 01:26:33,270
and these lists that are the statistics i summarise over here

1293
01:26:34,330 --> 01:26:36,250
so idea means dual feasible

1294
01:26:36,690 --> 01:26:41,000
so once this this number gets below ten to the minus i think i have

1295
01:26:41,000 --> 01:26:42,600
to i six is my default

1296
01:26:43,430 --> 01:26:44,060
i clearer

1297
01:26:44,410 --> 01:26:47,190
the problem dual feasible tomorrow the primal feasibility

1298
01:26:49,480 --> 01:26:50,910
here are not primal feasible

1299
01:26:51,390 --> 01:26:53,710
well i guess i used to analyze five is my threshold

1300
01:26:54,230 --> 01:26:54,810
here i am

1301
01:26:55,440 --> 01:26:56,190
primal feasible

1302
01:26:58,460 --> 01:27:01,710
so is not but you can see it is much better than just bad

1303
01:27:02,810 --> 01:27:08,270
it got way down the determinize twelve internalize fifteen which is almost machine percent precision

1304
01:27:09,710 --> 01:27:11,850
any objective function values have to get

1305
01:27:12,600 --> 01:27:17,500
i s the stopping this column here reflects the objective function values how many significant

1306
01:27:17,500 --> 01:27:19,810
figures in agreement there are are between them

1307
01:27:21,850 --> 01:27:22,230
and so

1308
01:27:22,830 --> 01:27:27,270
and my and my rule is if there are eight digits of agreement between the objective functions

1309
01:27:27,940 --> 01:27:32,640
so you know it just agreement in your primal feasible dual feasible you stop and declare yourself optimal

1310
01:27:33,230 --> 01:27:39,620
and you can see that this is behaving as i indicated before r that the infeasibility has dropped dramatically

1311
01:27:42,190 --> 01:27:43,160
at fast

1312
01:27:43,690 --> 01:27:44,600
right from the beginning

1313
01:27:45,440 --> 01:27:48,940
although i am i still plagiarism ninety nine percent otherwise so

1314
01:27:49,480 --> 01:27:50,540
go can go quickly

1315
01:27:52,770 --> 01:27:53,040
but they

1316
01:27:53,540 --> 01:27:58,020
but the objective function values you know they they sort of don't do much

1317
01:27:58,500 --> 01:28:00,790
until you get starting close the feasibility

1318
01:28:01,290 --> 01:28:01,660
and so

1319
01:28:02,190 --> 01:28:07,210
you know around here were forced both the primal and the dual turning cluster feasible that's when the

1320
01:28:09,120 --> 01:28:13,940
the objective function complementarity starts to kick in and we started a closed valley and

1321
01:28:14,140 --> 01:28:18,330
so these numbers come these things always starting later than getting feasible

1322
01:28:19,290 --> 01:28:21,660
and that's what the theorem was telling us before

1323
01:28:26,980 --> 01:28:29,290
see i started a few minutes late which we take a break

1324
01:28:30,620 --> 01:28:34,290
well actually maybe them if i feel there's a few more slides and take a break

1325
01:28:39,390 --> 01:28:40,460
so this whole idea

1326
01:28:40,930 --> 01:28:46,160
so i'm gonna generalize to nonlinear programming a few minutes and so give give you the framework it's

1327
01:28:46,580 --> 01:28:48,830
natural for the purposes and generalization

1328
01:28:50,960 --> 01:28:52,100
if we start with a

1329
01:28:52,230 --> 01:28:54,230
optimization problem and are favorite form

1330
01:28:54,870 --> 01:28:55,460
like this

1331
01:28:56,270 --> 01:28:57,080
standard form

1332
01:28:57,750 --> 01:29:00,290
end introduce slack variables like this

1333
01:29:00,710 --> 01:29:05,580
and these are the same steps we're gonna do nonlinear just a few minutes we start with a problem

1334
01:29:06,190 --> 01:29:08,250
we're gonna introduce slack variables

1335
01:29:09,640 --> 01:29:13,020
and here's the next step that's you haven't seen

1336
01:29:14,330 --> 01:29:15,710
but we're gonna get to the same place

1337
01:29:15,710 --> 01:29:20,540
microscopic description of these energy so OK that first president

1338
01:29:20,540 --> 01:29:24,570
second here you don't have any time evolution don't describe

1339
01:29:24,620 --> 01:29:30,380
the time evolution from the beginning of the from the fissioning system fragments

1340
01:29:30,410 --> 01:29:31,540
and then here

1341
01:29:31,540 --> 01:29:34,880
you don't have all these possible fragmentation was two

1342
01:29:34,890 --> 01:29:37,220
from the same as big one school

1343
01:29:37,330 --> 01:29:40,950
the small one so we have to this graph we should describe

1344
01:29:40,970 --> 01:29:44,140
all these different fragmentations

1345
01:29:44,160 --> 01:29:45,770
so that

1346
01:29:45,800 --> 01:29:50,560
to summarise what was missing this schematic view

1347
01:29:51,280 --> 01:29:52,290
that's why

1348
01:29:52,310 --> 01:29:58,030
we have done an approach with these assumptions the first suppose

1349
01:29:58,050 --> 01:29:59,070
that's the

1350
01:29:59,080 --> 01:30:05,080
fission dynamics is governed them by not only elongation but also asymmetry so we have

1351
01:30:05,090 --> 01:30:06,910
two degrees of freedom

1352
01:30:07,470 --> 01:30:09,470
and then all the shapes are

1353
01:30:09,520 --> 01:30:12,780
law allowed so this one is important

1354
01:30:12,790 --> 01:30:14,620
so these ranks priority

1355
01:30:14,640 --> 01:30:19,420
but it is important then to have a small one into one

1356
01:30:19,440 --> 01:30:21,890
we also have to make an assumption

1357
01:30:21,930 --> 01:30:28,370
which is in fact very valid and strong following education

1358
01:30:28,370 --> 01:30:30,810
with support there are two different speeds

1359
01:30:30,830 --> 01:30:37,160
one speed is for internal structures of that the motion of the nucleons inside places

1360
01:30:37,160 --> 01:30:39,140
so it's something very rapidly

1361
01:30:39,200 --> 01:30:44,270
and you have this small motion which is efficient so long

1362
01:30:44,460 --> 01:30:49,040
time which is bigger than ten minutes nineteen seconds so it's slow motion so you

1363
01:30:49,040 --> 01:30:50,340
can separate

1364
01:30:50,390 --> 01:30:55,380
the two motions so that's why do you suppose that at each stage of the

1365
01:30:55,380 --> 01:30:56,770
collective motion

1366
01:30:56,810 --> 01:31:01,360
you have internet structure which is at equilibrium

1367
01:31:01,380 --> 01:31:04,120
so that's why we can have here

1368
01:31:04,130 --> 01:31:06,490
these basis states so

1369
01:31:06,510 --> 01:31:11,340
so is that for different information which do not depend on time

1370
01:31:11,360 --> 01:31:16,280
and then so this is the lowest energy state for each deformation and then you

1371
01:31:16,280 --> 01:31:18,370
have this time evolution here

1372
01:31:18,390 --> 01:31:24,370
and you can constrict this wavefunction depending on time as a linear combination of these

1373
01:31:24,390 --> 01:31:26,310
static state with different

1374
01:31:26,330 --> 01:31:28,310
weight functions

1375
01:31:28,330 --> 01:31:31,050
when you do that you really have fission dynamics

1376
01:31:31,060 --> 01:31:34,450
which is the time evolution in a collective space

1377
01:31:34,480 --> 01:31:39,150
so how do we solve that it's a two steps formalism so first

1378
01:31:39,960 --> 01:31:42,060
we have first determine that

1379
01:31:42,070 --> 01:31:44,430
and second to determine that one

1380
01:31:44,450 --> 01:31:46,710
so to to determine these things

1381
01:31:46,720 --> 01:31:49,360
so we have in fact we use these

1382
01:31:49,370 --> 01:31:53,540
i admit that presented yesterday two days ago

1383
01:31:53,620 --> 01:31:58,530
which is the constrained hartree fock method so we impose some constraint i look at

1384
01:31:58,530 --> 01:31:59,680
the energy

1385
01:31:59,700 --> 01:32:04,160
of the system when you impose information for example

1386
01:32:04,950 --> 01:32:07,820
just recall you that it's based

1387
01:32:07,830 --> 01:32:12,660
i mean the musician principle his lack large parameter approach so you

1388
01:32:12,680 --> 01:32:15,190
and we impose to minimize these

1389
01:32:15,200 --> 01:32:17,560
so i mean name is constrained

1390
01:32:17,560 --> 01:32:21,710
we some imposed constraints here

1391
01:32:22,850 --> 01:32:24,260
the second part

1392
01:32:24,590 --> 01:32:28,910
we have to determine this time dependent weight function so

1393
01:32:28,930 --> 01:32:33,710
i just put the equations some equations just to explain what it is so it's

1394
01:32:33,710 --> 01:32:39,160
also based on the minimization principle what is important is that the same and so

1395
01:32:39,160 --> 01:32:40,580
we don't put any

1396
01:32:40,590 --> 01:32:43,240
all the interactions are all that

1397
01:32:43,270 --> 01:32:44,530
parameters of

1398
01:32:44,550 --> 01:32:49,650
it's the same and that we use that's very important for the query runs

1399
01:32:49,680 --> 01:32:51,110
of the method

1400
01:32:51,110 --> 01:32:56,470
and then at the end we have to collective shouting that equation depending on the

1401
01:32:58,120 --> 01:33:03,890
to explain the collective soul here it's like it's the kinetic terms like the square

1402
01:33:03,890 --> 01:33:08,580
of the two and so this is the inverse of these cases the potential

1403
01:33:08,590 --> 01:33:12,010
this is a o point energy corrections because we have

1404
01:33:12,020 --> 01:33:14,850
to deal with the quantum mechanical approach

1405
01:33:15,620 --> 01:33:18,800
that means that this is the potential so imagine

1406
01:33:18,810 --> 01:33:21,340
q is no translation

1407
01:33:22,440 --> 01:33:26,330
for example you have to go from one point to the other point so what

1408
01:33:26,330 --> 01:33:30,280
you don't like to climb more plain so you don't like with the potential energy

1409
01:33:30,280 --> 01:33:31,580
is very high

1410
01:33:31,580 --> 01:33:33,990
but you have also uses the term here

1411
01:33:34,020 --> 01:33:35,860
in this area you

1412
01:33:35,880 --> 01:33:42,350
when you're dealing with concentration this initiative this b here is the mass your mass

1413
01:33:42,370 --> 01:33:44,590
so it means that when you have to carry

1414
01:33:44,600 --> 01:33:48,640
a suitcase are bag you don't like to go there so there is a compromise

1415
01:33:48,640 --> 01:33:49,930
between great

1416
01:33:49,990 --> 01:33:52,960
when you want to go from one point to the other one

1417
01:33:52,970 --> 01:33:56,520
there is a compromise between the lengths of the way

1418
01:33:56,530 --> 01:33:59,490
the height the mountain that you have to climb

1419
01:33:59,520 --> 01:34:05,890
and the average that you're carrying so sometimes prefer condiment without any bags rather than

1420
01:34:05,890 --> 01:34:08,230
going through something flat

1421
01:34:08,260 --> 01:34:10,030
but something very heavy

1422
01:34:10,040 --> 01:34:12,810
so that's our really dynamical effects

1423
01:34:13,690 --> 01:34:16,110
so the balance between energy

1424
01:34:16,120 --> 01:34:18,210
and inertia

1425
01:34:18,230 --> 01:34:19,200
so that it

1426
01:34:19,210 --> 01:34:23,630
what this equation means in fact

1427
01:34:24,550 --> 01:34:28,150
now i'm going to present some results so

1428
01:34:28,180 --> 01:34:32,710
i will present some typical results of properties of the fissioning systems

1429
01:34:32,840 --> 01:34:36,870
we can see some potential energy landscape of the fissioning system

1430
01:34:37,070 --> 01:34:42,260
properties of the fragments and then i will present some dynamical reasons

1431
01:34:43,510 --> 01:34:50,410
i remind you aesthetic reason so i presented yesterday you have imposed information whether poland

1432
01:34:50,440 --> 01:34:55,760
and these so it on gation asymmetry for uranium two hundred thirty eight

1433
01:34:55,770 --> 01:35:00,390
and you find these problems state second there is

1434
01:35:00,420 --> 01:35:05,210
i minimum and then to valley the valley which leads to fragments with the same

1435
01:35:06,210 --> 01:35:08,680
and here this one which leads to

1436
01:35:08,720 --> 01:35:11,520
a big mass and the smallest mass so

1437
01:35:13,130 --> 01:35:20,440
static properties of the fissioning system so from the ground state to scission configurations

1438
01:35:20,470 --> 01:35:24,970
if you do that for many many actinides we find that all the topologies of

1439
01:35:24,980 --> 01:35:27,740
the surfaces are different so for example

1440
01:35:27,740 --> 01:35:30,920
of the system and the users but

1441
01:35:31,350 --> 01:35:33,280
even a simple sale

1442
01:35:33,300 --> 01:35:35,560
biologists do not know

1443
01:35:35,570 --> 01:35:40,480
the overall structures the underlying mechanisms of

1444
01:35:40,490 --> 01:35:42,680
of signaling of control

1445
01:35:42,700 --> 01:35:49,990
the current directly measured the these particular things

1446
01:35:50,000 --> 01:35:55,590
and so consequently there is a huge amount of uncertainty and the coming to two

1447
01:35:55,710 --> 01:36:00,860
to people with machine learning backgrounds in machine learning skills and ask them to help

1448
01:36:01,240 --> 01:36:07,390
understand this data to mine this data to help them design subsequent experiments that would

1449
01:36:07,390 --> 01:36:12,450
be maximally informative about the systems that they are trying to study

1450
01:36:12,450 --> 01:36:16,190
and so this whole area is motivating

1451
01:36:16,210 --> 01:36:23,910
an awful lot of development in machine learning methodology which is really great what's motivating

1452
01:36:23,920 --> 01:36:27,080
the development of methodologies not just its own

1453
01:36:27,140 --> 01:36:33,780
could what has previously existed in many ways it's it's developing novel methodology and and

1454
01:36:33,780 --> 01:36:38,470
pushing for home use tons of methodological research so

1455
01:36:38,900 --> 01:36:42,440
there are lots of opportunities for people there

1456
01:36:42,490 --> 01:36:48,230
on the other hand on very much more classical machine learning problems are i also

1457
01:36:48,230 --> 01:36:50,140
work with a number of companies

1458
01:36:50,160 --> 01:36:59,190
microsoft national cash register's appear they are interested in very classical pattern recognition problems someone

1459
01:36:59,190 --> 01:37:02,850
is in front of an ATM and you put some money into it

1460
01:37:02,960 --> 01:37:08,290
is the person standing in front of eighty in who say is the money that

1461
01:37:08,290 --> 01:37:12,110
has been deposited into the aegean is actually genuine

1462
01:37:12,140 --> 01:37:14,940
or is counterfeit and these are very

1463
01:37:14,960 --> 01:37:17,160
as i said classical problems which

1464
01:37:17,170 --> 01:37:21,650
some of the machine learning background could come with toolbox and say OK this is

1465
01:37:21,650 --> 01:37:26,650
are a regression problem this is a classification problem let's look at it from that

1466
01:37:26,660 --> 01:37:34,380
perspective but again these are very non-standard problems that do not

1467
01:37:34,400 --> 01:37:40,580
i can do this the the the don't you

1468
01:37:40,600 --> 01:37:46,640
two saying can i could use a neural network to thank to the neural network

1469
01:37:46,650 --> 01:37:52,490
to identify counterfeit currency well enough to fight we can't because in many cases we

1470
01:37:52,490 --> 01:37:54,850
don't know what the counterfeit currency is like

1471
01:37:54,900 --> 01:38:01,020
you've never seen counterfeit currency counterfeit currency depends on the counterfeiters how good they are

1472
01:38:01,030 --> 01:38:06,820
what type of inks whatever equipment use in so far as to subsequently again novel

1473
01:38:06,820 --> 01:38:12,020
methods have to be developed to track these sorts of problems but i should also

1474
01:38:12,020 --> 01:38:14,480
point out and this is the

1475
01:38:14,610 --> 01:38:19,970
something that is been mentioned sometimes you don't need to bother with developing novel methodology

1476
01:38:20,000 --> 01:38:22,890
sometimes you just have to be aware that there are

1477
01:38:22,890 --> 01:38:26,400
methods that can be used for example

1478
01:38:26,420 --> 01:38:28,530
on the problem of

1479
01:38:28,560 --> 01:38:36,430
identifying counterfeit currency and i had doc you want to this problem for six months

1480
01:38:38,160 --> 01:38:42,900
the solution that he came up with was one which the company patented

1481
01:38:42,940 --> 01:38:51,220
the higher the post-doc to develop the technology and now every ten years

1482
01:38:51,240 --> 01:38:58,920
images of euro banknotes will be scanned it will be processed by the method developed

1483
01:38:59,410 --> 01:39:01,010
and that was the methods

1484
01:39:01,060 --> 01:39:04,490
well and he actually just used

1485
01:39:06,900 --> 01:39:08,130
OK statistics

1486
01:39:08,150 --> 01:39:13,690
just use a level of significance so there wasn't so in terms of getting people

1487
01:39:13,690 --> 01:39:19,740
published in that's the people would never be published in that's five because you're using

1488
01:39:19,740 --> 01:39:22,490
something that was developed in the thirties

1489
01:39:22,500 --> 01:39:29,240
something like that it was the solution to the problem was required so we might

1490
01:39:29,240 --> 01:39:34,680
perhaps one to engineer more elegant type of these variational

1491
01:39:34,690 --> 01:39:35,730
one of

1492
01:39:35,810 --> 01:39:37,790
at the end of the day all we need to do

1493
01:39:37,830 --> 01:39:39,750
was defined outlets

1494
01:39:39,770 --> 01:39:46,310
and there are standard tables which devastated the levels of significance of that particular taste

1495
01:39:46,310 --> 01:39:52,460
testing so the problem provided a robust solution to the problem so

1496
01:39:52,550 --> 01:39:57,430
i guess the message here is that on one hand you know you have to

1497
01:39:57,430 --> 01:40:04,350
be invented and in terms of the development of methodology in technical aspects of of

1498
01:40:04,440 --> 01:40:08,090
machine learning but on the other hand you should also be just looking to solve

1499
01:40:08,090 --> 01:40:13,630
the problem and the most elegant way possible and that means using

1500
01:40:13,660 --> 01:40:16,130
simple statistics are simple

1501
01:40:16,140 --> 01:40:23,820
algorithms that are existing and that's what that's we should develop and you should use

1502
01:40:23,850 --> 01:40:30,080
maybe i'll stop about but if anyone is i i do here to the to

1503
01:40:30,080 --> 01:40:34,260
the notion of machine learning as it is at the moment is applied statistics and

1504
01:40:34,260 --> 01:40:37,860
designer branch statistics

1505
01:40:37,880 --> 01:40:42,670
and what to buy me obviously to get heated debate

1506
01:40:42,690 --> 01:40:47,980
and maybe just collecting nineteen ninety n

1507
01:40:48,000 --> 01:40:53,340
going around in the last interventions is of course one of the reasons for this

1508
01:40:53,340 --> 01:40:54,960
panel is trying to do

1509
01:40:54,980 --> 01:40:59,940
but i still feel to you for that we want to want to do for

1510
01:40:59,940 --> 01:41:02,480
the field and

1511
01:41:02,480 --> 01:41:06,300
something i would like to propose maybe can use a is is that i think

1512
01:41:06,300 --> 01:41:11,050
that one of the richest of machine learning is that if you are having with

1513
01:41:11,300 --> 01:41:13,320
background in it for you can

1514
01:41:13,340 --> 01:41:18,840
probably make yourself useful in a variety of fields many more than if you're doing

1515
01:41:18,980 --> 01:41:26,590
cryptography computational geometry we have a very narrow field of application the

1516
01:41:26,650 --> 01:41:30,320
you agree with me that actually works

1517
01:41:30,340 --> 01:41:31,590
six to

1518
01:41:31,590 --> 01:41:36,980
i mean i think certainly examples i gave my work my my phd

1519
01:41:38,260 --> 01:41:43,260
i have found themselves i is no academic or working in companies like NCI like

1520
01:41:44,610 --> 01:41:49,960
and working on various and diverse ranges of problems so

1521
01:41:50,050 --> 01:41:54,590
you will be channeled into one particular area which is really nice

1522
01:41:54,590 --> 01:41:56,170
now usual grading

1523
01:41:56,190 --> 01:42:00,150
then you begin to understand the idea of spectral resolution

1524
01:42:00,170 --> 01:42:01,750
you will now begin to see

1525
01:42:01,750 --> 01:42:02,940
the individual

1526
01:42:03,050 --> 01:42:06,900
former lines nicely separated for ukraine

1527
01:42:06,960 --> 01:42:09,250
so we'll turn this on

1528
01:42:09,250 --> 01:42:11,730
i will make it completely dark

1529
01:42:11,750 --> 01:42:13,400
and then

1530
01:42:13,420 --> 01:42:15,730
i want to thank you and i want you to

1531
01:42:15,750 --> 01:42:18,730
i appreciate this and spend some time looking

1532
01:42:21,300 --> 01:42:23,030
these lines

1533
01:42:23,090 --> 01:42:25,800
is really quite remarkable

1534
01:42:25,860 --> 01:42:29,480
this of course you could never do is to show

1535
01:42:31,800 --> 01:42:35,320
these many many lines you look so about sixteen hundred

1536
01:42:35,340 --> 01:42:37,670
this very strong

1537
01:42:37,730 --> 01:42:40,190
yellow in the

1538
01:42:40,230 --> 01:42:41,820
in the helium

1539
01:42:41,880 --> 01:42:44,940
is a well-known helium line

1540
01:42:44,990 --> 01:42:46,860
and it has a year

1541
01:42:46,920 --> 01:42:50,880
the wavelengths of five hundred and eighty seven nanometers

1542
01:42:50,900 --> 01:42:53,480
the brightest star in the

1543
01:42:53,480 --> 01:42:56,280
in helium

1544
01:42:56,300 --> 01:43:00,010
you can see them in first order you can see them in second order

1545
01:43:00,070 --> 01:43:03,990
and then gradually when you go to high order the various colors

1546
01:43:04,050 --> 01:43:08,510
begin to overlap with each other because each live their own lives

1547
01:43:08,570 --> 01:43:14,530
the angles are only depend on the land of the

1548
01:43:14,570 --> 01:43:17,460
now i can show you neon

1549
01:43:17,460 --> 01:43:20,210
which has even more lines

1550
01:43:20,210 --> 01:43:33,210
so you can you can you grading amazing into is growing doesn't cost more than

1551
01:43:33,210 --> 01:43:35,260
a billion dollar

1552
01:43:35,280 --> 01:43:37,880
it is absolutely stunning

1553
01:43:37,880 --> 01:43:40,920
and it has an incredible spectral resolution

1554
01:43:40,950 --> 01:43:43,820
because you need a prepared line source

1555
01:43:43,840 --> 01:43:46,880
to take advantage of that spectral resolution

1556
01:43:46,880 --> 01:43:48,070
and as i said

1557
01:43:48,090 --> 01:43:50,050
you will probably approach

1558
01:43:50,090 --> 01:43:52,230
certainly there

1559
01:43:52,250 --> 01:43:55,630
the audience all the way in the back of six one twenty

1560
01:43:55,800 --> 01:43:57,460
one approaching this

1561
01:43:57,460 --> 01:43:59,880
angular resolution ones that closer

1562
01:43:59,900 --> 01:44:02,780
may not approach because they see the line source

1563
01:44:02,800 --> 01:44:04,210
of course wider

1564
01:44:04,230 --> 01:44:06,880
the angle at which they sing the line source

1565
01:44:06,940 --> 01:44:08,610
may well be larger than

1566
01:44:08,630 --> 01:44:10,050
this which

1567
01:44:10,070 --> 01:44:11,940
the ones in the back of the audience

1568
01:44:11,960 --> 01:44:15,630
i therefore little better off

1569
01:44:15,650 --> 01:44:17,030
forty angle that you

1570
01:44:17,030 --> 01:44:18,590
c two the line source

1571
01:44:18,590 --> 01:44:22,490
with some the line source become smaller the farther away

1572
01:44:22,510 --> 01:44:25,070
this is remarkable isn't it

1573
01:44:25,070 --> 01:44:34,630
absolutely incredible

1574
01:44:34,670 --> 01:44:38,670
i think this is a great moment to rest and to digest this wonderful experience

1575
01:44:38,670 --> 01:44:42,400
and to have the form minute break

1576
01:44:42,460 --> 01:44:45,650
thank you very much

1577
01:44:45,710 --> 01:44:49,440
so what i want to discuss now

1578
01:44:49,460 --> 01:44:51,380
is the logical consequence

1579
01:44:51,400 --> 01:44:55,510
all this whole concept avoiding sources where spherical waves

1580
01:44:55,550 --> 01:44:57,920
come from each point

1581
01:44:57,940 --> 01:44:59,460
the aperture

1582
01:44:59,610 --> 01:45:01,360
we're not going to expand it

1583
01:45:01,420 --> 01:45:04,340
through a single opening

1584
01:45:04,380 --> 01:45:07,360
not notable with one single opening

1585
01:45:07,380 --> 01:45:09,360
and the opening

1586
01:45:09,420 --> 01:45:11,050
is now

1587
01:45:12,840 --> 01:45:16,730
this separation is now these openings the

1588
01:45:16,780 --> 01:45:21,400
think of them as being is that which has with these open singles slip

1589
01:45:21,400 --> 01:45:23,570
and we have plane waves coming in

1590
01:45:23,610 --> 01:45:24,780
like this

1591
01:45:24,800 --> 01:45:29,690
and now the question is

1592
01:45:29,730 --> 01:45:31,960
if i look

1593
01:45:31,980 --> 01:45:37,130
in various directions and that's my famous angle data

1594
01:45:37,170 --> 01:45:39,150
what will i see now

1595
01:45:39,170 --> 01:45:44,590
on the screen which i placed very far away

1596
01:45:46,860 --> 01:45:49,460
these points in this aperture

1597
01:45:49,480 --> 01:45:52,530
can now be considered according to the audience female principle

1598
01:45:52,530 --> 01:45:55,250
as a source of spherical wave

1599
01:45:55,250 --> 01:45:58,820
and they are going to interfere with each other strangely enough already

1600
01:45:58,840 --> 01:45:59,750
don't be

1601
01:45:59,750 --> 01:46:01,730
we call this the fraction

1602
01:46:01,780 --> 01:46:07,260
this is exactly the same phenomenon as interference but we draw strange distinction in physics

1603
01:46:07,260 --> 01:46:10,550
between interference which was degrading in the doubles

1604
01:46:10,590 --> 01:46:14,260
with a double split into two and no one would ever see double slit diffraction

1605
01:46:14,300 --> 01:46:15,460
is the same thing

1606
01:46:15,510 --> 01:46:20,210
somehow when we deal with individual openings we call the fraction

1607
01:46:20,260 --> 01:46:22,940
so it is the same you can use it anywhere you want to you can

1608
01:46:22,940 --> 01:46:26,400
call it in the field but the individual point sources

1609
01:46:26,420 --> 01:46:30,980
there they are all going to do the wrong thing

1610
01:46:30,990 --> 01:46:33,800
and i think this one of the top number one

1611
01:46:33,800 --> 01:46:40,070
and i think this one number two right in the middle

1612
01:46:40,130 --> 01:46:42,940
i don't the right you will see why do that

1613
01:46:42,960 --> 01:46:44,960
so now

1614
01:46:44,980 --> 01:46:48,250
i can calculate what the past differences

1615
01:46:48,300 --> 01:46:53,630
between the hurricane source right in the middle and organ source right at the top

1616
01:46:53,630 --> 01:46:56,130
well this past difference here

1617
01:46:56,150 --> 01:46:58,480
it's clearly one have the

1618
01:46:58,490 --> 01:47:01,320
by the sign of data

1619
01:47:01,420 --> 01:47:05,260
we did that before we had a little the

1620
01:47:05,260 --> 01:47:08,510
if i make that one half

1621
01:47:08,550 --> 01:47:10,230
i claim

1622
01:47:10,400 --> 01:47:11,670
in that direction

1623
01:47:11,710 --> 01:47:13,630
there will be darkness

1624
01:47:13,690 --> 01:47:16,170
while there the dark

1625
01:47:16,170 --> 01:47:18,210
because it's source number one

1626
01:47:18,780 --> 01:47:22,840
tell number two because they hundred eighty degrees out of phase then the source just

1627
01:47:22,840 --> 01:47:25,550
below one can kill this one belonged to

1628
01:47:25,550 --> 01:47:27,440
just check one activity

1629
01:47:27,480 --> 01:47:30,440
and say well if this person

1630
01:47:31,550 --> 01:47:35,650
more than this then he will never before the

1631
01:47:35,650 --> 01:47:40,570
and if he learns less than that he will the full

1632
01:47:40,690 --> 01:47:41,820
if you could see

1633
01:47:41,850 --> 01:47:44,150
there must be very happy

1634
01:47:45,110 --> 01:47:47,130
probably you can not

1635
01:47:48,500 --> 01:47:50,400
the same

1636
01:47:50,400 --> 01:47:52,150
we have found that you would

1637
01:47:52,170 --> 01:47:53,880
on the split

1638
01:47:53,940 --> 01:47:57,860
one keeps splitting we would see be in the in the

1639
01:47:58,030 --> 01:48:02,570
to measure how it was in use is the set of observations

1640
01:48:02,630 --> 01:48:03,960
and try to

1641
01:48:04,150 --> 01:48:07,630
use this heterogeneity

1642
01:48:07,670 --> 01:48:11,570
one test that uses that originated and this way

1643
01:48:13,340 --> 01:48:17,480
splitting and you at this time of the process we would see them in

1644
01:48:18,800 --> 01:48:26,730
after they decompose input space in axis parallel cuboids are kind of high dimensional

1645
01:48:28,130 --> 01:48:31,130
the best community in a number of ways

1646
01:48:31,250 --> 01:48:35,630
this those value equal set to be equal to the value or comparison is more

1647
01:48:35,630 --> 01:48:37,170
than or less than

1648
01:48:37,210 --> 01:48:42,840
and of course also these are several possible notions of originality and they didn't write

1649
01:48:42,840 --> 01:48:47,670
it in that once you are in the leaf you can either say what for

1650
01:48:47,670 --> 01:48:52,320
the majority are applied to are just for enough find way by any known professionally

1651
01:48:53,230 --> 01:48:56,050
the differences between

1652
01:48:56,070 --> 01:49:00,840
nice algorithms and i feel that is how do we how they

1653
01:49:00,880 --> 01:49:02,300
this same the tests

1654
01:49:02,300 --> 01:49:04,190
when the qualitative

1655
01:49:04,210 --> 01:49:05,610
and what happens believe

1656
01:49:05,630 --> 01:49:11,150
let me spend a few minutes saying what it was in a piece

1657
01:49:11,170 --> 01:49:14,630
in addition it is this

1658
01:49:14,690 --> 01:49:19,460
what do you mean that this just a few seconds would be enough to value

1659
01:49:19,460 --> 01:49:21,440
two possible values

1660
01:49:21,480 --> 01:49:23,610
there are high or low

1661
01:49:23,610 --> 01:49:25,130
so if you must

1662
01:49:27,400 --> 01:49:29,900
or low order must have high

1663
01:49:29,920 --> 01:49:33,360
it iteration eighty is low

1664
01:49:33,400 --> 01:49:37,780
if you have of them high on fire half of the metro

1665
01:49:37,800 --> 01:49:41,650
if there is any i

1666
01:49:41,800 --> 01:49:42,980
so this

1667
01:49:43,000 --> 01:49:48,650
axis x axis is right here of observations that fall on one side

1668
01:49:48,670 --> 01:49:50,280
this is the

1669
01:49:50,300 --> 01:49:54,790
when you hear almost all of them forty one side when you here almost all

1670
01:49:54,790 --> 01:49:56,150
of them same

1671
01:49:56,170 --> 01:49:58,570
when you're midway half of them

1672
01:49:58,610 --> 01:50:00,420
which way

1673
01:50:01,440 --> 01:50:05,400
half of them which way iteration it is high

1674
01:50:05,440 --> 01:50:08,210
when most of them go the same way

1675
01:50:08,210 --> 01:50:09,780
it originated low

1676
01:50:09,980 --> 01:50:11,550
but when this

1677
01:50:11,600 --> 01:50:15,380
o to one of these

1678
01:50:15,420 --> 01:50:18,070
i don't care

1679
01:50:18,150 --> 01:50:20,130
when you to choose

1680
01:50:20,170 --> 01:50:26,090
you have to program intestinal helping compute that originated and you may choose between in

1681
01:50:26,090 --> 01:50:31,530
dexter which you know these are eventually shannon information in the values of entropy and

1682
01:50:31,530 --> 01:50:33,050
this this

1683
01:50:33,090 --> 01:50:36,690
on the idea of doing nothing

1684
01:50:36,710 --> 01:50:38,460
but the outcome is if

1685
01:50:38,500 --> 01:50:43,150
and decision to howard pair or audience

1686
01:50:43,170 --> 01:50:45,250
very small changes in the data

1687
01:50:45,280 --> 01:50:51,960
make your changes on the page

1688
01:50:52,030 --> 01:50:56,420
your classifier to has some sort of very likely act as part of the

1689
01:50:56,440 --> 01:51:02,230
hyperplanes decisions to be just one test site westminster before you do one test

1690
01:51:02,270 --> 01:51:04,420
and this is what from here

1691
01:51:04,480 --> 01:51:06,860
the left from here to right

1692
01:51:06,860 --> 01:51:09,530
in this is you are doing unions like this

1693
01:51:09,550 --> 01:51:16,820
in a and b and knn have much complex shapes the most natural generalisation is

1694
01:51:16,990 --> 01:51:18,250
pretty much

1695
01:51:18,300 --> 01:51:20,630
with a hyperplane separating

1696
01:51:20,670 --> 01:51:23,550
not necessarily in the same input space

1697
01:51:23,610 --> 01:51:31,630
and not necessarily with hard threshold maybe you can get a password in the other

1698
01:51:31,650 --> 01:51:37,440
the first the hyperplane west indicating just before logistics missions

1699
01:51:37,480 --> 01:51:42,530
which way way even though it's called logistic to john correct me if i'm wrong

1700
01:51:42,530 --> 01:51:44,440
but logistic regression

1701
01:51:44,590 --> 01:51:46,780
its ability to classify

1702
01:51:46,860 --> 01:51:53,070
so g is not english

1703
01:51:53,090 --> 01:51:57,230
an example of you have probably the nice

1704
01:51:57,250 --> 01:52:02,340
the mother of predictors this a single fly out to sea and sand will show

1705
01:52:02,340 --> 01:52:04,550
up and they have shown up

1706
01:52:04,630 --> 01:52:07,050
things i have two place

1707
01:52:07,130 --> 01:52:13,750
this slogan of the CMCS maximum margin so they don't get any closer than then

1708
01:52:13,750 --> 01:52:16,530
you need to any of us

1709
01:52:16,570 --> 01:52:18,900
and this can be done by maximizing

1710
01:52:18,920 --> 01:52:20,230
the distance

1711
01:52:20,340 --> 01:52:23,360
of the hyperplane both

1712
01:52:24,710 --> 01:52:25,880
this means that

1713
01:52:25,900 --> 01:52:27,650
the labelled

1714
01:52:27,730 --> 01:52:29,730
thanks the prediction

1715
01:52:30,670 --> 01:52:36,800
as it should at least in yes and yes and with a high value possible

1716
01:52:36,840 --> 01:52:41,360
or you say no and it is non with high absolute value

1717
01:52:41,360 --> 01:52:46,020
you will want you might want to capture that structure as well so you can

1718
01:52:46,020 --> 01:52:51,840
say OK with that spherical space which have got by doing PCA then choose basis

1719
01:52:51,840 --> 01:52:53,270
that's well aligned

1720
01:52:53,290 --> 01:52:57,400
so that i can pull out particular pieces of structure

1721
01:52:59,250 --> 01:53:01,000
and the way that you do this

1722
01:53:01,020 --> 01:53:02,880
is you

1723
01:53:02,900 --> 01:53:08,310
typically maximize some kind of measure of kurtosis

1724
01:53:08,360 --> 01:53:14,080
nongaussianity of the distribution long tail of the distribution of particular you do this with

1725
01:53:14,080 --> 01:53:16,830
an iterative process pulls out the basis vector

1726
01:53:16,860 --> 01:53:22,230
and then what should pull out one suspect you do it again with the residual

1727
01:53:22,290 --> 01:53:28,420
second basis vector so in particular signals which are like this so some two nongaussian

1728
01:53:28,420 --> 01:53:33,810
signals which have been some together rotated and some space you know seen by many

1729
01:53:33,810 --> 01:53:40,230
different observers some linear transformation matrix from the signal source to the observer then you

1730
01:53:40,230 --> 01:53:42,810
can do it in a short tension PCA

1731
01:53:42,830 --> 01:53:45,170
two to get rid of the overall

1732
01:53:45,230 --> 01:53:49,650
magnitude the second moments in the final analysis by CA

1733
01:53:49,730 --> 01:53:52,040
to pull out the particular

1734
01:53:52,060 --> 01:53:53,770
individual cell structures

1735
01:53:53,790 --> 01:53:57,650
so you can do this with images and you get filters i want to show

1736
01:53:57,650 --> 01:54:01,230
it's a good way of pulling out independent information

1737
01:54:01,250 --> 01:54:04,020
genetic signals

1738
01:54:05,650 --> 01:54:08,610
onto another set of descriptors

1739
01:54:09,900 --> 01:54:12,770
another way to do things is just based on

1740
01:54:12,830 --> 01:54:14,790
very crude morphology

1741
01:54:14,830 --> 01:54:19,330
a great was storage so is there an nature this position in the image or

1742
01:54:19,330 --> 01:54:21,790
not is

1743
01:54:21,810 --> 01:54:25,310
the pixel next to me bigger than me or smaller than the

1744
01:54:25,360 --> 01:54:30,360
in terms of its great intensity value so local binary patterns basically take small patch

1745
01:54:30,500 --> 01:54:34,880
image here three by three patch you take the central pixel is threshold the other

1746
01:54:34,880 --> 01:54:39,540
pixels as with a smaller or larger than the central pixel and you get a

1747
01:54:39,540 --> 01:54:44,960
binary code corresponds to that the digital bunny victor here it's long which you can

1748
01:54:44,960 --> 01:54:47,520
use the code for the local structure of the image

1749
01:54:47,650 --> 01:54:49,520
that code you can for example

1750
01:54:49,520 --> 01:54:55,190
histogram locally of somewhat larger spatial region just to say what kind of structures there

1751
01:54:55,190 --> 01:54:56,340
are in the image

1752
01:54:56,360 --> 01:54:58,230
and that local local regions

1753
01:54:58,230 --> 01:55:03,600
OK this recruit is quite variance in lighting and things like that it turns out

1754
01:55:03,600 --> 01:55:07,210
to be a fairly powerful description if you want to do for example face recognition

1755
01:55:07,210 --> 01:55:10,110
is one of the best description face recognition

1756
01:55:11,080 --> 01:55:12,020
simple to do

1757
01:55:12,290 --> 01:55:15,650
OK another thing along the same lines

1758
01:55:15,710 --> 01:55:20,460
shape context here you have a low polar cities spatial bins

1759
01:55:20,520 --> 01:55:23,150
you have some kind of innate structure or

1760
01:55:23,520 --> 01:55:27,360
but binarized image in some way or another doesn't really matter how but it is

1761
01:55:27,360 --> 01:55:31,400
probably the easiest thing in what you're doing is you're going to place this descriptor

1762
01:55:31,730 --> 01:55:35,560
on all the edges and count the number of

1763
01:55:35,560 --> 01:55:40,100
the edges of the rich pieces that fall into a particular events so the count

1764
01:55:40,110 --> 01:55:41,420
of the bins

1765
01:55:41,460 --> 01:55:45,540
is the signal for that particular age signals multidimensional sigma fridge

1766
01:55:45,560 --> 01:55:48,630
for each piece of age and then you run it around to get sit these

1767
01:55:48,630 --> 01:55:49,900
different signals

1768
01:55:50,230 --> 01:55:54,250
which you can then use the characteristic of the image turns out to be very

1769
01:55:54,250 --> 01:55:55,690
good for matching

1770
01:55:55,750 --> 01:55:59,150
in particular things like text and things like that

1771
01:55:59,170 --> 01:56:04,580
this is quite characteristic of got clear binary signal gives you higher highly characteristic

1772
01:56:04,630 --> 01:56:09,400
i think we use it for example silhouettes to characterize human poses we get good

1773
01:56:09,400 --> 01:56:12,060
results the machine learning approach for the

1774
01:56:12,100 --> 01:56:14,170
t to use that information later

1775
01:56:15,100 --> 01:56:22,460
i think this is the final descriptively going to talk about so

1776
01:56:22,500 --> 01:56:28,920
each gradient orientation histograms so these have been very very successful descriptor over the last

1777
01:56:28,920 --> 01:56:30,170
ten years or so

1778
01:56:31,920 --> 01:56:36,060
again divide the local image into a certain number of orientation bins

1779
01:56:36,360 --> 01:56:41,920
and simply vote into been that corresponds to the orientation of the pixel typically with

1780
01:56:41,920 --> 01:56:44,110
the vote depends on the links

1781
01:56:44,130 --> 01:56:49,480
of the gradient at a pixel so you can see all per preferentially waiting

1782
01:56:49,480 --> 01:56:54,170
pixels of high gradients and then you pull that information in some small so special

1783
01:56:54,940 --> 01:56:56,630
so in spatial cell

1784
01:56:56,650 --> 01:56:59,060
there's a little histogram over orientations

1785
01:56:59,580 --> 01:57:01,670
and the whole descriptor is

1786
01:57:01,710 --> 01:57:05,420
six of histograms for each of the cells

1787
01:57:05,480 --> 01:57:07,810
so this is the basis of

1788
01:57:07,920 --> 01:57:12,520
SIFT scale invariant feature transform which is one of the best image matching techniques moment

1789
01:57:12,690 --> 01:57:16,830
histogram of oriented gradients which is one of the biggest human detection techniques at the

1790
01:57:16,830 --> 01:57:22,420
moment i think generally shape context which is business equipment use the shape const exist

1791
01:57:22,440 --> 01:57:25,250
no polygons we saw in the previous slide

1792
01:57:25,340 --> 01:57:29,650
but some things that turns out to be a very good arrangement as well

1793
01:57:30,540 --> 01:57:34,440
that's all of script is now we have to figure out where we're going to

1794
01:57:34,440 --> 01:57:37,880
evaluate and the the number of different choices

1795
01:57:37,960 --> 01:57:41,130
on the it's good of image location so that's the most obvious choice we just

1796
01:57:41,130 --> 01:57:46,170
evaluate descriptions that every pixel or if you pixels typically will do this also multiple

1797
01:57:46,170 --> 01:57:49,770
scales because we want to be able to scale invariant analysis things

1798
01:57:49,790 --> 01:57:54,540
so the output will be scaled spatial pyramid of local descriptive vectors and if we

1799
01:57:54,540 --> 01:57:58,520
want for example to do object detection in an image will typically take a small

1800
01:57:58,520 --> 01:58:02,690
window that corresponds to the size of the object around it over the image multiple

1801
01:58:02,690 --> 01:58:06,110
scales and positions and do some kind of local

1802
01:58:06,900 --> 01:58:11,230
classifier based calculation based on the descriptors that we have in that the wind

1803
01:58:12,100 --> 01:58:16,920
in order to decide whether this account our personal something in that particular window runs

1804
01:58:16,920 --> 01:58:19,460
over the whole image like that so that's

1805
01:58:19,520 --> 01:58:23,500
very classical way to do object recognition types of tasks we see some examples of

1806
01:58:23,500 --> 01:58:26,520
this in the next slide of course in the next lecture course

1807
01:58:28,130 --> 01:58:31,380
because regular simplifies the details of

1808
01:58:31,400 --> 01:58:33,380
so the extraction

1809
01:58:33,440 --> 01:58:35,980
things like that

1810
01:58:36,000 --> 01:58:38,710
another way to do this is to just sample randomly

1811
01:58:38,750 --> 01:58:42,130
and i think that's crazy there's no structure in that at all

1812
01:58:42,150 --> 01:58:46,290
but it turns out because patches are actually quite characteristic of images

