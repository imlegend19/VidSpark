1
00:00:00,000 --> 00:00:01,790
and it's relatively elementary

2
00:00:03,540 --> 00:00:08,040
i was hoping that maybe also some other parts we can do together are you do do some things

3
00:00:08,490 --> 00:00:11,000
when i weight here and then we saw them together on the whiteboard

4
00:00:11,530 --> 00:00:12,440
so let's see how it goes

5
00:00:14,090 --> 00:00:14,390
this is

6
00:00:15,280 --> 00:00:16,530
so let me see if i think

7
00:00:25,050 --> 00:00:26,120
so uh

8
00:00:27,510 --> 00:00:28,390
i think this two

9
00:00:29,160 --> 00:00:31,810
cases are relatively simple and maybe we can start

10
00:00:32,510 --> 00:00:33,960
with these two so i'm going to

11
00:00:34,810 --> 00:00:35,350
show you the

12
00:00:35,840 --> 00:00:39,210
definition again maybe you have them you have it on the slides if you don't

13
00:00:39,210 --> 00:00:43,290
know them is not recommend that you briefly write it down so a positive definite

14
00:00:43,290 --> 00:00:44,100
kernel is a kernel

15
00:00:44,690 --> 00:00:46,900
it's symmetric and that has this property that

16
00:00:48,490 --> 00:00:49,270
x i x and

17
00:00:49,780 --> 00:00:51,470
in x and for all real numbers

18
00:00:52,360 --> 00:00:54,220
this quantity here is non-negative

19
00:00:54,650 --> 00:00:57,980
so if you want to if you don't have it already maybe briefly write this down

20
00:01:02,450 --> 00:01:02,910
and dr

21
00:01:07,750 --> 00:01:09,610
could not gonna need this one down here now

22
00:01:10,970 --> 00:01:11,620
so i

23
00:01:12,140 --> 00:01:12,570
move to

24
00:01:13,410 --> 00:01:14,130
this thing here

25
00:01:14,980 --> 00:01:21,000
and now i would like you to try to prove because usually when one sees definition one rated but maybe

26
00:01:21,520 --> 00:01:24,270
one doesn't understand the details before one has worked with it a bit so i

27
00:01:24,270 --> 00:01:25,740
wanted to work a bit with this definition

28
00:01:26,480 --> 00:01:26,810
and end

29
00:01:28,330 --> 00:01:30,150
and want do first to these two things

30
00:01:31,000 --> 00:01:31,440
one is

31
00:01:32,930 --> 00:01:35,150
if we define a function of two arguments

32
00:01:35,530 --> 00:01:35,970
like this

33
00:01:36,850 --> 00:01:37,280
where we

34
00:01:37,680 --> 00:01:39,090
assume that five is a mapping

35
00:01:39,790 --> 00:01:43,710
from some nonempty sets carry graphic x into a dot product space age

36
00:01:44,640 --> 00:01:46,020
so we define a mapping like this

37
00:01:46,720 --> 00:01:48,290
we don't claim is a positive definite kernel

38
00:01:48,870 --> 00:01:51,620
then from this definition is already follows

39
00:01:52,330 --> 00:01:54,170
that this kernel is positive definite

40
00:01:54,850 --> 00:01:59,100
so i wanted to prove that this thing here satisfies the equation is required

41
00:01:59,760 --> 00:02:02,820
he inequality required for a positive definite kernel that's the first thing

42
00:02:03,820 --> 00:02:04,890
and the second thing is

43
00:02:05,960 --> 00:02:06,680
i would like to

44
00:02:07,410 --> 00:02:09,830
also show that this thing here

45
00:02:10,360 --> 00:02:11,620
is a positive definite kernel

46
00:02:13,050 --> 00:02:13,800
so what is this thing

47
00:02:14,390 --> 00:02:16,120
it's a kernel defined on sets

48
00:02:17,340 --> 00:02:20,860
and in be of finite arbitrary finite subsets of x

49
00:02:21,800 --> 00:02:25,390
and the value of case defined to be the sum of all

50
00:02:27,110 --> 00:02:28,890
uh in the metric could also

51
00:02:29,870 --> 00:02:33,330
the queen we now assume that case is that positive definite kernel

52
00:02:34,070 --> 00:02:36,860
so we take the sum over all these positive definite kernels

53
00:02:37,470 --> 00:02:39,250
over the sets in being

54
00:02:40,050 --> 00:02:42,830
and it turns out that this is again a positive definite kernel

55
00:02:44,230 --> 00:02:46,080
this time on this finite subsets

56
00:02:46,970 --> 00:02:49,780
so in the first one we don't assume anything about kay we just assume

57
00:02:50,190 --> 00:02:53,210
that phi such a mapping and the second one we assume

58
00:02:53,820 --> 00:02:54,900
that this small kay

59
00:02:55,780 --> 00:02:57,340
there is a positive definite kernel

60
00:02:58,310 --> 00:03:01,020
so i'll give you a few minutes and try to play around

61
00:03:01,510 --> 00:03:04,070
we didn't i'm not gonna force anyone to do it but

62
00:03:04,630 --> 00:03:05,910
i will ask for volunteers that in

63
00:03:06,610 --> 00:03:07,000
if you want

64
00:03:08,160 --> 00:03:09,920
how many have solved the problem

65
00:03:12,760 --> 00:03:15,920
okay so i'm not going to ask you to get up so you can you can be

66
00:03:16,420 --> 00:03:17,970
open in assessment

67
00:03:18,660 --> 00:03:19,750
so i give you a bit more time

68
00:03:20,860 --> 00:03:25,900
and i think we we should actually do the second one later because if we have the

69
00:03:26,330 --> 00:03:26,790
prove it

70
00:03:27,380 --> 00:03:31,510
based on what you know already is a bit complicated and later on it would be much easier

71
00:03:32,070 --> 00:03:35,950
so just to the first one hour take optional if you want you contrite

72
00:03:37,860 --> 00:03:39,980
uh i give you a bit more time to do the first one

73
00:03:40,690 --> 00:03:41,740
these half done it

74
00:03:43,710 --> 00:03:46,970
so how many of you have so the first one

75
00:03:46,970 --> 00:03:51,140
they are quite common denotes different concept from mark twain

76
00:03:51,800 --> 00:03:53,440
this analysis

77
00:03:54,670 --> 00:04:00,290
quite those to the right but anyway this is what's included in the list

78
00:04:04,300 --> 00:04:06,580
it just depends on

79
00:04:06,580 --> 00:04:10,490
might have different sections daniel

80
00:04:10,600 --> 00:04:13,290
the first single concept

81
00:04:13,420 --> 00:04:15,500
people might have different beliefs

82
00:04:15,620 --> 00:04:19,690
but from the comments and they have the mark twain

83
00:04:19,690 --> 00:04:23,020
this is why we

84
00:04:27,950 --> 00:04:33,920
that means you to something conceptually more concepts

85
00:04:33,930 --> 00:04:38,480
OK great so namestring

86
00:04:38,490 --> 00:04:40,420
there's namestring but are

87
00:04:40,420 --> 00:04:45,830
a whole lot of other predicates that relates strings to concepts directly into these

88
00:04:45,880 --> 00:04:48,490
all these over here can relate to people

89
00:04:48,740 --> 00:04:54,250
i don't have the hip-hop moniker but if i did it with p

90
00:04:54,680 --> 00:04:59,140
listed in the cave with the predicate hip-hop moniker

91
00:04:59,180 --> 00:05:01,170
and then these are for

92
00:05:01,180 --> 00:05:02,890
sort of inanimate objects

93
00:05:02,900 --> 00:05:05,500
so named files

94
00:05:05,670 --> 00:05:09,890
the atomic symbol of an element that serve to predicate

95
00:05:09,910 --> 00:05:15,110
like mainstreaming in the sense that relates the concept history

96
00:05:15,590 --> 00:05:23,730
cwm head injury CCW in KB that means conceptual work this lot of interesting

97
00:05:23,790 --> 00:05:27,190
work done by philosophers on conceptual works

98
00:05:27,740 --> 00:05:31,470
i recommend browsing around in that area

99
00:05:31,480 --> 00:05:34,400
a conceptual work is

100
00:05:34,410 --> 00:05:36,180
like a novel

101
00:05:36,940 --> 00:05:38,850
their predicates for

102
00:05:39,610 --> 00:05:42,960
talking about the content of the novel

103
00:05:42,990 --> 00:05:46,190
OK will be

104
00:05:46,230 --> 00:05:53,080
self-explanatory OK so this just shows that it's not all namestring there are other namestring

105
00:05:53,080 --> 00:05:54,350
like predicates

106
00:05:54,700 --> 00:05:56,620
but the

107
00:05:58,290 --> 00:06:01,180
you know

108
00:06:01,350 --> 00:06:03,300
with the life of

109
00:06:03,340 --> 00:06:06,410
these are all specs

110
00:06:06,430 --> 00:06:08,370
string indexing

111
00:06:09,690 --> 00:06:12,650
specs of string indexing slot

112
00:06:12,770 --> 00:06:14,260
the collection

113
00:06:14,390 --> 00:06:16,240
of binary

114
00:06:16,370 --> 00:06:20,680
i could on string indexing slot

115
00:06:21,030 --> 00:06:27,690
OK so here's an example of generative and acyclic

116
00:06:27,690 --> 00:06:30,430
it's asserted

117
00:06:30,440 --> 00:06:32,930
my family name is

118
00:06:32,940 --> 00:06:35,550
my given name is also the

119
00:06:39,130 --> 00:06:40,990
we should do this with the

120
00:06:41,090 --> 00:06:42,380
yes KB

121
00:06:44,620 --> 00:06:48,220
what's to this was to keep it

122
00:06:48,610 --> 00:06:52,670
o but i might not be in and out

123
00:07:04,640 --> 00:07:07,470
and then it's derived

124
00:07:07,490 --> 00:07:10,700
my name string is elizabeth coppock through

125
00:07:10,780 --> 00:07:13,900
a rule that can can it's the family name

126
00:07:13,950 --> 00:07:17,090
given names in language like hungarian where the

127
00:07:17,220 --> 00:07:19,740
given names follows the

128
00:07:19,750 --> 00:07:21,130
family names

129
00:07:21,130 --> 00:07:23,860
the rule for deriving the full name

130
00:07:23,880 --> 00:07:29,780
would be different so this would be an english specific rules for generating forming

131
00:07:30,860 --> 00:07:37,590
the well actually it should hold

132
00:07:37,630 --> 00:07:40,700
it should hold in the hungarian and

133
00:07:40,800 --> 00:07:42,040
perhaps the

134
00:07:42,970 --> 00:07:44,540
the name string

135
00:07:44,550 --> 00:07:46,280
it was discovered

136
00:07:46,300 --> 00:07:48,490
couple course the best

137
00:07:49,890 --> 00:07:55,540
it it's only the only have rules in english team

138
00:07:55,940 --> 00:08:00,700
if that rule were asserted in the hungarian MT then it would show up here

139
00:08:00,740 --> 00:08:01,740
among the

140
00:08:07,150 --> 00:08:09,370
right from the phd in

141
00:08:09,380 --> 00:08:12,430
two thousand nine it generates

142
00:08:12,600 --> 00:08:15,230
it was discovered come up phd

143
00:08:15,290 --> 00:08:16,910
using the fact that

144
00:08:17,030 --> 00:08:19,220
i got my ph d in two thousand nine

145
00:08:19,350 --> 00:08:20,440
also this

146
00:08:20,520 --> 00:08:25,870
so if you click on this assertion was the phd

147
00:08:25,910 --> 00:08:29,530
and then

148
00:08:29,560 --> 00:08:31,350
click on arguments

149
00:08:31,350 --> 00:08:37,120
it'll tell you the rules and the fact that we're used to describe to derive

150
00:08:37,170 --> 00:08:38,340
the search

151
00:08:40,760 --> 00:08:43,170
it is this rule

152
00:08:44,800 --> 00:08:49,620
the first is phd level that the collection of people with phds

153
00:08:49,620 --> 00:08:50,830
given me

154
00:08:51,080 --> 00:08:53,450
a the person's given name

155
00:08:53,970 --> 00:08:55,140
which given the

156
00:09:03,980 --> 00:09:07,830
if you could have a given name

157
00:09:09,370 --> 00:09:11,160
family name

158
00:09:11,200 --> 00:09:13,530
carlos space

159
00:09:13,580 --> 00:09:14,990
ph d

160
00:09:15,090 --> 00:09:20,430
and get question mark name plus this predicate is defined in code

161
00:09:23,960 --> 00:09:26,350
the namestring persons

162
00:09:26,360 --> 00:09:29,200
concatenated string

163
00:09:30,390 --> 00:09:31,680
and it knows

164
00:09:31,690 --> 00:09:35,110
these things called because they were asserted

165
00:09:35,460 --> 00:09:42,810
call opaque indicates that it was

166
00:09:42,840 --> 00:09:45,000
not computed

167
00:09:45,080 --> 00:09:47,940
using regular secretions

168
00:09:53,420 --> 00:09:54,780
all right so

169
00:09:54,800 --> 00:09:57,720
the next kind of predicate is

170
00:09:58,400 --> 00:10:01,410
the kind that relates so word two concepts

171
00:10:01,500 --> 00:10:04,840
denotation is the prototypical example of that

172
00:10:05,990 --> 00:10:08,030
it takes four arguments actually

173
00:10:10,040 --> 00:10:12,140
part of speech for the word

174
00:10:12,200 --> 00:10:16,250
integer indicating the word sense

175
00:10:16,270 --> 00:10:18,010
don't get me started about

176
00:10:18,020 --> 00:10:21,280
that i i will mostly ignore it

177
00:10:21,330 --> 00:10:22,200
and then

178
00:10:22,220 --> 00:10:24,700
there are four is the concept that the word

179
00:10:27,290 --> 00:10:33,080
so let's look at some examples

180
00:10:33,550 --> 00:10:36,520
alright this is

181
00:10:36,610 --> 00:10:39,630
all the things you can use

182
00:10:39,670 --> 00:10:42,120
as a part of speech argument

183
00:10:42,220 --> 00:10:45,760
there are all the parts of speech

184
00:10:45,780 --> 00:10:47,560
in the KB

185
00:10:47,970 --> 00:10:52,650
point is six i think

186
00:10:55,170 --> 00:10:58,660
ring theword

187
00:10:58,700 --> 00:11:02,100
it's asserted to be an english word

188
00:11:03,580 --> 00:11:08,260
in the english MT it asserted that its singular form is writing

189
00:11:08,310 --> 00:11:12,960
and it's definitive form ring because ring can be either a noun or verb

190
00:11:12,970 --> 00:11:14,960
so now in

191
00:11:14,970 --> 00:11:16,410
he gave me a ring

192
00:11:18,990 --> 00:11:21,560
the verbal end

193
00:11:21,580 --> 00:11:22,840
the bell will ring

194
00:11:26,800 --> 00:11:29,380
as a verb

195
00:11:30,590 --> 00:11:33,080
the first sentence

196
00:11:33,100 --> 00:11:34,370
it means

197
00:11:34,380 --> 00:11:36,890
emitting sound the collection of

198
00:11:36,930 --> 00:11:40,690
sound emitting events

199
00:11:40,730 --> 00:11:43,610
according to this assertion

200
00:11:43,640 --> 00:11:45,610
as account now

201
00:11:47,730 --> 00:11:50,680
the first sentence which is the same as

202
00:11:50,700 --> 00:11:51,990
per cent

203
00:11:52,140 --> 00:11:54,370
it means a sound

204
00:11:54,500 --> 00:11:56,510
and novels

205
00:11:57,140 --> 00:11:59,220
the collection of audible sound

206
00:11:59,300 --> 00:12:03,720
as another set has as it can

207
00:12:03,730 --> 00:12:06,230
is a ring shaped object

208
00:12:06,270 --> 00:12:08,640
there's something has

209
00:12:10,600 --> 00:12:13,020
if you

210
00:12:13,020 --> 00:12:17,350
they there there's seemingly infinite can i stay way

211
00:12:17,410 --> 00:12:20,160
this is my chance to mention the name

212
00:12:20,220 --> 00:12:23,890
the two names winner hot

213
00:12:23,910 --> 00:12:28,740
so we hop was about to that should be mentioned too

214
00:12:28,790 --> 00:12:32,350
we are half is about matrices that

215
00:12:32,370 --> 00:12:34,890
that's a start

216
00:12:34,910 --> 00:12:37,970
and go on forever singly infinite

217
00:12:38,000 --> 00:12:42,160
but not dublin doubly infinite is the one where for we could call the fourier

218
00:12:42,160 --> 00:12:44,100
matrices we want

219
00:12:44,100 --> 00:12:45,390
as for it

220
00:12:45,410 --> 00:12:47,470
this is perfect

221
00:12:49,180 --> 00:12:55,100
where singly infinite we would have had to have a smart idea

222
00:12:55,120 --> 00:12:59,080
this this factors well into you times l

223
00:12:59,220 --> 00:13:03,180
that this is here's here's you want to know that we half idea

224
00:13:03,240 --> 00:13:07,100
we remark these are constant diagonal right

225
00:13:07,120 --> 00:13:10,970
we are half idea was that i can factor into

226
00:13:10,990 --> 00:13:13,310
upper time lower

227
00:13:13,350 --> 00:13:15,760
not i winner harbor

228
00:13:15,760 --> 00:13:19,000
ah so upper

229
00:13:19,020 --> 00:13:22,520
and lower

230
00:13:22,520 --> 00:13:26,180
and these will be constant diag

231
00:13:26,200 --> 00:13:28,830
that's the thing that's the thing that this

232
00:13:28,870 --> 00:13:31,500
the that multiplying that times that

233
00:13:31,560 --> 00:13:33,120
to give this

234
00:13:33,140 --> 00:13:37,430
will be no different from find that second row times the second column to give

235
00:13:37,450 --> 00:13:38,120
the two

236
00:13:38,140 --> 00:13:39,580
two two and

237
00:13:39,580 --> 00:13:43,310
you see that the is at least you can hope that

238
00:13:43,350 --> 00:13:49,600
the winner of factorizations preserves this constant diagonal which is what everybody is living in

239
00:13:49,600 --> 00:13:51,040
the fourier domain

240
00:13:51,060 --> 00:13:53,770
but it gives an upper times slower

241
00:13:53,790 --> 00:13:58,660
whereas when we did when we took that matrix so what what what if you

242
00:13:58,660 --> 00:14:02,490
try for the lower times up

243
00:14:02,500 --> 00:14:04,490
actually that's what we did

244
00:14:04,500 --> 00:14:09,370
we did the factorisation you remember the factorisation of the matrix k is not going

245
00:14:10,180 --> 00:14:12,200
remove this

246
00:14:12,200 --> 00:14:15,910
devilish problem

247
00:14:15,910 --> 00:14:18,540
and the q are idea

248
00:14:18,580 --> 00:14:20,370
square root filter idea

249
00:14:20,430 --> 00:14:25,000
just just to complete this book because i think maybe it's worth completing its it's

250
00:14:25,430 --> 00:14:29,680
a mantis evading here for a stuff but probably

251
00:14:30,200 --> 00:14:36,080
this is a good time to say what what happened you remember factorisation two of

252
00:14:36,080 --> 00:14:37,700
our favorite

253
00:14:40,990 --> 00:14:44,990
into LDA you what these numbers look like

254
00:14:45,000 --> 00:14:48,560
what did the pivots look like and what these things look like

255
00:14:48,620 --> 00:14:50,410
remember there was a

256
00:14:50,470 --> 00:14:54,740
well when the pivots for this thing

257
00:14:55,950 --> 00:14:59,910
three have four thirds crewmember

258
00:14:59,930 --> 00:15:03,000
the pivots were approaching one

259
00:15:03,020 --> 00:15:08,060
and what was the l l had ones on the diagonal and had minus

260
00:15:08,200 --> 00:15:11,620
half minus two thirds minus three four

261
00:15:11,680 --> 00:15:12,620
so on

262
00:15:14,790 --> 00:15:18,410
because it was symmetric you was the transport

263
00:15:18,430 --> 00:15:21,080
so these are approaching one

264
00:15:21,140 --> 00:15:23,120
minus one

265
00:15:23,120 --> 00:15:25,680
what i'm saying here is that

266
00:15:27,560 --> 00:15:31,720
l times you which is sort of the wrong order is not the perfect order

267
00:15:31,770 --> 00:15:33,560
but it's approaching

268
00:15:33,620 --> 00:15:35,120
it converges

269
00:15:35,180 --> 00:15:36,810
the perfect one the

270
00:15:36,830 --> 00:15:38,770
as i go down the list

271
00:15:38,770 --> 00:15:39,760
i get

272
00:15:39,770 --> 00:15:42,640
minus one one

273
00:15:42,770 --> 00:15:45,140
in in the limit in l

274
00:15:45,140 --> 00:15:48,410
and in the u i get minus one

275
00:15:48,470 --> 00:15:51,270
i get that this is sort of what i'm

276
00:15:51,270 --> 00:15:56,600
what i'm converging to of a minus ones and ones and ones and minus one

277
00:15:56,600 --> 00:15:57,810
those are

278
00:15:57,810 --> 00:15:59,490
the the

279
00:15:59,500 --> 00:16:00,640
here is that

280
00:16:00,720 --> 00:16:02,520
this was the l in the u

281
00:16:04,020 --> 00:16:05,830
only in the limit

282
00:16:05,890 --> 00:16:08,930
whereas if i switch put in the right order

283
00:16:08,990 --> 00:16:10,160
in the other order

284
00:16:10,180 --> 00:16:12,830
which is the right one for winter harbor

285
00:16:12,850 --> 00:16:16,870
there there are correct from the all the way along

286
00:16:16,870 --> 00:16:20,580
you see that because actually we sell that factorisation that

287
00:16:21,430 --> 00:16:22,350
so that

288
00:16:22,410 --> 00:16:25,830
it was red it one one one one one

289
00:16:25,910 --> 00:16:30,930
minus one minus one minus one minus one song

290
00:16:30,930 --> 00:16:35,680
one one one one one minus one minus one minus one OK so do that

291
00:16:35,680 --> 00:16:38,330
publication in the in the order you times l

292
00:16:38,350 --> 00:16:40,370
what do you get you get two

293
00:16:40,390 --> 00:16:44,500
yet i minus one you get k

294
00:16:44,560 --> 00:16:48,060
so i should have shown you that example from the start

295
00:16:48,080 --> 00:16:52,390
that if making gold forever

296
00:16:52,410 --> 00:16:55,140
so that was the winner have applied

297
00:16:55,200 --> 00:17:00,950
and actually now on the amount of having got this far unintentionally on we hop

298
00:17:00,970 --> 00:17:04,160
we're dysphoria come in

299
00:17:04,160 --> 00:17:08,810
why don't we see it it never hurts like see an idea a month

300
00:17:10,640 --> 00:17:15,790
so we're dysphoria coming of fourier a would say OK what's

301
00:17:15,830 --> 00:17:18,370
what's the

302
00:17:18,430 --> 00:17:19,370
what's the

303
00:17:19,390 --> 00:17:22,100
the polynomial

304
00:17:22,120 --> 00:17:24,870
get into the frequency domain

305
00:17:24,890 --> 00:17:28,240
so when i look at that in the or we we're looks at in the

306
00:17:28,240 --> 00:17:32,600
frequency domain he'd say OK we have one minus one times

307
00:17:32,620 --> 00:17:33,760
each the

308
00:17:33,760 --> 00:17:36,500
minus i omega

309
00:17:36,560 --> 00:17:38,830
and two for the constant term

310
00:17:38,850 --> 00:17:41,270
and i minus one is the

311
00:17:41,310 --> 00:17:43,200
plus i omega

312
00:17:43,240 --> 00:17:46,020
that would be

313
00:17:46,060 --> 00:17:52,810
are whatever i want to call it the frequency response function or transfer function symbol

314
00:17:52,810 --> 00:17:55,350
everybody is that it's the right thing

315
00:17:55,350 --> 00:17:58,680
to look at for for that kind of matrix

316
00:17:58,680 --> 00:18:02,490
and what is and what what i have here

317
00:18:02,490 --> 00:18:08,220
following the same reliable one for the constant however minus either omega

318
00:18:08,270 --> 00:18:09,950
and what i have here

319
00:18:09,970 --> 00:18:15,680
everyone from the diagonal and i minus e to the minus sign maker from

320
00:18:15,680 --> 00:18:19,270
lower diagonal and what is suppose

321
00:18:19,290 --> 00:18:24,000
a product of it so we never ended up factoring polynomials

322
00:18:24,020 --> 00:18:25,200
that's the point

323
00:18:25,240 --> 00:18:26,200
is that

324
00:18:26,290 --> 00:18:28,520
is that

325
00:18:28,540 --> 00:18:31,870
that you take the polynomial for k

326
00:18:31,870 --> 00:18:33,540
that's of what

327
00:18:33,580 --> 00:18:36,450
in other words you go into the frequency domain

328
00:18:36,470 --> 00:18:41,470
in the frequency domain you have some simple little polynomials of infinite matrix

329
00:18:41,490 --> 00:18:43,810
the factor that polynomial into

330
00:18:43,870 --> 00:18:47,870
you know into a series of positive power series going one way

331
00:18:47,890 --> 00:18:51,700
and then a power series in the inverse going the other way

332
00:18:51,740 --> 00:18:54,180
and that tells you well in your

333
00:18:54,240 --> 00:18:55,270
and of course

334
00:18:55,370 --> 00:18:58,560
the whole thing depends on this constant diag

335
00:18:58,580 --> 00:19:01,060
so common could

336
00:19:01,330 --> 00:19:06,120
and come would lose the structure the simplicity of the structure

337
00:19:08,220 --> 00:19:12,100
the flooding through the state space

338
00:19:12,100 --> 00:19:13,330
time domain

339
00:19:14,220 --> 00:19:19,500
but of course he gained enormously by having a much much more general

340
00:19:21,100 --> 00:19:25,890
is there a question or discussion i i hadn't intended to

341
00:19:25,910 --> 00:19:30,430
speak about this but it so highly we're kind of in between

342
00:19:30,490 --> 00:19:34,290
having seen the very first lectures these this k

343
00:19:35,500 --> 00:19:37,970
now seeing

344
00:19:38,020 --> 00:19:40,390
much more general stuff

345
00:19:40,390 --> 00:19:45,240
but then see in the future are returning to this

346
00:19:45,290 --> 00:19:49,770
constant diagonal we're probably winner of mentioned again but

347
00:19:49,830 --> 00:19:53,390
it's kind of fun to see here as

348
00:19:53,410 --> 00:19:55,660
having mentioned this

349
00:19:55,660 --> 00:20:00,270
factorisation UL in the reverse order in that crazy problems

350
00:20:00,640 --> 00:20:05,000
just popped into my mind is exactly the factorisation that we are

351
00:20:05,060 --> 00:20:06,640
it's only again

352
00:20:06,660 --> 00:20:08,830
that that's winter harbor

353
00:20:08,850 --> 00:20:10,930
we are half deals with

354
00:20:10,950 --> 00:20:13,000
one boundary

355
00:20:13,140 --> 00:20:15,950
matrix comes up and it stops at zero

356
00:20:15,950 --> 00:20:19,370
so we are half operates on a half

357
00:20:19,430 --> 00:20:22,720
four a operates on the whole no boundary at all

358
00:20:22,770 --> 00:20:26,910
or on a periodic one were repeats and repeats and repeats of the boundary is

359
00:20:26,910 --> 00:20:30,310
and again you cannot use a function that is one when the mean is in

360
00:20:30,310 --> 00:20:33,600
the box and zero otherwise the average that's function

361
00:20:34,270 --> 00:20:36,460
and the the posterior distribution over hypotheses

362
00:20:40,170 --> 00:20:41,480
questions that you have

363
00:20:42,800 --> 00:20:45,250
can usually be expressed as expectations

364
00:20:46,250 --> 00:20:49,770
questions of or what should i believe about this variable

365
00:20:50,520 --> 00:20:51,220
given the data

366
00:20:55,090 --> 00:21:01,010
gentleman what element that someone gives you a nasty distribution they give you some functions whose average values that what

367
00:21:03,200 --> 00:21:04,090
so that's the set up

368
00:21:05,090 --> 00:21:10,250
and now we can forget about clustering because motivated to want to solve these two problems

369
00:21:12,830 --> 00:21:16,310
just in case you don't care about clustering gonna give you one more problem that

370
00:21:16,310 --> 00:21:17,460
can be expressed in this way

371
00:21:19,560 --> 00:21:21,360
well that's why i was right

372
00:21:22,560 --> 00:21:23,340
you you know the problem

373
00:21:24,870 --> 00:21:25,350
so this

374
00:21:25,870 --> 00:21:28,290
the problem of understanding nanomagnets

375
00:21:30,010 --> 00:21:31,850
nanotechnology is really cool

376
00:21:33,370 --> 00:21:34,950
so let's say the magnet

377
00:21:35,780 --> 00:21:36,450
and you've got

378
00:21:37,920 --> 00:21:40,010
area will be energy america

379
00:21:43,200 --> 00:21:46,800
energy depends on the state's oldest remains and unanimous

380
00:21:52,480 --> 00:21:55,100
so let's call x the state-of-the-art nanomagnets

381
00:21:56,020 --> 00:21:57,930
and this is a tiny little minus

382
00:21:58,670 --> 00:22:03,620
made after ten by ten by ten thousand so only

383
00:22:04,150 --> 00:22:05,960
thousands in that tiny

384
00:22:07,800 --> 00:22:08,880
or maybe it's a

385
00:22:08,940 --> 00:22:12,960
plane objective made at that time to use it as a

386
00:22:14,050 --> 00:22:15,120
one thousand students

387
00:22:16,420 --> 00:22:17,430
we've got ourselves

388
00:22:20,560 --> 00:22:21,420
the energy function

389
00:22:23,580 --> 00:22:26,440
is the energy of those things that are in state

390
00:22:28,700 --> 00:22:29,910
what might that look like

391
00:22:30,840 --> 00:22:32,950
well maybe it's a nice simple and

392
00:22:34,170 --> 00:22:34,810
in energy

393
00:22:35,690 --> 00:22:37,660
and all couplings between pairs

394
00:22:44,780 --> 00:22:45,810
it these acts

395
00:22:46,230 --> 00:22:49,290
x is it takes on the value minus one plus one

396
00:22:50,600 --> 00:22:53,150
so at least in space minus one

397
00:22:56,850 --> 00:22:57,880
and binary variables

398
00:23:01,880 --> 00:23:04,680
given the couplings between spins andy

399
00:23:05,170 --> 00:23:06,430
inverse temperature beta

400
00:23:08,100 --> 00:23:09,120
it's a minus beta

401
00:23:14,620 --> 00:23:17,030
divided by normalizing constant which depends on it

402
00:23:19,620 --> 00:23:20,550
the couplings where

403
00:23:21,040 --> 00:23:22,120
these couplings

404
00:23:23,080 --> 00:23:23,730
you all

405
00:23:26,440 --> 00:23:28,180
couplings jay determine energy

406
00:23:28,780 --> 00:23:32,030
in the following so i have a sum over all spins

407
00:23:32,550 --> 00:23:34,860
so some so and things so

408
00:23:35,430 --> 00:23:36,440
so end

409
00:23:39,780 --> 00:23:40,800
the coupling data

410
00:23:41,500 --> 00:23:42,620
prime x and

411
00:23:47,390 --> 00:23:49,880
so that's a fairly standard expression before

412
00:23:51,100 --> 00:23:53,120
the energy of a man with

413
00:23:53,810 --> 00:23:55,610
second order coupling expense

414
00:23:56,780 --> 00:23:59,750
what sort of things that someone wants to know about the nanomagnets

415
00:24:01,070 --> 00:24:03,890
you might want to know what's the average energy

416
00:24:04,320 --> 00:24:10,070
of the things under the distribution take the overall distribution in red

417
00:24:10,680 --> 00:24:11,340
this is red

418
00:24:12,350 --> 00:24:13,440
he is relevant

419
00:24:14,770 --> 00:24:16,580
it's not that's just the sum

420
00:24:19,310 --> 00:24:20,860
got fairly high standards

421
00:24:22,420 --> 00:24:22,930
read this

422
00:24:24,690 --> 00:24:28,690
but is nasty because actually if i tell you okay his the coupling matrix and

423
00:24:28,780 --> 00:24:30,150
say what is the average energy

424
00:24:30,870 --> 00:24:34,590
you'll struggle to answer questions you could try to do a complete enumeration

425
00:24:35,450 --> 00:24:39,430
if you wanted to do like you actually have some overall value

426
00:24:41,350 --> 00:24:42,460
that's referred

427
00:24:47,370 --> 00:24:48,320
the average energy

428
00:24:48,320 --> 00:24:50,090
on but we also we're working

429
00:24:50,130 --> 00:24:51,930
in the same feature space

430
00:24:51,970 --> 00:24:59,010
actually corresponds to the our probabilistic models corresponds to exactly two lin lin model

431
00:24:59,030 --> 00:25:02,590
living instead of in input space living in the feature space

432
00:25:02,600 --> 00:25:04,190
which is

433
00:25:04,200 --> 00:25:07,080
which is which corresponds to this kind

434
00:25:08,200 --> 00:25:09,380
these things are

435
00:25:09,400 --> 00:25:11,880
i extremely close to each other

436
00:25:33,170 --> 00:25:39,450
OK sorry about that so the question is why do i sometimes right

437
00:25:39,460 --> 00:25:40,930
p of star

438
00:25:40,940 --> 00:25:43,170
and sometimes i don't write your star

439
00:25:43,190 --> 00:25:44,540
so this is just sloppy

440
00:25:44,550 --> 00:25:50,220
this is the only ever write down things are probabilities i let p

441
00:25:50,240 --> 00:25:54,880
i apologize for anything that looks as though it could be a probability is the

442
00:26:05,350 --> 00:26:07,620
OK so that was the was the

443
00:26:07,630 --> 00:26:09,260
i mean function

444
00:26:09,290 --> 00:26:13,320
it's a little bit at the at the at the peak of the covariance function

445
00:26:13,320 --> 00:26:14,050
of the

446
00:26:14,130 --> 00:26:16,330
other covariance function

447
00:26:16,340 --> 00:26:18,080
the other term here

448
00:26:18,090 --> 00:26:19,380
then the

449
00:26:19,400 --> 00:26:24,620
the covariance function evaluated at x star and actor itself

450
00:26:24,970 --> 00:26:30,350
minus this this this term here which is the quadratic terms

451
00:26:30,570 --> 00:26:32,190
depending on the

452
00:26:32,200 --> 00:26:35,930
this the vector of covariances between has been put in the training

453
00:26:36,180 --> 00:26:39,300
and the inverse matrix in the middle and this

454
00:26:39,310 --> 00:26:42,120
better again

455
00:26:42,150 --> 00:26:47,040
what this thing that question the brain about what do you mean by a of

456
00:26:47,070 --> 00:26:48,550
text documents are

457
00:26:48,560 --> 00:26:51,010
this is this is just the priors

458
00:26:51,080 --> 00:26:52,650
the variance of the function

459
00:26:52,660 --> 00:26:59,160
this is the variance of the function at x star this is what we what

460
00:27:01,080 --> 00:27:08,340
so if it is invariant because the covariance between identical argument that the variance of

461
00:27:08,350 --> 00:27:14,200
the function without any reference to the training set the prior variance

462
00:27:14,240 --> 00:27:17,100
and then we have the posterior variance

463
00:27:17,110 --> 00:27:20,910
it's going to be the prior variance minus positive for

464
00:27:21,240 --> 00:27:28,010
that means we always if we if we get observations that always reduce the variance

465
00:27:28,130 --> 00:27:32,330
how much will reduce the variance that depends on this term

466
00:27:32,330 --> 00:27:34,580
so it's it's a quadratic form

467
00:27:34,590 --> 00:27:35,810
it depends on the

468
00:27:35,820 --> 00:27:39,710
on the inverse of the covariance matrix evaluated at

469
00:27:39,870 --> 00:27:44,280
all the training input come on the twenty with of all the all pairs of

470
00:27:44,290 --> 00:27:45,980
writing input

471
00:27:45,990 --> 00:27:47,360
this is the noise level

472
00:27:47,370 --> 00:27:51,290
and that the matrix unit this matrix and then

473
00:27:51,300 --> 00:27:57,290
it's depends on the vector of covariances between x starring x

474
00:27:57,310 --> 00:28:01,530
let's say that we have to test input which happens to be very far away

475
00:28:02,930 --> 00:28:04,730
from the training input

476
00:28:04,780 --> 00:28:08,240
the covariance function of the talk about so far the covariance function words e to

477
00:28:08,240 --> 00:28:11,830
the minus distance where between input

478
00:28:12,580 --> 00:28:16,000
so that if the if if they have a large distance between each other and

479
00:28:16,010 --> 00:28:18,240
the grants will be almost zero

480
00:28:18,300 --> 00:28:22,430
i mean is that these terms we always zero these terms will be able to

481
00:28:22,470 --> 00:28:25,690
i mean the whole thing would be almost zero

482
00:28:25,710 --> 00:28:29,510
what does that mean what that means that the posterior variance is almost the same

483
00:28:29,510 --> 00:28:31,160
as the five

484
00:28:31,170 --> 00:28:36,150
it means that the training data didn't tell you very much about what's going on

485
00:28:36,160 --> 00:28:41,200
conversely if you think the largely large covariance between a start next

486
00:28:41,260 --> 00:28:44,400
then this thing can be a sizeable

487
00:28:44,470 --> 00:28:50,160
a sizable contribution and the posterior variance can be much smaller than the prior variance

488
00:28:50,240 --> 00:28:53,680
in the data to a lot of what's going on

489
00:28:54,070 --> 00:29:01,290
and this thing will always remain positive visual words about

490
00:29:01,330 --> 00:29:05,330
OK so that means that we can sort of we can sort of we can

491
00:29:05,330 --> 00:29:08,870
look at this expression and we can sort of understand the they seem to have

492
00:29:09,080 --> 00:29:10,190
a reasonable

493
00:29:14,400 --> 00:29:18,940
up until now i just focused on the on the predictions what about the marginal

494
00:29:18,940 --> 00:29:22,050
likelihood what about it and we and we can compute

495
00:29:22,060 --> 00:29:23,060
what is the

496
00:29:23,070 --> 00:29:27,040
but the probability of the of the of the model and we can try out

497
00:29:27,220 --> 00:29:31,180
different things also matter now of course i want to show you examples with a

498
00:29:31,180 --> 00:29:32,670
particular covariance function

499
00:29:32,680 --> 00:29:34,990
it has a very small properties

500
00:29:35,170 --> 00:29:37,990
so we should worry about what could be my model

501
00:29:38,800 --> 00:29:40,930
interesting function

502
00:29:41,380 --> 00:29:45,460
but now the the the marginal likelihood

503
00:29:45,550 --> 00:29:48,830
is again just the has a thousand four

504
00:29:48,840 --> 00:29:51,040
remember it's the interval of the

505
00:29:51,060 --> 00:29:54,710
the prior times the likelihood people have gotten pawned although

506
00:29:54,710 --> 00:30:00,840
some of the infinite dimensional gaussians come there but then the the all the people

507
00:30:00,860 --> 00:30:04,680
in many variables are marginalized out when you do the modernization

508
00:30:04,710 --> 00:30:06,660
the marginal likelihood

509
00:30:07,370 --> 00:30:11,410
so the so they actually expression you get when you when you do that goes

510
00:30:11,420 --> 00:30:12,260
into the

511
00:30:12,810 --> 00:30:17,680
as the form written not that consists of a are in the log domain of

512
00:30:17,680 --> 00:30:20,200
because it looks pretty

513
00:30:20,240 --> 00:30:22,780
three terms

514
00:30:22,790 --> 00:30:25,600
the first time

515
00:30:26,030 --> 00:30:28,300
i call the data for term

516
00:30:28,320 --> 00:30:31,820
the reason i call that the term is the only term that depend on the

517
00:30:33,330 --> 00:30:37,220
and i can think of the data as being the observations of the of the

518
00:30:37,220 --> 00:30:40,340
function value not the existence of the axes are

519
00:30:40,360 --> 00:30:41,970
consider the

520
00:30:41,980 --> 00:30:43,320
of course

521
00:30:43,330 --> 00:30:44,830
k matrix itself

522
00:30:44,900 --> 00:30:47,930
depends on the on the input

523
00:30:47,950 --> 00:30:52,190
recall that the benefits

524
00:30:52,200 --> 00:30:56,570
and then as the second term here is called complexity penalty

525
00:30:56,680 --> 00:30:58,730
and i had just you

526
00:30:58,750 --> 00:31:01,750
the minute why called the complexity penalty

527
00:31:01,760 --> 00:31:05,240
and then there is a large and that was very boring

528
00:31:08,930 --> 00:31:10,180
the marginal

529
00:31:10,190 --> 00:31:16,680
likelihood that that should somehow tell us something about how to select a different model

530
00:31:16,690 --> 00:31:19,870
essentially consists of these two terms

531
00:31:21,180 --> 00:31:25,730
something to do with how where we're getting the data and one with

532
00:31:25,740 --> 00:31:30,830
it has to do with with the complexity

533
00:31:30,840 --> 00:31:32,350
OK so now

534
00:31:32,870 --> 00:31:37,600
when we want to do learning in processes then

535
00:31:39,710 --> 00:31:40,750
the learning

536
00:31:40,780 --> 00:31:46,170
corresponds to or or involves finding out what the property should be of the underlying

537
00:31:46,170 --> 00:31:51,950
function but if learning amounts to trying to find a good covariance functions

538
00:31:52,300 --> 00:31:55,950
we can talk about you i want to show you one particular variant

539
00:31:56,590 --> 00:32:00,690
but you have to put the covariance function somehow encodes the the properties of the

540
00:32:00,690 --> 00:32:05,460
so this is the situation that we're not really says it is large and we

541
00:32:05,460 --> 00:32:06,900
also use kernels

542
00:32:06,900 --> 00:32:14,230
so that you can also get very huge matrix the first technique you want to

543
00:32:14,230 --> 00:32:15,000
do or you don't

544
00:32:15,000 --> 00:32:20,920
china is of course a subset of this is very simple and often effective just

545
00:32:20,920 --> 00:32:25,170
like clear consensus on when they're going to do you want to try k nearest

546
00:32:26,130 --> 00:32:28,230
founded in classification

547
00:32:28,340 --> 00:32:31,360
but if you have a very large data sets

548
00:32:31,400 --> 00:32:33,030
very large as it

549
00:32:33,900 --> 00:32:36,690
if you find out the training is going to be

550
00:32:36,710 --> 00:32:42,400
very low subsampling is what you wanted to find just try smaller subset in the

551
00:32:44,090 --> 00:32:45,900
so is

552
00:32:45,980 --> 00:32:50,940
and the starting from this we can do all kinds of

553
00:32:50,960 --> 00:32:56,670
more sophisticated techniques but i'm going to show some

554
00:32:56,690 --> 00:32:57,960
this is just one of the

555
00:32:58,420 --> 00:33:04,030
one extension so we use the that you want to go straight find version that

556
00:33:04,030 --> 00:33:10,210
is always more complicated to be useful for your application if you're a citizen that's

557
00:33:10,590 --> 00:33:19,480
the as it is only doing subsampling is that we can we will more

558
00:33:19,840 --> 00:33:23,780
all kinds of approximation techniques but this one is

559
00:33:23,800 --> 00:33:28,590
it's quite simple to increase the with so we say all we can source

560
00:33:28,590 --> 00:33:30,880
we can solve very

561
00:33:30,880 --> 00:33:32,730
that still problem

562
00:33:32,780 --> 00:33:36,070
so why don't we separate the two different parts

563
00:33:36,230 --> 00:33:37,320
for example

564
00:33:38,460 --> 00:33:40,530
he said that they had to ten parts

565
00:33:43,570 --> 00:33:45,270
which for the first part

566
00:33:45,320 --> 00:33:51,500
OK this is only one has already so suppose training is short because the training

567
00:33:51,860 --> 00:33:58,320
training is the reason is that the new company the complexity is not

568
00:33:58,320 --> 00:34:00,050
this usually a hundred

569
00:34:00,050 --> 00:34:00,980
or more

570
00:34:03,090 --> 00:34:10,650
so so training only what is that they actually like

571
00:34:10,670 --> 00:34:18,460
i you only a very small amount of ten of time compared the training hope

572
00:34:18,480 --> 00:34:20,420
so each in the first part

573
00:34:20,510 --> 00:34:23,210
and we obtain a set of support vectors right

574
00:34:23,230 --> 00:34:28,690
but there's property saying that support vectors may be only a small subset

575
00:34:28,800 --> 00:34:34,320
the only a small subset y will we hope that the situation so we collect

576
00:34:34,320 --> 00:34:36,340
support vectors then

577
00:34:36,420 --> 00:34:40,360
we try another which the second part class

578
00:34:40,510 --> 00:34:43,510
the support vectors from the training the first

579
00:34:43,530 --> 00:34:45,420
so this is slightly more

580
00:34:45,480 --> 00:34:47,650
so more but

581
00:34:47,670 --> 00:34:53,800
still only a a little bit more than one test data so from these

582
00:34:53,820 --> 00:34:56,840
we get a set of support vectors

583
00:34:57,320 --> 00:35:01,840
we we call it has to be it all seems to certain part

584
00:35:01,900 --> 00:35:05,500
so it is and you all ten past action

585
00:35:05,510 --> 00:35:09,920
well this is an approximation

586
00:35:09,920 --> 00:35:11,960
it is also quite simple

587
00:35:11,960 --> 00:35:18,110
the problem is you don't know what it is always faced solving

588
00:35:18,130 --> 00:35:22,670
hold your problem you know even for this still problems

589
00:35:23,500 --> 00:35:28,230
the total number of support vectors is so small that you have something to do

590
00:35:28,280 --> 00:35:32,550
is OK there is don't need to use such technique

591
00:35:33,570 --> 00:35:38,440
then there are also some ways of doing this is OK if the number of

592
00:35:38,460 --> 00:35:40,530
days is is so

593
00:35:40,530 --> 00:35:42,110
it is so large

594
00:35:42,170 --> 00:35:48,000
instead of doing the stuff simply by the title a little bit more is to

595
00:35:48,000 --> 00:35:50,090
select some good points

596
00:35:50,110 --> 00:35:54,360
so i only the only each training set of good points

597
00:35:54,360 --> 00:35:57,980
four on the other hand you can say that you are trying to remove some

598
00:35:58,000 --> 00:35:59,530
necessary points

599
00:35:59,570 --> 00:36:06,280
so you get a smaller smaller said please use how can you say him because

600
00:36:06,280 --> 00:36:10,630
kenya is as simple is a list right is first

601
00:36:11,010 --> 00:36:15,650
then you can always use of any other heuristics as selecting these

602
00:36:15,670 --> 00:36:17,800
this is the so-called good points

603
00:36:17,800 --> 00:36:20,010
there are also many papers

604
00:36:20,010 --> 00:36:22,030
i think that's the method

605
00:36:26,070 --> 00:36:26,990
just two

606
00:36:27,180 --> 00:36:33,490
so i think

607
00:36:33,590 --> 00:36:40,620
that is also the question so you can have these classes were members of themselves

608
00:36:40,820 --> 00:36:45,100
and that is where begins to really hurt lot it's not so that if you

609
00:36:45,100 --> 00:36:46,370
have instances

610
00:36:46,390 --> 00:36:51,870
classes class of level one can be instances of classes level two and was able

611
00:36:51,870 --> 00:36:53,910
to convince class level three

612
00:36:53,910 --> 00:36:58,340
and so on this vertical stratification which you could not cross the borders that way

613
00:36:58,340 --> 00:37:03,320
so this is also known three to get clean

614
00:37:03,470 --> 00:37:09,570
and there is a paper by you and you can insist on the stratified first

615
00:37:09,620 --> 00:37:13,090
of all so that is going somewhere in the middle here it's more out

616
00:37:13,840 --> 00:37:15,280
but less than of fools

617
00:37:16,410 --> 00:37:17,640
the more stuff

618
00:37:22,720 --> 00:37:25,410
o three

619
00:37:25,430 --> 00:37:28,090
these are all

620
00:37:30,930 --> 00:37:33,010
but it would be

621
00:37:33,300 --> 00:37:35,120
somewhere in between

622
00:37:35,180 --> 00:37:39,220
so just to give you OK a

623
00:37:39,240 --> 00:37:43,140
this is important comment this has been engineering the proper way so this is really

624
00:37:43,140 --> 00:37:46,490
syntactically layered that if you have a legal

625
00:37:46,510 --> 00:37:52,180
OWL lite statement it's also allow the statements also legal full statement not to around

626
00:37:52,600 --> 00:37:54,720
and it's also semantically probably

627
00:37:54,800 --> 00:38:00,680
so any valid OWL conclusion from the set of light axioms is alive actions also

628
00:38:00,700 --> 00:38:06,010
said about the elections and then conclusion is also valid OWL DL conclusion

629
00:38:06,030 --> 00:38:07,490
and the same level i

630
00:38:07,570 --> 00:38:10,620
so you can in no way so we are great

631
00:38:10,700 --> 00:38:13,010
you can start with a simple language

632
00:38:13,050 --> 00:38:18,070
later on decide you need ritual language or you do you could save you can

633
00:38:18,070 --> 00:38:20,320
also use tools in the same way

634
00:38:20,590 --> 00:38:21,590
so i'll

635
00:38:21,590 --> 00:38:27,470
the tools will do the right thing even if it's only a light expressions

636
00:38:31,470 --> 00:38:38,090
so this is probably the layering is really well done and there is this is

637
00:38:38,090 --> 00:38:39,370
the first WTC

638
00:38:39,370 --> 00:38:42,090
spec with a theorem in the theory says that

639
00:38:42,140 --> 00:38:44,300
is there in really

640
00:38:44,360 --> 00:38:49,760
OK now let's quickly look at these awards in these languages or in these layers

641
00:38:49,890 --> 00:38:53,450
let's do them all three at the same time so this is what we already

642
00:38:53,450 --> 00:39:00,840
seen from ISP subclasses hierarchy individuals domain and range of properties in conjunction conjunction is

643
00:39:00,840 --> 00:39:03,970
just saying that you're a member of two classes at the same time

644
00:39:04,090 --> 00:39:10,240
if is OK i'm human and physical objects or in special cases of of

645
00:39:12,570 --> 00:39:16,260
i'm an academic and father member of two classes

646
00:39:16,800 --> 00:39:18,300
therefore member of

647
00:39:18,320 --> 00:39:21,530
academics who are father the intersection of the two

648
00:39:22,010 --> 00:39:26,160
but that's all you have schema in OWL lite you can

649
00:39:26,300 --> 00:39:27,220
talk about

650
00:39:27,240 --> 00:39:30,240
equality and inequality that's a very big step

651
00:39:30,260 --> 00:39:34,570
various people have said to me well if only i like this idea schema plus

652
00:39:34,570 --> 00:39:41,030
this we would have been during this cardinality restrictions are so saying that something is

653
00:39:41,030 --> 00:39:44,890
at most two were at least forty two property values is restricted to the case

654
00:39:44,910 --> 00:39:50,240
of zero one which is enough to talk about required optional properties as single valued

655
00:39:50,240 --> 00:39:51,700
multivalued properties

656
00:39:51,780 --> 00:39:57,970
so the vocabulary is expressible in just over zero and values you could talk about

657
00:39:57,970 --> 00:40:04,360
inverse transitive symmetric properties stargate example being the author these are domain restrictions

658
00:40:04,370 --> 00:40:10,240
in the sense of of values from a particular domain for a particular property has

659
00:40:10,240 --> 00:40:11,570
to come from this

660
00:40:11,620 --> 00:40:15,950
this particular class if not you have consistency

661
00:40:16,140 --> 00:40:21,870
so that was allvaluesfrom value saying that you have a particular

662
00:40:23,660 --> 00:40:25,340
so that

663
00:40:25,760 --> 00:40:30,930
now you as it is basically a tool for linear algebra

664
00:40:30,970 --> 00:40:36,740
so the negation which is called the class disjunctions junior class you could about arbitrary

665
00:40:37,740 --> 00:40:40,370
not just about zero and one five four two

666
00:40:40,390 --> 00:40:42,340
of forty two

667
00:40:42,340 --> 00:40:48,200
in practice i've seen ontologies for the number two appears right like parents but i

668
00:40:48,200 --> 00:40:49,360
haven't seen any

669
00:40:49,390 --> 00:40:51,950
ontology with the number seven appears very often

670
00:40:52,010 --> 00:40:59,450
and you can define times but enumeration was see example it sometimes it's important it's

671
00:40:59,450 --> 00:41:03,820
impossible to define the clusters by properties how do you define the seven days a

672
00:41:04,720 --> 00:41:07,490
you can only defined by just listing

673
00:41:07,490 --> 00:41:08,850
have such low air

674
00:41:09,960 --> 00:41:12,110
the reason they have such low weight

675
00:41:12,160 --> 00:41:16,460
is because they were correctly classified on the first two rounds of boosting

676
00:41:16,480 --> 00:41:18,730
so we don't really care so much about them

677
00:41:18,740 --> 00:41:20,280
so it makes sense that

678
00:41:20,290 --> 00:41:24,280
they should have low weight and that we should be effectively ignoring them

679
00:41:24,340 --> 00:41:25,990
on the third round of boosting

680
00:41:26,170 --> 00:41:30,850
OK so now we run boosting for three rounds

681
00:41:30,870 --> 00:41:32,640
and the final step

682
00:41:32,690 --> 00:41:37,180
it is to take those three weak classifiers and combine them

683
00:41:37,210 --> 00:41:39,940
and to the final combine classifiers

684
00:41:39,980 --> 00:41:41,580
so here's a to one

685
00:41:41,580 --> 00:41:42,680
h two

686
00:41:42,700 --> 00:41:44,620
and h three

687
00:41:44,620 --> 00:41:47,000
and we wait them by alpha one

688
00:41:47,030 --> 00:41:49,570
alpha two and alpha three

689
00:41:49,580 --> 00:41:54,180
we add them together we think the weighted sum of those weak classifiers

690
00:41:54,200 --> 00:41:56,950
we take the sign with the result is

691
00:41:56,960 --> 00:42:00,680
and that gives us our final combined classifier which turns out to be

692
00:42:00,690 --> 00:42:04,630
this function here which classifies everything in this

693
00:42:05,150 --> 00:42:06,790
upside down l shape

694
00:42:06,820 --> 00:42:08,070
as positive

695
00:42:08,090 --> 00:42:10,000
and everything in the pink region as

696
00:42:11,370 --> 00:42:14,370
so we started out with these three weak classifiers

697
00:42:14,390 --> 00:42:17,800
and each one of them missed three out of the ten examples so each one

698
00:42:17,800 --> 00:42:19,660
hundred thirty percent area

699
00:42:19,670 --> 00:42:21,920
with respect to the original data

700
00:42:21,940 --> 00:42:23,160
but we ended up

701
00:42:23,180 --> 00:42:32,010
in only three rounds with something correctly classifies all ten examples

702
00:42:32,020 --> 00:42:35,570
four questions

703
00:42:37,510 --> 00:42:42,850
what is the u

704
00:42:42,890 --> 00:42:45,840
the point

705
00:42:45,850 --> 00:42:48,200
well because we're missing

706
00:42:48,260 --> 00:42:52,140
it would be classified as negative here negative here

707
00:42:52,180 --> 00:42:55,210
so these two negative for that upper right corner

708
00:42:55,270 --> 00:42:58,240
outweigh the positive for the upper right corner there

709
00:43:00,360 --> 00:43:12,740
that is the half plane take into account the weights

710
00:43:15,410 --> 00:43:18,750
what we're trying to do on each round of boosting is we're trying to

711
00:43:18,770 --> 00:43:21,570
minimize the weighted error rate

712
00:43:21,570 --> 00:43:22,530
OK so

713
00:43:22,550 --> 00:43:28,900
so for instance by the best example of the last last round

714
00:43:28,900 --> 00:43:30,690
where these three examples

715
00:43:30,710 --> 00:43:32,990
i have very low weight

716
00:43:33,040 --> 00:43:37,270
and so were motivated to choose this week classifier because it has very low error

717
00:43:37,270 --> 00:43:38,280
rate because

718
00:43:38,300 --> 00:43:42,080
it's only missing these three examples which have really low error

719
00:43:42,080 --> 00:43:44,660
you would not for instance one to choose

720
00:43:44,670 --> 00:43:46,140
h one again

721
00:43:46,180 --> 00:43:48,440
on this round because if you did

722
00:43:48,450 --> 00:43:53,270
we would be missing these three points which have fairly higher

723
00:43:54,050 --> 00:44:01,150
three using the waiting to guide the selection of the weak classifiers

724
00:44:15,180 --> 00:44:22,930
right right that certainly could happen they could have

725
00:44:23,080 --> 00:44:24,130
high weight

726
00:44:24,710 --> 00:44:29,510
one of the examples and so one of the classifiers can have very high weight

727
00:44:29,520 --> 00:44:31,170
and that could lead to bad

728
00:44:31,180 --> 00:44:34,980
decisions so in principle that could happen is that you're asking

729
00:44:35,030 --> 00:44:37,330
yes in principle that could happen so

730
00:44:37,370 --> 00:44:40,140
it doesn't happen on this contrived example but

731
00:44:40,180 --> 00:44:44,590
the bigger question is what can we say about it in general right

732
00:44:44,630 --> 00:44:48,840
so we want to argue that it doesn't that tends not to happen

733
00:45:08,220 --> 00:45:12,980
right so so the formula so that formula for alpha t is giving more weight

734
00:45:12,980 --> 00:45:16,980
to that classifiers weak classifiers which have low error rate

735
00:45:16,990 --> 00:45:18,200
which is true

736
00:45:18,220 --> 00:45:21,680
but even so a priori it seems like a lot of bad things can happen

737
00:45:21,680 --> 00:45:26,490
with this algorithm so what i want to try to argue next to that

738
00:45:28,090 --> 00:45:32,220
that there are some interesting things we can say about the algorithm

739
00:45:36,380 --> 00:45:41,000
well in this case so we don't really know what

740
00:45:41,030 --> 00:45:45,380
but the question so where in the world of these weak classifiers coming from

741
00:45:45,420 --> 00:45:50,130
so we're leaving that deliberately as a black box so they can take this method

742
00:45:50,130 --> 00:45:52,350
and combined it with any

743
00:45:52,370 --> 00:45:56,380
method that you sitting around for doing that in this particular example

744
00:45:56,430 --> 00:46:00,490
i was always finding the weak classifier of this particular form

745
00:46:00,510 --> 00:46:02,780
which has the lowest weighted area

746
00:46:02,790 --> 00:46:05,470
OK with searching over all of them there are many

747
00:46:05,520 --> 00:46:08,180
and finding the best one

748
00:46:09,630 --> 00:46:15,440
if you keep updating the weights because going the the biggest point example would you

749
00:46:15,440 --> 00:46:17,940
know sometimes chasing out

750
00:46:17,970 --> 00:46:22,500
yes that's the problem so let me come back to that question was about outliers

751
00:46:22,500 --> 00:46:26,270
and does does and this time cause is now a lot of problems with outliers

752
00:46:26,270 --> 00:46:29,940
in this asylum come back to that point

753
00:46:29,950 --> 00:46:31,280
OK good

754
00:46:31,300 --> 00:46:32,400
OK yes o

755
00:46:32,400 --> 00:46:37,440
clearly this is a contrived example we have ten examples in three rounds of boosting

756
00:46:37,460 --> 00:46:39,770
works great on this contrived example

757
00:46:39,790 --> 00:46:41,950
likely say in general

758
00:46:42,600 --> 00:46:44,770
the first natural question is

759
00:46:44,820 --> 00:46:49,970
what can we say about how well it does on the training set itself

760
00:46:49,980 --> 00:46:52,920
so here's what we can prove about this

761
00:46:52,930 --> 00:46:55,600
OK this is the theorem we can prove

762
00:46:55,620 --> 00:46:59,700
so suppose we run the algorithm that i just described to you

763
00:46:59,750 --> 00:47:04,760
and we get these ar plenty on every round of boosting

764
00:47:04,770 --> 00:47:09,070
and then we just rewrite that air raid in a slightly different form

765
00:47:09,070 --> 00:47:10,580
try this thing out

766
00:47:10,630 --> 00:47:14,590
so let me write that area as one-half minus gamma t

767
00:47:14,600 --> 00:47:18,480
remember you just randomly you'll be right exactly half the time

768
00:47:18,500 --> 00:47:22,540
so gamma t is just how much better than random guessing we're dealing with thinking

769
00:47:22,540 --> 00:47:27,210
of gamma t is a small the positive number one percent five percent ten percent

770
00:47:27,210 --> 00:47:29,180
something like that

771
00:47:30,410 --> 00:47:32,180
this is what we can prove

772
00:47:32,200 --> 00:47:34,320
we can prove that the training year

773
00:47:34,320 --> 00:47:37,210
of the final combined classifier

774
00:47:37,230 --> 00:47:38,470
is that most

775
00:47:38,480 --> 00:47:42,850
this kind of airy formula in terms of the one season

776
00:47:42,850 --> 00:47:47,090
which can be re written as this slightly nicer formula in terms of the gamma

777
00:47:48,870 --> 00:47:49,650
in which

778
00:47:49,670 --> 00:47:52,180
using a standard approximation

779
00:47:52,200 --> 00:47:57,150
can be upper bounded by this form which is the former like the best for

780
00:47:57,150 --> 00:47:58,880
in terms of motivating

781
00:48:00,170 --> 00:48:02,640
which says that the training areas that most

782
00:48:02,650 --> 00:48:08,480
e to the minus two times the sum of the squares of the gamma t

783
00:48:08,510 --> 00:48:11,610
OK so how how can we think about that last found

784
00:48:11,610 --> 00:48:16,020
so you popular these high energy and then it would get either here or here

785
00:48:16,040 --> 00:48:18,650
so you can populate these

786
00:48:18,660 --> 00:48:23,820
for example the first spectrum that i showed you on the first day twenty one

787
00:48:23,860 --> 00:48:28,670
so you have this but it very difficult it is to be able to form

788
00:48:29,840 --> 00:48:32,350
so this correspondence between sixty

789
00:48:32,510 --> 00:48:35,290
to spin twenty six

790
00:48:35,310 --> 00:48:40,200
so this has been up to in military one ninety three

791
00:48:41,320 --> 00:48:46,830
concerning fission isomers many predictions have been made in actinides so

792
00:48:46,840 --> 00:48:53,850
fission isomers it appearing i so all the three kv system that are going

793
00:48:53,870 --> 00:48:55,590
the fish and the one

794
00:48:55,600 --> 00:49:00,700
so what is calculated here the energy of the state respect my respect this one

795
00:49:00,900 --> 00:49:03,790
is in in here

796
00:49:03,810 --> 00:49:07,250
you have experimental data the open

797
00:49:07,270 --> 00:49:10,840
compare these experiments the black dots

798
00:49:10,840 --> 00:49:13,790
so first will see these global decrease

799
00:49:13,820 --> 00:49:16,380
of these isomers so this is decreasing

800
00:49:16,390 --> 00:49:19,360
and here you have three to spatial nuclei

801
00:49:19,400 --> 00:49:21,110
where you have

802
00:49:21,130 --> 00:49:24,260
the states which is below this one

803
00:49:24,290 --> 00:49:26,390
so what does it mean

804
00:49:26,400 --> 00:49:27,680
it means that

805
00:49:27,700 --> 00:49:29,120
with this picture

806
00:49:29,130 --> 00:49:34,110
we have predicted that we have to go from ground states of grants the nuclear

807
00:49:34,230 --> 00:49:37,930
which have very huge deformation

808
00:49:37,950 --> 00:49:41,810
but in fact you have to be careful and you have to calculate the lifetime

809
00:49:41,810 --> 00:49:43,230
of the states

810
00:49:43,250 --> 00:49:46,080
and this has been done in fact they don't

811
00:49:46,170 --> 00:49:48,140
very few lifetime

812
00:49:48,160 --> 00:49:52,260
so that's why we cannot say that they states that there

813
00:49:52,280 --> 00:49:56,140
smaller like reason and so they may not survive

814
00:49:56,150 --> 00:49:58,080
as bound states

815
00:49:58,100 --> 00:50:04,010
but that presented that because it's very controversial question has been much discussed

816
00:50:04,040 --> 00:50:05,970
in the community

817
00:50:05,990 --> 00:50:11,700
because superdeformed ground states has been predicted behavior in elements

818
00:50:11,720 --> 00:50:15,820
and in fact in the last ten seems that is not the case

819
00:50:15,820 --> 00:50:18,040
but it's very important because

820
00:50:18,070 --> 00:50:21,900
the superheavy elements they are detected by the flood gates

821
00:50:21,920 --> 00:50:26,560
so i tried to get has no small amount time that fish

822
00:50:26,580 --> 00:50:31,320
but imagine that you have the superdeformed superheavy elements

823
00:50:31,330 --> 00:50:35,110
so then it means that it we've got to feature very easy so then you

824
00:50:35,110 --> 00:50:37,630
have to find something else to detect it

825
00:50:37,650 --> 00:50:39,630
on friday there will not be

826
00:50:39,640 --> 00:50:41,590
a good way to detect it

827
00:50:41,610 --> 00:50:42,810
and also

828
00:50:42,830 --> 00:50:44,750
if you want to produce it

829
00:50:44,770 --> 00:50:47,890
you have to take care of these

830
00:50:47,900 --> 00:50:49,570
so that's why

831
00:50:49,590 --> 00:50:53,190
i think it's an important question now it's concerning the deformation

832
00:50:53,250 --> 00:50:55,350
the superheavy elements

833
00:50:55,410 --> 00:50:59,110
these isomers do also have a role

834
00:50:59,130 --> 00:51:00,820
in vision

835
00:51:00,870 --> 00:51:03,150
what we call resonant transmission

836
00:51:03,170 --> 00:51:07,140
when you have different states in different well if you look at the prose sections

837
00:51:07,140 --> 00:51:08,750
so this is the section

838
00:51:08,770 --> 00:51:11,400
depending on the neutron energy energy

839
00:51:11,440 --> 00:51:14,190
of this here consider the energy of the state here

840
00:51:14,220 --> 00:51:16,310
used in different

841
00:51:16,320 --> 00:51:17,910
and structure

842
00:51:18,000 --> 00:51:22,180
this structure there really are when you have two states

843
00:51:22,210 --> 00:51:24,820
in the different ways of the seminar

844
00:51:24,860 --> 00:51:28,180
it's something resonant tunnelling through a barrier

845
00:51:28,200 --> 00:51:31,490
and the structure are very important when you are

846
00:51:31,850 --> 00:51:35,660
when you want to have a precision for different

847
00:51:38,910 --> 00:51:41,520
now shape coexistence so

848
00:51:41,620 --> 00:51:43,660
it's also related to push

849
00:51:44,180 --> 00:51:47,950
shape isomers but now it's

850
00:51:48,000 --> 00:51:51,870
it's also related to shape so there are many nuclei where

851
00:51:51,870 --> 00:51:54,660
different european states have been observed

852
00:51:55,460 --> 00:51:58,430
they have been observed almost the same energy

853
00:51:58,480 --> 00:52:02,720
so this is the schematic view here where you have zero place here to request

854
00:52:02,720 --> 00:52:04,570
always the same energy

855
00:52:04,580 --> 00:52:08,090
there are associated with two different bands

856
00:52:08,100 --> 00:52:12,070
and you see that they are predicted to be blatant trolling

857
00:52:12,090 --> 00:52:17,730
so the reason for that that you get different gaps for it and already formations

858
00:52:17,750 --> 00:52:19,230
they are in competition

859
00:52:19,250 --> 00:52:24,010
so a situation very complicated where we don't know

860
00:52:24,720 --> 00:52:31,070
if it's spherical prolate coordinates so very precise calculations should be performed

861
00:52:31,070 --> 00:52:35,800
and the experiments so experiments have been done for example here i have been using

862
00:52:35,800 --> 00:52:39,390
spare and so radioactive beams of krypton

863
00:52:39,430 --> 00:52:44,090
they have used call on the citation so that populated many benefits

864
00:52:44,110 --> 00:52:46,680
and then after calculated

865
00:52:46,770 --> 00:52:51,600
the quadrupole moment and they have seen that they have opposite signs so

866
00:52:51,600 --> 00:52:54,190
that means that they have different information

867
00:52:54,190 --> 00:52:58,890
of you who are have some dynamical systems background their applications of these in a

868
00:52:58,890 --> 00:53:04,200
new area in the control community called how systems in which one has a mixture

869
00:53:04,260 --> 00:53:09,230
of dynamical models that which among time and one wants identify

870
00:53:09,240 --> 00:53:11,060
a model for each one of them

871
00:53:13,830 --> 00:53:16,510
let me get back on two

872
00:53:16,530 --> 00:53:19,390
well the mathematics is and try to

873
00:53:19,420 --> 00:53:22,480
describes a little bit what the problem is

874
00:53:23,450 --> 00:53:25,900
one is given a set of points

875
00:53:25,920 --> 00:53:28,490
but in multiple subspaces

876
00:53:28,500 --> 00:53:33,240
the number of subspaces for most of the time is actually going to be known

877
00:53:33,240 --> 00:53:36,460
but it actually can be can be known as well

878
00:53:36,470 --> 00:53:40,710
the dimensions of the subspaces are unknown

879
00:53:40,720 --> 00:53:45,940
the basis for every subspaces and no and the segmentation of the data points is

880
00:53:47,820 --> 00:53:52,210
so this is given the data points in one has to solve for

881
00:53:52,220 --> 00:53:54,420
all these things

882
00:53:56,240 --> 00:53:58,050
this is

883
00:53:58,130 --> 00:54:02,000
and at least this has been considered traditionally chicken and egg problem

884
00:54:02,050 --> 00:54:04,120
if you just think about the

885
00:54:04,540 --> 00:54:07,360
the two lines example that i given here

886
00:54:07,380 --> 00:54:10,420
the venue which point belongs to which line

887
00:54:10,440 --> 00:54:12,560
i could easily defeat one nine

888
00:54:12,580 --> 00:54:14,830
for every

889
00:54:16,040 --> 00:54:18,340
if i knew the equations of the lines

890
00:54:18,350 --> 00:54:19,610
i could easily

891
00:54:19,620 --> 00:54:24,110
cluster the data point by assigning every point to the closest line

892
00:54:24,130 --> 00:54:26,670
the big challenge was that

893
00:54:26,680 --> 00:54:27,950
i don't know the

894
00:54:27,970 --> 00:54:29,300
equations of the line

895
00:54:29,310 --> 00:54:31,680
i don't know the segmentation of the day

896
00:54:31,730 --> 00:54:36,860
so what should i do first

897
00:54:36,880 --> 00:54:38,310
so naturally

898
00:54:38,310 --> 00:54:40,210
this has motivated

899
00:54:40,220 --> 00:54:43,490
a wide variety of iterative algorithms

900
00:54:43,520 --> 00:54:48,110
that's a i don't know which to first so let's say i have some segmentation

901
00:54:48,110 --> 00:54:49,680
that somebody is given to me

902
00:54:49,690 --> 00:54:51,450
given that segmentation

903
00:54:51,460 --> 00:54:54,730
i could solve for the basis for the subspace

904
00:54:54,750 --> 00:54:58,240
given the basis i could resolve the segmentation

905
00:54:58,240 --> 00:55:01,030
and then i could just a back and forth

906
00:55:01,050 --> 00:55:02,750
until it converges

907
00:55:02,770 --> 00:55:07,780
and one can prove that is that it converges and essentially be the ideas behind

908
00:55:07,780 --> 00:55:14,850
these are essentially the same ideas and the k means algorithm for standard classrooms

909
00:55:17,590 --> 00:55:19,180
can also be

910
00:55:19,290 --> 00:55:24,180
done using probabilistic approaches such as the expectation maximisation

911
00:55:24,360 --> 00:55:28,350
which essentially that's exactly the same thing except that there is

912
00:55:28,380 --> 00:55:32,420
a probability underlying probability distribution for each other

913
00:55:32,480 --> 00:55:36,810
group in this case for the noise distribution orthogonal to the subspace

914
00:55:36,860 --> 00:55:41,430
but essentially the idea is the same one starts with a certain prior

915
00:55:41,450 --> 00:55:43,850
of what the segmentation is like

916
00:55:43,860 --> 00:55:49,160
given the prior one solves for the parameters meaning one solves for the basis for

917
00:55:49,160 --> 00:55:54,170
the subspace in an optimal fashion and that's called the maximisation step n given the

918
00:55:54,170 --> 00:55:59,400
parameters one can go back and we compute with the membership of what's the probability

919
00:55:59,420 --> 00:56:02,590
that eight point belongs to one of the groups

920
00:56:02,670 --> 00:56:07,750
and by iterating this procedure again one obtains an optimal solution

921
00:56:07,910 --> 00:56:10,590
the main challenges that

922
00:56:10,600 --> 00:56:12,820
one of these approaches

923
00:56:14,200 --> 00:56:21,750
either optimization based or statistically basically correct and one can prove convergence first the convergence

924
00:56:21,750 --> 00:56:23,930
is very slow

925
00:56:23,950 --> 00:56:26,150
and second and most importantly

926
00:56:26,300 --> 00:56:31,520
although segmentation problems our optimisation problems with

927
00:56:31,520 --> 00:56:33,380
a lot of local minima

928
00:56:33,400 --> 00:56:38,650
so this thing that you're minimizing is not convex and therefore it matters quite a

929
00:56:38,650 --> 00:56:41,350
lot where you start

930
00:56:41,360 --> 00:56:45,550
if you start in the wrong place you may hit the wrong solution

931
00:56:46,790 --> 00:56:51,090
many of these techniques for them to work you have to really do multiple restarts

932
00:56:51,100 --> 00:56:55,850
you start with some random initialisation you iterate you convert and you repeat many many

933
00:56:55,850 --> 00:56:59,000
times until you happy with the final

934
00:56:59,000 --> 00:57:07,480
it is precisely this issue of initilisation what is the main motivation for looking for

935
00:57:07,480 --> 00:57:09,980
alternative approaches there are

936
00:57:10,140 --> 00:57:11,580
more geometric

937
00:57:11,580 --> 00:57:13,980
in a sense if just look at the picture

938
00:57:13,990 --> 00:57:16,550
at least with not a lot of nice

939
00:57:16,570 --> 00:57:18,860
it is rather apparent that

940
00:57:18,890 --> 00:57:23,670
there should be a way of fitting to align to the set of points

941
00:57:23,690 --> 00:57:26,860
i mean i had to do from high school how to ft

942
00:57:26,880 --> 00:57:31,540
aligned to the set of points but how do i fit to simultaneously without knowing

943
00:57:31,550 --> 00:57:33,030
the membership

944
00:57:33,110 --> 00:57:36,320
and just in a purely algebraic way

945
00:57:37,980 --> 00:57:41,340
in nineteen ninety one one

946
00:57:41,350 --> 00:57:45,090
motivated by the computer vision problem of motion segmentation

947
00:57:45,100 --> 00:57:49,850
the first direct solution was introduced in that it was for the case of two

948
00:57:49,850 --> 00:57:52,200
planes in r three

949
00:57:52,210 --> 00:57:54,390
and it was shown that

950
00:57:54,410 --> 00:57:56,870
it can be solved with some

951
00:57:56,900 --> 00:57:59,480
polynomial algebra type techniques

952
00:57:59,490 --> 00:58:02,680
the second technique

953
00:58:02,940 --> 00:58:06,750
and this has been very popular over the past few years in computer vision

954
00:58:06,750 --> 00:58:09,760
is what is called factory station approaches

955
00:58:09,780 --> 00:58:15,090
by factorisation approaches it's meant essentially say the same thing to remember that the solution

956
00:58:15,090 --> 00:58:17,870
to PCA is essentially factoriza matrix

957
00:58:17,900 --> 00:58:20,450
using the value the composition to get

958
00:58:20,570 --> 00:58:21,810
the basis

959
00:58:22,850 --> 00:58:24,670
maybe what we could think

960
00:58:24,760 --> 00:58:29,390
how they have multiple subspaces maybe there is some sort of generalisation of the singular

961
00:58:29,390 --> 00:58:33,310
value decomposition that will allow me to do this

962
00:58:34,430 --> 00:58:37,910
and unfortunately the answer or at least until today

963
00:58:37,930 --> 00:58:39,490
the answer is no

964
00:58:43,590 --> 00:58:47,830
two groups you get tense rather than matrix and so how do you do the

965
00:58:47,830 --> 00:58:54,730
factory station becomes a big problem but in one particular case and this was the

966
00:58:54,730 --> 00:59:00,900
beauty of this approach that was also proposing ninety one in popularizing ninety eight

967
00:59:00,950 --> 00:59:07,300
is that in the particular case which is when the subspaces have equal dimensions

968
00:59:07,300 --> 00:59:08,160
number one

969
00:59:08,190 --> 00:59:12,520
number two there are perpendicular to each other

970
00:59:12,520 --> 00:59:17,210
and number three the intersection is only the origin meaning and this is what i

971
00:59:17,210 --> 00:59:20,530
call here independent subspaces in that particular case

972
00:59:20,550 --> 00:59:25,340
you can solve the segmentation problem using the singular value decomposition of the

973
00:59:25,360 --> 00:59:29,930
but that's not the general case

974
00:59:29,940 --> 00:59:31,680
that's not the case i mean

975
00:59:31,690 --> 00:59:34,840
subspace is not to be able to begin with

976
00:59:34,850 --> 00:59:37,250
they may clearly be intersections

977
00:59:37,270 --> 00:59:40,940
for instance two planes in r three they intersect in a line

978
00:59:41,030 --> 00:59:43,280
they will not just intersecting the origin

979
00:59:43,300 --> 00:59:48,510
and it's a r

980
00:59:48,510 --> 00:59:51,490
the dimensions may be different this one

981
00:59:53,270 --> 00:59:55,830
what i will talk today is precisely

982
00:59:56,700 --> 00:59:59,890
how do you solve this problem analytically

983
00:59:59,890 --> 01:00:02,140
if it receives no destined for

984
01:00:02,140 --> 01:00:03,170
for b

985
01:00:03,180 --> 01:00:07,210
it's going to say OK this is it is going to compute with it's going

986
01:00:07,250 --> 01:00:11,270
presumably wants to send it along whatever the shortest happiness and we'll talk about how

987
01:00:11,280 --> 01:00:15,250
the next step routing is actually deciding which things should be there but for example

988
01:00:15,640 --> 01:00:19,710
it might have helped to the next point towards the center c

989
01:00:19,720 --> 01:00:22,680
it might have one is the next slide so a to b is using it

990
01:00:22,690 --> 01:00:23,750
using l two

991
01:00:23,790 --> 01:00:25,880
and a to see it's using l one

992
01:00:26,800 --> 01:00:28,050
now node

993
01:00:28,120 --> 01:00:30,190
is going to civil wars to throughout the

994
01:00:30,210 --> 01:00:33,090
it's going to have to root through either b or c

995
01:00:33,640 --> 01:00:37,740
and so for now it's just a around one and then it's going to around

996
01:00:37,740 --> 01:00:38,410
to see

997
01:00:38,430 --> 01:00:41,370
it's going to allow c to go in for the pattern the

998
01:00:41,420 --> 01:00:46,190
directed e well we can again you can either have b or c but let's

999
01:00:46,190 --> 01:00:53,900
say maybe wants to round b because has a shorter path so right l two

1000
01:00:57,890 --> 01:01:01,130
so this is just a very simple example and you can see that any time

1001
01:01:01,280 --> 01:01:05,230
a wants to send a packet it has the next top introduce for every destination

1002
01:01:06,640 --> 01:01:10,070
so this is a very simple and this is sort of

1003
01:01:10,090 --> 01:01:12,730
a high level the way before works

1004
01:01:13,050 --> 01:01:16,170
of course there are some other details associated with

1005
01:01:16,230 --> 01:01:17,690
getting forwarding

1006
01:01:17,710 --> 01:01:23,420
to work properly in order to talk about how forwarding actually works at the interaction

1007
01:01:23,420 --> 01:01:25,110
between these lay in the in the

1008
01:01:25,150 --> 01:01:28,640
interaction between these layers what i wanna do is just give you some examples of

1009
01:01:28,640 --> 01:01:33,610
what the packet headers look like four different kinds of particles are used in the

1010
01:01:33,610 --> 01:01:35,130
real world so

1011
01:01:35,170 --> 01:01:38,980
we're talking about here this is the ITV for headers remember i p

1012
01:01:38,990 --> 01:01:42,270
is a protocol that runs at the network layer

1013
01:01:42,290 --> 01:01:45,440
and this header has the following information so

1014
01:01:45,470 --> 01:01:48,910
what i've shown here down the left side is the sequence of

1015
01:01:49,120 --> 01:01:54,230
words so these are thirty two bit thirty thirty two bit words that i just

1016
01:01:54,260 --> 01:01:58,420
broken up the bit by number along the top so

1017
01:01:58,470 --> 01:02:03,400
the first sort of word has some so has information about the version of the

1018
01:02:03,400 --> 01:02:06,350
protocol the running and so in this case the version to be

1019
01:02:06,390 --> 01:02:09,350
four because this ITV for there's new

1020
01:02:09,360 --> 01:02:13,070
IP protocol that is being in the in the process of being deployed called IP

1021
01:02:13,230 --> 01:02:15,770
six which is sometimes you referenced

1022
01:02:15,780 --> 01:02:17,850
there's a header link field

1023
01:02:17,860 --> 01:02:22,340
it specifies the length header there's TOS field is called this type of service is

1024
01:02:22,340 --> 01:02:25,050
typically not used in most packets

1025
01:02:25,060 --> 01:02:27,990
and then there's a packet length which is the entire length of the whole package

1026
01:02:27,990 --> 01:02:32,220
not just like the header so this is fairly should be fairly straightforward except for

1027
01:02:32,220 --> 01:02:34,710
the type of service field which you don't need to

1028
01:02:34,730 --> 01:02:36,860
paying attention to

1029
01:02:36,960 --> 01:02:41,920
so the next field is the there's this

1030
01:02:41,930 --> 01:02:45,390
so next to the idea is this identification the next

1031
01:02:45,390 --> 01:02:48,130
by the next word has identification

1032
01:02:48,140 --> 01:02:53,420
flags and fragment offset let's this look at the next one and the next one

1033
01:02:53,420 --> 01:02:55,910
has time to live and intra college texts

1034
01:03:00,350 --> 01:03:06,520
important field lighting on what the identification field contains which is why i stalling but

1035
01:03:06,520 --> 01:03:09,390
basically the so identification field

1036
01:03:09,430 --> 01:03:13,870
so there's flight so let's just talk about the other fields will come back to

1037
01:03:13,870 --> 01:03:15,680
the addition of our members of the

1038
01:03:15,690 --> 01:03:22,010
the flags field is the flags field simply contains some information about whether this package

1039
01:03:22,010 --> 01:03:22,790
has been

1040
01:03:23,270 --> 01:03:28,360
has been fragmented or should be fragmented so fragmentation something that the package this this

1041
01:03:28,360 --> 01:03:31,810
package can be split into multiples of packets you need to worry about it too

1042
01:03:33,220 --> 01:03:38,230
and the fragment offset says if this has been split into these packets these fragments

1043
01:03:38,230 --> 01:03:41,840
then this is the number of fragments being transmitted

1044
01:03:41,860 --> 01:03:44,890
so they interesting ones that come in the next row so we have the time

1045
01:03:44,890 --> 01:03:50,380
to live flag the end-to-end and protocol checks so that time to live flag is

1046
01:03:50,380 --> 01:03:53,920
the time to live field is sometimes abbreviated ttl

1047
01:03:53,940 --> 01:03:57,230
and this is it's time to live with a little bit of a strange name

1048
01:03:57,230 --> 01:04:01,060
for basically what this says is as the packet is being forwarded to the network

1049
01:04:01,110 --> 01:04:03,610
we're going to decrement the time to live by one

1050
01:04:03,660 --> 01:04:07,460
and every step forward into the network and at the time delivery to zero for

1051
01:04:07,460 --> 01:04:11,330
going to stop for this so the reason we care about i believe is that

1052
01:04:11,330 --> 01:04:15,590
there can sometimes be out there that can sometimes be loops in our foreign graphs

1053
01:04:15,690 --> 01:04:19,950
suppose that we have set this up so they sent messages destined for

1054
01:04:19,990 --> 01:04:25,180
say e to c eighty thousand messages destined three forty per se

1055
01:04:25,190 --> 01:04:30,680
and see also that c sent messages destined through e for a that would be

1056
01:04:30,680 --> 01:04:34,330
a loop that would be a problem so we were trying to avoid forming these

1057
01:04:34,330 --> 01:04:37,940
loops when we run when we do our routing protocol that they're going to be

1058
01:04:37,950 --> 01:04:42,570
certain situations when nodes fail for example the loops can occasionally occur we use the

1059
01:04:42,570 --> 01:04:45,350
time to live field to eliminate those

1060
01:04:45,780 --> 01:04:50,010
the next thing is the and the and particles we and and particles specification of

1061
01:04:52,490 --> 01:04:55,700
next is specification of which and and protocol

1062
01:04:55,770 --> 01:04:58,410
we should forward this message on two

1063
01:04:58,420 --> 01:05:02,660
so i'll talk about that more second and then there's the checksum field which can

1064
01:05:02,710 --> 01:05:06,410
it is the which which can be used to determine whether the message was fully-formed

1065
01:05:06,410 --> 01:05:12,580
although it turns out that in most cases an IP this field probably is used

1066
01:05:12,630 --> 01:05:16,300
so and then there's the source address and destination address so these are IP addresses

1067
01:05:16,310 --> 01:05:19,000
the and identify the end point of the PAC

1068
01:05:19,160 --> 01:05:24,760
and finally there are some additional optional information which

1069
01:05:24,770 --> 01:05:27,750
you can specify a a huge range of different things that can be up to

1070
01:05:27,750 --> 01:05:29,280
forty four bytes in this case

1071
01:05:29,330 --> 01:05:32,780
you don't need to worry about it and then finally there's the payload which is

1072
01:05:32,780 --> 01:05:37,430
the actual message was sent from the and then later into the IP layer

1073
01:05:37,530 --> 01:05:45,300
so what you see happening here if we look at a diagram of

1074
01:05:45,310 --> 01:05:48,700
with the interface between the

1075
01:05:48,710 --> 01:05:53,940
with the interface between the end and layer and the web the interface between the

1076
01:05:53,940 --> 01:05:55,680
network layer the link layer is

1077
01:05:55,720 --> 01:05:58,790
so we have

1078
01:05:58,840 --> 01:06:02,270
are let's just look at our three layer and layer

1079
01:06:02,310 --> 01:06:04,750
we have our network layer

1080
01:06:04,790 --> 01:06:07,220
we have really clear

1081
01:06:07,230 --> 01:06:11,880
and one of the things that we can pick out here is that knows that

1082
01:06:11,880 --> 01:06:15,690
this we have this and the empirical that's actually in our IP packets what this

1083
01:06:15,690 --> 01:06:19,770
means is that if i have a network protocols like IP this running here

1084
01:06:19,770 --> 01:06:21,660
very low-tech

1085
01:06:21,720 --> 01:06:23,590
very high tech

1086
01:06:23,590 --> 01:06:24,960
and i will start

1087
01:06:24,980 --> 01:06:28,100
with myself which is very low-tech

1088
01:06:28,140 --> 01:06:30,590
and that is this

1089
01:06:30,630 --> 01:06:33,380
i was sitting in my office and i said to myself she would like to

1090
01:06:34,910 --> 01:06:37,340
so i took a pencil and i just get

1091
01:06:37,380 --> 01:06:39,610
very roughly

1092
01:06:39,640 --> 01:06:41,330
the sinusoidal

1093
01:06:41,380 --> 01:06:43,640
right here

1094
01:06:43,650 --> 01:06:47,440
and i know according to the solutions you accept

1095
01:06:47,450 --> 01:06:49,830
that these beats these

1096
01:06:49,830 --> 01:06:52,070
particles must lie on that

1097
01:06:54,270 --> 01:06:58,400
and c one is that the choice of the amplitude of the sinusoid

1098
01:06:58,460 --> 01:07:00,140
in the second mode

1099
01:07:00,150 --> 01:07:03,330
you pick another value for forty amplitude stacey two

1100
01:07:03,430 --> 01:07:06,100
then he beats have to rely on that

1101
01:07:06,150 --> 01:07:09,700
sinusoid and so on

1102
01:07:09,710 --> 01:07:11,430
what you will see however

1103
01:07:11,460 --> 01:07:15,280
is that these beats i connected with straight wires

1104
01:07:15,320 --> 01:07:18,340
so you will not see those nice arcs

1105
01:07:18,350 --> 01:07:19,860
what you'll see

1106
01:07:20,120 --> 01:07:22,340
of course this

1107
01:07:22,350 --> 01:07:24,120
the red lines

1108
01:07:24,140 --> 01:07:27,350
are the actual strings

1109
01:07:27,360 --> 01:07:30,840
so for instance if you go to the second

1110
01:07:32,350 --> 01:07:34,720
we call that the second

1111
01:07:34,740 --> 01:07:36,190
monic if you like

1112
01:07:36,190 --> 01:07:38,960
notice that this point you on this point here

1113
01:07:38,960 --> 01:07:42,290
never reached the amplitude c two

1114
01:07:42,350 --> 01:07:43,980
sinusoid does

1115
01:07:44,060 --> 01:07:46,560
and this is true that equation does

1116
01:07:46,580 --> 01:07:49,830
but those points will never reach that because their location

1117
01:07:49,870 --> 01:07:50,870
as such

1118
01:07:50,880 --> 01:07:53,560
but they never make it to the point here

1119
01:07:53,560 --> 01:07:57,100
this what standstill then it was intuitive

1120
01:07:57,190 --> 01:07:59,640
we had that year

1121
01:07:59,670 --> 01:08:01,650
and if you go to higher

1122
01:08:03,000 --> 01:08:05,250
particularly the very highest one

1123
01:08:06,270 --> 01:08:09,560
it's always out of phase which is so you see that

1124
01:08:09,580 --> 01:08:12,060
up down up down up

1125
01:08:12,100 --> 01:08:13,560
and again this little

1126
01:08:13,580 --> 01:08:17,190
article will never reach the amplitude c five

1127
01:08:17,190 --> 01:08:19,170
this one does

1128
01:08:19,210 --> 01:08:20,730
this one does not

1129
01:08:20,790 --> 01:08:25,900
this one does not this one does not this one but this one does

1130
01:08:25,920 --> 01:08:29,350
so as i'm going to show you the simulation we will keep this going

1131
01:08:29,480 --> 01:08:31,040
it will be great

1132
01:08:32,120 --> 01:08:33,540
this debate

1133
01:08:33,560 --> 01:08:37,270
what we may be seeing

1134
01:08:37,310 --> 01:08:39,980
so the first thing and going going to show you

1135
01:08:42,140 --> 01:08:46,100
one complete oscillation in the normal mode number one

1136
01:08:46,120 --> 01:08:49,190
which i have said to be fifteen

1137
01:08:49,210 --> 01:08:50,830
second which they help

1138
01:08:50,850 --> 01:08:52,600
you're just model all

1139
01:08:53,230 --> 01:08:54,900
has guided me greatly

1140
01:08:54,900 --> 01:08:55,870
in this

1141
01:08:57,400 --> 01:08:59,640
so let me

1142
01:08:59,670 --> 01:09:01,600
first of all

1143
01:09:01,690 --> 01:09:03,350
give was the right

1144
01:09:03,400 --> 01:09:05,330
light conditions

1145
01:09:05,350 --> 01:09:07,650
and you know i will start

1146
01:09:07,730 --> 01:09:09,960
the last fifteen seconds

1147
01:09:10,000 --> 01:09:12,810
i have given the amplitude

1148
01:09:12,940 --> 01:09:17,370
two which is very large and of course it's unrealistic these high and it but

1149
01:09:17,370 --> 01:09:19,020
i wanted to see

1150
01:09:19,330 --> 01:09:29,040
the relative position of these particles

1151
01:09:29,100 --> 01:09:30,750
there we go

1152
01:09:30,790 --> 01:09:36,900
you all seem to make one complete oscillation

1153
01:09:37,020 --> 01:09:39,710
that of course is no surprise

1154
01:09:39,750 --> 01:09:44,710
i think only one will stop now and will be exactly twice will start thank

1155
01:09:44,900 --> 01:09:46,640
you don't want

1156
01:09:48,330 --> 01:09:51,310
i'm going to show you the second one

1157
01:09:51,370 --> 01:09:55,290
and i give the same amplitude so c two i give it to

1158
01:09:55,290 --> 01:09:57,620
and i want you to count

1159
01:09:57,620 --> 01:10:02,330
how many oscillations it makes before it comes to stop

1160
01:10:02,370 --> 01:10:05,080
well can be exactly fifteen seconds

1161
01:10:05,080 --> 01:10:06,810
so we'll have to agree

1162
01:10:06,830 --> 01:10:11,040
that the number of oscillations that makes is now one point nine

1163
01:10:11,140 --> 01:10:14,210
missus the two you will not get back to the

1164
01:10:14,210 --> 01:10:15,880
the complete also

1165
01:10:15,940 --> 01:10:17,790
so you're going to look at

1166
01:10:18,000 --> 01:10:20,440
this mode

1167
01:10:20,440 --> 01:10:24,640
so these two particles will never reach the value two

1168
01:10:24,650 --> 01:10:26,900
two is marked here

1169
01:10:27,830 --> 01:10:29,400
it will go down

1170
01:10:32,690 --> 01:10:33,670
down and

1171
01:10:33,730 --> 01:10:37,810
stop just short of two also

1172
01:10:37,810 --> 01:10:40,310
so that no

1173
01:10:40,350 --> 01:10:42,560
make sure

1174
01:10:42,580 --> 01:10:44,980
the first one zero

1175
01:10:45,040 --> 01:10:47,350
now we get the features

1176
01:10:47,370 --> 01:10:49,580
the two

1177
01:10:49,600 --> 01:10:51,330
i mean the two

1178
01:10:51,350 --> 01:10:57,500
that we will not count

1179
01:10:57,560 --> 01:11:02,830
one also relation

1180
01:11:03,810 --> 01:11:06,810
well just short of two you can tell that of course

1181
01:11:06,870 --> 01:11:11,600
because you don't have that resolution chop stop just short of two

1182
01:11:11,600 --> 01:11:13,480
now we go into this one

1183
01:11:13,500 --> 01:11:17,230
amplitude of two

1184
01:11:17,290 --> 01:11:19,000
and now we're going to come

1185
01:11:19,040 --> 01:11:22,080
and you will definitely be able to see

1186
01:11:22,140 --> 01:11:26,790
it just missus two point seven five because two point seven five

1187
01:11:26,850 --> 01:11:29,600
it's something that you can ibl

1188
01:11:29,650 --> 01:11:31,460
so we're going to number

1189
01:11:31,520 --> 01:11:33,350
three now

1190
01:11:33,400 --> 01:11:34,650
number three

1191
01:11:34,650 --> 01:11:36,020
it already has

1192
01:11:36,060 --> 01:11:39,230
two interesting points which don't move at all

1193
01:11:39,270 --> 01:11:40,290
this point one move

1194
01:11:40,420 --> 01:11:41,790
that point one

1195
01:11:41,830 --> 01:11:50,560
never reach the plus still in the plus two

1196
01:11:52,480 --> 01:11:55,140
and the other two

1197
01:11:55,150 --> 01:12:02,670
there we go

1198
01:12:02,770 --> 01:12:07,560
this one

1199
01:12:07,580 --> 01:12:10,040
it's too

1200
01:12:10,140 --> 01:12:12,250
two half

1201
01:12:12,670 --> 01:12:18,310
and that's the point seven three just short of two point seven five

1202
01:12:18,420 --> 01:12:21,040
there's this number

1203
01:12:21,140 --> 01:12:23,440
now we're going to number four

1204
01:12:23,440 --> 01:12:25,960
number before this one will stand still

1205
01:12:26,000 --> 01:12:26,940
many others

1206
01:12:26,960 --> 01:12:37,190
do not have the maximum amplitude of two

1207
01:12:42,980 --> 01:12:47,940
we go

1208
01:13:00,730 --> 01:13:03,500
three point three five

1209
01:13:03,540 --> 01:13:04,920
the last one

1210
01:13:04,960 --> 01:13:06,940
before we go into cocktail the

1211
01:13:07,170 --> 01:13:09,140
this one

1212
01:13:09,310 --> 01:13:12,060
again this point will be close to

1213
01:13:12,120 --> 01:13:13,750
this one will never reach that

1214
01:13:13,750 --> 01:13:24,620
this will lead to cluster

1215
01:13:32,620 --> 01:13:35,210
it doesn't make it to get high

1216
01:13:40,620 --> 01:13:42,290
three and a half

1217
01:13:42,370 --> 01:13:43,440
it's not

1218
01:13:43,500 --> 01:13:46,420
just on the three point five watts

1219
01:13:46,420 --> 01:13:49,720
lovely to see you all back again

1220
01:13:49,730 --> 01:13:53,570
i'm sorry about missing the sunshine

1221
01:13:53,580 --> 01:13:56,020
but you know serious things to be done

1222
01:13:56,040 --> 01:14:00,600
the last time we were talking about

1223
01:14:00,620 --> 01:14:01,760
how to make

1224
01:14:01,770 --> 01:14:07,660
active contour filters that statistical filters working in a space of curves

1225
01:14:07,700 --> 01:14:09,990
and we had a bit of digression

1226
01:14:10,000 --> 01:14:11,080
in two

1227
01:14:11,090 --> 01:14:13,430
filtering with points just

1228
01:14:13,450 --> 01:14:17,200
people who haven't come across kalman filters before to see

1229
01:14:18,440 --> 01:14:21,710
basic things go on in the kalman filter but now

1230
01:14:21,760 --> 01:14:26,300
we want to go back to that space of curves again and see how filtering

1231
01:14:26,300 --> 01:14:29,270
works back in the space of curves

1232
01:14:32,290 --> 01:14:34,640
you may remember when

1233
01:14:35,120 --> 01:14:40,550
what you remember when we were doing the recursive algorithm for fitting curves there was

1234
01:14:40,550 --> 01:14:46,020
a regularisation matrix s north terrace bar i think it was called

1235
01:14:46,030 --> 01:14:48,860
and i said the the time of that looks a little bit like

1236
01:14:48,870 --> 01:14:54,930
the term in a prior distribution over curves so now we want to go back

1237
01:14:54,930 --> 01:14:56,160
to that

1238
01:15:02,090 --> 01:15:04,920
so now we want to really think explicitly that

1239
01:15:04,970 --> 01:15:10,610
in this shape space parameterized by catholics that we have gas distribution

1240
01:15:10,620 --> 01:15:12,260
let's say

1241
01:15:12,270 --> 01:15:16,190
better began as in otherwise we we start with the kalman filter

1242
01:15:16,200 --> 01:15:19,650
over the curves with min x by

1243
01:15:19,660 --> 01:15:23,110
and covariance p not

1244
01:15:28,820 --> 01:15:30,690
actually this

1245
01:15:30,770 --> 01:15:34,060
so i i could sort of set up such a distribution and i could sample

1246
01:15:34,060 --> 01:15:38,020
around random from the distribution let's say in the euclidean similarity space which was to

1247
01:15:38,020 --> 01:15:42,730
run the full degree of freedom space with the translations rotations and scalings and phi

1248
01:15:42,780 --> 01:15:46,320
sample random from this template shape

1249
01:15:47,070 --> 01:15:52,120
for distribution with this mean shape and i get

1250
01:15:52,330 --> 01:15:57,550
sample bottles like this at different orientations and scales and positions and so on

1251
01:16:00,410 --> 01:16:01,470
actually this

1252
01:16:01,550 --> 01:16:04,340
it's probably worth mentioning

1253
01:16:04,380 --> 01:16:09,050
that given what you've been doing during the week that this is a guassian process

1254
01:16:09,050 --> 01:16:11,410
it implies a guassian process over

1255
01:16:11,420 --> 01:16:14,680
over curves in some ways it's a bit of

1256
01:16:14,730 --> 01:16:17,010
a crude way to do

1257
01:16:17,030 --> 01:16:18,850
guassian process

1258
01:16:18,870 --> 01:16:23,120
so actually what we have is

1259
01:16:23,130 --> 01:16:25,200
if i only neglect the mean of this

1260
01:16:28,940 --> 01:16:31,330
and then what we got is

1261
01:16:31,370 --> 01:16:36,700
covariance over you know across two points on the curve if you like which has

1262
01:16:36,700 --> 01:16:38,240
the form

1263
01:16:40,760 --> 01:16:42,840
this is

1264
01:16:42,850 --> 01:16:45,300
an exciting experiment from the net

1265
01:16:45,320 --> 01:16:47,420
i heard that little cool people do this

1266
01:16:50,940 --> 01:16:54,080
this is this is

1267
01:16:54,110 --> 01:16:55,760
more interesting than ever

1268
01:16:57,090 --> 01:17:02,290
it's pretty isn't it i think

1269
01:17:02,310 --> 01:17:06,530
i was a practising this last night and this didn't happen but you read the

1270
01:17:06,630 --> 01:17:08,750
dark blue on black writing

1271
01:17:16,750 --> 01:17:18,650
very excited

1272
01:17:18,730 --> 01:17:26,000
so anyway the point is we implied a marriage

1273
01:17:26,010 --> 01:17:28,910
is this what happens when people do this

1274
01:17:28,950 --> 01:17:33,560
well the least i won't send a message to microsoft

1275
01:17:34,020 --> 01:17:35,700
miss out on that

1276
01:17:38,330 --> 01:17:41,530
i got some very exciting new technology over here

1277
01:17:45,110 --> 01:17:46,480
i technology

1278
01:17:46,510 --> 01:17:48,470
i can't actually see whether ten is

1279
01:17:48,490 --> 01:17:49,460
this is an

1280
01:17:49,520 --> 01:17:54,890
this is this is a valid and

1281
01:17:56,270 --> 01:18:00,230
OK yes so what i actually was attempting to write and who knows what will

1282
01:18:00,230 --> 01:18:02,890
ever get a powerpoint presentation back again

1283
01:18:03,390 --> 01:18:06,490
was that we've implied

1284
01:18:06,660 --> 01:18:12,240
forgetting about the means setting the mean zero something moment just sort of oil mess

1285
01:18:12,490 --> 01:18:18,840
we can apply to guassian process of this kind this is the the covariance function

1286
01:18:18,850 --> 01:18:21,180
that this parametric

1287
01:18:21,190 --> 01:18:23,720
guassian implies and actually

1288
01:18:23,730 --> 01:18:25,910
if we were free to to

1289
01:18:25,960 --> 01:18:30,250
design covariance functions that in some ways would be more appealing and maybe you guys

1290
01:18:30,530 --> 01:18:34,290
really know how to do this now i'm not sure if the whole thing

1291
01:18:34,300 --> 01:18:36,980
will carry through because the

1292
01:18:36,980 --> 01:18:41,320
only a function of the difference between those two arguments so that's that's that's often

1293
01:18:41,320 --> 01:18:42,720
green tea i'm honesty jay

1294
01:18:43,180 --> 01:18:45,770
or just just just kate cave tau

1295
01:18:47,050 --> 01:18:49,550
and you can see that here in the squared exponential

1296
01:18:50,020 --> 01:18:51,350
so in in a picture

1297
01:18:51,950 --> 01:18:53,510
what is a stationary kernel

1298
01:18:53,890 --> 01:18:55,370
essentially you'll notice at

1299
01:18:56,100 --> 01:18:59,670
it doesn't really matter where i am over time the prior looks the same no

1300
01:18:59,670 --> 01:19:03,780
matter where i am in time the only thing that matters is is is that

1301
01:19:03,890 --> 01:19:07,800
the difference between time points how much how much how much variance there's between time

1302
01:19:11,070 --> 01:19:13,630
there are also plenty of nonstationary kernel so

1303
01:19:14,620 --> 01:19:18,640
one one on one of my favorite ones that a using example is is that

1304
01:19:18,640 --> 01:19:20,620
we process is a standard brownian motion

1305
01:19:22,690 --> 01:19:23,450
we remember has

1306
01:19:24,580 --> 01:19:27,710
this infinite limit of eighty of a random walk

1307
01:19:28,440 --> 01:19:29,210
so that's

1308
01:19:30,000 --> 01:19:31,540
has the following that's a jeep

1309
01:19:32,030 --> 01:19:33,480
and has the following kernel function

1310
01:19:34,680 --> 01:19:38,360
which is the the the mean of other two time points and so you can

1311
01:19:38,360 --> 01:19:40,200
see this can be written in stationary way

1312
01:19:40,610 --> 01:19:42,130
as the difference between these two time points

1313
01:19:43,530 --> 01:19:44,990
this is still a jeep e

1314
01:19:45,180 --> 01:19:45,510
and so

1315
01:19:45,940 --> 01:19:46,820
here's the pryr

1316
01:19:48,010 --> 01:19:51,300
here's the prior in winter process so you can see what's happening

1317
01:19:52,350 --> 01:19:55,860
is there these variances growing linearly over time

1318
01:19:56,530 --> 01:19:59,210
again this is standard deviation so that's what looks like a square root

1319
01:20:00,540 --> 01:20:05,720
variance grows linearly over time and so this is this is nonstationary because what's happening

1320
01:20:05,720 --> 01:20:10,680
at time zero war small time the space that you can explorer that you expect

1321
01:20:10,680 --> 01:20:13,810
to be exploring is quite a bit smaller than when you further out here

1322
01:20:15,430 --> 01:20:19,320
so again in the same way we can just take draws from this nonstationarity

1323
01:20:19,940 --> 01:20:21,770
and this looks like familiar draws

1324
01:20:22,200 --> 01:20:26,010
a f of other particle diffusing or something like this which is all which is

1325
01:20:26,010 --> 01:20:27,430
all very familiar from brownian motion

1326
01:20:30,190 --> 01:20:34,850
another interesting non stationary kernel which i think is important connected because if i

1327
01:20:34,900 --> 01:20:35,960
because evolve e

1328
01:20:37,150 --> 01:20:39,180
all familiar with linear linear kernels

1329
01:20:39,620 --> 01:20:40,090
so now

1330
01:20:40,630 --> 01:20:41,320
let's say that

1331
01:20:42,170 --> 01:20:45,480
let's say that were saying that are regression function fifty

1332
01:20:47,600 --> 01:20:48,580
a linear function of tea

1333
01:20:49,130 --> 01:20:51,210
with some random random weight w

1334
01:20:53,540 --> 01:21:00,050
then there will evaluate out if you evaluate this kernel you've after university i then evaluates out the following

1335
01:21:00,500 --> 01:21:01,770
and there is this

1336
01:21:02,350 --> 01:21:03,300
growing variance

1337
01:21:03,900 --> 01:21:07,460
variance on envelope and you can see then when we take draws from this kernel

1338
01:21:08,790 --> 01:21:10,400
you just get you just get straight lines

1339
01:21:13,490 --> 01:21:14,120
okay so

1340
01:21:16,320 --> 01:21:17,390
building our own kernels

1341
01:21:18,380 --> 01:21:19,070
so we've seen

1342
01:21:19,680 --> 01:21:20,740
a handful of examples

1343
01:21:21,220 --> 01:21:25,010
and there are a number of operations that we can use that will be helpful

1344
01:21:25,150 --> 01:21:28,140
in building a kernel why would you want a billion kernel

1345
01:21:28,740 --> 01:21:33,240
because in your particular application area you collect this this specific type of data and you say

1346
01:21:33,820 --> 01:21:34,570
i really like

1347
01:21:35,490 --> 01:21:38,860
i really like some of this feature in some this feature and i'd like to

1348
01:21:38,860 --> 01:21:41,690
be able to design my own kernel that's model appropriate

1349
01:21:43,130 --> 01:21:47,140
so the first thing is that kernels are linear we saw this when we took

1350
01:21:47,720 --> 01:21:48,720
one which are latent

1351
01:21:50,080 --> 01:21:52,630
half an hour noise and we add them together

1352
01:21:53,060 --> 01:21:57,980
we add those two gy piece together and again got a teepee with covariance with

1353
01:21:57,980 --> 01:21:59,550
the kernel function as the sum of these two

1354
01:22:00,440 --> 01:22:03,620
it is linear in other words you can add you can add in scale these

1355
01:22:03,620 --> 01:22:05,800
two as long as you respect positive scale cause

1356
01:22:06,920 --> 01:22:09,350
you can add to kernels together to get another valid kernel

1357
01:22:11,800 --> 01:22:13,330
what is interesting is that here

1358
01:22:14,460 --> 01:22:17,360
we're talking about adding kernels on on on time

1359
01:22:17,760 --> 01:22:19,900
but if we look at a multi dimensional case

1360
01:22:21,100 --> 01:22:21,640
we've got

1361
01:22:22,470 --> 01:22:27,280
you know latitude longitude we can add kernels better we can we can compose

1362
01:22:28,170 --> 01:22:34,560
a multidimensional kernel by adding kernels on each dimension individually so we can have a single kernel latitude

1363
01:22:35,060 --> 01:22:39,580
i think we should be you know just on x one this single-dimensional single dimensional

1364
01:22:39,580 --> 01:22:42,680
kernel on launched to we can add those two together and i will give us

1365
01:22:43,120 --> 01:22:44,420
a multidimensional kernel

1366
01:22:46,530 --> 01:22:48,630
the products of kernels are again kernels

1367
01:22:52,650 --> 01:22:58,290
integration gives you a kernel for filtering context so if you have here is a cheap e

1368
01:22:58,410 --> 01:23:00,570
and we've got some fixed function gene duty

1369
01:23:01,090 --> 01:23:04,520
and we integrate across it again this is just this is just a linear operation

1370
01:23:05,970 --> 01:23:06,810
then set of tea

1371
01:23:08,380 --> 01:23:08,960
that gives us so

1372
01:23:09,490 --> 01:23:12,160
that's also a cheapy and has the following kernel

1373
01:23:13,770 --> 01:23:15,260
this is just the standard result from

1374
01:23:15,820 --> 01:23:18,170
b filtering or signal processing

1375
01:23:21,200 --> 01:23:28,140
differentiation again later operation and so this again gives us this again gives the cheap so effort a cheap e

1376
01:23:28,190 --> 01:23:31,220
if we take the derivative at over time called at z eighty

1377
01:23:31,800 --> 01:23:33,470
then kesey if ti i ti jay

1378
01:23:34,020 --> 01:23:34,510
is again

1379
01:23:35,100 --> 01:23:36,470
sorry as it is again

1380
01:23:36,900 --> 01:23:38,610
the jeep e with the following kernel

1381
01:23:42,580 --> 01:23:44,830
one thing is not discussed has as much

1382
01:23:46,000 --> 01:23:51,590
is is warping and this comes up in again in geostatistics or the court spatial spatial defamation

1383
01:23:52,740 --> 01:23:54,420
but we've talked a lot about

1384
01:23:55,420 --> 01:23:59,240
regression in time or this this fixed input space to u x

1385
01:24:01,430 --> 01:24:05,730
you can also impose a function in between there are two two two toward your input space

1386
01:24:07,250 --> 01:24:09,470
such that if you've got some cheapy half

1387
01:24:10,000 --> 01:24:11,780
which is usually if you know fonte

1388
01:24:12,290 --> 01:24:15,790
you can make it for age fifty as well and i will again be a jeep

1389
01:24:16,620 --> 01:24:18,230
something nice about this is that

1390
01:24:18,580 --> 01:24:19,720
because you've

1391
01:24:20,960 --> 01:24:23,280
because you have a fixed kernel the kernel is

1392
01:24:23,960 --> 01:24:24,660
the kernel is

1393
01:24:25,130 --> 01:24:26,100
positive semi definite

1394
01:24:27,500 --> 01:24:30,640
in it doesn't care about what the inputs are and so you can you can

1395
01:24:30,640 --> 01:24:33,100
mess around quite a bit more with what the input warping

1396
01:24:34,630 --> 01:24:36,920
in the periodic kernel that we looked at before r

1397
01:24:37,110 --> 01:24:40,830
you could look it that's almost as a squared exponential kernel where we've done some

1398
01:24:41,090 --> 01:24:43,940
warping input space on to on a periodic basis

1399
01:24:46,840 --> 01:24:52,880
one thing is cool about this i want to point out is that many of these operations preserve joint gaussianity

1400
01:24:53,600 --> 01:24:57,400
what i mean say instead let's look at this linear case where we took why

1401
01:24:57,980 --> 01:24:59,390
equals at f

1402
01:24:59,470 --> 01:25:00,460
plus noise

1403
01:25:01,850 --> 01:25:04,490
the respects to a joint gaussianity between all these

1404
01:25:04,980 --> 01:25:09,060
so that is why and at and why in an effort and they all remain

1405
01:25:09,060 --> 01:25:14,030
jointly guassian that's true linearity that's not true of products but that's true love

1406
01:25:14,720 --> 01:25:18,840
all of these other features so differentiation is is an interesting example

1407
01:25:19,230 --> 01:25:21,210
so let's take a cheapy on at

1408
01:25:21,700 --> 01:25:26,890
end we we see we have this this this this cheapy on the derivative at which recalling see

1409
01:25:28,320 --> 01:25:32,600
well it's an interesting fact and that these are still jointly guassian with one another

1410
01:25:33,130 --> 01:25:37,550
then for still jointly guassian one another and so that gives us this nice opportunity to

1411
01:25:38,380 --> 01:25:39,500
for example if we measure

1412
01:25:39,960 --> 01:25:43,530
both the function and its derivative we can still use energy to modeling context

1413
01:25:47,510 --> 01:25:51,250
another way to build your own kernel is via the frequency domain

1414
01:25:53,640 --> 01:25:55,170
we talk about stationary kernels

1415
01:25:56,530 --> 01:25:57,890
so stationary kernel

1416
01:25:58,930 --> 01:26:00,230
is positive semi definite

1417
01:26:00,230 --> 01:26:03,490
if i have categories

1418
01:26:03,530 --> 01:26:10,280
the generic category of paper i have journal papers conference papers and workshop papers

1419
01:26:10,280 --> 01:26:12,440
then potentially what i can do

1420
01:26:13,120 --> 01:26:15,230
i can go through and say

1421
01:26:17,200 --> 01:26:18,910
i can have the different

1422
01:26:18,930 --> 01:26:23,680
conditional probability distribution for the acceptance of the workshop paper

1423
01:26:23,680 --> 01:26:24,820
rather than

1424
01:26:24,840 --> 01:26:26,110
the acceptance

1425
01:26:26,860 --> 01:26:28,280
conference paper

1426
01:26:29,340 --> 01:26:32,450
are so generic paper so

1427
01:26:32,510 --> 01:26:33,550
for these

1428
01:26:33,560 --> 01:26:36,630
i would come along and

1429
01:26:36,650 --> 01:26:38,340
find the lowest level

1430
01:26:38,340 --> 01:26:39,740
and this

1431
01:26:40,690 --> 01:26:43,800
where there is distribution defined

1432
01:26:43,870 --> 01:26:46,660
and be able to use that for

1433
01:26:48,370 --> 01:26:52,150
the conditional probability of whether the papers accepted

1434
01:26:52,200 --> 01:26:55,160
now four

1435
01:26:55,190 --> 01:26:58,130
learning the class hierarchies the

1436
01:26:58,150 --> 01:26:59,660
issue is we

1437
01:26:59,660 --> 01:27:00,920
potentially have

1438
01:27:00,940 --> 01:27:05,010
first really observable data

1439
01:27:05,510 --> 01:27:09,840
and the approach

1440
01:27:11,010 --> 01:27:12,430
again too

1441
01:27:12,440 --> 01:27:13,990
build the decision tree

1442
01:27:16,040 --> 01:27:17,400
the training set

1443
01:27:18,700 --> 01:27:21,440
we have

1444
01:27:21,460 --> 01:27:24,240
attributes which

1445
01:27:24,290 --> 01:27:27,130
flip on

1446
01:27:31,180 --> 01:27:32,940
class attribute

1447
01:27:33,020 --> 01:27:39,960
and again we can split on

1448
01:27:39,980 --> 01:27:41,660
attributes of

1449
01:27:44,090 --> 01:27:46,540
entities second but on

1450
01:27:47,620 --> 01:27:50,350
papers authors fame

1451
01:27:50,380 --> 01:27:52,880
and get out these classes

1452
01:27:53,070 --> 01:27:55,430
this is the hidden variable they're going to be

1453
01:27:55,820 --> 01:28:00,790
need to be inferred potentially

1454
01:28:00,850 --> 01:28:02,600
when i apply this model

1455
01:28:02,930 --> 01:28:08,890
at least here what we did was defined over the attributes observed in the training

1456
01:28:08,890 --> 01:28:11,170
set which are the attributes

1457
01:28:11,530 --> 01:28:14,670
entities and attributes

1458
01:28:15,000 --> 01:28:18,730
other entities you could also do something worrying doing more for clustering

1459
01:28:18,750 --> 01:28:21,320
approach that's not what we did here

1460
01:28:24,100 --> 01:28:26,290
this is the way to

1461
01:28:28,200 --> 01:28:29,410
we're fine

1462
01:28:29,420 --> 01:28:30,380
kind of

1463
01:28:30,410 --> 01:28:33,440
more abstract heterogeneous classes into

1464
01:28:33,500 --> 01:28:36,060
potentially more coherent subclasses

1465
01:28:36,070 --> 01:28:39,600
and refine the probabilistic model along the path

1466
01:28:40,440 --> 01:28:44,410
we can start off with something that very abstract

1467
01:28:44,450 --> 01:28:46,040
and refine it

1468
01:28:46,040 --> 01:28:49,260
and potentially in the places where we need to go

1469
01:28:50,130 --> 01:28:51,260
two level

1470
01:28:51,260 --> 01:28:54,320
of instances

1471
01:28:56,350 --> 01:28:57,200
OK so

1472
01:28:57,660 --> 01:29:03,060
that was a whirlwind tour of directed frame based approaches

1473
01:29:03,170 --> 01:29:06,380
i hope that you could see

1474
01:29:06,390 --> 01:29:09,110
in many cases can the parallel

1475
01:29:09,130 --> 01:29:10,810
ideas involved

1476
01:29:10,820 --> 01:29:17,470
the rule base directed approaches and the frame based on approaches and get a flavor

1477
01:29:17,470 --> 01:29:23,350
for the inference and learning algorithms and potentially the types of objects are

1478
01:29:23,390 --> 01:29:26,330
take the task you would want to solve with these

1479
01:29:28,380 --> 01:29:39,510
questions here about the directed approaches

1480
01:29:40,420 --> 01:29:49,750
everybody structures of that we get that

1481
01:29:51,070 --> 01:29:52,010
i can here

1482
01:29:59,720 --> 01:30:00,880
OK so

1483
01:30:01,060 --> 01:30:06,140
the question was you are curious about the other approaches based on

1484
01:30:06,200 --> 01:30:12,200
my probably misnomer the programming languages approaches of first i am going to go over

1485
01:30:12,200 --> 01:30:18,820
the undirected approaches which is another major category and then they say something about the

1486
01:30:18,820 --> 01:30:23,450
programming approaches to programming approach is what i'm capturing under the problem

1487
01:30:23,510 --> 01:30:28,650
which i don't know anybody works on them and is wincing an idea and i

1488
01:30:28,710 --> 01:30:30,900
are both kind

1489
01:30:30,950 --> 01:30:35,780
proof based methods the stochastic context free grammar method

1490
01:30:35,800 --> 01:30:40,550
and things like eyeball which is something that i be that for

1491
01:30:40,590 --> 01:30:41,510
works on

1492
01:30:44,940 --> 01:30:47,870
to some extent

1493
01:30:47,920 --> 01:30:49,580
prism as well

1494
01:30:49,780 --> 01:30:52,320
so there

1495
01:30:52,320 --> 01:30:55,210
they don't categorize quite neatly

1496
01:30:55,210 --> 01:30:59,580
this is in the standard language that exist today but

1497
01:30:59,650 --> 01:31:04,210
it's easy to simulate this is a picture of our simulator if your traffic models

1498
01:31:04,210 --> 01:31:09,460
and what not so just a couple of motivating examples so what is monte carlo

1499
01:31:09,460 --> 01:31:13,170
planning and i should say if you do have questions you can raise your hand

1500
01:31:13,170 --> 01:31:17,850
in interrupt me cut you off it's if i think it should be dealt with

1501
01:31:17,850 --> 01:31:18,780
later but

1502
01:31:18,830 --> 01:31:20,740
feel free to ask questions

1503
01:31:20,800 --> 01:31:23,830
so what is monte carlo planning

1504
01:31:23,960 --> 01:31:28,360
roughly defined as we want to compute a good policy for an MDP

1505
01:31:28,370 --> 01:31:30,980
by interacting with an MDP simulator

1506
01:31:30,990 --> 01:31:32,170
as opposed to

1507
01:31:32,190 --> 01:31:37,670
taking into compact representation of the model in planning with that source about

1508
01:31:37,690 --> 01:31:42,550
computing good policies by interacting with a simulator and will

1509
01:31:42,560 --> 01:31:47,350
formally define what we mean by simulator moment but now we have average here he's

1510
01:31:47,350 --> 01:31:50,450
going to world simulator instead and

1511
01:31:50,490 --> 01:31:54,190
every step is going to use the simulator play around to figure out what to

1512
01:31:54,840 --> 01:31:57,670
and get some feedback from the world and is going to use the simulator to

1513
01:31:57,670 --> 01:32:01,650
figure out what to do and take another action

1514
01:32:01,670 --> 01:32:09,460
so good question so so if we define the simulator that now mention that difference

1515
01:32:10,830 --> 01:32:15,560
you can look at this on your own basically there's a whole list of domains

1516
01:32:16,470 --> 01:32:19,760
there's simulators and i should mention these slides will be on

1517
01:32:19,780 --> 01:32:22,800
in fact if you wait a couple of days are going to be updated with

1518
01:32:22,800 --> 01:32:28,960
references and what these are very fresh from fifteen minutes ago in fact

1519
01:32:29,200 --> 01:32:31,330
so so

1520
01:32:31,340 --> 01:32:37,330
so let's define what we want from our simulation based representation of an MDP there's

1521
01:32:37,330 --> 01:32:42,570
still some states and actions but our instead of being given probability distributions in some

1522
01:32:42,570 --> 01:32:47,990
language we're going to be given r and c are is simply a stochastic function

1523
01:32:48,090 --> 01:32:54,080
to see program also a that returns a reward when you pass in stay in

1524
01:32:54,080 --> 01:32:57,780
action to think of it as a function that you right and see the returns

1525
01:32:57,780 --> 01:33:02,550
to random reward according to the distribution of our underlying MDP

1526
01:33:02,570 --> 01:33:06,170
and the same thing where t is the transition function you pass it to stay

1527
01:33:06,170 --> 01:33:07,090
in action

1528
01:33:07,280 --> 01:33:09,460
and it's going to spit out the next day

1529
01:33:09,470 --> 01:33:14,860
so these are simply stochastic functions that we don't care about their implementation you could

1530
01:33:14,860 --> 01:33:18,660
use c java whatever language you like

1531
01:33:18,680 --> 01:33:24,680
you could encoded in planning languages simulate that if you care to so so that's

1532
01:33:24,680 --> 01:33:29,190
what we mean when we say a simulation based representation now coming back to reinforcement

1533
01:33:29,190 --> 01:33:30,730
learning so

1534
01:33:30,750 --> 01:33:35,240
again in reinforcement learning we don't have a compact description of the environment in fact

1535
01:33:35,240 --> 01:33:39,760
we start off with no knowledge of the environment whatsoever was allowed to take actions

1536
01:33:39,760 --> 01:33:41,210
in the environment

1537
01:33:41,230 --> 01:33:45,320
here we start off with a simulator of the environment in our head

1538
01:33:45,330 --> 01:33:48,840
so the so the the key distinction is

1539
01:33:48,850 --> 01:33:55,670
sometimes users we're using a strong simulator here and a strong simulator simulator is basically

1540
01:33:55,670 --> 01:33:59,690
you can teleport to any state at any moment and take any action that you

1541
01:33:59,690 --> 01:34:07,080
want in reinforcement learning we sort of in a week simulator model where you you're

1542
01:34:07,090 --> 01:34:11,060
you are where you are you cannot teleport to any state that you want if

1543
01:34:11,060 --> 01:34:13,320
you want to get to stay state you have to figure out how to get

1544
01:34:14,510 --> 01:34:20,540
possibly in some cases you could ever reset the resets to an initial state so

1545
01:34:20,540 --> 01:34:24,420
that sort of the key distinction weak simulators reinforcement learning

1546
01:34:24,440 --> 01:34:29,970
we're in the strong simulator case of and that there are some

1547
01:34:29,980 --> 01:34:35,120
theoretical relations between these two so

1548
01:34:35,150 --> 01:34:41,060
so let's move on so that's our our simulation based MDP representation now we're going

1549
01:34:41,060 --> 01:34:46,260
to talk about the basic monte carlo algorithms

1550
01:34:46,280 --> 01:34:47,650
now i

1551
01:34:47,820 --> 01:34:52,160
i said our motivation is to scale to very large state spaces but on the

1552
01:34:52,170 --> 01:34:56,310
start out by talking about how do we use monte carlo techniques for single state

1553
01:34:57,100 --> 01:35:02,330
so that's not exciting by itself but we're going to try to build on that

1554
01:35:02,430 --> 01:35:07,860
and then create things that can be applied to large mdps so going to start

1555
01:35:07,860 --> 01:35:09,870
with a single say state case

1556
01:35:09,920 --> 01:35:12,080
this turns out to be

1557
01:35:12,100 --> 01:35:16,240
identical to what's called the multi armed bandit problem some of you might have heard

1558
01:35:16,240 --> 01:35:21,170
of it so i

1559
01:35:21,190 --> 01:35:26,230
suppose we have a single state MDP sources are only state we have these actions

1560
01:35:26,230 --> 01:35:28,100
we have k actions OK

1561
01:35:29,780 --> 01:35:33,880
what happens when we take one of these actions well you get a random reward

1562
01:35:33,880 --> 01:35:36,580
by calling this reward function

1563
01:35:36,580 --> 01:35:41,010
and you can view this is pulling the arm of a slot machine in getting

1564
01:35:41,010 --> 01:35:45,000
what do you have is this cases child alexander mackenzie

1565
01:35:45,010 --> 01:35:46,710
you can kind of be

1566
01:35:46,740 --> 01:35:49,880
proceeding of the old is that it is

1567
01:35:50,240 --> 01:35:52,630
and the judiciary

1568
01:35:52,660 --> 01:35:57,940
archives he said well he was an apprentice and used to leaving castle street

1569
01:35:57,960 --> 01:36:03,210
while on the other that the public record office was not john alexander mackenzie was

1570
01:36:03,210 --> 01:36:06,580
alexander mackenzie spent in a different way

1571
01:36:06,590 --> 01:36:12,240
he said was an apprentice but you to to leave on old across street and

1572
01:36:12,420 --> 01:36:15,430
in one case in the old bailey the guy was

1573
01:36:17,940 --> 01:36:19,760
condemned to that

1574
01:36:19,840 --> 01:36:21,560
couple penalty

1575
01:36:21,570 --> 01:36:23,240
in the other guy

1576
01:36:23,260 --> 01:36:24,380
he was

1577
01:36:24,410 --> 01:36:30,910
actually don't another one was sent to australia deported to australia is this the same

1578
01:36:30,910 --> 01:36:32,420
person one

1579
01:36:32,450 --> 01:36:36,970
this is all the reason that you have to do it implies some form of

1580
01:36:36,970 --> 01:36:40,890
imprecise reasoning is typically not supported by

1581
01:36:40,900 --> 01:36:43,140
the current tools

1582
01:36:43,150 --> 01:36:46,120
there are ways of doing this is one of the

1583
01:36:46,120 --> 01:36:50,940
sources of a of information the UK so sorry to see you can use this

1584
01:36:50,940 --> 01:36:56,500
is about three metrics is uses the names of objects like john alexander mackenzie alexander

1585
01:36:56,500 --> 01:36:59,410
mckenzie and this also applies to record

1586
01:36:59,430 --> 01:37:01,980
and tries to tell you which one is which

1587
01:37:01,990 --> 01:37:05,100
and this is something you can download from the web

1588
01:37:05,140 --> 01:37:07,130
but now we have just caption

1589
01:37:07,150 --> 01:37:10,620
information what we have to do the next step is to share

1590
01:37:10,620 --> 01:37:15,790
to let people finding it so when we talk about sharing views

1591
01:37:15,810 --> 01:37:21,260
now we have all this database this knowledge base of facts of instances

1592
01:37:21,290 --> 01:37:22,360
it is there

1593
01:37:22,370 --> 01:37:26,480
and we want to search we want to make the information available at the right

1594
01:37:27,420 --> 01:37:29,130
to the right people

1595
01:37:29,410 --> 01:37:30,920
now the

1596
01:37:30,930 --> 01:37:35,900
task is incredibly complex i can speak for four hours on this so we simplify

1597
01:37:35,900 --> 01:37:36,710
the view

1598
01:37:36,730 --> 01:37:40,340
exactly ten minutes so i simplified if you to searching

1599
01:37:40,350 --> 01:37:48,900
so what is ontology based annotation enables you to the two things is is to

1600
01:37:49,690 --> 01:37:56,650
collect information across documents and reasoning talk just searching two types of things that you

1601
01:37:56,650 --> 01:37:57,600
can search

1602
01:37:57,600 --> 01:37:59,590
one is the document

1603
01:37:59,610 --> 01:38:04,190
you can search documents especially collections not necessarily on the web

1604
01:38:04,210 --> 01:38:07,990
on the world wide web unless you really have large coverage

1605
01:38:09,430 --> 01:38:13,520
but you can also search for knowledge i said before you can you remember the

1606
01:38:13,520 --> 01:38:17,290
hamster examples you can use the information to

1607
01:38:17,300 --> 01:38:18,520
find the page

1608
01:38:18,550 --> 01:38:19,630
you can use

1609
01:38:19,640 --> 01:38:23,680
the information just to say this guy says force expanses knowledge

1610
01:38:24,460 --> 01:38:27,050
well or information according to how you define

1611
01:38:29,870 --> 01:38:31,020
there are

1612
01:38:32,200 --> 01:38:33,910
but if you look at the

1613
01:38:33,990 --> 01:38:37,420
possibilities for search you will have keyword based search and that is what we do

1614
01:38:37,420 --> 01:38:40,880
with google or any other search engines or whatever

1615
01:38:41,850 --> 01:38:45,610
you typically have two problems one is the one of ambiguity

1616
01:38:45,630 --> 01:38:49,390
he was can be police and so they can have multiple meanings

1617
01:38:49,410 --> 01:38:52,550
so you receive losses for these documents

1618
01:38:52,560 --> 01:38:56,620
and synonymity an object can be identified by

1619
01:38:56,630 --> 01:38:57,960
multiple terms

1620
01:38:57,990 --> 01:39:02,270
so you don't find documents because you're using c nineteen eighty and people have seen

1621
01:39:02,270 --> 01:39:08,900
only be in the in the text of low recorded what he said now

1622
01:39:08,920 --> 01:39:12,220
you tend not to notice that with google

1623
01:39:12,330 --> 01:39:18,760
on the way because they do some normalisation and also because you have such an

1624
01:39:18,760 --> 01:39:24,430
amount of documents that even if you have low recall you would still find documents

1625
01:39:24,560 --> 01:39:29,540
you will find something relevant and then you navigate but especially that power is the

1626
01:39:29,550 --> 01:39:36,000
interlinking interlinking gives you the most popular pages and this is quite useful

1627
01:39:36,100 --> 01:39:40,300
because people tend to be interested in the most popular pages but when you work

1628
01:39:40,300 --> 01:39:44,940
within the company on the organizational memory that is not linking on if there is

1629
01:39:44,940 --> 01:39:46,890
a hyperlink is not good enough

1630
01:39:46,910 --> 01:39:50,800
well actually the most important resources so

1631
01:39:50,810 --> 01:39:54,400
this is where you really find that people search doesn't work

1632
01:39:54,420 --> 01:40:01,480
semantic search if you just use the information that is in your knowledge base quite

1633
01:40:01,490 --> 01:40:04,710
i suppose knowledge search suppose document search

1634
01:40:05,560 --> 01:40:06,890
the problem is

1635
01:40:06,900 --> 01:40:10,560
if you look at the lead there whether different methods of doing that one going

1636
01:40:10,560 --> 01:40:11,410
to be used

1637
01:40:11,430 --> 01:40:16,230
you can use keywords and he was into concepts you can use the graph based

1638
01:40:16,230 --> 01:40:21,880
methods to create queries using the ontology you can use natural language approach is that

1639
01:40:22,220 --> 01:40:28,960
your query natural language query into an ontology can form based approaches the problem with

1640
01:40:28,960 --> 01:40:31,870
ontology based approaches is their metadata

1641
01:40:31,880 --> 01:40:34,750
can cover only part of the information

1642
01:40:36,170 --> 01:40:38,080
and the page about the hamsters

1643
01:40:38,080 --> 01:40:42,430
we can say anything about the address with the markup the rest of the show

1644
01:40:42,440 --> 01:40:45,930
but actually it turns out it's quite useful isn't but it was not in your

1645
01:40:45,930 --> 01:40:47,800
apology no one marketed that

1646
01:40:48,340 --> 01:40:50,060
so what do you do when

1647
01:40:50,100 --> 01:40:53,890
that happens and can be quite dramatic

1648
01:40:53,910 --> 01:40:56,710
well we did an experiment in jet engines

1649
01:40:56,730 --> 01:41:01,140
someone had developed an ontology and we were analysing

1650
01:41:01,210 --> 01:41:03,870
the typical topics of research there some

1651
01:41:03,910 --> 01:41:10,480
engineers data voice and we discovered that the ontology that was designed carefully

1652
01:41:10,510 --> 01:41:14,560
actually didn't call fourteen of out of twenty one topics

1653
01:41:14,640 --> 01:41:19,390
they were looking for just because they had so many things that were interested in

1654
01:41:19,420 --> 01:41:21,440
not everything was annotated

1655
01:41:21,460 --> 01:41:26,200
so the solution is that we have proposed to c was

1656
01:41:26,220 --> 01:41:28,600
well why do you want to do that well

1657
01:41:28,630 --> 01:41:33,460
you know you can't index everything you content think everything is two times spent time

1658
01:41:33,560 --> 01:41:39,500
consuming and it is impossible to foresee all the users and also automated means there

1659
01:41:39,500 --> 01:41:43,630
are pieces of information you can extract another simply you can't

1660
01:41:43,640 --> 01:41:49,090
because the system is not good enough because information is still in place

1661
01:41:49,100 --> 01:41:51,410
so what you are proposing cyprus search

1662
01:41:52,350 --> 01:41:54,190
enables you to combine

1663
01:41:54,470 --> 01:41:58,930
semantic semantic search and he was within the same query

1664
01:41:58,950 --> 01:42:04,210
basically it's three types of complex combination of three types of

1665
01:42:04,680 --> 01:42:07,860
of that's one is

1666
01:42:07,860 --> 01:42:09,050
pure semantic

1667
01:42:09,070 --> 01:42:10,130
so use

1668
01:42:10,210 --> 01:42:11,760
your eyes

1669
01:42:11,770 --> 01:42:14,850
he was based in which you match

1670
01:42:15,240 --> 01:42:17,110
the whole documents like good

1671
01:42:17,110 --> 01:42:22,510
but also you can add keywords in context in which much keywords within the annotations

1672
01:42:23,220 --> 01:42:26,650
when hamsters is annotated

1673
01:42:26,660 --> 01:42:27,510
you have

1674
01:42:27,520 --> 01:42:30,320
the concept hamster and the term memes

1675
01:42:30,320 --> 01:42:32,310
and you can use well within the

1676
01:42:32,330 --> 01:42:35,090
whatever is this as pet

1677
01:42:35,110 --> 01:42:36,420
find me

1678
01:42:36,420 --> 01:42:39,560
combine the semantic information that is on the web

1679
01:42:39,610 --> 01:42:45,180
like using watson is like using the semantic web or not and what you want

1680
01:42:45,180 --> 01:42:48,430
to put stop basically so

1681
01:42:49,260 --> 01:42:53,400
he was supposed to have an online demos i will talk about

1682
01:42:53,430 --> 01:42:58,660
basically it looks like in terms of interface it looks for space looks pretty much

1683
01:42:58,660 --> 01:43:04,390
like semantic web search search engine normal search engine you put keywords you obtain a

1684
01:43:04,390 --> 01:43:09,990
list of ontology is only stuff semantic documents that talked about you keywords you can

1685
01:43:10,730 --> 01:43:15,030
i want my keywords to correspond to class all i one i two as much

1686
01:43:15,030 --> 01:43:19,030
on is the idea of the entity i want to match to be exact meaning

1687
01:43:19,030 --> 01:43:21,610
that you can formulate ways like

1688
01:43:21,630 --> 01:43:24,270
i one semantic documents that contain

1689
01:43:26,170 --> 01:43:31,440
by its ideal stable researcher let's say and then you obtain all the semantic document

1690
01:43:31,440 --> 01:43:34,510
on the web that talks about research the class

1691
01:43:34,540 --> 01:43:36,170
and you get not only

1692
01:43:36,190 --> 01:43:41,860
the documents you entities you get a rid list of all the classes research that

1693
01:43:41,880 --> 01:43:43,930
has been described anywhere on the web

1694
01:43:44,020 --> 01:43:47,570
and then you can explore its you can click on the entity see that your

1695
01:43:47,570 --> 01:43:54,110
research a subclass of working person of that it does surprise which is research in

1696
01:43:54,110 --> 01:43:57,930
academia then you can click on research in academia i see that is the subject

1697
01:43:57,930 --> 01:44:03,400
of academic you can do graphene you can get metadata about

1698
01:44:03,430 --> 01:44:07,910
about the ontology containing your research being meaning that you can know

1699
01:44:07,920 --> 01:44:09,100
what is

1700
01:44:09,110 --> 01:44:13,990
the description logics underlying it you can know how it is

1701
01:44:14,000 --> 01:44:20,550
i mean the classes contain what also ontologies import what's what ontology that imported and

1702
01:44:20,580 --> 01:44:22,490
you can finally

1703
01:44:22,510 --> 01:44:25,550
the sparql query to it and gets with that

1704
01:44:25,600 --> 01:44:28,100
so this is what the interface that

1705
01:44:28,120 --> 01:44:32,160
basically it allows you to explore all

1706
01:44:32,180 --> 01:44:36,490
the same is the semantic information that is currently present on the web

1707
01:44:36,500 --> 01:44:40,100
now this is not what is interesting again

1708
01:44:40,120 --> 01:44:41,690
i mean this talk is about

1709
01:44:41,710 --> 01:44:45,060
building application and using it as the so

1710
01:44:45,060 --> 01:44:47,590
watson provides a set of API

1711
01:44:47,670 --> 01:44:53,040
which allows you to do whatever i do we use these interface within your application

1712
01:44:53,040 --> 01:44:54,740
in that you can find

1713
01:44:54,740 --> 01:44:58,500
semantic information you can explore it you can

1714
01:44:58,520 --> 01:45:02,740
find relations between entities and you can find anything

1715
01:45:02,790 --> 01:45:07,830
that is in the documents and trying to apply sparql queries and

1716
01:45:07,850 --> 01:45:10,100
in that case you know i i

1717
01:45:10,300 --> 01:45:15,270
how hard time trying to choose which application i'll show as an example

1718
01:45:15,290 --> 01:45:17,760
because there are many very

1719
01:45:17,780 --> 01:45:23,180
the watson plugin for ontology engineering for example for ontology reuse which some talked a

1720
01:45:23,180 --> 01:45:24,430
bit about

1721
01:45:24,460 --> 01:45:26,650
basically allows you to find

1722
01:45:27,330 --> 01:45:33,440
whatever is in your whatever and you have your ontology to find corresponding of entity

1723
01:45:33,730 --> 01:45:39,550
ontologies and to integrate any of the statements that is out there into your own

1724
01:45:39,620 --> 01:45:41,420
trading links between

1725
01:45:41,440 --> 01:45:47,070
the ontology and if you ever use we have an application for questions sharing

1726
01:45:47,070 --> 01:45:53,570
meaning that you can actually as natural language questions to the semantic web through watson

1727
01:45:53,580 --> 01:45:58,590
the semantic roles of dynamically annotate whatever you are browsing ontoweb

1728
01:45:58,600 --> 01:46:02,490
we have scarlet a nice application given two terms it tells you what is the

1729
01:46:02,490 --> 01:46:04,890
relation between the two terms by

1730
01:46:04,940 --> 01:46:07,060
going through ontologies on the web

1731
01:46:07,060 --> 01:46:11,550
moreover it is an adaptation feeding some other people are doing well since simulation

1732
01:46:11,560 --> 01:46:17,960
some people are doing folksonomy enrichment finding semantic information to put into tags synonym discovery

1733
01:46:17,960 --> 01:46:19,670
only sort of things

1734
01:46:19,690 --> 01:46:23,040
so with all application what i thought i would do

1735
01:46:23,140 --> 01:46:24,920
these are the show you want

1736
01:46:25,070 --> 01:46:28,990
which is not particularly impressive actually it's a very simple one

1737
01:46:29,000 --> 01:46:33,390
but it's a nice demonstrator of what you can do in today's two hundred lines

1738
01:46:33,390 --> 01:46:34,580
of javascript

1739
01:46:34,650 --> 01:46:40,580
and that actually provide a future but actually at semantics into something

1740
01:46:40,600 --> 01:46:46,070
without you having to collect semantics without you having to get to in the ontology

1741
01:46:46,080 --> 01:46:48,870
and two billion that set to put into other

1742
01:46:48,930 --> 01:46:50,730
he very simple

1743
01:46:50,730 --> 01:46:53,710
demonstrators caldwell who because it's

1744
01:46:53,740 --> 01:46:55,840
watson and yahoo

1745
01:46:55,870 --> 01:46:59,690
this user yahoo API to get

1746
01:46:59,810 --> 01:47:04,700
simple web search normal web search but what it does in addition is

1747
01:47:04,710 --> 01:47:06,300
it is part

1748
01:47:06,300 --> 01:47:10,410
which to you for each of the keywords you entered into the query follows the

1749
01:47:10,760 --> 01:47:15,500
search engine one of the keyword you could use to generalise the query

1750
01:47:16,250 --> 01:47:20,020
in the case of research can use working person you can use that

1751
01:47:20,040 --> 01:47:22,340
you can use academic staff

1752
01:47:23,450 --> 01:47:25,290
what keywords you could use

1753
01:47:25,290 --> 01:47:30,480
to specify your query in that case you could use scientists senior research fellow in

1754
01:47:30,480 --> 01:47:33,790
academia is a good one computer science researcher

1755
01:47:34,070 --> 01:47:35,430
does not four or

1756
01:47:35,460 --> 01:47:37,340
you're keywords

1757
01:47:37,420 --> 01:47:39,100
then every click

1758
01:47:39,250 --> 01:47:44,530
in other words korean you can run it when you click here it replaced the

1759
01:47:44,530 --> 01:47:48,450
keyword by your feet so i really this is not

1760
01:47:48,960 --> 01:47:54,510
particularly impressive is the big thing but the thing is that it is fully simple

1761
01:47:54,510 --> 01:47:55,240
to do

1762
01:47:55,300 --> 01:47:58,210
again i was supposed to do an on line demo

1763
01:47:58,240 --> 01:47:59,620
it's actually working

1764
01:48:00,690 --> 01:48:02,190
well i want to show

1765
01:48:02,250 --> 01:48:04,350
is that these application

1766
01:48:04,490 --> 01:48:05,790
the reason a bit

1767
01:48:05,800 --> 01:48:10,200
just a tiny bit of development can be done very easily any that can be

1768
01:48:10,200 --> 01:48:11,560
done very easily

1769
01:48:11,600 --> 01:48:15,120
what i mean you can imagine what you can do in one week two weeks

1770
01:48:15,450 --> 01:48:16,940
one month or year

1771
01:48:16,950 --> 01:48:19,940
so i we do actually see

1772
01:48:19,950 --> 01:48:25,150
using the watson API you can for example search for all the ontology is on

1773
01:48:25,150 --> 01:48:28,330
the web that contains a class researcher

1774
01:48:28,380 --> 01:48:30,760
this is one simple call

1775
01:48:30,790 --> 01:48:31,580
this one

1776
01:48:32,040 --> 01:48:35,890
to this particular USA we want semantic content

1777
01:48:35,920 --> 01:48:37,500
corresponding to the keywords

1778
01:48:38,600 --> 01:48:40,910
now this is not exactly

1779
01:48:40,960 --> 01:48:42,560
what you want

1780
01:48:42,560 --> 01:48:44,210
you want to tiny bit better

1781
01:48:44,230 --> 01:48:45,820
if you want something

1782
01:48:47,130 --> 01:48:48,160
you want to all

1783
01:48:48,170 --> 01:48:50,060
the ontology that contains

1784
01:48:50,150 --> 01:48:54,270
class researcher and that any thing that is research

1785
01:48:54,280 --> 01:49:00,060
which would be exactly researcher not research in academia not research computer science researcher

1786
01:49:00,070 --> 01:49:06,730
and that full content research and not in anything but in its ideal it's labelled

1787
01:49:06,730 --> 01:49:12,380
so and we will see why why this is useful in the second case

1788
01:49:14,030 --> 01:49:16,190
so we have a lower bound

1789
01:49:16,250 --> 01:49:22,040
that's palmer tries by these q functions so if we for different choices of q

1790
01:49:22,040 --> 01:49:24,600
we get different lower bound on the likelihood

1791
01:49:27,220 --> 01:49:30,600
all right so what we do now is

1792
01:49:30,600 --> 01:49:36,440
that's the wrong way we now take this lower bound

1793
01:49:36,470 --> 01:49:37,930
and we

1794
01:49:37,950 --> 01:49:43,310
optimize the lower bound for given parameters theta and pi we optimize with respect to

1795
01:49:43,310 --> 01:49:47,830
the variational distributions specific with respect to these q functions

1796
01:49:47,840 --> 01:49:49,350
OK so

1797
01:49:49,370 --> 01:49:54,520
actually we maximize it it's a lower bound it turns out we want to maximize

1798
01:49:54,520 --> 01:49:59,190
that we want to maximize the log likelihood function ultimately but you know we have

1799
01:49:59,190 --> 01:50:03,070
this lower bound so we basically maximize the lower bound and in this in this

1800
01:50:03,070 --> 01:50:05,830
step here what we do is we actually find

1801
01:50:05,850 --> 01:50:09,120
we can think of it that way we find the best lower bound in this

1802
01:50:09,120 --> 01:50:15,490
family that we've defined so it's the tightest bound at the current estimate of our

1803
01:50:15,520 --> 01:50:17,070
the parameters

1804
01:50:18,190 --> 01:50:21,480
so so we can look at you know what if we vary

1805
01:50:22,250 --> 01:50:26,770
you know and look at the end an extremal point well we see then you

1806
01:50:26,770 --> 01:50:29,270
know that just by performing that

1807
01:50:29,300 --> 01:50:34,500
we get the following equation namely that this q are fuzzy

1808
01:50:34,530 --> 01:50:37,700
equals for what's written on the right hand side here

1809
01:50:39,540 --> 01:50:44,680
so it's p of w given times of the given d and then

1810
01:50:44,720 --> 01:50:48,670
normalized well that is simply the posterior probability

1811
01:50:52,070 --> 01:50:53,530
d common w

1812
01:50:53,560 --> 01:50:55,590
right so

1813
01:50:56,250 --> 01:51:01,720
so so this is basically what we do in the east right so we can

1814
01:51:01,720 --> 01:51:04,520
we can think of the e step as as

1815
01:51:05,180 --> 01:51:10,440
basically finding as the tightest lower bound among this family of lower bounds that we've

1816
01:51:11,600 --> 01:51:17,120
OK once we have a particular lower bound what we can do then is optimise

1817
01:51:17,120 --> 01:51:21,440
or parameters maximise lower bound with respect to all parameters

1818
01:51:21,460 --> 01:51:24,920
and and this will then result in the m step equations case if we look

1819
01:51:24,920 --> 01:51:29,140
at the gradient of that with respect to fade as they are the derivatives with

1820
01:51:29,140 --> 01:51:36,790
respect to participate to zero respectively we get these two equations that are exactly the

1821
01:51:36,790 --> 01:51:39,330
same as the m step equations

1822
01:51:39,470 --> 01:51:43,630
and of course we just have these q functions in here but but the way

1823
01:51:43,630 --> 01:51:47,840
they been determined before is just to be the posteriors right

1824
01:51:47,850 --> 01:51:53,210
so this is slightly more general way of

1825
01:51:53,240 --> 01:51:55,430
you know motivating

1826
01:51:55,500 --> 01:51:59,850
these types of you know that that also applies to slightly more general class of

1827
01:51:59,850 --> 01:52:02,800
problems than does the model of them but

1828
01:52:02,810 --> 01:52:04,880
so this is basically the derivation

1829
01:52:04,910 --> 01:52:09,980
there's also there questions of

1830
01:52:09,990 --> 01:52:14,570
problems of over fitting in such models became what

1831
01:52:14,610 --> 01:52:16,440
what we have done

1832
01:52:16,630 --> 01:52:22,300
in our early work is used a trick called temperate TMN

1833
01:52:22,300 --> 01:52:26,280
we're basically if we look at these lower bounds again

1834
01:52:26,300 --> 01:52:31,820
right you can actually write them like this you have two contributions maybe i can

1835
01:52:31,820 --> 01:52:35,630
go back to show that here you know if you look at this what you

1836
01:52:35,640 --> 01:52:37,850
what you have here is

1837
01:52:37,900 --> 01:52:41,060
if you look at this part right is just

1838
01:52:41,140 --> 01:52:42,980
the expectation

1839
01:52:43,000 --> 01:52:50,030
so it is just the expectation of that block here

1840
01:52:50,080 --> 01:52:53,810
right and then also you get an entropy part if you get the you know

1841
01:52:54,150 --> 01:52:56,120
q r log

1842
01:52:56,150 --> 01:53:00,440
qr xe or one of accuracy so he pulled out with a minus sign so

1843
01:53:00,440 --> 01:53:04,830
basically you have one part that has to do in physics is called the average

1844
01:53:04,830 --> 01:53:08,110
energy and the other part it has to do with the

1845
01:53:08,130 --> 01:53:14,410
entropy actually of these variational distribution so we want that to be

1846
01:53:15,980 --> 01:53:17,700
and we want

1847
01:53:17,710 --> 01:53:19,150
this to be

1848
01:53:19,200 --> 01:53:23,990
large and that means we want the q functions to have high entropy now what

1849
01:53:23,990 --> 01:53:24,940
one can do

1850
01:53:24,950 --> 01:53:27,210
and this isn't just a heuristic approach

1851
01:53:27,230 --> 01:53:31,820
as i motivated here at least is increase

1852
01:53:31,830 --> 01:53:34,130
this entropy part OK you can

1853
01:53:34,140 --> 01:53:36,720
usually this is just the one here OK there

1854
01:53:36,740 --> 01:53:39,950
you know it's just this minus this but we can

1855
01:53:39,960 --> 01:53:42,600
amplify this entropy part

1856
01:53:42,640 --> 01:53:48,700
and forest that you know we're not actually computing posterior probabilities but that we smoothing

1857
01:53:48,700 --> 01:53:53,840
out posterior probabilities in some way and if you do that you can actually see

1858
01:53:53,840 --> 01:53:59,580
the following look at these graphs here OK so what's shown here is number of

1859
01:54:02,130 --> 01:54:06,000
and what's shown the y axis here is the

1860
01:54:06,140 --> 01:54:11,440
perplexity so this is the basically the test the likelihood on some

1861
01:54:11,470 --> 01:54:14,170
test data set of all the datasets

1862
01:54:14,190 --> 01:54:22,640
i mean basically scaled you know e to the minus test look like this anyway

1863
01:54:22,640 --> 01:54:26,300
so you know the the lower the better and you can see is train models

1864
01:54:26,310 --> 01:54:29,880
for instance here would be to equal one this would be the standard model of

1865
01:54:30,740 --> 01:54:34,130
OK the quality of the model gets better and better and better but then it

1866
01:54:34,140 --> 01:54:38,520
after about twenty five or iterations in this particular case here

1867
01:54:38,640 --> 01:54:40,540
it actually

1868
01:54:40,560 --> 01:54:45,450
deteriorates so the performance get worse and worse and worse again this is the typical

1869
01:54:45,450 --> 01:54:47,540
overfitting phenomenon OK

1870
01:54:47,560 --> 01:54:49,220
now as you

1871
01:54:49,270 --> 01:54:51,100
use this trick here we

1872
01:54:51,100 --> 01:54:55,950
maybe it's right translation and scale from one to five with five being perfect one

1873
01:54:55,950 --> 01:54:57,810
being completely terrible

1874
01:54:57,860 --> 01:54:59,670
and then

1875
01:54:59,680 --> 01:55:03,850
let people judge people don't even agree on that by the way

1876
01:55:03,950 --> 01:55:07,790
so that

1877
01:55:07,840 --> 01:55:12,250
the history of automatic metrics is that we take what works in speech recognition is

1878
01:55:12,260 --> 01:55:14,720
actually general model and machine translation

1879
01:55:14,740 --> 01:55:18,580
take what works this feature so we look at our right so we still do

1880
01:55:18,580 --> 01:55:23,650
the same thing where we have human translation we have all machine translation and just

1881
01:55:23,650 --> 01:55:29,870
look how many words match the error rate implies how many words match the same

1882
01:55:31,290 --> 01:55:36,210
but often things get kind of fundamentally order and you don't want to get completely

1883
01:55:36,210 --> 01:55:42,050
punished for that so there some adaptations for that way you allow some reordering

1884
01:55:42,090 --> 01:55:43,930
the metric is use

1885
01:55:43,940 --> 01:55:45,300
right now

1886
01:55:45,350 --> 01:55:49,480
pretty popular section of the old blue was suggested in two thousand two

1887
01:55:49,530 --> 01:55:55,570
and it's based the same idea you reference translation you have the system output and

1888
01:55:55,570 --> 01:55:59,600
the only trigger setting not only look at how many words get right

1889
01:55:59,610 --> 01:56:02,350
which would completely ignore reordering

1890
01:56:02,350 --> 01:56:05,010
we also look at how many pairs of birds and

1891
01:56:05,030 --> 01:56:08,380
triple suffered some four grams of words to get right and if you get forward

1892
01:56:08,380 --> 01:56:13,910
to write comparison to the reference sensation it is deemed to be a good thing

1893
01:56:13,910 --> 01:56:17,540
OK how does this work that say this is the reference translation here

1894
01:56:17,640 --> 01:56:21,970
slightly violent the gunman was shot to death by the police

1895
01:56:22,070 --> 01:56:28,210
and he system translations of human translations so whatever they are possible

1896
01:56:28,220 --> 01:56:31,010
candidate translation so you want to judge them

1897
01:56:31,040 --> 01:56:34,430
by michael how what they are so we we use

1898
01:56:34,980 --> 01:56:38,160
n gram overlap with the reference translation

1899
01:56:38,170 --> 01:56:41,580
so i have a little bit colour coded so they can afford can write kind

1900
01:56:41,580 --> 01:56:43,480
of the whole thing green fagin

1901
01:56:43,510 --> 01:56:45,040
by right

1902
01:56:45,130 --> 01:56:46,960
it's blue and

1903
01:56:46,970 --> 01:56:51,170
and if i get nothing to help produced the wrong word put in red

1904
01:56:51,190 --> 01:56:55,450
so you can kind of see that really terrible translations like the one that have

1905
01:56:55,460 --> 01:56:58,380
on subject larger than zero symbols

1906
01:56:58,390 --> 01:57:01,550
of course a lot of words that are at so these

1907
01:57:01,610 --> 01:57:03,210
just very lonely

1908
01:57:03,230 --> 01:57:08,280
but of course the perfect station has all matching programs

1909
01:57:10,520 --> 01:57:13,320
it doesn't work perfectly so the last sentence

1910
01:57:13,380 --> 01:57:16,720
police killed the gunman

1911
01:57:16,720 --> 01:57:22,690
missus elizabeth information that there was shooting involved so it that instead money thing

1912
01:57:22,740 --> 01:57:25,200
but is actually good translation but

1913
01:57:25,200 --> 01:57:26,360
we have

1914
01:57:26,410 --> 01:57:29,210
a single word arrived in the by grand

1915
01:57:29,220 --> 01:57:31,470
and the period in the right here

1916
01:57:31,490 --> 01:57:35,580
so that there wouldn't be just very highly

1917
01:57:35,590 --> 01:57:39,030
so it's very easy with this metric to always come with an example of a

1918
01:57:39,030 --> 01:57:40,870
perfectly nice to station

1919
01:57:40,910 --> 01:57:45,870
but it doesn't match the source matches you reference at all

1920
01:57:45,880 --> 01:57:48,490
and it would be judged fairly lowly

1921
01:57:48,510 --> 01:57:53,030
the argument is that you don't do that for one sentence you do that for

1922
01:57:53,060 --> 01:57:55,120
safe house sentences

1923
01:57:55,120 --> 01:57:59,800
and if you look in the decision corpus of a thousand sentences you have much

1924
01:57:59,800 --> 01:58:03,650
more matches with human reference you probably better

1925
01:58:03,660 --> 01:58:07,420
a system that doesn't have that much patches

1926
01:58:07,470 --> 01:58:11,300
OK so that's intuitive argument you might believe it to believe it or not

1927
01:58:11,330 --> 01:58:16,500
so what made this argument convincing that this is a good metric to evaluate machine

1928
01:58:16,500 --> 01:58:19,070
translation graphs like this

1929
01:58:19,130 --> 01:58:20,240
there you have

1930
01:58:20,270 --> 01:58:21,640
automatic scores

1931
01:58:21,650 --> 01:58:26,230
on the one hand axes and human discourse we make human society that what i

1932
01:58:26,230 --> 01:58:29,500
said earlier you give us a better person say

1933
01:58:29,570 --> 01:58:36,050
actually here yesterday for questions one is is the meaning preserved its called adequacy and

1934
01:58:36,260 --> 01:58:39,150
question is not fluent output in this confluence e

1935
01:58:39,150 --> 01:58:41,950
and because influence correlate a lot

1936
01:58:42,000 --> 01:58:44,450
but the main point is here

1937
01:58:44,490 --> 01:58:46,650
that also the automatic scores

1938
01:58:46,650 --> 01:58:50,810
so here's in this causes slight variant of the score

1939
01:58:50,830 --> 01:58:53,880
also correlate very very well with their

1940
01:58:53,920 --> 01:58:57,640
with the human judgments of the system on the top right it was judged

1941
01:58:57,690 --> 01:59:02,160
best by humans is also just best by the automatic metrics

1942
01:59:02,170 --> 01:59:03,770
and on the bottom left

1943
01:59:03,790 --> 01:59:08,470
the very system according to both humans and

1944
01:59:09,270 --> 01:59:11,300
automatic metric

1945
01:59:11,410 --> 01:59:12,220
so you can

1946
01:59:12,230 --> 01:59:15,170
draw a nice line here and say you know everything kind of is on that

1947
01:59:15,900 --> 01:59:19,700
and we can also compute the correlation

1948
01:59:19,750 --> 01:59:23,650
and say well the correlation i think what we have here

1949
01:59:23,730 --> 01:59:25,130
you can read the number

1950
01:59:25,140 --> 01:59:29,580
and when the eighties nineties that's a very strong correlation

1951
01:59:31,220 --> 01:59:35,320
and you say well OK that's one metric maybe have a different metric and it's

1952
01:59:35,330 --> 01:59:39,230
very nice game to play so once you have a corpus annotated with human judgments

