1
00:00:00,000 --> 00:00:04,740
it through and it does work but you are to get similar

2
00:00:06,690 --> 00:00:08,070
one of the things i like to say

3
00:00:08,080 --> 00:00:11,010
is this split merge stuff has really worked great for us

4
00:00:11,050 --> 00:00:15,540
basically there are been a lot of cases where there some hidden structure we only

5
00:00:15,540 --> 00:00:19,830
see the courses traces that we know there are some kind of incredibly complicated patterns

6
00:00:20,000 --> 00:00:24,090
and are hoping that the model of learning and really does so one other case

7
00:00:24,090 --> 00:00:29,530
where this work very well is in the sequence case looking at modeling phones and

8
00:00:29,530 --> 00:00:33,690
kind of kind of basic phonetic phonology patterns here where this is the task it's

9
00:00:33,690 --> 00:00:38,620
the speech recognition task this kind of the baby is to speech recognition tasks recognizing

10
00:00:39,640 --> 00:00:41,330
doing for recognition

11
00:00:41,340 --> 00:00:44,500
and kind of a kind of a

12
00:00:44,690 --> 00:00:49,300
a good baseline HMM the kind of the standard approach circa maybe

13
00:00:49,370 --> 00:00:52,580
i don't know five or ten years ago maybe more like ten years ago gives

14
00:00:52,580 --> 00:00:56,130
you about twenty five percent error rate

15
00:00:56,140 --> 00:00:59,530
and doing the same split merge thing for five rounds

16
00:00:59,540 --> 00:01:03,680
i improves on that substantially what's going on what's going on is are starting off

17
00:01:03,680 --> 00:01:07,010
saying is the sequence of phonemes i don't know what's going on beyond that but

18
00:01:07,010 --> 00:01:10,920
i suspect there some kind of tricky phonological patterns and there's there's a bunch of

19
00:01:10,920 --> 00:01:15,180
kind of phonological classes were kind of some sounds are similar and they have the

20
00:01:15,180 --> 00:01:19,520
same contextually like all nasals may influence the next sound in the same way

21
00:01:19,980 --> 00:01:24,760
and moreover this kind of duration patterns when certain contexts certain kind of sounds get

22
00:01:24,760 --> 00:01:28,880
longer shorter and all of this is kind of being learned automatically in a by

23
00:01:28,880 --> 00:01:33,320
the model so in this particular case i'm showing that the simplest structure here for

24
00:01:33,320 --> 00:01:36,990
one phone and what you what what the models learned here is that in this

25
00:01:36,990 --> 00:01:43,240
particular case that kind of stuff pattern one way on one side and

26
00:01:43,260 --> 00:01:48,540
various kinds of sirens pattern and on the other side and it's kind of learning

27
00:01:48,540 --> 00:01:52,310
these clusters which we could have imposed based on our knowledge of phenomenology but we

28
00:01:52,310 --> 00:01:53,600
didn't have to

29
00:01:53,650 --> 00:01:58,780
this is another case of linguistic structure that you know in some cases verifies what

30
00:01:58,780 --> 00:02:01,580
we suspect in other cases can improve on it

31
00:02:01,590 --> 00:02:04,360
so this works here too

32
00:02:04,390 --> 00:02:09,390
so in summary has so far been talking about this probably variable grammar refinement to

33
00:02:09,390 --> 00:02:14,760
take an kind of much more generally this is ICO ICO course traceable complex process

34
00:02:14,960 --> 00:02:19,860
and how to learn about that process in in an unsupervised fashion

35
00:02:19,910 --> 00:02:25,040
and to do this with grammars you get a very very good and interpretable grammar

36
00:02:25,900 --> 00:02:29,490
this gives rise to state-of-the-art parsing in this works as well as can any other

37
00:02:29,490 --> 00:02:35,530
class of techniques for passing and in my opinion is substantially simpler and moreover because

38
00:02:35,530 --> 00:02:39,330
you've got this ontology of an ontology of grammars you can and cannot use that

39
00:02:39,330 --> 00:02:42,660
again at inference time you kind of get your inference algorithm along with your learning

40
00:02:42,660 --> 00:02:44,140
algorithm for free

41
00:02:44,150 --> 00:02:48,230
i suspect there's a lot of applications beyond passing and the other linguistic i keep

42
00:02:48,240 --> 00:02:52,020
cases we looked at so i encourage you to think about it and if you

43
00:02:52,020 --> 00:02:55,270
can think of any find these great if you can tell me of problems that

44
00:02:55,310 --> 00:02:58,230
you think this character looking

45
00:03:00,630 --> 00:03:01,960
so back to outline

46
00:03:02,130 --> 00:03:04,650
the next thing i'm going to talk about is a very different kind of linguistic

47
00:03:04,650 --> 00:03:08,860
structure but we're going to be using some of the same methods so in this

48
00:03:08,860 --> 00:03:12,420
case again we're going do some unsupervised learning but we're going to be looking at

49
00:03:12,420 --> 00:03:16,510
the problem of co reference

50
00:03:16,560 --> 00:03:19,510
what's the problem about so here's a short discourse

51
00:03:19,600 --> 00:03:24,330
the weir group whose headquarters is in the US is a large specialized corporation

52
00:03:24,400 --> 00:03:29,030
this power plant which will be situated in jiangsu has a large generation capacity

53
00:03:29,070 --> 00:03:32,030
so what's coreference about there's a lot going on here one thing we could want

54
00:03:32,030 --> 00:03:35,890
to do this course can break it up and some syntactic fashion with the past

55
00:03:35,900 --> 00:03:37,160
very talked about that

56
00:03:38,120 --> 00:03:39,820
the last five minutes

57
00:03:39,860 --> 00:03:42,850
what i wanna do and co reference is i want to basically say

58
00:03:42,860 --> 00:03:44,620
what are the mentions here

59
00:03:44,630 --> 00:03:47,550
so what are the entities being talked about

60
00:03:47,620 --> 00:03:49,570
and which of them are the same

61
00:03:49,640 --> 00:03:52,890
right so each of these boxes what i'm going to call mentioned

62
00:03:52,930 --> 00:03:54,170
so it some

63
00:03:54,180 --> 00:03:58,430
syntactic item which evokes some underlying entity

64
00:03:58,480 --> 00:04:01,130
and i want to know which of them is the same

65
00:04:01,150 --> 00:04:04,020
maybe inside documents not across documents

66
00:04:04,020 --> 00:04:06,780
i'm going to talk about model to basically ignore the rest of the structure of

67
00:04:06,830 --> 00:04:08,180
discourse looks like this

68
00:04:08,240 --> 00:04:11,230
so like the rest of the structure is important but we can model everything

69
00:04:11,340 --> 00:04:15,480
so we're going to talk about models to which are basically view discourse as

70
00:04:15,530 --> 00:04:17,100
kind of a sequence of

71
00:04:17,120 --> 00:04:20,290
one of nominal mentions of various kinds

72
00:04:20,300 --> 00:04:23,130
and that means we're going to have some kind of generative model

73
00:04:23,190 --> 00:04:26,260
the generative model and i'm being deliberately vague because we're going to go through a

74
00:04:26,260 --> 00:04:29,850
lot of iterations of this fairly quickly the generative model is going to have this

75
00:04:29,850 --> 00:04:31,180
kind of flavor

76
00:04:31,230 --> 00:04:34,760
there's a bunch of slots in which i have to say something

77
00:04:34,770 --> 00:04:38,130
and i'm going to draw a bunch of entities that i'm going to keep talking

78
00:04:38,130 --> 00:04:41,270
about for some reason discourses seem to be about

79
00:04:42,210 --> 00:04:46,270
so i have some sequence of entities that comes from some prior distribution and we'll

80
00:04:46,270 --> 00:04:47,470
talk about that

81
00:04:47,480 --> 00:04:48,610
and then

82
00:04:48,630 --> 00:04:53,050
i'm gonna go one by one to each of these entities figuring out some linguistic

83
00:04:53,720 --> 00:04:55,530
how to render into a strength

84
00:04:55,610 --> 00:05:00,610
so these underlying entity sometimes pop up as proper name sometimes they pop up pronouns

85
00:05:00,610 --> 00:05:04,060
according to some other data process that the world's going to talk about go through

86
00:05:04,060 --> 00:05:05,940
some iteration

87
00:05:06,710 --> 00:05:07,920
an inference time

88
00:05:07,960 --> 00:05:11,550
what we're going to observe is the mentions and we're going to want to infer

89
00:05:11,550 --> 00:05:13,870
what the entities are

90
00:05:13,890 --> 00:05:16,220
and the reason we're going to want that is basically so we can figure out

91
00:05:16,220 --> 00:05:19,750
which of them are the same and so on

92
00:05:19,760 --> 00:05:22,510
i want to repeat that this particular

93
00:05:23,590 --> 00:05:25,330
it's going to take as input

94
00:05:25,380 --> 00:05:27,300
a bunch of documents

95
00:05:27,330 --> 00:05:31,150
each of which is basically kind of pre segmented into

96
00:05:31,210 --> 00:05:34,680
these kinds of lists of nouns

97
00:05:34,690 --> 00:05:38,040
but we're never going to see an example occurrence this is going to be done

98
00:05:38,040 --> 00:05:43,710
entirely in an unsupervised fashion and model based

99
00:05:46,370 --> 00:05:50,540
this includes the classical anaphora problem thank you a good point this includes the anaphora

100
00:05:50,540 --> 00:05:54,600
problem which is what are the pronouns refer to but also includes is that bush

101
00:05:54,600 --> 00:05:59,230
is the president and the george bush and this document is mister bush not document

102
00:05:59,270 --> 00:06:03,560
so it is a generalization of reference problem thank you

103
00:06:04,210 --> 00:06:06,870
so let's talk about the simplest thing that could possibly work

104
00:06:06,930 --> 00:06:07,830
it will not

105
00:06:07,870 --> 00:06:09,380
and then fix it

106
00:06:09,430 --> 00:06:13,060
a simple thing you can imagine saying well i have a bunch of slots to

107
00:06:13,890 --> 00:06:17,120
for each one i'm going to have a random variable which has to do with

108
00:06:17,120 --> 00:06:18,470
this value is

109
00:06:18,480 --> 00:06:20,250
among the space of entities

110
00:06:20,850 --> 00:06:24,830
i'm writing things like where group and where headquarters really probably just going to be

111
00:06:24,830 --> 00:06:26,830
integers or something like that

112
00:06:26,880 --> 00:06:30,800
and i can imagine that i have some prior distribution over which entities get used

113
00:06:31,020 --> 00:06:35,370
and again it says we're group that's probably entered the number three hundred seventy

114
00:06:35,410 --> 00:06:40,920
then each one of these slots independently gets transformed into a string

115
00:06:41,420 --> 00:06:42,920
what is this parameterized by

116
00:06:42,930 --> 00:06:47,420
well the simplest thing that could possibly work and will not is that each entity

117
00:06:47,420 --> 00:06:50,070
kind comes along with the distribution over strings

118
00:06:50,120 --> 00:06:53,830
this with these would be the mention parameters these characterize

119
00:06:53,880 --> 00:06:57,560
what strings are appropriate for each entity we would have to learn both of these

120
00:06:57,560 --> 00:07:00,800
things from data in an unsupervised fashion in this kind of and

121
00:07:00,860 --> 00:07:06,270
going to call this entity distribution can just the relative frequencies really of these entities

122
00:07:06,420 --> 00:07:11,400
going to be data can be some vector proportions and the mention parameter is there's

123
00:07:11,400 --> 00:07:12,240
going to be

124
00:07:12,250 --> 00:07:13,860
one distribution

125
00:07:13,880 --> 00:07:15,560
for every entity

126
00:07:15,570 --> 00:07:17,980
which is here to the multinomial over strings

127
00:07:17,980 --> 00:07:20,190
we're going to call this fee

128
00:07:20,220 --> 00:07:21,570
those two

129
00:07:21,580 --> 00:07:23,720
we're here there are k different entities

130
00:07:23,720 --> 00:07:24,770
and what this would do

131
00:07:24,780 --> 00:07:28,430
is any position you choose according to their entity and then you look at the

132
00:07:28,430 --> 00:07:30,710
role of multinomial and choose the string

133
00:07:30,770 --> 00:07:34,160
if we happen to have the strings that might even work because then we know

134
00:07:34,160 --> 00:07:38,220
that bush and the president are both strings for the same entity

135
00:07:38,280 --> 00:07:42,120
but we don't have the strings this doesn't work very well and in particular we

136
00:07:42,120 --> 00:07:44,630
don't even know the number of entities k

137
00:07:44,650 --> 00:07:49,370
so this kind of the easiest answer is just erase k right infinity

138
00:07:49,480 --> 00:07:52,000
and we call it the dirichlet process mixture model

139
00:07:52,070 --> 00:07:53,300
that's all going to do

140
00:07:53,310 --> 00:07:56,490
we're not going to determine the number in advance is going to be some inference

141
00:07:56,490 --> 00:07:58,790
that i'm not going to talk about here we use the sampler

142
00:07:59,420 --> 00:08:04,010
that allocates its so how does this work group running numbers there are a lot

143
00:08:04,010 --> 00:08:07,120
of measure of how well you doing in this task the measure can be reporting

144
00:08:07,120 --> 00:08:09,580
here is the mark f one measure

145
00:08:09,640 --> 00:08:13,020
and it turns out you get about fifty four percent this is not a good

146
00:08:13,020 --> 00:08:16,150
number but it may be higher than you think and the reason is higher than

147
00:08:16,150 --> 00:08:20,880
this assumption whether actually walk around in the environment and they touch everything right like

148
00:08:20,900 --> 00:08:23,990
a bomb themselves in every because they don't know maybe that's the magic way out

149
00:08:23,990 --> 00:08:27,150
of the box is to do this crazy thing so so we need to take

150
00:08:27,150 --> 00:08:31,030
advantage of the fact that the world has some regularity therefore the transition functions can

151
00:08:31,030 --> 00:08:35,340
have some regularity so quick learners have some regularity they can exploit that the whole

152
00:08:35,340 --> 00:08:36,820
thing could go a lot faster

153
00:08:36,820 --> 00:08:41,280
so here's a show you one concrete example of this idea

154
00:08:41,300 --> 00:08:44,760
all right so we've got

155
00:08:44,780 --> 00:08:48,670
same kind of environment as before the robot its environment it's got x and y

156
00:08:48,670 --> 00:08:52,780
coordinates it also has an orientation and one more thing to it which is there's

157
00:08:52,780 --> 00:08:58,050
of a ball and environment obstacle moving obstacle stationary obstacles or whatever they are but

158
00:08:58,220 --> 00:09:01,530
moving obstacles are interesting because they become part of the state space so the right

159
00:09:01,530 --> 00:09:04,570
thing to do at a given point in time may depend not only were the

160
00:09:04,570 --> 00:09:07,990
robot is but the obstacle is in relation to the robot

161
00:09:08,550 --> 00:09:11,260
now we've got

162
00:09:11,260 --> 00:09:15,380
potentially very high dimensional even if we discretize the x and y the ball x

163
00:09:15,400 --> 00:09:18,900
y the robot and the state of the robot molotov states and the robots can

164
00:09:18,970 --> 00:09:22,820
you really really slowly it's very often going to encounter stated never seen before it's

165
00:09:22,820 --> 00:09:26,300
going to be exploring for a very long time so you're going to use domain

166
00:09:26,300 --> 00:09:29,670
knowledge to say hey you know what there's some structure in this environment we're going

167
00:09:29,670 --> 00:09:31,110
to build a little

168
00:09:31,130 --> 00:09:34,280
graphical models of that says

169
00:09:34,280 --> 00:09:39,450
as the woman to win at the age and the the robot uses the forward

170
00:09:39,450 --> 00:09:44,530
action the change in the balls x and y coordinates is independent doesn't depend on

171
00:09:44,530 --> 00:09:48,490
anything about where the ball the robot are just move around randomly

172
00:09:48,490 --> 00:09:53,700
i mean this ball really does show the video insects i thought it would have

173
00:09:53,700 --> 00:09:56,860
all kinds of history but i think now it's it's pretty much like i i

174
00:09:56,860 --> 00:09:58,280
d movement

175
00:09:58,300 --> 00:10:03,450
the brownian motion so the robotics and why we're on the forward action the change

176
00:10:03,450 --> 00:10:06,860
in the x and y doesn't change that is only affected by the angle that

177
00:10:06,860 --> 00:10:09,970
the robot is facing so right so if it's space in this way to goes

178
00:10:09,990 --> 00:10:14,700
forward then x y changes certain way according to go robust face in this way

179
00:10:14,700 --> 00:10:18,920
that's why change differently when it moves forward but the but the robots angle actually

180
00:10:18,920 --> 00:10:22,610
is again independent of everything else change in angle so that for the for action

181
00:10:22,630 --> 00:10:25,150
we just write down this model but we don't have to fill in the numbers

182
00:10:25,150 --> 00:10:27,320
here we just say it depends on what

183
00:10:27,340 --> 00:10:30,320
moving but takes a right hand turn actually

184
00:10:30,340 --> 00:10:35,920
there are also independent i think that's the model we use that that x and

185
00:10:35,920 --> 00:10:38,880
y don't depend on the previous section why because it's really not moving x y

186
00:10:38,900 --> 00:10:39,740
just turning

187
00:10:39,880 --> 00:10:41,530
the robotics and why

188
00:10:41,720 --> 00:10:45,360
so ball in the robot and the angle actually does change but it doesn't matter

189
00:10:45,360 --> 00:10:49,150
what the current was going to change relative to whichever way facing so now we

190
00:10:49,150 --> 00:10:51,780
have a model where there's lots of things that are independent of a lot less

191
00:10:51,780 --> 00:10:54,510
to learn this way fewer parameters

192
00:10:55,200 --> 00:10:57,030
and what's nice is that we can

193
00:10:57,050 --> 00:11:00,940
can show you video and then i'll talk about learning so let's say that's not

194
00:11:00,970 --> 00:11:04,420
robot but that is the number hears about wall

195
00:11:04,450 --> 00:11:08,550
just rule all around like that and this is the robot again wearing patterns on

196
00:11:08,550 --> 00:11:11,470
the back on its back so we can detect it from the above camera also

197
00:11:11,470 --> 00:11:14,880
detecting the ball which is distinctive colour and trying to get to this goal location

198
00:11:14,880 --> 00:11:18,170
at the end the reward function is it gets higher reward for getting to the

199
00:11:18,840 --> 00:11:20,570
it gets

200
00:11:20,590 --> 00:11:24,170
a big negative or every time it bumps into the ball gets small negative or

201
00:11:24,170 --> 00:11:27,470
just for not being the goal yet so just can encourage you to get on

202
00:11:27,470 --> 00:11:29,950
with his life

203
00:11:34,340 --> 00:11:38,150
we tried to this is very sort of function approximation with with

204
00:11:38,340 --> 00:11:42,320
with q learning that didn't didn't go very well so we're trying to get down

205
00:11:42,320 --> 00:11:47,490
here and to forty five episodes now this is now thirteen episodes using other model

206
00:11:47,490 --> 00:11:53,740
based algorithm which actually learning the transition probabilities with the conditional independencies from

207
00:11:54,220 --> 00:11:55,920
he was quick learning

208
00:11:55,920 --> 00:11:57,110
from experience

209
00:11:57,130 --> 00:11:59,860
and so you can see it already doing a very good job of pointing in

210
00:11:59,860 --> 00:12:04,130
the right direction the bottom of the falls out of the way this particular case

211
00:12:04,130 --> 00:12:07,590
and it's heading fairly directly towards this corner

212
00:12:07,670 --> 00:12:11,440
and if the ball is close to it you can see it it's a little

213
00:12:11,440 --> 00:12:12,300
bit more

214
00:12:12,320 --> 00:12:14,150
careful get by the ball

215
00:12:14,170 --> 00:12:19,820
so it doesn't look scared but i don't think it's feeling frightened the value function

216
00:12:19,820 --> 00:12:23,320
does drop when the ball gets closer depends on how you define fright now here's

217
00:12:23,320 --> 00:12:26,400
the robot strider down here to the corner here's about ball

218
00:12:26,420 --> 00:12:28,720
but unfortunately placed in its way

219
00:12:28,720 --> 00:12:33,270
but that's OK the robot can wait it out since it's moving randomly it's a

220
00:12:33,270 --> 00:12:37,300
just wait longer no no that's not helping

221
00:12:39,360 --> 00:12:43,170
sometimes it looks less random than others

222
00:12:43,260 --> 00:12:48,400
the the balls kind of attacking good an opening got opening i really had right

223
00:12:48,400 --> 00:12:49,720
to the goal now here we go

224
00:12:49,720 --> 00:12:54,650
just just got kind of face in the right direction and start tracking and unfortunately

225
00:12:54,650 --> 00:12:59,300
the ball has other plans are so

226
00:12:59,320 --> 00:13:03,260
i don't what the couple computational models of frustration look like

227
00:13:03,260 --> 00:13:08,150
i guess if we ever saw the robot actually go up to the biologists

228
00:13:08,240 --> 00:13:11,550
that would count as basically giving up but

229
00:13:11,570 --> 00:13:15,440
it it's patiently waiting for the ball to be in a position where can actually

230
00:13:15,440 --> 00:13:22,090
get towards to go just over there right there is so

231
00:13:22,400 --> 00:13:27,200
so in this example the what's going on here is because we know the conditional

232
00:13:28,440 --> 00:13:33,470
but really it's learning instead of learning the whole transition probability matrix says from this

233
00:13:33,470 --> 00:13:38,900
combination of opposition robot position robert engle here's what the next one looks like instead

234
00:13:38,900 --> 00:13:41,650
of just finding little want to say when you do the forward action what's the

235
00:13:41,650 --> 00:13:47,050
probability distribution over the change in the ball's position robots position and robots state of

236
00:13:47,130 --> 00:13:52,180
all independently and so what you things to learn these are all independently quickly ball

237
00:13:52,200 --> 00:13:57,070
this this idea i think i had on on the previous slide this this observation

238
00:13:57,070 --> 00:14:00,650
that if you know the conditional independence structure you can learn

239
00:14:00,680 --> 00:14:06,090
the transition matrices really efficiently even though there is an exponential number of combinations of

240
00:14:06,840 --> 00:14:11,030
the values of state variables we don't need anywhere near that we need polynomial in

241
00:14:11,030 --> 00:14:14,220
the size of these little many transition

242
00:14:14,240 --> 00:14:16,630
state diagrams so

243
00:14:16,700 --> 00:14:20,570
that observation was made by color and

244
00:14:20,590 --> 00:14:23,300
so they show how you can do this and so i'm not saying anything new

245
00:14:23,300 --> 00:14:26,280
beyond what they said in this particular case but we did discover that for the

246
00:14:26,280 --> 00:14:29,950
role of examples it was nice to have it be the change in variable instead

247
00:14:29,950 --> 00:14:35,010
of predicting from the old value the variable the new values variables so sometimes sometimes

248
00:14:35,010 --> 00:14:38,280
that's the thing that you want to be predicting that which is kind of odd

249
00:14:40,110 --> 00:14:45,990
that's that's a different kind of representation of the transition function that is efficiently learnable

250
00:14:45,990 --> 00:14:49,840
and applies really well and in lots of different circumstances here's another one this is

251
00:14:49,840 --> 00:14:53,550
what we found to be useful in in several settings are continuing to work on

252
00:14:53,720 --> 00:14:59,150
where we represent transitions instead of from state to state or from features to features

253
00:14:59,150 --> 00:15:03,240
like in the DBN case we're going to represent them in terms of object attributes

254
00:15:03,240 --> 00:15:07,030
to object attributes so talking about the world as consisting of a set of objects

255
00:15:07,400 --> 00:15:10,550
so this is this is a figure from a paper which is the taxi problem

256
00:15:10,590 --> 00:15:11,950
i showed you the beginning

257
00:15:11,970 --> 00:15:16,260
it's very convenient in fact i think if you ask yourself what you're doing when

258
00:15:16,260 --> 00:15:19,530
you're exploring in this game to talk about the state in terms of a set

259
00:15:19,530 --> 00:15:23,860
of objects there's taxi it's in some place there's a passenger it in some place

260
00:15:24,110 --> 00:15:28,860
there's walls there in some place that there's a destination it also in some places

261
00:15:28,860 --> 00:15:32,340
and whether two things are in the same place is very relevant to figuring out

262
00:15:32,340 --> 00:15:34,490
what's going to happen when you choose an action

263
00:15:36,090 --> 00:15:39,760
so now instead of yes so so what happens when objects interact that's what the

264
00:15:39,760 --> 00:15:42,740
learner is going to try to figure out and we run these sort of things

265
00:15:42,740 --> 00:15:44,760
if you see this

266
00:15:44,780 --> 00:15:48,030
they also from that

267
00:15:50,780 --> 00:15:53,070
well in this region

268
00:15:53,080 --> 00:15:56,910
next is that you're able to find some

269
00:16:06,260 --> 00:16:07,670
if you want to

270
00:16:11,030 --> 00:16:14,210
so the

271
00:16:14,210 --> 00:16:16,290
this is the result

272
00:16:19,350 --> 00:16:22,000
it is in fact

273
00:16:22,070 --> 00:16:24,640
we years

274
00:16:24,850 --> 00:16:28,280
there is some

275
00:16:29,690 --> 00:16:33,270
if we have

276
00:16:34,690 --> 00:16:38,840
she for that

277
00:17:09,510 --> 00:17:15,070
we have

278
00:17:15,090 --> 00:17:19,800
so this

279
00:17:20,960 --> 00:17:22,480
here is

280
00:17:22,500 --> 00:17:25,230
so these

281
00:17:25,290 --> 00:17:26,520
this is

282
00:17:26,530 --> 00:17:29,160
the universe is

283
00:17:38,280 --> 00:17:40,330
all one

284
00:17:46,550 --> 00:17:52,980
and i should

285
00:17:53,330 --> 00:17:55,790
it doesn't

286
00:18:01,540 --> 00:18:06,420
and that was a

287
00:18:06,430 --> 00:18:09,760
so thing

288
00:18:09,790 --> 00:18:11,470
at some point in the

289
00:18:11,480 --> 00:18:16,320
very sensitive to the presence of

290
00:18:16,350 --> 00:18:19,780
not all these

291
00:18:22,850 --> 00:18:27,060
sensitive to the service

292
00:18:32,310 --> 00:18:37,240
so we

293
00:18:49,660 --> 00:18:57,890
and the the the the the and

294
00:18:58,180 --> 00:19:00,690
point three

295
00:19:00,700 --> 00:19:04,530
many of you

296
00:19:34,110 --> 00:19:36,570
one four

297
00:20:03,270 --> 00:20:05,730
it would me

298
00:20:07,190 --> 00:20:09,000
this me

299
00:20:11,160 --> 00:20:17,000
he's that trying to make sure that people said

300
00:20:17,020 --> 00:20:18,510
thank you you

301
00:20:20,740 --> 00:20:21,830
every image

302
00:20:26,850 --> 00:20:28,390
what they believe that

303
00:20:28,420 --> 00:20:31,000
you can

304
00:20:31,050 --> 00:20:34,480
to use the next two years

305
00:20:34,500 --> 00:20:37,440
the reason i'm trying to identify

306
00:20:41,710 --> 00:20:43,160
where are you

307
00:20:43,180 --> 00:20:44,270
article twenty three

308
00:20:49,310 --> 00:20:50,750
they want to get

309
00:20:53,650 --> 00:20:55,730
looking more

310
00:20:55,950 --> 00:20:59,090
information about

311
00:20:59,260 --> 00:21:10,760
so what

312
00:21:10,770 --> 00:21:13,760
the action

313
00:21:13,770 --> 00:21:17,830
actually they can

314
00:21:17,870 --> 00:21:20,410
by this to me

315
00:21:20,410 --> 00:21:23,270
so basically it is used as prior

316
00:21:23,270 --> 00:21:25,540
if we model the FCC

317
00:21:25,750 --> 00:21:28,420
crystal as hard spheres touching

318
00:21:28,480 --> 00:21:33,160
how much of this is no matter how much of it is free space it's

319
00:21:33,160 --> 00:21:35,190
given by the packing density

320
00:21:35,190 --> 00:21:41,270
packing density that's equal to the volume of the atoms

321
00:21:41,350 --> 00:21:44,230
divided by total volume

322
00:21:44,390 --> 00:21:48,480
the total volume so if we choose the unit cell is the basis so we've

323
00:21:48,480 --> 00:21:50,940
got something to count on let's

324
00:21:50,960 --> 00:21:55,600
let's go back so we know that the going the other like this secure drawn

325
00:21:55,600 --> 00:22:01,480
from ok so the volume of the atoms while I get Adams equivalent of atoms

326
00:22:01,480 --> 00:22:05,730
and what's the volume of an atom it's going to be 4 thirds times pi

327
00:22:05,750 --> 00:22:11,850
times the radius of the atom Q and what's the volume of that unit cell

328
00:22:12,190 --> 00:22:16,020
that unit cell is a cube and now I need to know the relationship between

329
00:22:16,020 --> 00:22:20,480
a and are assigned convert 1 to the other and then I'll have a dimensionless

330
00:22:20,480 --> 00:22:24,620
group here so there you can see the relationship between an based where was a

331
00:22:24,620 --> 00:22:26,690
squared is

332
00:22:26,690 --> 00:22:30,410
the for the year 16 hours whereas if you go through all that you can

333
00:22:30,410 --> 00:22:37,120
convince yourself that a equals to the roots of the 2 are in FCC so

334
00:22:37,120 --> 00:22:42,830
plug that into their and you'll end up with the packing density will be equal

335
00:22:42,830 --> 00:22:48,820
to pi divided by 3 times the square root of 2 which is value of

336
00:22:48,820 --> 00:22:50,370
0 . 7 4

337
00:22:50,480 --> 00:22:56,350
so that means in face centered cubic and face centered cubic lattice you have 74

338
00:22:56,350 --> 00:23:05,770
per cent of matter this is assuming hard-sphere model hard-sphere model here so big that

339
00:23:05,770 --> 00:23:07,040
they touch

340
00:23:07,270 --> 00:23:13,790
so that's 26 % avoid 26 % voice and you go through and you can

341
00:23:13,790 --> 00:23:19,040
see that as you look at this table that the packing decreases in the case

342
00:23:19,040 --> 00:23:23,790
of a body centered cubic is a little more open spaces 68 % and in

343
00:23:23,790 --> 00:23:25,020
case of

344
00:23:25,210 --> 00:23:29,640
In the case of our simple cubic it's only about half pack so

345
00:23:30,420 --> 00:23:32,310
this table here is something that

346
00:23:32,850 --> 00:23:38,460
you wanna get confronted with but they're going to show you is the crystallographic notation

347
00:23:38,910 --> 00:23:45,160
and whether they want to do is are invited to look at the website this

348
00:23:45,160 --> 00:23:47,600
this will give you the answer

349
00:23:47,970 --> 00:23:54,330
the nature of the crystal graphic notation position is basically as it is in your

350
00:23:54,330 --> 00:23:59,870
math classes the only differences we don't put enclosures no parenthesis around the point of

351
00:23:59,870 --> 00:24:04,440
the origin is 0 0 0 but don't put parenthesis around so point is simply

352
00:24:04,440 --> 00:24:04,980
written as

353
00:24:06,790 --> 00:24:12,050
follows so origin 4 . 0 3 0

354
00:24:12,330 --> 00:24:16,870
and that's all there is to it in the case of direction so that this

355
00:24:16,870 --> 00:24:19,120
is going to take a little more time than I have

356
00:24:19,140 --> 00:24:23,310
a set aside because of got a few items of business that I need to

357
00:24:23,350 --> 00:24:28,210
take care of so I think I'm going to have a little hold at this

358
00:24:28,210 --> 00:24:33,660
point and will jump ahead to this is to always have a few things obviously

359
00:24:34,270 --> 00:24:38,690
OK so let's let's go back to the beginning of when people came in as

360
00:24:38,690 --> 00:24:43,790
is the custom we have music playing and what was the music today

361
00:24:45,110 --> 00:24:48,310
yeah I'm talking heads and the song was

362
00:24:48,510 --> 00:24:51,880
burning down the house so why would be playing burning down the house in a

363
00:24:51,880 --> 00:24:58,530
lecture about crystallography well what inspired the inflated to choose that piece of music was

364
00:24:58,530 --> 00:25:04,510
this painting houses that was status was painted by George Braque in 1908 stack is

365
00:25:04,510 --> 00:25:07,290
a community down in the southern part of France

366
00:25:08,020 --> 00:25:14,500
and about when fractionated this he outraged the critics of the someone in Paris who

367
00:25:14,500 --> 00:25:18,940
condemned this painting they said look at it doesn't have a common perspective that that

368
00:25:18,940 --> 00:25:23,270
but actually all of the problems

369
00:25:23,290 --> 00:25:25,560
he talked about that i've talked about

370
00:25:25,610 --> 00:25:26,670
until now

371
00:25:26,690 --> 00:25:27,860
still exists

372
00:25:28,420 --> 00:25:31,480
we're going to structure it can do the job

373
00:25:31,540 --> 00:25:36,110
if we can pick these

374
00:25:36,210 --> 00:25:39,420
OK so

375
00:25:39,460 --> 00:25:44,360
one of the questions we might ask first of all is what does it do

376
00:25:44,360 --> 00:25:47,630
this is the only equation early on in the show

377
00:25:48,000 --> 00:25:50,730
i'm going to talk about this in terms of

378
00:25:50,750 --> 00:25:55,710
minimizing the sum of squares error but this this similar results for

379
00:25:55,790 --> 00:25:59,480
a whole variety of tasks that we might be

380
00:26:00,610 --> 00:26:02,960
seeking to carry out

381
00:26:03,000 --> 00:26:05,190
OK it turns out

382
00:26:05,270 --> 00:26:08,150
it's not actually that difficult to prove that

383
00:26:08,210 --> 00:26:13,350
minimizing the sum of squares error is equivalent to finding the conditional mean

384
00:26:13,350 --> 00:26:15,130
the target date

385
00:26:15,190 --> 00:26:19,310
it is precisely what we want to do this is this progression property

386
00:26:19,710 --> 00:26:22,250
but it was mentioned earlier

387
00:26:23,150 --> 00:26:28,170
so if we just look at the the theoretical problem rather than our real problem

388
00:26:28,170 --> 00:26:31,080
which is yes we're going to go to the data

389
00:26:32,900 --> 00:26:37,090
this question which i discussed you will look at the minute if we assume that

390
00:26:37,090 --> 00:26:42,440
we found the set of weights that gives us the best possible answer

391
00:26:42,480 --> 00:26:44,540
to this problem

392
00:26:44,540 --> 00:26:47,380
and the we've got infinite amount of data

393
00:26:47,380 --> 00:26:51,360
then we are in a position to rewrite the

394
00:26:51,400 --> 00:26:53,710
sum of squares error

395
00:26:53,730 --> 00:26:57,610
according j infinity infinity infinity there just to indicate

396
00:26:58,210 --> 00:27:00,520
it's a theoretical

397
00:27:03,420 --> 00:27:08,040
we we can rewrite is the sum of two components became

398
00:27:08,060 --> 00:27:11,540
it's the average of the square of of this

399
00:27:11,540 --> 00:27:13,020
o thing in brackets here

400
00:27:13,040 --> 00:27:15,310
plus the average of the square

401
00:27:15,310 --> 00:27:16,590
this thing

402
00:27:16,650 --> 00:27:18,650
records here OK

403
00:27:18,670 --> 00:27:23,130
the some asian here is because of this in the general form where you might

404
00:27:23,130 --> 00:27:24,400
have multiple

405
00:27:30,110 --> 00:27:31,830
it's just the probability

406
00:27:31,850 --> 00:27:33,060
density function

407
00:27:33,110 --> 00:27:34,860
of x

408
00:27:35,210 --> 00:27:38,830
he is the expectation operator

409
00:27:38,880 --> 00:27:41,690
so we've got two components here

410
00:27:42,880 --> 00:27:44,650
the first one

411
00:27:44,730 --> 00:27:46,130
is the

412
00:27:46,170 --> 00:27:48,850
expectation mean value of

413
00:27:48,900 --> 00:27:51,130
the target data

414
00:27:51,130 --> 00:27:53,380
at a particular point ex

415
00:27:53,440 --> 00:27:54,940
in particular

416
00:27:55,000 --> 00:27:56,290
value of

417
00:27:56,310 --> 00:27:58,630
the input

418
00:28:01,250 --> 00:28:03,400
of the function

419
00:28:03,400 --> 00:28:06,810
which turns out to be the multilayer perceptron itself of course

420
00:28:06,860 --> 00:28:09,380
which is self is a function of x

421
00:28:09,400 --> 00:28:14,730
and the function of these the curly w to indicate all of the weights

422
00:28:14,750 --> 00:28:15,670
and the thing

423
00:28:15,670 --> 00:28:18,230
not just a single vector y

424
00:28:19,230 --> 00:28:20,580
so we take that

425
00:28:20,610 --> 00:28:24,790
difference we square and we find the average of it

426
00:28:25,880 --> 00:28:28,150
and added to that is

427
00:28:28,150 --> 00:28:32,440
so the term here which is the difference between

428
00:28:32,520 --> 00:28:34,060
the target data

429
00:28:34,060 --> 00:28:39,790
the average of the target data a particular value of x

430
00:28:40,230 --> 00:28:45,310
actually should be substituted with i suppose x is

431
00:28:45,520 --> 00:28:49,020
look at it

432
00:28:49,610 --> 00:28:53,960
should a

433
00:28:55,420 --> 00:28:59,460
the crucial thing is this one here doesn't depend on the weights

434
00:28:59,520 --> 00:29:00,350
of the

435
00:29:00,400 --> 00:29:02,520
multilayer perceptron at all

436
00:29:03,460 --> 00:29:04,920
this is just some

437
00:29:04,960 --> 00:29:06,610
residual error

438
00:29:06,670 --> 00:29:07,960
but cannot be

439
00:29:08,020 --> 00:29:11,710
can be changed by the introduction of this

440
00:29:12,360 --> 00:29:15,000
this neural network

441
00:29:15,060 --> 00:29:18,210
OK so we can say is that if we want to make j as small

442
00:29:18,210 --> 00:29:23,400
as possible we can't do anything about that by choosing w so we can ignore

443
00:29:23,400 --> 00:29:25,060
to make

444
00:29:25,060 --> 00:29:27,900
the error as small as possible we have to make

445
00:29:27,940 --> 00:29:29,610
the function itself

446
00:29:29,630 --> 00:29:31,460
equal to

447
00:29:31,560 --> 00:29:36,540
what's called the conditional mean the expected value of z given the x

448
00:29:38,460 --> 00:29:41,500
so under these very extreme

449
00:29:41,500 --> 00:29:44,170
theoretical conditions we see that

450
00:29:45,770 --> 00:29:48,020
what we're trying to do is

451
00:29:48,060 --> 00:29:49,460
the right thing

452
00:29:49,770 --> 00:29:54,730
in reality we have a finite amount of data so

453
00:29:54,790 --> 00:29:56,900
it will just simply be

454
00:29:56,940 --> 00:29:57,960
and estimate

455
00:29:57,980 --> 00:29:59,650
of the right thing

456
00:29:59,810 --> 00:30:05,400
nonetheless it's encouraging

457
00:30:11,230 --> 00:30:17,420
if you knew it yes

458
00:30:17,480 --> 00:30:20,080
what is indicating is that if you if you

459
00:30:20,130 --> 00:30:20,810
in the

460
00:30:20,860 --> 00:30:23,880
extreme case where you have an infinite amount of data

461
00:30:24,150 --> 00:30:27,110
covering all of x basically

462
00:30:28,270 --> 00:30:29,630
by making

463
00:30:29,670 --> 00:30:34,880
by driving j to its global minimum by an appropriate choice of w

464
00:30:34,900 --> 00:30:36,540
you get a quality there

465
00:30:36,540 --> 00:30:40,860
right so this morning i'm going to talk about forces in this talk and then

466
00:30:40,860 --> 00:30:42,610
we'll talk about putting it all together

467
00:30:42,620 --> 00:30:43,880
in the next one

468
00:30:43,900 --> 00:30:45,750
let me start with gravity

469
00:30:45,770 --> 00:30:51,090
seeing as particle physics tend to ignore gravity but sometimes you can't but let me

470
00:30:51,090 --> 00:30:54,080
show you what it is

471
00:30:54,130 --> 00:30:58,770
the forces that we tend to deal with our electromagnetic weak and strong

472
00:30:59,430 --> 00:31:01,110
electrostatic force

473
00:31:01,110 --> 00:31:03,470
you know in hydrogen is an inverse square law

474
00:31:03,490 --> 00:31:06,060
and gravity is also an inverse square law

475
00:31:06,080 --> 00:31:10,640
so to give an idea of how feeble gravity is you could ask

476
00:31:10,660 --> 00:31:15,030
the electron and the proton hydrogen they have an electrostatic potential energy

477
00:31:15,050 --> 00:31:20,420
and they have also gravitational potential energy between them as is the ratio of the

478
00:31:20,440 --> 00:31:21,740
potential energy

479
00:31:21,760 --> 00:31:24,420
it is ten to the minus forty

480
00:31:24,480 --> 00:31:26,160
it is a small number

481
00:31:26,190 --> 00:31:29,850
and to give you an idea of how small it is if you took the

482
00:31:29,850 --> 00:31:34,420
size of a single proton ten to the minus fifteen metres

483
00:31:34,420 --> 00:31:37,990
and compared it to the size of the observable universe which is about ten to

484
00:31:37,990 --> 00:31:39,730
twenty six metres

485
00:31:39,740 --> 00:31:42,900
that is about ten to forty to to forty one

486
00:31:42,930 --> 00:31:46,850
what is the magnitude so if you imagine a single proton compared to the whole

487
00:31:46,850 --> 00:31:52,060
observable universe that is like the strength of gravity compared to electromagnetic forces between individual

488
00:31:52,060 --> 00:31:57,180
particles therefore we ignore them between individual particles

489
00:31:57,200 --> 00:32:00,700
of course gravity has the property adding up the wall part you have the more

490
00:32:01,090 --> 00:32:06,320
the gravitational forces and there are some places where it reveals some unusual things not

491
00:32:06,320 --> 00:32:10,310
just in falling apples in the problem of dark matter so let me to show

492
00:32:10,310 --> 00:32:13,040
you the background to this

493
00:32:13,060 --> 00:32:15,710
if you have a large mass like the sun

494
00:32:15,730 --> 00:32:20,620
with small matters like planets going around and then you know newton's inverse square law

495
00:32:20,620 --> 00:32:22,880
tells us the force between them

496
00:32:22,900 --> 00:32:25,350
you equate that with these were are

497
00:32:25,400 --> 00:32:29,510
you can work out what the velocity profile of those objects is a function of

498
00:32:29,510 --> 00:32:35,120
distance you find that the speed drop like the square root of are so the

499
00:32:35,120 --> 00:32:38,490
outer planets moving slower than the inner ones

500
00:32:38,510 --> 00:32:42,210
that is indeed what happens to the planet

501
00:32:42,210 --> 00:32:48,140
but if you look at some spiral galaxies where you can measure the relative speeds

502
00:32:48,140 --> 00:32:53,360
of the stars by the doppler shift of the spectral line you find those there's

503
00:32:53,380 --> 00:32:55,380
clearly a lot of mass in the middle of

504
00:32:55,410 --> 00:32:57,850
these stars on the outside

505
00:32:57,910 --> 00:33:00,160
but the velocity profile seems to me

506
00:33:01,160 --> 00:33:04,010
independent distance rather dying away

507
00:33:04,030 --> 00:33:09,220
this is large error bars on this and varies from one galaxy to another overall

508
00:33:09,280 --> 00:33:10,690
you get this impression

509
00:33:11,580 --> 00:33:14,640
stars are not following the rules

510
00:33:14,660 --> 00:33:18,380
and one of two opportunities are either you say this shows that on these distance

511
00:33:18,380 --> 00:33:20,290
scales newton's laws not correct

512
00:33:21,410 --> 00:33:25,380
you can play that game is very difficult to convince convince you fit it with

513
00:33:25,380 --> 00:33:28,980
other data or that small

514
00:33:29,480 --> 00:33:30,850
stuff there

515
00:33:30,860 --> 00:33:34,630
giving gravitational forces that you're actually seeing your telescope

516
00:33:34,910 --> 00:33:39,030
and this is what gives rise to the idea of dark matter namely stuff that

517
00:33:39,030 --> 00:33:40,320
does not shine

518
00:33:40,360 --> 00:33:44,010
in electromagnetic radiation of any wavelength

519
00:33:45,380 --> 00:33:47,720
in gravitational force which affects the motion

520
00:33:47,730 --> 00:33:48,540
one of the

521
00:33:48,540 --> 00:33:50,320
stars and galaxies

522
00:33:50,330 --> 00:33:53,780
it turns out when you do the sums that you need

523
00:33:53,790 --> 00:33:57,760
the dominant stuff in the universe to be this dark stuff unseen

524
00:33:57,780 --> 00:33:59,230
so that's the observation

525
00:33:59,260 --> 00:34:02,950
from cosmology and the nature of the dark matter is a big puzzle

526
00:34:02,970 --> 00:34:08,780
one of the possibilities at the LHC is that we will discover new particles

527
00:34:08,790 --> 00:34:14,420
call supersymmetric particles which have perhaps the properties of dark matter and i'll mention that

528
00:34:14,420 --> 00:34:17,500
later on that if it's true it will be wonderful

529
00:34:17,530 --> 00:34:20,110
synthesis between particle physics

530
00:34:20,130 --> 00:34:22,640
and cosmology

531
00:34:22,660 --> 00:34:27,410
so what is that we know that whatever consists of has to be electrically neutral

532
00:34:27,420 --> 00:34:33,330
and those two scenarios called hot dark matter and cold dark matter hotly moving relativistic

533
00:34:33,330 --> 00:34:34,420
for example

534
00:34:34,440 --> 00:34:37,100
if neutrinos had about

535
00:34:37,110 --> 00:34:38,820
small mass

536
00:34:38,830 --> 00:34:42,360
it's possible that they collectively

537
00:34:42,380 --> 00:34:47,040
because i wouldn't shine they don't feel electromagnetic forces the electric charge

538
00:34:47,160 --> 00:34:51,800
but the gravity could add up and out why everything it turns out when you

539
00:34:51,800 --> 00:34:54,360
do the details sums it doesn't seem to work

540
00:34:54,420 --> 00:34:58,650
the extreme cold dark matter very massive electrically neutral object

541
00:34:58,690 --> 00:34:59,970
that is

542
00:34:59,980 --> 00:35:03,930
what city might provide us with and we'll talk about that the next lecture

543
00:35:04,100 --> 00:35:09,030
that's not to say that gravity and i want to talk about the strong and

544
00:35:09,030 --> 00:35:12,830
the electromagnetic and weak forces so the strong force

545
00:35:12,840 --> 00:35:17,360
which we met i guess first of all students between the protons and neutrons the

546
00:35:17,360 --> 00:35:18,640
biosphere the nuclei

547
00:35:19,240 --> 00:35:20,710
we now know

548
00:35:20,780 --> 00:35:27,870
is remnant of deeper force acting on the quarks inside the proton the neutron

549
00:35:27,890 --> 00:35:31,150
and i'm going to show you how that force works

550
00:35:31,160 --> 00:35:32,280
and show you

551
00:35:32,290 --> 00:35:33,900
how similar it is

552
00:35:34,920 --> 00:35:37,430
with the good old electromagnetic force

553
00:35:38,470 --> 00:35:42,530
the idea is that quarks in addition to carrying electric charge

554
00:35:42,540 --> 00:35:45,000
carry another form of charge

555
00:35:45,030 --> 00:35:46,640
which we call color

556
00:35:46,650 --> 00:35:49,280
nine of twenty one

557
00:35:49,290 --> 00:35:53,390
and the rules of attraction and repulsion between color charges

558
00:35:53,410 --> 00:35:57,580
are the same as you meet between electric charges

559
00:35:57,590 --> 00:35:58,980
so for example

560
00:35:59,000 --> 00:36:01,170
quarks carry positive

561
00:36:01,180 --> 00:36:06,540
colour charges it whereas electric charge comes in only one variety plus the number of

562
00:36:06,540 --> 00:36:07,920
minus the number

563
00:36:07,930 --> 00:36:10,850
this charge

564
00:36:10,900 --> 00:36:14,540
comes in three varieties and just to give them in a little red yellow green

565
00:36:14,740 --> 00:36:16,050
red yellow and blue

566
00:36:17,420 --> 00:36:23,730
three different coloured possibilities for quarks positive and clerks will have three

567
00:36:23,750 --> 00:36:25,280
colours negative

568
00:36:27,500 --> 00:36:34,440
the problem is somebody can turn this thing down

569
00:36:37,600 --> 00:36:41,800
they apply some of the rules that you used to electrostatics where the rules are

570
00:36:41,800 --> 00:36:43,990
and to reconstruct

571
00:36:43,990 --> 00:36:45,800
it's an interesting thing to do

572
00:36:45,810 --> 00:36:47,550
and if you feel like it

573
00:36:47,590 --> 00:36:51,480
i would say give it a shot

574
00:36:51,550 --> 00:36:53,290
all right so far

575
00:36:53,330 --> 00:36:56,890
about speeds and average velocities

576
00:36:56,890 --> 00:36:58,930
and accelerations

577
00:36:59,070 --> 00:37:01,680
let's not go to

578
00:37:04,030 --> 00:37:05,520
three dimensional

579
00:37:05,650 --> 00:37:07,440
trajectories trajectories

580
00:37:08,390 --> 00:37:09,920
thank goodness

581
00:37:10,180 --> 00:37:13,220
almost never three-dimensional

582
00:37:13,270 --> 00:37:17,580
they're always two-dimensional because the trajectory itself is in a vertical plane

583
00:37:17,590 --> 00:37:18,590
and so we

584
00:37:19,920 --> 00:37:23,880
when we throw up an object in a gravitational field

585
00:37:23,920 --> 00:37:25,800
you have

586
00:37:25,850 --> 00:37:28,440
the trajectory in the plane

587
00:37:30,230 --> 00:37:34,740
so we'll going to

588
00:37:34,750 --> 00:37:37,460
have long-term trajectory

589
00:37:37,470 --> 00:37:39,030
this b

590
00:37:39,260 --> 00:37:41,530
next direction

591
00:37:41,550 --> 00:37:45,250
and that this week

592
00:37:45,250 --> 00:37:47,970
one direction increasing values

593
00:37:47,990 --> 00:37:50,380
why increasing values

594
00:37:50,400 --> 00:37:52,810
of x

595
00:37:52,830 --> 00:37:54,850
i think an object

596
00:37:54,860 --> 00:37:58,010
and i throw it out

597
00:37:58,030 --> 00:38:00,190
with an initial velocity

598
00:38:00,190 --> 00:38:01,240
the zero

599
00:38:01,320 --> 00:38:03,270
what is the object going to do

600
00:38:03,360 --> 00:38:06,210
you're going to get bradley

601
00:38:06,240 --> 00:38:09,390
and and for friends of gravity

602
00:38:09,460 --> 00:38:12,960
and it comes down here again

603
00:38:13,000 --> 00:38:15,250
and then we have this kind of problem

604
00:38:15,250 --> 00:38:17,270
we will decompose it

605
00:38:18,270 --> 00:38:22,050
two one dimensional motions one in the x direction

606
00:38:22,100 --> 00:38:23,960
and one in

607
00:38:23,960 --> 00:38:26,280
the y direction

608
00:38:26,340 --> 00:38:29,200
we already decompose right away

609
00:38:31,810 --> 00:38:34,390
at time t equals zero

610
00:38:34,390 --> 00:38:35,470
in two

611
00:38:35,520 --> 00:38:38,830
component which i call the zero x

612
00:38:38,840 --> 00:38:41,030
and that of course is the zero

613
00:38:41,050 --> 00:38:43,180
times the cosine of of

614
00:38:43,180 --> 00:38:44,220
if the angle

615
00:38:44,230 --> 00:38:46,430
it's all far

616
00:38:46,480 --> 00:38:47,390
and the

617
00:38:47,400 --> 00:38:50,220
o velocity in the y direction

618
00:38:50,260 --> 00:38:52,470
at time t equals zero

619
00:38:52,470 --> 00:38:55,900
i will call that the zero in the y direction

620
00:38:55,950 --> 00:38:57,810
and that is the zero

621
00:38:57,830 --> 00:38:59,470
i'm sign of

622
00:38:59,550 --> 00:39:04,090
and now

623
00:39:04,190 --> 00:39:05,490
i have to know how

624
00:39:05,520 --> 00:39:08,410
the object moves in the x direction as a function of time and how it

625
00:39:08,410 --> 00:39:10,220
behaves as a function

626
00:39:10,220 --> 00:39:13,510
time in the y direction so here come the equations

627
00:39:14,300 --> 00:39:16,250
forty x direction

628
00:39:16,330 --> 00:39:18,730
x as a function of time

629
00:39:18,770 --> 00:39:20,490
because x zero

630
00:39:20,570 --> 00:39:23,890
plus the zero x times t

631
00:39:23,890 --> 00:39:29,110
that's all there is no acceleration

632
00:39:29,170 --> 00:39:32,450
the velocity in the x direction as a function of time

633
00:39:33,430 --> 00:39:34,650
the zero x

634
00:39:34,660 --> 00:39:36,860
you never changes

635
00:39:36,950 --> 00:39:37,890
so that

636
00:39:38,470 --> 00:39:41,490
x direction

637
00:39:41,510 --> 00:39:44,700
now we take the y direction

638
00:39:44,730 --> 00:39:49,180
y as a function of time equals y zero

639
00:39:52,080 --> 00:39:53,850
these are white t

640
00:39:53,900 --> 00:39:58,180
plus one half a square

641
00:39:59,560 --> 00:40:01,780
g value that i'm going to use

642
00:40:01,810 --> 00:40:06,800
is always positive you nine point eight meters per second squared or sometimes i make

643
00:40:06,800 --> 00:40:07,600
it easy

644
00:40:07,610 --> 00:40:09,150
to use ten

645
00:40:09,150 --> 00:40:11,150
mine is always positive

646
00:40:11,190 --> 00:40:13,420
and since in this case

647
00:40:13,470 --> 00:40:17,410
i have chosen this could be the increasing value of y that's the only reason

648
00:40:17,410 --> 00:40:19,380
why i would now have to put in

649
00:40:19,420 --> 00:40:21,020
minus one have

650
00:40:21,040 --> 00:40:26,190
GT square not as some of you think because the acceleration is down that's not

651
00:40:26,190 --> 00:40:30,560
a reason because i could have called this direction increasing y

652
00:40:30,580 --> 00:40:32,160
then it would have been plus

653
00:40:32,200 --> 00:40:36,610
one how do you square so the consequence of my choosing this the direction which

654
00:40:36,660 --> 00:40:37,940
why increases

655
00:40:37,950 --> 00:40:40,470
therefore the plus one half

656
00:40:40,480 --> 00:40:43,480
eighty square and that you would normally see

657
00:40:43,520 --> 00:40:45,360
i'm going to replace that now

658
00:40:45,380 --> 00:40:46,750
by minus

659
00:40:46,770 --> 00:40:48,450
one half

660
00:40:48,520 --> 00:40:51,710
GT square

661
00:40:51,710 --> 00:40:54,910
than the velocity in the y direction as a function of time would be because

662
00:40:54,920 --> 00:40:56,340
the derivative

663
00:40:56,360 --> 00:40:57,570
that is the zero

664
00:41:01,900 --> 00:41:03,460
any acceleration

665
00:41:03,470 --> 00:41:05,200
equals minus two

666
00:41:05,250 --> 00:41:08,340
so these ideas three equations that govern the motion

667
00:41:08,360 --> 00:41:09,450
in the

668
00:41:09,490 --> 00:41:14,670
y direction

669
00:41:14,700 --> 00:41:15,680
there's only

670
00:41:15,690 --> 00:41:17,700
holds if there is no

671
00:41:17,750 --> 00:41:22,590
and ragno friction of any kind that is very unrealistic

672
00:41:22,650 --> 00:41:26,040
if any earth but when we are far away from earth

673
00:41:26,100 --> 00:41:26,960
as we

674
00:41:26,960 --> 00:41:31,110
where was the KC one thirty five

675
00:41:31,140 --> 00:41:35,470
which was flying at an altitude of above thirty thousand feet

676
00:41:35,490 --> 00:41:37,750
that of course is a little bit more

677
00:41:38,920 --> 00:41:42,660
and therefore the example that i have picked to throw up an object

678
00:41:42,730 --> 00:41:43,580
is the one

679
00:41:43,600 --> 00:41:45,670
by the KC one thirty five

680
00:41:45,710 --> 00:41:49,300
at an altitude somewhere around twenty five thirty thousand feet

681
00:41:49,390 --> 00:41:50,990
comes in at speed

682
00:41:51,050 --> 00:41:53,550
of four hundred twenty five miles per hour

683
00:41:53,560 --> 00:41:54,980
turns the engines of

684
00:41:54,980 --> 00:41:57,550
and then for the remaining

685
00:41:57,650 --> 00:42:00,130
whatever it was about thirty seconds

686
00:42:00,180 --> 00:42:02,210
everyone including the airplane

687
00:42:02,220 --> 00:42:03,340
has no weight

688
00:42:03,360 --> 00:42:04,510
that's the case

689
00:42:04,520 --> 00:42:06,280
and i want to work out

690
00:42:06,290 --> 00:42:08,500
quantitatively with

691
00:42:08,510 --> 00:42:09,750
in the case of the

692
00:42:09,790 --> 00:42:13,940
KC thirty five will take an angle of forty five degrees

693
00:42:13,940 --> 00:42:15,920
and we'll take zero

694
00:42:15,940 --> 00:42:17,420
which was

695
00:42:17,460 --> 00:42:21,480
about four hundred twenty five miles per hour may remember that

696
00:42:22,330 --> 00:42:23,260
that lectures

697
00:42:23,260 --> 00:42:29,140
supposedly if you walk towards the coast

698
00:42:29,180 --> 00:42:32,990
with some cellphones you will be able to get some signal at least that seen

699
00:42:33,890 --> 00:42:35,650
occasionally succeeding

700
00:42:35,730 --> 00:42:37,490
three years ago so

701
00:42:40,810 --> 00:42:45,310
now i mean i've seen people making phone calls them so

702
00:42:46,780 --> 00:42:50,190
and i talking about wasn't very tall

703
00:42:51,710 --> 00:42:54,760
for those united states was clock

704
00:42:58,380 --> 00:43:00,060
third lecture after

705
00:43:00,070 --> 00:43:03,570
this afternoon's present running can so now will be

706
00:43:03,590 --> 00:43:05,680
to getting a little bit too

707
00:43:06,670 --> 00:43:09,290
this is my around the fifties

708
00:43:09,350 --> 00:43:13,670
this actuary get thing combination into the mind

709
00:43:13,770 --> 00:43:15,740
moving slightly forward

710
00:43:16,490 --> 00:43:17,160
we have

711
00:43:17,170 --> 00:43:21,570
pretty much sidestepping the real networks

712
00:43:21,670 --> 00:43:24,430
i will talk about him through all the perceptron

713
00:43:26,560 --> 00:43:31,070
depending on how we go will actually do the convergence proof

714
00:43:31,120 --> 00:43:35,280
and then we'll talk about feature mapping

715
00:43:38,340 --> 00:43:42,950
i would start with heavy the have been the first you into positive feedback

716
00:43:43,010 --> 00:43:45,870
then we look at linear separability

717
00:43:45,880 --> 00:43:47,100
and then will go

718
00:43:47,120 --> 00:43:48,990
and explicitly look at

719
00:43:49,070 --> 00:43:52,240
features how we can construct another can do it efficiently

720
00:43:53,340 --> 00:43:57,060
in the end well few shortcuts and get to the kernel

721
00:43:57,100 --> 00:43:58,710
as kernels and nothing else

722
00:43:58,760 --> 00:44:05,400
then explicit feature constructions which i can compute very efficiently

723
00:44:07,290 --> 00:44:11,350
what i have always come from so this is where the biology was actually useful

724
00:44:11,490 --> 00:44:16,930
so the idea is that that behaviour should be somehow rewarding bad behavior punished

725
00:44:16,980 --> 00:44:18,170
so for instance

726
00:44:18,180 --> 00:44:21,430
if you hit the target of the hidden temple you kill the tiger other the

727
00:44:22,380 --> 00:44:26,230
this should be rewarded private i the big piece of meat and maybe you will

728
00:44:26,840 --> 00:44:28,290
able to procreate

729
00:44:28,310 --> 00:44:33,690
so the other thing is correlated means should be combined so for instance pavlov's salivating

730
00:44:33,690 --> 00:44:35,040
dogs the

731
00:44:35,080 --> 00:44:39,450
really for for example when they learned that the feature of him getting food was

732
00:44:44,140 --> 00:44:47,790
you can have training mechanisms in the following way that you

733
00:44:47,850 --> 00:44:50,920
what if i individuals are you know just showing them

734
00:44:50,980 --> 00:44:55,760
how successful behaviours is rewarding but you could essentially also eliminate and it

735
00:44:55,760 --> 00:44:59,350
all individuals from the gene pool

736
00:44:59,390 --> 00:45:02,230
and that thing you could will put into interesting

737
00:45:02,600 --> 00:45:04,920
the point is

738
00:45:05,540 --> 00:45:10,300
the machine learning purpose that distinction doesn't actually matter very much

739
00:45:10,320 --> 00:45:12,730
we're just talking about learning

740
00:45:12,730 --> 00:45:14,820
it's just push this

741
00:45:14,860 --> 00:45:18,130
biological knowledge a little bit further so these are two neurons and they talk to

742
00:45:18,130 --> 00:45:18,820
each other

743
00:45:19,010 --> 00:45:24,800
so that's the cell body like the CPU that we combined signals

744
00:45:24,820 --> 00:45:28,570
then dried where you get all the inputs from

745
00:45:29,320 --> 00:45:30,600
it's like an

746
00:45:30,640 --> 00:45:32,070
input terminal

747
00:45:32,080 --> 00:45:36,140
finances well that's connected between two neurons

748
00:45:36,140 --> 00:45:39,380
accent will like an output cable

749
00:45:41,080 --> 00:45:42,760
these are the cables can be

750
00:45:42,770 --> 00:45:46,100
like one and a half metres long so there are neurons which go from the

751
00:45:46,760 --> 00:45:49,230
somewhere through the entire spine

752
00:45:49,250 --> 00:45:50,070
in two

753
00:45:50,070 --> 00:45:51,880
whatever muscles

754
00:45:51,920 --> 00:45:52,860
they can be

755
00:45:52,880 --> 00:45:55,020
serious along

756
00:45:55,510 --> 00:45:59,480
here's another neuron

757
00:45:59,480 --> 00:46:01,260
the cell body

758
00:46:03,350 --> 00:46:04,820
and at the lights

759
00:46:04,820 --> 00:46:06,630
the inputs

760
00:46:06,760 --> 00:46:10,380
so what i'm getting is an output here

761
00:46:10,390 --> 00:46:14,050
y one on x one like two times six two and so on and so

762
00:46:18,700 --> 00:46:20,640
you can write it like this

763
00:46:23,480 --> 00:46:26,260
and then i have some

764
00:46:26,360 --> 00:46:28,110
non linear function

765
00:46:28,130 --> 00:46:29,980
of this linear combination

766
00:46:29,980 --> 00:46:32,660
nothing the of

767
00:46:33,290 --> 00:46:36,350
this output doesn't necessarily have to be exactly

768
00:46:36,390 --> 00:46:38,940
this linear function but it could be someone tonic

769
00:46:38,980 --> 00:46:40,580
function there

770
00:46:40,610 --> 00:46:42,940
so this is what i can

771
00:46:42,990 --> 00:46:45,880
now when people went into neural networks states

772
00:46:45,930 --> 00:46:49,260
this realized high neuron is really cool

773
00:46:49,260 --> 00:46:53,280
so when better if i had even more than one neuron

774
00:46:53,320 --> 00:47:00,500
so they basically concatenated several of those functions and created one fairly complex maze

775
00:47:00,510 --> 00:47:03,180
and then you had about ten years people writing

776
00:47:03,640 --> 00:47:09,510
how to actually clean up the mess and optimize efficient stuff works pretty well

777
00:47:10,550 --> 00:47:14,220
what you can also do is you could say well i know run with in

778
00:47:14,220 --> 00:47:18,780
features is really good wouldn't it be better if i had a lot more features

779
00:47:18,900 --> 00:47:22,460
this is basically the direction that kernel

780
00:47:22,520 --> 00:47:23,510
they can just

781
00:47:23,550 --> 00:47:25,200
now we have seen

782
00:47:25,280 --> 00:47:28,930
really which feature space and the the things with it

783
00:47:28,940 --> 00:47:31,200
cause somebody might ask

784
00:47:31,210 --> 00:47:34,570
well wouldn't it be great if i you had a neural network and then crawls

785
00:47:34,570 --> 00:47:35,380
in there

786
00:47:35,780 --> 00:47:39,370
and sadly enough people have actually tried to publish papers

787
00:47:39,570 --> 00:47:45,250
but you basically combining the worst of both worlds throughout

788
00:47:45,820 --> 00:47:47,450
OK now

789
00:47:47,500 --> 00:47:50,700
as a model of what we have here and i'm just ignoring that

790
00:47:50,720 --> 00:47:52,610
that the function is just a fixed

791
00:47:52,700 --> 00:47:55,050
w plus b

792
00:47:55,340 --> 00:48:00,960
just the same in the next are in dimensional vectors and these just the scalar

793
00:48:01,010 --> 00:48:01,880
OK so

794
00:48:01,890 --> 00:48:07,140
this is not just an attic lights modifications just whatever the sun does

795
00:48:07,140 --> 00:48:12,360
and then after summing over things in the so what was not quite exactly how

796
00:48:12,360 --> 00:48:15,580
the brain works so sorry abstract the web of things

797
00:48:15,640 --> 00:48:18,200
but this is something i can easily implement

798
00:48:18,260 --> 00:48:21,490
on the computer

799
00:48:21,530 --> 00:48:26,690
so what can i use it for spam filtering echo cancellation

800
00:48:26,720 --> 00:48:28,030
and the cool thing is

801
00:48:29,050 --> 00:48:30,510
well the weights are

802
00:48:30,550 --> 00:48:34,410
what bothers you call you called the plastic plates another they can

803
00:48:35,570 --> 00:48:40,140
and so you that there was some training data

804
00:48:42,090 --> 00:48:43,970
now here's the same picture again

805
00:48:46,830 --> 00:48:50,000
the function has its value zero here

806
00:48:50,010 --> 00:48:52,940
but everybody has seen what the hyperplane looks like

807
00:48:52,990 --> 00:48:55,140
basically that's the normal vector

808
00:48:55,160 --> 00:48:58,800
and be just tell me what the offset from the origin is

809
00:48:59,820 --> 00:49:01,740
and this of course will separate

810
00:49:01,780 --> 00:49:06,140
right dots from green dot

811
00:49:06,190 --> 00:49:08,780
and i'm going to show you the simplest

812
00:49:08,830 --> 00:49:13,640
what linear classifier algorithms

813
00:49:13,690 --> 00:49:16,910
and it's actually really nice very nice properties

814
00:49:16,930 --> 00:49:19,910
people still use it even today

815
00:49:19,930 --> 00:49:23,770
that's a few dozen an argument in a series of x's

816
00:49:23,820 --> 00:49:27,510
and why status plus minus one

817
00:49:27,520 --> 00:49:29,830
i initialize the NPV zero

818
00:49:29,990 --> 00:49:33,050
and i'm thinking it's i want hear from the bottom

819
00:49:33,060 --> 00:49:37,110
and if y i times the i plus b

820
00:49:37,130 --> 00:49:39,360
is less equal in sierra

821
00:49:39,440 --> 00:49:41,530
i have to explain what that means in a moment

822
00:49:41,640 --> 00:49:44,140
and just updating w

823
00:49:44,150 --> 00:49:46,370
i at one time so excited to it

824
00:49:46,410 --> 00:49:49,200
and adding y to be

825
00:49:49,260 --> 00:49:50,370
if that science

826
00:49:50,380 --> 00:49:54,330
because the other way around i don't do anything do this until this sea

827
00:49:54,340 --> 00:49:58,070
condition hope now let me explain what this means

828
00:49:59,070 --> 00:50:02,000
what i want is at the end the w thirty six

829
00:50:02,130 --> 00:50:03,300
plus b

830
00:50:03,340 --> 00:50:05,150
is greater than zero

831
00:50:05,160 --> 00:50:06,880
four positive y

832
00:50:06,930 --> 00:50:09,310
and it's less than zero for negative one

833
00:50:10,180 --> 00:50:12,360
so this means that this product here

834
00:50:12,380 --> 00:50:13,300
has to be

835
00:50:13,310 --> 00:50:14,950
greater than zero

836
00:50:14,950 --> 00:50:18,370
for all y and x x and y here

837
00:50:18,450 --> 00:50:20,280
that's when i'm done

838
00:50:20,970 --> 00:50:22,240
how do get there

839
00:50:22,250 --> 00:50:25,820
what we need for this condition is satisfied yet

840
00:50:25,880 --> 00:50:28,310
the update the weight vector

841
00:50:29,870 --> 00:50:32,880
essentially a copy of what i've actually seen

842
00:50:32,880 --> 00:50:37,520
and i'm just playing according to whether it's positive and negative class

843
00:50:37,520 --> 00:50:39,650
so i want to

844
00:50:39,670 --> 00:50:40,900
talked about some

845
00:50:41,350 --> 00:50:43,940
work that i've been doing jointly with

846
00:50:44,970 --> 00:50:49,880
because that's who was a student of duncan watts in sociology at columbia more recently

847
00:50:49,880 --> 00:50:54,720
was post-doc in the social department cornell re-joining

848
00:50:54,730 --> 00:50:56,110
google fall

849
00:50:56,520 --> 00:50:57,900
so we're

850
00:50:58,750 --> 00:50:59,840
george in

851
00:50:59,860 --> 00:51:01,070
can have been doing

852
00:51:01,090 --> 00:51:06,200
looking at some issues in social networks as structures for communication and the flow of

853
00:51:07,200 --> 00:51:10,440
so if you think about the kinds of social network data that we often work

854
00:51:10,440 --> 00:51:14,830
with one property that has one more actually

855
00:51:15,510 --> 00:51:17,290
science communities

856
00:51:18,370 --> 00:51:21,720
on of work in the last few years is

857
00:51:21,740 --> 00:51:26,470
so i started consists of discrete communication right around think of it as nodes with

858
00:51:26,470 --> 00:51:30,140
these permanent links the title together rather we think of it as a collection of

859
00:51:30,150 --> 00:51:30,930
people who

860
00:51:30,940 --> 00:51:36,130
and various intermittent times talk to each other in way exchange information so this view

861
00:51:36,130 --> 00:51:38,320
of the social network is dynamic

862
00:51:38,400 --> 00:51:43,130
object with constant active communication and of course communication comes in many

863
00:51:43,190 --> 00:51:48,630
many different forms like recent work has been about what you might call event-driven communication

864
00:52:00,830 --> 00:52:09,080
so saw negation than what i call event-driven application which is creation triggered by a

865
00:52:09,080 --> 00:52:12,570
particular strength here

866
00:52:19,990 --> 00:52:24,320
will will show that this too microphone things

867
00:52:24,880 --> 00:52:31,660
so during communication which is triggered by some kind of action rates

868
00:52:31,710 --> 00:52:36,430
as for example here from work you're asking which led to mention procurement and cascading

869
00:52:36,430 --> 00:52:40,350
book recommendations we recommend things to your friends there are many of their friends everything

870
00:52:40,350 --> 00:52:45,670
triggered by the initial personal book or this small piece of the spread of a

871
00:52:45,680 --> 00:52:49,520
gigantic chain letter across the internet or just redirect the

872
00:52:49,650 --> 00:52:52,710
david liben nowell and i have been studying for the past few years so this

873
00:52:52,710 --> 00:52:56,710
is things triggered by physical action that then spread rapidly

874
00:52:56,830 --> 00:53:00,260
but i guess i would argue that this kind of negation is really israeli-born on

875
00:53:00,260 --> 00:53:03,010
top of something that's

876
00:53:03,020 --> 00:53:07,280
but for intrinsic to the social networks and has been largely invisible but arguably determines

877
00:53:07,300 --> 00:53:11,860
a lot of how information actually flows we might call systemic communication and this simply

878
00:53:11,860 --> 00:53:13,520
the rhythm of everyday communication

879
00:53:13,990 --> 00:53:16,210
the rate at which you talk to

880
00:53:16,240 --> 00:53:22,030
your colleagues co-workers or office made your friends and relatives just acquaintances all of this

881
00:53:22,030 --> 00:53:27,780
is this sort of title patterns of communication that flow across time and

882
00:53:27,800 --> 00:53:29,700
allow other information to

883
00:53:29,710 --> 00:53:31,110
payback on top of it

884
00:53:31,120 --> 00:53:33,990
now this is very hard to observe prey it's easier to do

885
00:53:33,990 --> 00:53:36,060
log book sales or

886
00:53:36,080 --> 00:53:40,600
she was the internet then is to simply watch minute-by-minute day by day the rate

887
00:53:40,600 --> 00:53:44,360
different rates which you talk people which information flows that's what

888
00:53:44,480 --> 00:53:48,620
we would like to to do to sort of watch this nearly invisible backdrop and

889
00:53:48,620 --> 00:53:50,450
which these other events have

890
00:53:50,520 --> 00:53:54,120
OK so what kind of questions to ask essentially the goal is going to be

891
00:53:54,120 --> 00:54:00,550
to try inferring structural measures about the network based on these rhythmic patterns in which

892
00:54:00,550 --> 00:54:03,780
information and communication flows so

893
00:54:03,800 --> 00:54:04,990
we start with

894
00:54:06,010 --> 00:54:09,180
the basic question and that's the following if i have to say

895
00:54:09,270 --> 00:54:13,680
a data set consisting of communication among larger peoples in the social network five people

896
00:54:13,730 --> 00:54:16,520
with the stamps which people actually communicate

897
00:54:16,520 --> 00:54:20,640
i can ask questions like what is the most recent information that one might be

898
00:54:20,640 --> 00:54:23,990
here could possibly have about all the other

899
00:54:24,080 --> 00:54:27,490
right if these were the times which all communication to play some let's assume there

900
00:54:27,510 --> 00:54:30,990
is no indication outside the group for purposes of this pedagogical example

901
00:54:30,990 --> 00:54:35,300
that's of relevance to us so for example was most recent information because have friday

902
00:54:35,300 --> 00:54:36,930
at five PM about body

903
00:54:36,980 --> 00:54:39,330
and if you work it out you see that

904
00:54:39,400 --> 00:54:44,180
the latter from c three p o information could piggyback from a is

905
00:54:44,240 --> 00:54:46,460
as recently as friday night and when

906
00:54:46,480 --> 00:54:49,610
there's a sort of the laws of nature going on

907
00:54:49,710 --> 00:54:53,710
if this is really all communication of emails phone calls face-to-face whatever

908
00:54:55,200 --> 00:55:00,040
nothing happened after friend in a very literal sense could possibly be known by b

909
00:55:00,060 --> 00:55:03,710
friday five it simply cannot go there was no communication

910
00:55:03,720 --> 00:55:07,440
any in this way we can find these view of the network is sort of

911
00:55:07,440 --> 00:55:10,440
smear out in this strange time shifted away

912
00:55:10,460 --> 00:55:12,940
it's too much things becoming increasingly

913
00:55:15,100 --> 00:55:15,940
right so

914
00:55:15,950 --> 00:55:19,440
different views of different people and also one of the things for example

915
00:55:19,450 --> 00:55:23,140
it's clear that although a sometimes talks to be that's not in general the fastest

916
00:55:23,140 --> 00:55:27,950
potential means for information flow rather the fact that a talks alot to CC talks

917
00:55:27,950 --> 00:55:33,100
about to be is much faster interaction again we're trying to learn structural measures the

918
00:55:33,100 --> 00:55:39,170
network sort out of date time shifted nature and fast communication channels when that we

919
00:55:39,170 --> 00:55:43,190
know in particular what float is already gave the the potential for the

920
00:55:43,190 --> 00:55:47,340
the axe so so

921
00:55:47,460 --> 00:55:51,460
so all of this over again this is these are very good questions that any

922
00:55:51,490 --> 00:55:56,010
colours have to worry about but i was hoping not to worry about here so

923
00:55:56,030 --> 00:55:57,590
all of these models

924
00:55:57,650 --> 00:56:00,590
i have a slightly different units and there

925
00:56:00,990 --> 00:56:04,780
there are also sort of sort of transform to normalise the scales have the same

926
00:56:04,780 --> 00:56:08,010
minimax so we we don't necessarily think that

927
00:56:08,070 --> 00:56:14,740
this particular skilled people using is is directly meaningful basically there's what the the the

928
00:56:14,740 --> 00:56:17,010
units of this are measured in

929
00:56:17,010 --> 00:56:20,820
you know it this this is the this is the measure it's this bayes factor

930
00:56:21,150 --> 00:56:25,420
but then we're putting it through sort of a non-linear power law scaling which all

931
00:56:25,420 --> 00:56:27,720
the models go through that kind of thing which takes into account the fact that

932
00:56:27,720 --> 00:56:33,300
when people use it every judgement rating scale they may not use the ratings

933
00:56:34,380 --> 00:56:35,460
that makes sense

934
00:56:35,510 --> 00:56:40,570
so there's weak nonlinearity for all of these models that's just a parameter that

935
00:56:40,590 --> 00:56:42,690
two to the data

936
00:56:44,320 --> 00:56:47,990
although actually it turns out you don't need that i mean it turns out that

937
00:56:48,010 --> 00:56:51,880
if you if you just computed posterior probability you get basically the same so if

938
00:56:52,630 --> 00:56:56,550
that's not what i'm showing here but that is to the right people are made

939
00:56:57,210 --> 00:57:01,340
that many of the natural environment

940
00:57:01,380 --> 00:57:06,710
so i didn't collect these data although we have collected we replicated the experiment and

941
00:57:06,710 --> 00:57:12,510
got basically the same data and i actually remember those are data budarin change but

942
00:57:12,570 --> 00:57:15,490
because the error bars probably data

943
00:57:15,510 --> 00:57:21,670
no i mean just not that we're better just i didn't have there what some

944
00:57:21,670 --> 00:57:26,990
sorts are with us yet people are placed on individuals that's a good question i

945
00:57:28,210 --> 00:57:32,720
so one thing that i i don't i don't know exactly that we we

946
00:57:32,740 --> 00:57:37,740
well so in in this experiment you have the basic quality shapes are the individual

947
00:57:37,740 --> 00:57:42,070
subject so we tested for example do you have this linear effect in individual subjects

948
00:57:42,220 --> 00:57:46,240
another thing we tested is do you have a slightly statistically significant kind of u

949
00:57:46,240 --> 00:57:49,170
shape here which is interesting because the bayesian model predicts that and the other ones

950
00:57:49,170 --> 00:57:53,570
don't so those are those that those are present in

951
00:57:54,240 --> 00:57:57,800
and of individual subjects more more than you'd expect by chance so i don't i

952
00:57:57,800 --> 00:58:01,800
can't exactly tell you what the individual plots the light but they have the same

953
00:58:01,800 --> 00:58:06,840
quality shapes most of them do but yes there certainly certainly averaging here you know

954
00:58:06,840 --> 00:58:09,300
on the order of

955
00:58:09,320 --> 00:58:13,460
fifty subjects the standard areas so you know multiply by

956
00:58:13,460 --> 00:58:15,800
square root of fifty to get the actual

957
00:58:15,840 --> 00:58:20,670
standard deviations in judgement and you know there are some significant variation in that standard

958
00:58:20,670 --> 00:58:22,990
thing in cognitive psychology

959
00:58:23,050 --> 00:58:32,090
these once again

960
00:58:32,110 --> 00:58:36,570
i think it's statistically significant in intuitively why for the real reason or

961
00:58:38,420 --> 00:58:42,490
in general the i mean from the point of view of the bayesian model

962
00:58:42,530 --> 00:58:45,710
the reason why and the there's a difference in the bayesian model but the scale

963
00:58:45,760 --> 00:58:53,190
the the nonlinearity kind of scares but in general the reason why this kind of

964
00:58:53,190 --> 00:58:55,820
well it's a version of the same kind of thing that was was going on

965
00:58:55,820 --> 00:58:59,760
here but its most dramatic in this case as the base rate decreases the of

966
00:58:59,760 --> 00:59:03,400
the there's more dynamic range in a sense for cause of the same strength to

967
00:59:03,400 --> 00:59:07,280
express itself so you get stronger evidence from the same

968
00:59:07,440 --> 00:59:12,530
the same absolute difference in conditional probabilities for the presence of course

969
00:59:12,550 --> 00:59:16,550
and if you you can actually look at the form of the the posteriors over

970
00:59:16,550 --> 00:59:18,820
the over the model parameters and see that

971
00:59:19,960 --> 00:59:25,610
OK that i hope that helps for this i mean it's worth going through one

972
00:59:25,610 --> 00:59:29,360
experiment in some detail which i i guess i should have anticipated that would be

973
00:59:29,360 --> 00:59:32,380
a good idea but i didn't but i if possible i'd like to move on

974
00:59:33,010 --> 00:59:36,090
there's much more but i want to cover all right

975
00:59:37,130 --> 00:59:42,940
so tell you very quickly about two other experiments and hopefully many of the same

976
00:59:42,940 --> 00:59:46,130
issues are relevant like if you want to know what the scale is it's the

977
00:59:46,920 --> 00:59:48,740
and i'm gonna compare

978
00:59:48,760 --> 00:59:50,400
bayes factor log

979
00:59:50,420 --> 00:59:52,130
the likelihood ratio two

980
00:59:52,150 --> 00:59:54,570
human judgments with some arbitrary nonlinear

981
00:59:54,590 --> 01:00:00,690
monotonic scaling this this is also kind of causal judgement but it's one it's a

982
01:00:00,690 --> 01:00:05,030
little bit more interesting a little bit less textbook in which here you're trying to

983
01:00:05,030 --> 01:00:06,550
make sense of peoples

984
01:00:07,530 --> 01:00:08,960
intriguing abilities

985
01:00:10,380 --> 01:00:13,170
remarkable sometimes frustrating to see

986
01:00:13,190 --> 01:00:17,960
hidden causes in from coincidences right we're off the ground we often sensitive to

987
01:00:18,050 --> 01:00:21,960
you know coincidences that that in retrospect turn out to be wrong

988
01:00:22,070 --> 01:00:26,210
and so when we think of our sense of coincidence we might think of it

989
01:00:26,210 --> 01:00:30,630
as this kind of mystical thing whatever i write papers on coincidences i get a

990
01:00:30,630 --> 01:00:32,570
lot of attention from the popular press

991
01:00:32,590 --> 01:00:37,490
because they think oh that's like real psychology and we're mysterious

992
01:00:37,510 --> 01:00:40,590
per something or other

993
01:00:40,590 --> 01:00:45,440
there's there's the most recognition i've ever had in any kind of media which is

994
01:00:45,490 --> 01:00:49,550
that same very much is this show called criminal minds

995
01:00:49,570 --> 01:00:55,130
and then the some something i said about quences was quoted by

996
01:00:55,880 --> 01:01:01,300
protect the by the suspect of the serial killer thing anyway was on CBS ten

997
01:01:01,300 --> 01:01:05,710
million people watched it so i guess this is a good topic to work on

998
01:01:05,940 --> 01:01:09,960
but but the reason we're interested in this is because so much of our actual

999
01:01:09,960 --> 01:01:14,030
called knowledge of the world we think is driven by again noticing coincidence which to

1000
01:01:14,030 --> 01:01:19,280
a statistical point of view and you probably not remarkable but to provide some evidence

1001
01:01:19,280 --> 01:01:23,190
for hidden cause it's worth exploring so here's an example this may be motivated by

1002
01:01:23,190 --> 01:01:26,820
something you might kind of thing that sometimes gets written up in the press you've

1003
01:01:26,820 --> 01:01:30,530
may be heard of cancer clusters of disease clusters like somebody notes that you know

1004
01:01:30,530 --> 01:01:32,210
in a certain city

1005
01:01:32,220 --> 01:01:36,380
there's an unusually high number of cases of some rare cancer and maybe it's because

1006
01:01:36,380 --> 01:01:40,570
some evil company was making something into the water and we should go and see

1007
01:01:40,570 --> 01:01:45,320
them it turns out most of the time those all those claims don't turn out

1008
01:01:45,320 --> 01:01:48,610
the whole but sometimes that kind of reasoning really leads you to a new disease

1009
01:01:48,610 --> 01:01:53,130
like lyme disease and today have one disease here now OK well it's terrible thing

1010
01:01:53,400 --> 01:01:57,300
my daughter actually had it if she was treated for but it's this is this

1011
01:01:57,300 --> 01:02:00,130
thing you can get in the US from being bitten by certain kind of take

1012
01:02:00,130 --> 01:02:02,670
a deer tick and

1013
01:02:02,690 --> 01:02:06,550
if it is untreated it can cause serious neurological damage is kind of like syphilis

1014
01:02:06,550 --> 01:02:07,610
i mean that

1015
01:02:07,630 --> 01:02:10,380
you get from being that might take

1016
01:02:10,510 --> 01:02:18,090
anyway so so that this was only rediscovered you know maybe terror twenty years ago

1017
01:02:18,090 --> 01:02:21,570
in lyme connecticut and that's why it's called like this is

1018
01:02:21,590 --> 01:02:26,690
but anyway so imagine that you know this is this is one square mile of

1019
01:02:26,690 --> 01:02:31,200
the city and i'm plotting each incidence of a rare cancer in over a year

1020
01:02:31,440 --> 01:02:35,030
and i say well you see like little potential cluster over there in the upper

1021
01:02:35,030 --> 01:02:39,690
left-hand corner how much evidence you think there's that there's actually some hidden localized cause

1022
01:02:39,710 --> 01:02:44,380
some actual thing causing people to get this disease more often in some part of

1023
01:02:44,380 --> 01:02:46,240
the city than others

1024
01:02:46,240 --> 01:02:48,360
you know here you might say it's not

1025
01:02:48,530 --> 01:02:51,570
might be stronger reached this is kind of the middle in case the the way

1026
01:02:51,570 --> 01:02:55,360
we model this sort of measures we said OK again we're looking at the kind

1027
01:02:55,360 --> 01:03:00,880
of model selection thing but one model is just the this is a uniform distribution

1028
01:03:00,880 --> 01:03:05,120
of these events over this space but another model that the interesting one says there's

1029
01:03:05,120 --> 01:03:09,980
some hidden common cause which is spatially localized and it's it's kids giving rise to

1030
01:03:09,980 --> 01:03:12,670
some of these events but not necessarily all of them in this idea of picking

1031
01:03:12,670 --> 01:03:16,170
out a hidden common cause in in sea of noise again a lot of what

1032
01:03:16,190 --> 01:03:19,630
the brain is designed to do looks like that so that the space of

1033
01:03:19,670 --> 01:03:24,170
possible hidden causes we use we're just gaussians

1034
01:03:24,220 --> 01:03:25,980
sort of

1035
01:03:25,990 --> 01:03:31,150
gaussians of different scale not rotation in different positions

1036
01:03:31,190 --> 01:03:35,420
of the space so we so there's a sort of continuous parameter space here varying

1037
01:03:35,420 --> 01:03:38,260
in in mean and variance

1038
01:03:38,280 --> 01:03:40,690
these are all possible represent all possible

1039
01:03:40,690 --> 01:03:47,280
causes for these disease clusters and in computing the evidence for this model over that

1040
01:03:47,280 --> 01:03:49,880
when we have to integrate out those

1041
01:03:49,880 --> 01:03:53,700
OK so how how a particular piece of information propagates over the network and the

1042
01:03:53,700 --> 01:03:59,140
reason why we can get this is because these disposable timestamp so we know that

1043
01:03:59,180 --> 01:04:03,450
i know yesterday yesterday somebody had to write this one so that today i was

1044
01:04:03,450 --> 01:04:05,720
able to prefer OK

1045
01:04:05,740 --> 01:04:09,660
so this is the blogs and we have have a crawl of forty five thousand

1046
01:04:09,660 --> 01:04:10,880
books for you

1047
01:04:10,910 --> 01:04:14,140
which is more than ten million posts and we can extract like two hundred fifty

1048
01:04:14,140 --> 01:04:15,400
thousand cascades

1049
01:04:15,410 --> 01:04:17,910
so this propagation graphs out

1050
01:04:20,700 --> 01:04:23,510
the viral marketing programme i talk about

1051
01:04:23,590 --> 01:04:28,840
what's the problem with what have said that the followers of the recommendations and they

1052
01:04:29,220 --> 01:04:33,880
they they receive discounts on products of the recommendations of success so

1053
01:04:33,880 --> 01:04:37,470
here here's how the thing works let's have a person who buys a book

1054
01:04:37,470 --> 01:04:42,800
and makes recommendation to another person knows this person decides to buy a book also

1055
01:04:42,820 --> 01:04:48,260
they get ten percent discount and successfully commanded gets ten percent so there is an

1056
01:04:48,260 --> 01:04:50,660
incentive on both sides

1057
01:04:50,700 --> 01:04:51,410
to make

1058
01:04:51,970 --> 01:04:56,880
on one side to make successful recommendations and on the other side to buy products

1059
01:04:56,880 --> 01:05:03,010
through these recommendations and now you can imagine that this so both people have been

1060
01:05:03,010 --> 01:05:07,530
and you can imagine that this now spread over and people continue to call commanding

1061
01:05:07,530 --> 01:05:08,680
for the

1062
01:05:08,700 --> 01:05:09,410
the thing

1063
01:05:09,410 --> 01:05:14,490
and they will be using this incentivized viral marketing programme at some of the big

1064
01:05:14,860 --> 01:05:21,610
online stores where they can the data contained like sixteen million recommendations between four million

1065
01:05:21,610 --> 01:05:25,990
people and we have confirmation of products and i will go into more detail about

1066
01:05:25,990 --> 01:05:27,030
his life

1067
01:05:27,050 --> 01:05:28,700
and then the last

1068
01:05:29,530 --> 01:05:34,410
diffusion of the community membership is basically have a big social networks where people belong

1069
01:05:34,450 --> 01:05:38,260
to explicitly defined communities and

1070
01:05:38,320 --> 01:05:41,090
what we will be asking is how the people who

1071
01:05:41,140 --> 01:05:42,860
joined these communities

1072
01:05:43,180 --> 01:05:47,450
as more and more of their friends are inside so for example communities can be

1073
01:05:47,450 --> 01:05:53,860
on on livejournal blogging website people explicitly say what the want friends or a community

1074
01:05:53,860 --> 01:05:59,590
with computers i call scientific conference and now you asking what the likelihood that particular

1075
01:05:59,590 --> 01:06:03,740
that someone will publish the particular conference given the number of their friends or peers

1076
01:06:03,740 --> 01:06:05,970
that are already published in the conference

1077
01:06:05,990 --> 01:06:08,280
and again this data is

1078
01:06:08,280 --> 01:06:11,090
one of the same size we have a million users

1079
01:06:11,760 --> 01:06:15,180
two hundred quarter familiar of groups to join

1080
01:06:15,930 --> 01:06:19,780
so the first thing i want to touch upon are how do diffusion curves look

1081
01:06:19,780 --> 01:06:23,090
like it is diminishing returns or

1082
01:06:23,110 --> 01:06:27,530
or a critical mass type of and this comes from binary marketing when we see

1083
01:06:27,530 --> 01:06:33,410
people recommended is to each other and the the x axis is the number of

1084
01:06:33,450 --> 01:06:35,550
recommendations of particular person got

1085
01:06:35,630 --> 01:06:37,860
and y axis is the probability that

1086
01:06:37,900 --> 01:06:43,070
that the article the person buys the DVD after receiving OK recommendation

1087
01:06:43,090 --> 01:06:48,400
and what you can see from this is that it sort of diminishing returns

1088
01:06:48,400 --> 01:06:50,180
good where where the

1089
01:06:50,200 --> 01:06:55,590
where the probability such right so if you buy marketing company

1090
01:06:55,630 --> 01:07:00,180
you don't want to send i more than ten or fifteen recommendations for

1091
01:07:00,300 --> 01:07:05,180
on one particular DVD otherwise people will start paying attention

1092
01:07:05,260 --> 01:07:09,090
and the other thing is also that part

1093
01:07:09,110 --> 01:07:13,610
the probability that increases very quickly and then and then it flattens out so this

1094
01:07:13,610 --> 01:07:19,110
is for DVD purchases now if you go on on the livejournal we have communities

1095
01:07:19,110 --> 01:07:23,410
in the US what's the what's the probability of a person joining a community given

1096
01:07:23,410 --> 01:07:26,510
that they already have a number of friends in the community

1097
01:07:26,510 --> 01:07:30,050
you can again see very similar to the

1098
01:07:30,680 --> 01:07:32,640
the first few method along the

1099
01:07:33,010 --> 01:07:37,470
increases and then sort of let flippers and if you if you do if it

1100
01:07:37,800 --> 01:07:42,780
like a lot of such a logarithmic it would be to well this

1101
01:07:42,800 --> 01:07:46,820
OK so this is another diminishing returns

1102
01:07:46,860 --> 01:07:47,860
and then

1103
01:07:47,860 --> 01:07:53,160
a series of these for sending email when we have an email network of large

1104
01:07:53,160 --> 01:07:56,200
university and he asked what's the probability of

1105
01:07:56,470 --> 01:08:00,160
to persons exchanging email given the number of the number of friends they have in

1106
01:08:00,160 --> 01:08:04,800
common and again we observed seem similar type of behavior went on the x axis

1107
01:08:04,800 --> 01:08:06,260
we plot the number of

1108
01:08:06,320 --> 01:08:13,260
friends that he has in common that the probability that the exchange in inside some

1109
01:08:13,260 --> 01:08:16,760
and again it's it's monotone slowly increases

1110
01:08:16,780 --> 01:08:22,990
and at the lower numbers thinking is more significant as its like

1111
01:08:23,050 --> 01:08:28,140
so so these are the diffusion and this is the question i was asked

1112
01:08:28,900 --> 01:08:31,530
i want to be really is this

1113
01:08:31,550 --> 01:08:36,300
how much how much how much is it for example for viral marketing what what

1114
01:08:36,470 --> 01:08:39,450
what we see on what is in the data is that we see that know

1115
01:08:39,470 --> 01:08:43,700
the received recommendation and then purchased the product but we don't know whether they also

1116
01:08:43,700 --> 01:08:45,410
saw an apple TV

1117
01:08:45,410 --> 01:08:50,840
whether they also had personal communication with that particular command and so for example

1118
01:08:50,860 --> 01:08:55,200
four when people joining communities it seems i feel we only see the behaviour of

1119
01:08:55,200 --> 01:09:00,760
a particular particular time and the questions are again like how did the particular become

1120
01:09:00,760 --> 01:09:04,430
aware of the recommendation for how do they become aware that their friends join the

1121
01:09:04,430 --> 01:09:10,340
community or how what was the lag between the time the the null decided to

1122
01:09:10,340 --> 01:09:11,510
join the community

1123
01:09:11,510 --> 01:09:13,720
and the time the joint

1124
01:09:14,410 --> 01:09:15,800
things like that

1125
01:09:15,860 --> 01:09:19,240
and here is what i want to show you next is the things that maybe

1126
01:09:19,240 --> 01:09:23,430
not so simple as they seem as they as they look so one question that

1127
01:09:23,550 --> 01:09:27,550
very actually is how this so we saw the dependence on the number of fans

1128
01:09:27,550 --> 01:09:31,200
sort of sexual attraction had diminishing returns but

1129
01:09:31,200 --> 01:09:35,180
you can ask how does the connectedness of my friends in influence

1130
01:09:35,220 --> 01:09:39,280
what the probability of me joining a community or adopting a particular behavior yet so

1131
01:09:39,470 --> 01:09:41,490
here i have to explain examples

1132
01:09:41,800 --> 01:09:46,840
who is more likely to to to join the community to to join this community

1133
01:09:46,840 --> 01:09:51,220
so you know that's has his friends friends in the community and they are not

1134
01:09:51,220 --> 01:09:53,680
connected their independent node y

1135
01:09:53,700 --> 01:09:56,430
this difference in their are connected so

1136
01:09:56,450 --> 01:09:59,510
regional is more likely to join the community

1137
01:09:59,570 --> 01:10:05,220
and there are two competing so competing social theories theories that could explain explain

1138
01:10:05,660 --> 01:10:11,910
the vision the first one is called the information arguement and it says that

1139
01:10:11,930 --> 01:10:16,800
unconnected friends give independent support so that would mean that node x is more likely

1140
01:10:16,800 --> 01:10:20,200
to to join the community because they have different for know each other so they

1141
01:10:20,200 --> 01:10:21,160
have like three

1142
01:10:21,220 --> 01:10:28,720
independent opinions about particular particular thing and having independent opinions that's the other one is

1143
01:10:28,720 --> 01:10:33,430
called the social capital arguement which says that safety or trust

1144
01:10:33,490 --> 01:10:37,900
is something that that you want from the so if this is a community

1145
01:10:37,950 --> 01:10:42,240
i don't know why would be more likely to join because they have a very

1146
01:10:42,300 --> 01:10:46,720
they have friends they have a group of friends that are all good friends among

1147
01:10:46,720 --> 01:10:49,930
which among each other so if this would be

1148
01:10:49,970 --> 01:10:54,300
joining a community i would prefer i would know why we would be more likely

1149
01:10:54,300 --> 01:10:58,030
to join because then they could hang out these three friends and everything will be

1150
01:10:58,050 --> 01:11:02,180
fine instead of having concealed chemical conformation

1151
01:11:04,640 --> 01:11:08,780
these are these are the two competing hypotheses not going to see what's going on

1152
01:11:09,820 --> 01:11:16,640
for example if you look at the legend then the the community joining probability increases

1153
01:11:17,200 --> 01:11:21,090
as there are more connections between the friends who are already members of the community

1154
01:11:21,090 --> 01:11:32,010
announcements six

1155
01:11:33,240 --> 01:11:38,800
good time to use it earlier

1156
01:11:40,980 --> 01:11:42,260
this afternoon

1157
01:11:43,640 --> 01:11:47,870
it's like thirty

1158
01:11:48,120 --> 01:11:50,530
we can stop

1159
01:11:50,550 --> 01:11:53,010
the stuff about quite

1160
01:11:53,260 --> 01:11:57,550
so with

1161
01:11:59,160 --> 01:12:05,010
now before before and let's talk about

1162
01:12:05,020 --> 01:12:07,270
the slightly more specific

1163
01:12:07,290 --> 01:12:11,200
OK so this is a gibbs sampler

1164
01:12:12,900 --> 01:12:15,100
in summary the spatial

1165
01:12:15,120 --> 01:12:17,150
case of MCMC algorithms

1166
01:12:17,810 --> 01:12:21,640
will see the special case of the metropolis hastings

1167
01:12:21,650 --> 01:12:23,060
if you look at it

1168
01:12:23,110 --> 01:12:24,800
the right way

1169
01:12:24,880 --> 01:12:26,780
i do with the

1170
01:12:26,800 --> 01:12:28,990
this apparently because

1171
01:12:29,010 --> 01:12:31,710
if there is that it comes from

1172
01:12:31,760 --> 01:12:32,940
from nineteen

1173
01:12:32,960 --> 01:12:34,630
but was

1174
01:12:35,760 --> 01:12:37,450
used in the

1175
01:12:37,610 --> 01:12:39,280
the main community

1176
01:12:39,540 --> 01:12:43,130
from all over europe then me present things four

1177
01:12:45,400 --> 01:12:46,790
well i just

1178
01:12:47,740 --> 01:12:49,510
the top of all

1179
01:12:49,530 --> 01:12:51,960
i need to say about misrepresenting so

1180
01:12:51,980 --> 01:12:54,640
this is just just to give you a rough idea

1181
01:12:54,650 --> 01:12:56,150
about the principle

1182
01:12:57,680 --> 01:12:59,740
of the first chapter is that

1183
01:12:59,750 --> 01:13:02,760
there is this universal algorithm

1184
01:13:02,780 --> 01:13:04,670
that in principle were

1185
01:13:04,680 --> 01:13:06,170
for every problem

1186
01:13:06,190 --> 01:13:07,990
the same practice

1187
01:13:08,000 --> 01:13:12,070
the number of iterations required just two numbers

1188
01:13:12,090 --> 01:13:13,990
to be useful

1189
01:13:14,000 --> 01:13:16,290
and finally to it

1190
01:13:16,300 --> 01:13:17,030
to get

1191
01:13:17,080 --> 01:13:19,470
the right picture

1192
01:13:19,490 --> 01:13:22,300
i one and i mean i don't have time to mention

1193
01:13:23,430 --> 01:13:30,950
the work that has been going on controlling MCMC agrees and during this parameters according

1194
01:13:30,950 --> 01:13:32,960
to different criteria

1195
01:13:33,070 --> 01:13:35,110
if you want to know more

1196
01:13:35,310 --> 01:13:37,520
there is a chapter in in the book

1197
01:13:38,000 --> 01:13:39,390
there is also

1198
01:13:39,400 --> 01:13:41,290
this software

1199
01:13:41,310 --> 01:13:43,290
that is ridiculous and recalled but

1200
01:13:43,310 --> 01:13:46,030
the UGC gs

1201
01:13:46,050 --> 01:13:47,640
that is being developed by

1202
01:13:47,690 --> 01:13:49,800
people the north sea

1203
01:13:49,910 --> 01:13:51,470
need it

1204
01:13:51,700 --> 01:13:53,750
preceding throughout her

1205
01:13:55,290 --> 01:13:58,700
and implement MCMC principle

1206
01:13:59,050 --> 01:14:00,740
in his own language

1207
01:14:00,750 --> 01:14:03,600
and are associated with with bugs

1208
01:14:03,870 --> 01:14:05,870
there is an issue

1209
01:14:05,890 --> 01:14:07,590
unicode code

1210
01:14:07,610 --> 01:14:09,700
that is that kind of control

1211
01:14:09,720 --> 01:14:11,150
that the convergence

1212
01:14:13,720 --> 01:14:19,490
two hours to go to

1213
01:14:19,540 --> 01:14:22,870
there exist some

1214
01:14:23,750 --> 01:14:27,420
for example

1215
01:14:27,660 --> 01:14:33,330
a different slide you want to see the connection present because

1216
01:14:33,390 --> 01:14:35,200
it seems like a very different

1217
01:14:36,720 --> 01:14:38,500
that barroso

1218
01:14:39,670 --> 01:14:42,330
two from the

1219
01:14:43,050 --> 01:14:45,970
from metropolis hastings

1220
01:14:48,520 --> 01:14:51,910
is like when you do have an optimisation if you have an accumulation to do

1221
01:14:51,910 --> 01:14:54,920
not dimensional call

1222
01:14:55,610 --> 01:14:56,600
if you want to go

1223
01:14:56,610 --> 01:14:59,140
so we can

1224
01:14:59,240 --> 01:15:04,370
if you don't to solve the optimisation directly

1225
01:15:04,420 --> 01:15:08,000
you may want to start to look at one direction at a time

1226
01:15:08,010 --> 01:15:09,900
if you have to maximize the function

1227
01:15:09,910 --> 01:15:11,840
of XYZ

1228
01:15:11,890 --> 01:15:15,440
you x x and y and you maximizing c

1229
01:15:15,450 --> 01:15:16,250
you get

1230
01:15:16,270 --> 01:15:17,380
c one

1231
01:15:17,490 --> 01:15:19,090
and then maximise

1232
01:15:19,110 --> 01:15:20,160
if x y

1233
01:15:20,170 --> 01:15:24,530
insy the one you maximizing access to get x one and you keep going is

1234
01:15:24,530 --> 01:15:28,620
that and if you're lucky you get to the to the to the optimum of

1235
01:15:28,620 --> 01:15:34,240
the function of the function is is is convex

1236
01:15:34,260 --> 01:15:37,730
concave you get you get to the global maximum

1237
01:15:37,750 --> 01:15:38,500
well here

1238
01:15:38,520 --> 01:15:42,490
what to do is try to make this principle for simulation

1239
01:15:44,040 --> 01:15:44,940
we start

1240
01:15:44,950 --> 01:15:50,460
again the target distributions and here we assume we are in dimensional setting

1241
01:15:50,480 --> 01:15:51,800
of course all i said

1242
01:15:51,810 --> 01:15:52,650
the idea

1243
01:15:52,670 --> 01:15:56,960
didn't require dimension one was working for any dimension

1244
01:15:57,100 --> 01:16:01,410
but here it's require more than dimension one because we need to know

1245
01:16:01,420 --> 01:16:08,020
the conditional distributions as it was for all the components of x

1246
01:16:08,070 --> 01:16:10,620
so what i denote by f one

1247
01:16:10,640 --> 01:16:15,700
it is the conditional distribution of x one given x two x three x p

1248
01:16:15,720 --> 01:16:18,070
as it is

1249
01:16:18,080 --> 01:16:19,390
OK from

1250
01:16:19,400 --> 01:16:24,320
we define f one f two f p that are the conditional distributions as it

1251
01:16:26,630 --> 01:16:30,430
and if we know that of course we we know the distribution

1252
01:16:31,770 --> 01:16:35,150
f one

1253
01:16:35,160 --> 01:16:37,630
of x one x given x two

1254
01:16:39,590 --> 01:16:41,610
is by bayes theorem

1255
01:16:41,620 --> 01:16:43,060
directly proportional

1256
01:16:44,070 --> 01:16:45,260
if x y

1257
01:16:45,300 --> 01:16:46,820
x two

1258
01:16:48,310 --> 01:16:53,060
because if we know that we know that the missing the constant but as usual

1259
01:16:53,080 --> 01:16:55,800
the constant doesn't matter

1260
01:16:58,090 --> 01:16:59,710
we start from

1261
01:16:59,730 --> 01:17:02,440
now arbitrary point again ergodicity

1262
01:17:02,500 --> 01:17:05,930
say is that it doesn't matter and

1263
01:17:05,960 --> 01:17:08,510
we will simulate successively

1264
01:17:08,530 --> 01:17:11,350
x one x two x three x p

1265
01:17:11,360 --> 01:17:12,490
from this

1266
01:17:12,530 --> 01:17:14,710
full conditionals

1267
01:17:15,680 --> 01:17:19,790
and then we start again and again and again

1268
01:17:19,800 --> 01:17:24,060
so this is a gibbs sampling agrees to give some

1269
01:17:24,070 --> 01:17:27,530
the name is used to the wrong with score like that because

1270
01:17:27,550 --> 01:17:30,280
originally it was applied for gibbs

1271
01:17:31,680 --> 01:17:34,350
and it's just to name stuck

1272
01:17:34,360 --> 01:17:36,000
and structures

1273
01:17:37,600 --> 01:17:39,620
the markov chain is therefore

1274
01:17:39,630 --> 01:17:41,920
this sequence of p

1275
01:17:43,280 --> 01:17:46,170
OK you add xt x one x two x

1276
01:17:46,180 --> 01:17:47,980
x two x t

1277
01:17:47,990 --> 01:17:53,430
and you successively generate x one t plus one from this conditional could still depends

1278
01:17:53,430 --> 01:17:54,650
on x two t

1279
01:17:55,830 --> 01:17:56,850
next two

1280
01:17:56,870 --> 01:17:58,010
t plus one

1281
01:17:58,040 --> 01:18:02,240
is the full conditional text given those now six one t plus one

1282
01:18:02,250 --> 01:18:03,810
but still it's t

1283
01:18:05,050 --> 01:18:10,270
and we keep moving in one direction at the time to reach t plus one

1284
01:18:10,270 --> 01:18:15,240
which depends on the current values of x one x two x p minus one

1285
01:18:15,260 --> 01:18:16,320
we should now

1286
01:18:16,340 --> 01:18:19,580
all indexed by t plus one

1287
01:18:20,210 --> 01:18:23,740
so you can see that it doesn't matter that you can see that that as

1288
01:18:23,740 --> 01:18:26,770
one iteration of the algorithm

1289
01:18:27,590 --> 01:18:30,980
other sequence of p iterations of

1290
01:18:32,330 --> 01:18:33,500
and the reason

1291
01:18:35,070 --> 01:18:37,790
well the point is that

1292
01:18:37,790 --> 01:18:44,730
sufficiently high level to make intelligent decisions about in some cases you want to appeal

1293
01:18:44,730 --> 01:18:49,990
to experts who have special knowledge of it and you have to evaluate the knowledge

1294
01:18:50,230 --> 01:18:53,820
so by and large i think with regard to the second part of the question

1295
01:18:53,820 --> 01:18:56,360
that most human affairs are

1296
01:18:56,370 --> 01:19:01,350
well within the range of ordinary people's

1297
01:19:01,360 --> 01:19:08,400
judgments and decisions and there's a suggested in the talk

1298
01:19:08,450 --> 01:19:14,400
this in my opinion the judgments of the large majority of the american people are

1299
01:19:14,400 --> 01:19:17,910
far more sane and rational

1300
01:19:18,580 --> 01:19:30,890
justifiable on moral than those of the intellectual elites and the political class of managers

1301
01:19:30,890 --> 01:19:33,430
as the european union

1302
01:19:33,440 --> 01:19:39,330
the of course slovenia has a rather special history being conquered by everyone

1303
01:19:39,340 --> 01:19:45,650
not having its empire but the most of europe is somewhat different the history the

1304
01:19:45,660 --> 01:19:51,080
modern nation-state system was essentially developed in europe

1305
01:19:51,100 --> 01:19:54,240
and it's a hideous

1306
01:19:54,320 --> 01:19:58,860
that's why you one of the main reasons why europe such as savage history in

1307
01:19:58,860 --> 01:20:06,250
the past has many centuries a lot of the savagery of european history

1308
01:20:06,290 --> 01:20:09,360
results from the effort to impose

1309
01:20:09,370 --> 01:20:14,440
the nation-state systems and societies where they don't make any sense at all

1310
01:20:14,450 --> 01:20:20,670
the society diverse many cultural patterns different languages the cross every

1311
01:20:20,710 --> 01:20:26,240
dimension you can think of any effort to impose the nation-state system on

1312
01:20:26,310 --> 01:20:34,090
lead to wars massacres destruction it's not just the twentieth century after only the thirty

1313
01:20:34,090 --> 01:20:37,070
years war probably killed forty percent of the

1314
01:20:37,420 --> 01:20:38,860
population germany

1315
01:20:38,870 --> 01:20:44,300
i mean the entire history of europe is barbarous savages as part of the reason

1316
01:20:44,300 --> 01:20:48,850
why europe was able to conquer the world it had died that year europe was

1317
01:20:48,850 --> 01:20:58,530
mostly barbarian fringe the world the centres of world civilization and culture and economic and

1318
01:20:58,530 --> 01:21:00,090
so on were age

1319
01:21:00,140 --> 01:21:08,300
china and india that europe did conquered world primarily because it had a huge advantage

1320
01:21:08,300 --> 01:21:11,940
in a kind of a culture of savagery

1321
01:21:11,960 --> 01:21:19,670
and also in the techniques of warfare military historians often point out that in europe

1322
01:21:19,780 --> 01:21:22,230
the war became the science

1323
01:21:22,460 --> 01:21:25,850
in the rest of the world it was kind of

1324
01:21:28,240 --> 01:21:30,450
hobby it now and then

1325
01:21:30,470 --> 01:21:36,730
but in europe was just the science simply exterminated most of the conflicts in the

1326
01:21:36,730 --> 01:21:38,310
world today

1327
01:21:38,330 --> 01:21:40,330
are the result of the

1328
01:21:40,340 --> 01:21:48,050
efforts european invaders to impose the nation-state system on other societies to take horrible wars

1329
01:21:48,070 --> 01:21:49,920
raging in africa

1330
01:21:49,930 --> 01:21:55,970
the worst is in eastern congo were probably forming people have been killed in the

1331
01:21:55,970 --> 01:21:59,540
last few years and all over the place and the same and the same of

1332
01:21:59,540 --> 01:22:07,450
the major conflicts in pakistan and israel palestinians just go across the world plenty of

1333
01:22:07,450 --> 01:22:09,020
examples here by

1334
01:22:09,040 --> 01:22:11,820
those wars are

1335
01:22:11,830 --> 01:22:15,200
to a large extent not completely the result of

1336
01:22:15,220 --> 01:22:18,050
european imposition of

1337
01:22:18,080 --> 01:22:23,090
nation-state systems ons in societies and cultures within make any sense

1338
01:22:23,100 --> 01:22:25,040
which is just about everywhere

1339
01:22:25,050 --> 01:22:27,360
because every part of the world is

1340
01:22:27,390 --> 01:22:28,950
sufficiently diverse

1341
01:22:28,960 --> 01:22:35,000
so the forcing a nation-state system on them is going to violate human rights actually

1342
01:22:35,000 --> 01:22:37,610
the united states and australia

1343
01:22:37,630 --> 01:22:40,430
are two exceptions to this

1344
01:22:40,440 --> 01:22:45,370
but the reason is that they don't have much internal diversity the ways to europe

1345
01:22:45,420 --> 01:22:47,310
but the reason is what i mentioned

1346
01:22:48,820 --> 01:22:52,090
english invaders just exterminated the population

1347
01:22:52,140 --> 01:22:56,110
if you exterminate population you don't have the version

1348
01:22:56,200 --> 01:22:59,680
so in the united states now there is basically one language

1349
01:22:59,690 --> 01:23:06,130
but when the colonists came there with thousands of languages thousands of cultures want to

1350
01:23:06,130 --> 01:23:10,570
exterminate everybody uniform but short of extermination

1351
01:23:10,630 --> 01:23:15,040
it's extremely hard to impose the nation-state system with violence

1352
01:23:15,060 --> 01:23:19,410
europe finally in nineteen forty five

1353
01:23:19,610 --> 01:23:21,850
reached a point where it

1354
01:23:21,860 --> 01:23:28,400
it doesn't have any more wars and there's a big theory in political science and

1355
01:23:28,690 --> 01:23:33,580
international relations called the theory of the democratic peace

1356
01:23:33,770 --> 01:23:40,200
perspective cont the ideas that one societies become democratic they don't make war on each

1357
01:23:40,200 --> 01:23:41,870
other anymore

1358
01:23:41,880 --> 01:23:44,520
very hard to find any evidence for that

1359
01:23:44,540 --> 01:23:51,030
but you can find evidence for something much simpler one societies reached the point where

1360
01:23:51,030 --> 01:23:56,480
they realize that the next time they have war it's convenient everybody

1361
01:23:56,500 --> 01:23:58,460
then they don't have were

1362
01:23:58,510 --> 01:24:06,590
by nineteen forty five europe europe and its offshoots had achieved levels of destructive capacity

1363
01:24:06,590 --> 01:24:08,620
so enormous

1364
01:24:08,660 --> 01:24:14,100
that they simply cannot go to war with each other anymore for centuries from france

1365
01:24:14,100 --> 01:24:18,820
french and germans considered the highest duty in life to slaughter each other

1366
01:24:18,860 --> 01:24:21,420
same with the english and so on

1367
01:24:21,450 --> 01:24:26,870
but by nineteen forty five it's an impossibility they can go to war any longer

1368
01:24:26,930 --> 01:24:28,830
because the next began

1369
01:24:29,560 --> 01:24:31,640
the level of destruction is too high

1370
01:24:31,660 --> 01:24:34,780
so you get move towards the european union

1371
01:24:34,790 --> 01:24:38,870
and this is good or bad well maybe it has

1372
01:24:38,890 --> 01:24:40,470
both aspects

1373
01:24:40,490 --> 01:24:45,540
there's this centralizing aspect to the european union which is dangerous

1374
01:24:45,540 --> 01:24:49,210
so the power of the central bank in europe

1375
01:24:49,230 --> 01:24:54,720
it is so extreme that it's condemned even by the right wing right wingers in

1376
01:24:54,750 --> 01:24:59,870
the united states is just too much of an attack on democracy have that much

1377
01:24:59,870 --> 01:25:03,260
power in an unaccountable central bank in another

1378
01:25:03,320 --> 01:25:09,160
aspects of european integration by me raise similar questions on the other hand it's a

1379
01:25:09,170 --> 01:25:10,910
good idea to

1380
01:25:10,930 --> 01:25:18,070
have more integration among societies and what instead of sharp boundaries in conflict since all

1381
01:25:18,110 --> 01:25:22,080
the good idea to be able to travel from

1382
01:25:22,080 --> 01:25:29,490
linear to england passing through lot of complications and other interchanges as well one of

1383
01:25:29,490 --> 01:25:31,210
the reactions to

1384
01:25:31,280 --> 01:25:34,690
european unity which in my view this is pretty healthy

1385
01:25:34,700 --> 01:25:38,180
is the rise of

1386
01:25:38,230 --> 01:25:40,320
visible increase in

1387
01:25:40,320 --> 01:25:42,370
pressures towards regional

1388
01:25:42,390 --> 01:25:44,640
cultural autonomy

1389
01:25:46,290 --> 01:25:48,950
so in spain for example

1390
01:25:48,970 --> 01:25:55,110
catalonia and the basque country in the world and the others have a fair degree

1391
01:25:55,110 --> 01:25:58,070
of autonomy and independence

1392
01:25:58,140 --> 01:26:00,550
i was just in scotland

1393
01:26:00,570 --> 01:26:03,940
we care so go and scotland now has

1394
01:26:04,090 --> 01:26:09,660
parliament still part of the united kingdom but it has a degree

1395
01:26:09,740 --> 01:26:15,340
cultural and to a limited extent political independent and that's probably a good thing it's

1396
01:26:15,340 --> 01:26:22,050
happening over a lot of you're revival of regional languages regional cultures some political economy

1397
01:26:22,050 --> 01:26:22,930
and so on

1398
01:26:22,970 --> 01:26:27,470
and the to move towards what sometimes called the europe of the regions

1399
01:26:27,490 --> 01:26:29,980
with the nation-state system

1400
01:26:29,990 --> 01:26:32,780
being reduced

1401
01:26:33,900 --> 01:26:37,290
and with under the federalism but also more

1402
01:26:40,380 --> 01:26:43,480
diversity tolerance and support for diversity

1403
01:26:43,510 --> 01:26:45,590
you should say that

1404
01:26:45,660 --> 01:26:46,650
year two

1405
01:26:46,670 --> 01:26:47,790
maybe still

1406
01:26:47,790 --> 01:26:50,460
i was under investigation by the

1407
01:26:50,480 --> 01:26:52,680
turkish state security

1408
01:26:52,690 --> 01:26:56,970
authorities for the crimean separatism

1409
01:26:56,980 --> 01:27:03,080
the series ran three was based on the fact that in the talk in southeastern

1410
01:27:03,080 --> 01:27:05,580
o one beta one one beta two

1411
01:27:05,620 --> 01:27:07,200
and yes important

1412
01:27:07,220 --> 01:27:10,750
that if want to be to sum to one

1413
01:27:11,480 --> 01:27:12,390
we take

1414
01:27:12,390 --> 01:27:13,500
i one with the

1415
01:27:13,930 --> 01:27:16,500
i want i want i want to

1416
01:27:16,500 --> 01:27:20,890
you know think one of the things also one

1417
01:27:21,430 --> 01:27:24,180
so this thing is also a dirichlet

1418
01:27:24,200 --> 01:27:26,390
both of these are drawn independently

1419
01:27:26,410 --> 01:27:30,890
and the racially web this parameter alpha one is that the two parameters

1420
01:27:30,950 --> 01:27:34,080
in exactly the fashion

1421
01:27:34,100 --> 01:27:36,750
so that's the decimative property of interest

1422
01:27:38,450 --> 01:27:44,200
so now the the idea is the following so dirichlet process is basically an infinitely

1423
01:27:44,580 --> 01:27:48,120
decimated dirichlet distribution

1424
01:27:48,140 --> 01:27:49,620
so we started

1425
01:27:51,810 --> 01:27:54,060
is that recently of alpha

1426
01:27:54,080 --> 01:27:55,200
so this is

1427
01:27:55,220 --> 01:27:59,750
a one dimensional dirichlet distribution is simply appointed one

1428
01:27:59,750 --> 01:28:03,750
we take this thing and was split into two pi one pi two

1429
01:28:04,740 --> 01:28:06,310
the way we do it is

1430
01:28:06,450 --> 01:28:08,740
basically we draw

1431
01:28:08,790 --> 01:28:13,910
in independently with rock island how to

1432
01:28:13,930 --> 01:28:15,770
and that if i one

1433
01:28:15,790 --> 01:28:20,240
i could i want one one with one hand how to

1434
01:28:22,120 --> 01:28:24,270
because of the that the been decimated

1435
01:28:24,350 --> 01:28:29,270
we know that we can be helped by itself but rather innovative

1436
01:28:29,290 --> 01:28:31,540
that we can take i one i two

1437
01:28:31,560 --> 01:28:35,330
these i want to i want what i want to

1438
01:28:35,330 --> 01:28:36,700
so that this

1439
01:28:36,790 --> 01:28:38,480
o thing i want

1440
01:28:38,520 --> 01:28:41,750
we see the hyperlink i one two

1441
01:28:41,750 --> 01:28:44,250
so is end is some compact

1442
01:28:44,270 --> 01:28:49,220
and again this thing would still be dirichlet distributed with those parameters

1443
01:28:49,350 --> 01:28:53,220
we basically do this infinitely often

1444
01:28:53,430 --> 01:28:56,660
and let's see how the

1445
01:28:56,720 --> 01:29:03,140
however it lot

1446
01:29:11,310 --> 01:29:13,830
but little rectangle here

1447
01:29:14,410 --> 01:29:17,720
and the area of the rectangle corresponds to

1448
01:29:19,790 --> 01:29:21,890
the value of one of the high

1449
01:29:21,890 --> 01:29:23,640
so in this case

1450
01:29:23,680 --> 01:29:29,520
we have one because basically had their one is one the best possible probability mass

1451
01:29:29,520 --> 01:29:32,500
which we can find to the whole space

1452
01:29:32,520 --> 01:29:36,250
we take this thing i was split into who

1453
01:29:36,270 --> 01:29:38,720
and that i one and i two

1454
01:29:38,740 --> 01:29:39,850
so the area

1455
01:29:39,870 --> 01:29:42,000
within this rectangle i want

1456
01:29:42,060 --> 01:29:44,410
the area within the i two

1457
01:29:45,160 --> 01:29:49,870
again we take this rectangle with the two pi one one pi one two

1458
01:29:49,890 --> 01:29:53,770
take this thing was that i one and i two two

1459
01:29:54,750 --> 01:29:57,390
so if we repeat this we may get

1460
01:29:57,450 --> 01:30:00,140
something with the fact that

1461
01:30:00,200 --> 01:30:01,560
if we actually

1462
01:30:01,560 --> 01:30:03,790
do this of like

1463
01:30:04,040 --> 01:30:06,020
a lot of time

1464
01:30:06,040 --> 01:30:07,600
yes what we get

1465
01:30:07,640 --> 01:30:09,350
we had one thing each

1466
01:30:09,370 --> 01:30:11,470
rectangle into two parts

1467
01:30:11,500 --> 01:30:13,270
and we keep on doing it

1468
01:30:13,390 --> 01:30:14,180
thing it

1469
01:30:16,350 --> 01:30:20,720
basically draw from our trees the process

1470
01:30:20,740 --> 01:30:22,870
noticed that

1471
01:30:22,890 --> 01:30:25,720
it almost looks like a bunch of

1472
01:30:25,770 --> 01:30:27,270
o point masses

1473
01:30:27,470 --> 01:30:29,640
and in fact

1474
01:30:29,970 --> 01:30:34,910
so what this means is that draws from dirichlet process

1475
01:30:34,930 --> 01:30:37,350
it's basically a discrete distribution

1476
01:30:39,640 --> 01:30:42,390
property is that if

1477
01:30:49,770 --> 01:30:54,520
properly what is the dirichlet process dirichlet process is basically

1478
01:30:54,540 --> 01:30:58,660
a distribution over probability measures

1479
01:30:58,700 --> 01:31:02,890
what probability measures we can think of probability measures

1480
01:31:02,910 --> 01:31:04,620
as simply

1481
01:31:04,950 --> 01:31:07,080
functions as well

1482
01:31:07,950 --> 01:31:13,950
it it's a probability measure is a function from subsets of some base act

1483
01:31:13,970 --> 01:31:15,180
two zero one

1484
01:31:15,200 --> 01:31:19,100
which satisfy certain properties which makes the them probability measure

1485
01:31:19,160 --> 01:31:21,640
so these are properties which are things like

1486
01:31:21,640 --> 01:31:26,410
variable we eliminate everything else we can get any information about the ability of the

1487
01:31:26,410 --> 01:31:30,950
variable given to know something else so if you actually want to compute marginals for

1488
01:31:30,950 --> 01:31:36,980
everything which is a common occurrence when you're doing see learning is not very useful

1489
01:31:37,000 --> 01:31:39,510
the junction tree algorithm essentially

1490
01:31:39,530 --> 01:31:44,260
do variable elimination serve in all directions at the same time

1491
01:31:44,260 --> 01:31:48,040
right so that instead of doing so if you want to compute marginals are possible

1492
01:31:49,140 --> 01:31:54,760
running variable elimination and times your junction tree once you get all the marginals

1493
01:31:54,790 --> 01:31:57,490
it's basically the same amount of time

1494
01:31:57,500 --> 01:32:01,640
as everyone manage OK so it's just a way to organize competitions

1495
01:32:01,650 --> 01:32:05,790
so the things are removed because when computing the marginal for this guy and give

1496
01:32:05,910 --> 01:32:07,250
model for this guy

1497
01:32:07,260 --> 01:32:10,310
i'm doing a lot of the same kind of propagation

1498
01:32:10,330 --> 01:32:15,500
by the same sort of elimination so junction trees a data structure that that allows

1499
01:32:15,500 --> 01:32:17,090
you to

1500
01:32:17,160 --> 01:32:21,100
basically do everything at once and keep track of it without

1501
01:32:21,120 --> 01:32:25,340
retracing your steps

1502
01:32:27,250 --> 01:32:32,000
so individual modules is one thing you might want but you might want

1503
01:32:32,020 --> 01:32:36,140
things that are likely to unknown parents injection to do that

1504
01:32:36,150 --> 01:32:38,960
there are a few other things so

1505
01:32:38,970 --> 01:32:43,860
OK let's contention trees we have

1506
01:32:43,870 --> 01:32:45,210
graph right

1507
01:32:45,210 --> 01:32:47,200
and this is the set of

1508
01:32:47,220 --> 01:32:55,990
cliques in it right so cluster graph is just sort of a set of

1509
01:32:56,040 --> 01:32:58,010
groupings of variables

1510
01:32:58,940 --> 01:33:05,430
right so bring variables and things are connected in this in this cluster of they

1511
01:33:05,440 --> 01:33:11,860
shared nodes but now a cluster graph is the junction tree has the following properties

1512
01:33:12,080 --> 01:33:16,270
one singly connected so we want the actual tree

1513
01:33:16,290 --> 01:33:22,510
right so we have exactly one path between each pair of clusters are this questions

1514
01:33:23,260 --> 01:33:24,390
from the graph

1515
01:33:24,440 --> 01:33:27,620
they get

1516
01:33:27,640 --> 01:33:30,510
so the green cluster and the purple one

1517
01:33:32,140 --> 01:33:34,000
and the light blue

1518
01:33:34,420 --> 01:33:38,900
and i want exactly one between

1519
01:33:38,940 --> 01:33:43,720
OK so this is going to allow us to do

1520
01:33:43,760 --> 01:33:46,160
their elimination without introducing

1521
01:33:46,170 --> 01:33:47,690
any more cliques

1522
01:33:47,700 --> 01:33:52,050
it turns out the things covering free to create

1523
01:33:55,410 --> 01:34:00,710
this this the input graph then there's a cluster c

1524
01:34:00,760 --> 01:34:02,180
that contains

1525
01:34:02,340 --> 01:34:04,500
so if i had

1526
01:34:06,330 --> 01:34:07,800
you know click here

1527
01:34:09,000 --> 01:34:15,390
have potentials associated with them and we take the potentials and represent them in this

1528
01:34:15,390 --> 01:34:16,320
cluster of

1529
01:34:16,370 --> 01:34:17,990
OK so

1530
01:34:17,990 --> 01:34:19,910
i know a place to put this thing

1531
01:34:19,930 --> 01:34:22,990
OK so always ensures that i can take the potential

1532
01:34:23,000 --> 01:34:26,500
and stick it into the street it's going to be square present my

1533
01:34:26,500 --> 01:34:27,600
i graph

1534
01:34:27,620 --> 01:34:31,070
and the third thing you need is was called the running intersection

1535
01:34:31,900 --> 01:34:34,290
and learning social properties that

1536
01:34:34,720 --> 01:34:37,560
is that if you have

1537
01:34:38,280 --> 01:34:40,550
a pair of clusters

1538
01:34:41,590 --> 01:34:44,590
this one and this one and they both contain

1539
01:34:48,390 --> 01:34:53,520
each cluster on the path between these things contains b

1540
01:34:53,540 --> 01:34:54,810
france so

1541
01:34:54,830 --> 01:35:00,280
this is satisfied here because these here here is unique between them and these in

1542
01:35:01,250 --> 01:35:06,190
what that guarantees it turns out that when we compute all marginals are going to

1543
01:35:06,190 --> 01:35:11,490
be doing this running variable elimination or something like the information our computer models on

1544
01:35:11,490 --> 01:35:12,720
the street

1545
01:35:12,730 --> 01:35:18,070
so in the end get a marginal rate and marginal would be marginal BC

1546
01:35:18,170 --> 01:35:23,910
and these marginals are going to agree with each other and if if he was

1547
01:35:23,910 --> 01:35:25,310
missing from here

1548
01:35:25,320 --> 01:35:29,410
it might happen is that the marginal get here in the morning get here i

1549
01:35:29,410 --> 01:35:31,530
don't really know about each other

1550
01:35:31,540 --> 01:35:35,040
and they completely disagree so the running intersection property property

1551
01:35:35,140 --> 01:35:36,920
in nineteen here

1552
01:35:36,940 --> 01:35:42,270
the intuition for it's trying to essentially enforce consistency

1553
01:35:42,290 --> 01:35:43,710
OK so

1554
01:35:44,240 --> 01:35:45,290
we start with our

1555
01:35:45,310 --> 01:35:48,260
just to restate we start with the graph has a bunch of

1556
01:35:48,300 --> 01:35:49,700
clicks on it

1557
01:35:49,740 --> 01:35:56,230
there we want to build a junction tree with these properties it has these clusters

1558
01:35:56,230 --> 01:35:59,100
for each clique where we can accommodate all are

1559
01:36:00,470 --> 01:36:04,210
it it singly connected

1560
01:36:04,260 --> 01:36:06,480
and running intersection

1561
01:36:06,500 --> 01:36:10,530
so going from set of cliques in two clusters

1562
01:36:10,540 --> 01:36:15,150
so how do you do this how do you do this

1563
01:36:15,150 --> 01:36:21,850
basically you know you start was kind of variable elimination idea june ordering and use

1564
01:36:21,850 --> 01:36:23,220
node elimination

1565
01:36:23,230 --> 01:36:26,250
two things sort of these creeks

1566
01:36:27,140 --> 01:36:29,010
that gives you a set of these

1567
01:36:29,440 --> 01:36:33,390
so complete cluster graphs or

1568
01:36:33,390 --> 01:36:38,010
the mission figs are maximal so you don't take part the clique over a b

1569
01:36:38,120 --> 01:36:39,410
can c

1570
01:36:39,410 --> 01:36:43,850
the end creek or b and c astonishing click don't worry about it so i

1571
01:36:43,850 --> 01:36:45,530
we take the maximum ones

1572
01:36:45,830 --> 01:36:49,720
so i remember elimination i get these factors

1573
01:36:49,730 --> 01:36:53,070
right i look at the maximal ones and so

1574
01:36:53,220 --> 01:36:55,310
filter the ones that are subsets

1575
01:36:55,330 --> 01:36:58,610
and i believe that you know

1576
01:36:58,620 --> 01:37:04,320
compute cluster graph so now

1577
01:37:04,330 --> 01:37:07,990
clusters are connected if they if they share variables like a certain cluster of in

1578
01:37:08,000 --> 01:37:10,750
their connected to share it was so

1579
01:37:10,750 --> 01:37:14,260
now we have no proof those

1580
01:37:16,400 --> 01:37:22,130
the edges by running a maximum weight spanning tree OK so expansion is going to

1581
01:37:22,130 --> 01:37:29,020
to ensure that there is exactly a unique path between everybody and

1582
01:37:29,050 --> 01:37:31,320
and in this case running intersection

1583
01:37:32,290 --> 01:37:41,660
the property of singly connected bring section covering so the weight on each and each

1584
01:37:42,320 --> 01:37:45,100
it is actually usually done by

1585
01:37:45,120 --> 01:37:48,000
but this computing the size of the in c

1586
01:37:48,050 --> 01:37:51,490
the reason for this is what we're doing eventually is

1587
01:37:51,490 --> 01:37:54,700
computing kind of factors that can move back and forth

1588
01:37:54,710 --> 01:38:00,320
red and the weight of each edge like the expense the the expense that we

1589
01:38:00,710 --> 01:38:04,570
could incur by using that edge is corresponds to

1590
01:38:04,650 --> 01:38:08,240
kind of the number of possible values these things can take

1591
01:38:08,660 --> 01:38:12,770
passing messages that are the size of the joint assignments of thing

1592
01:38:15,270 --> 01:38:18,920
by finding

1593
01:38:19,010 --> 01:38:24,290
maximum weight one actually we ensure that consistency so

1594
01:38:24,320 --> 01:38:28,600
there's not going to go going to prove why this works but this is how

1595
01:38:28,600 --> 01:38:32,380
you do it i mean you need to pick a good orderings right and heuristics

1596
01:38:32,380 --> 01:38:33,770
for doing that

1597
01:38:33,780 --> 01:38:38,320
you but after that it's it's not you it's very simple

1598
01:38:40,410 --> 01:38:43,710
here's an example

1599
01:38:43,740 --> 01:38:45,100
to make this country

1600
01:38:45,120 --> 01:38:48,530
we sort of creeks

1601
01:38:48,550 --> 01:38:51,350
one we eliminated the order was

1602
01:38:51,360 --> 01:38:55,560
really the the then even see the OK

1603
01:38:55,570 --> 01:38:57,700
these are the nation cliques

1604
01:38:57,710 --> 01:39:03,800
and then what we have done is looked at the maximum one so e

1605
01:39:03,830 --> 01:39:06,770
it's not that someone is contained here

1606
01:39:06,790 --> 01:39:11,120
and the rest are sent from a complete graph where there is an edge since

1607
01:39:11,120 --> 01:39:13,400
so there is the kind of shot into this room

1608
01:39:13,410 --> 01:39:16,670
the receiver the person called the receiver is the one who is going to try

1609
01:39:16,670 --> 01:39:18,350
to be psychic

1610
01:39:18,390 --> 01:39:21,490
she in this picture is going to try to receive information

1611
01:39:21,500 --> 01:39:23,260
from somewhere else

1612
01:39:23,320 --> 01:39:27,150
the monitor is kind of like coach who going to explain to her what she

1613
01:39:27,150 --> 01:39:28,100
needs to do

1614
01:39:28,140 --> 01:39:32,150
the monitor has no idea what the right answer is and this picture here

1615
01:39:32,210 --> 01:39:36,890
is there basically just described her with her task is going to be and to

1616
01:39:36,890 --> 01:39:40,170
make sure that she does

1617
01:39:40,200 --> 01:39:44,250
OK so once they are in the room and assistant does not roll dice in

1618
01:39:44,330 --> 01:39:52,510
system uses a computer but harder to illustrate with computer so assistant randomly picks

1619
01:39:52,550 --> 01:39:53,830
the target

1620
01:39:53,840 --> 01:39:57,290
the target comes from in this case it's shown that there's a bunch of files

1621
01:39:57,290 --> 01:39:58,420
in if file can

1622
01:39:58,480 --> 01:40:02,450
in some experiments we actually did have folders with photographs and

1623
01:40:02,470 --> 01:40:05,780
the photographs were all taken from national geographic magazine

1624
01:40:05,800 --> 01:40:09,700
and so the receiver we know that it was going to be a photograph from

1625
01:40:09,720 --> 01:40:13,290
national geographic magazine that's all they were now

1626
01:40:13,300 --> 01:40:16,610
so one of the photographs is chosen as the target oh i should mention in

1627
01:40:16,610 --> 01:40:21,980
some laboratories especially these days instead of photographs they use very short clips from movies

1628
01:40:21,980 --> 01:40:23,720
maybe fifteen second

1629
01:40:24,450 --> 01:40:26,950
from the movie because it seems like

1630
01:40:26,960 --> 01:40:30,350
the more information there is to receive the more information

1631
01:40:30,370 --> 01:40:31,600
is received

1632
01:40:31,610 --> 01:40:32,820
we'll see that

1633
01:40:32,840 --> 01:40:37,210
OK so let's say came out as a picture of the town hall

1634
01:40:38,640 --> 01:40:42,990
the receiver spend fifteen or so minutes trying to describe what she thinks the target

1635
01:40:42,990 --> 01:40:44,300
actually is

1636
01:40:44,330 --> 01:40:45,790
and we have

1637
01:40:45,870 --> 01:40:51,320
this is what happens

1638
01:40:52,140 --> 01:40:56,690
so that's the basic outline of the experiment now how is

1639
01:40:56,700 --> 01:40:58,320
in evaluating

1640
01:40:58,330 --> 01:40:59,540
give you some details

1641
01:40:59,780 --> 01:41:04,730
so first of all for security purposes after the session the drawings and descriptions are

1642
01:41:04,730 --> 01:41:09,370
copied and secured so they can't be changed so the originals are locked away so

1643
01:41:09,370 --> 01:41:11,770
nobody can alter the

1644
01:41:11,830 --> 01:41:15,940
and then the person the receiver is given feedback showing him or her the copy

1645
01:41:15,940 --> 01:41:20,900
of what they true and also what the correct answer was

1646
01:41:20,940 --> 01:41:24,750
and then the results are judged now in some laboratories the person who actually did

1647
01:41:24,750 --> 01:41:26,940
the description is the judge

1648
01:41:26,950 --> 01:41:30,670
and then of course before they actually show the right answer

1649
01:41:30,680 --> 01:41:33,750
in other laboratories is an independent judge

1650
01:41:33,760 --> 01:41:37,680
so it just depends on the structure

1651
01:41:37,720 --> 01:41:41,810
so this meets condition number one safeguards to rule out cheating and ordinary means of

1652
01:41:45,150 --> 01:41:48,820
but i haven't shown you how we need condition number two about knowing probability the

1653
01:41:51,360 --> 01:41:55,350
here's an example of an excellent match this was one of the experiments done well

1654
01:41:55,350 --> 01:42:00,380
actually so the program moved up one point from sir i to another think tank

1655
01:42:00,380 --> 01:42:05,510
in the stanford area called SAIC science applications international corporation so here's an example of

1656
01:42:05,510 --> 01:42:07,770
an experiment that was done there and yellow

1657
01:42:07,790 --> 01:42:10,040
what about the writing it because it would be kind of hard for you to

1658
01:42:11,010 --> 01:42:14,260
but basically this is what the receiver drew

1659
01:42:14,280 --> 01:42:19,560
you just do this picture he said here there would like he mountain here

1660
01:42:19,580 --> 01:42:21,670
barn or large cabin

1661
01:42:21,690 --> 01:42:28,410
shadow trees road path american rockies or maybe alps

1662
01:42:28,430 --> 01:42:31,670
and the correct target was

1663
01:42:31,690 --> 01:42:35,450
better than i could have done if i had been looking at a picture

1664
01:42:35,460 --> 01:42:37,840
so this is the kind of thing that i would say once in a while

1665
01:42:37,840 --> 01:42:42,390
that would really be striking and yet the evaluation process is you'll see

1666
01:42:42,830 --> 01:42:46,410
it doesn't take advantage of how good that is it just basically

1667
01:42:46,450 --> 01:42:48,790
you'll see how do the judging

1668
01:42:48,820 --> 01:42:51,470
which is this guy

1669
01:42:51,480 --> 01:42:53,490
it's actually corrected mistakes

1670
01:43:01,840 --> 01:43:04,890
but the first

1671
01:43:04,900 --> 01:43:08,470
OK so here's a more typical example in the early days they actually didn't use

1672
01:43:08,470 --> 01:43:10,240
photographs they actually had

1673
01:43:10,250 --> 01:43:13,050
but the current outbound experimenter who would

1674
01:43:13,300 --> 01:43:18,210
the given a randomly selected and the target location based on an envelope would be

1675
01:43:18,210 --> 01:43:21,710
picked from a series of possibilities and the person would be forced to drive to

1676
01:43:21,710 --> 01:43:22,960
that location

1677
01:43:22,990 --> 01:43:26,180
and current trends send information from that location

1678
01:43:26,190 --> 01:43:30,270
they called them the outbound founder experiment so here's an example from those days

1679
01:43:30,300 --> 01:43:33,440
basically just says parking

1680
01:43:33,450 --> 01:43:40,390
a single structure building that brown's next edge water boats large parking area

1681
01:43:40,410 --> 01:43:44,400
and you are shown this to be subliminally give you a hand held and that

1682
01:43:44,400 --> 01:43:47,720
this was the location where person had gone

1683
01:43:47,770 --> 01:43:49,010
so again

1684
01:43:49,050 --> 01:43:52,550
pretty good match however not skeptics

1685
01:43:52,560 --> 01:43:54,940
this is the san francisco bay area

1686
01:43:54,990 --> 01:44:00,170
water gas lots of it's only the water that is surprising gas

1687
01:44:00,300 --> 01:44:04,220
large parking areas so this person supposed to drive locations

1688
01:44:04,260 --> 01:44:08,300
get out and looks of course let's choose locations that have parking areas

1689
01:44:08,350 --> 01:44:10,420
OK so what may seem like

1690
01:44:10,450 --> 01:44:14,630
a surprising match might just have some elements that would make sense boats and so

1691
01:44:15,500 --> 01:44:20,480
so we can judge it by looking in just trying to compare like that

1692
01:44:20,490 --> 01:44:23,490
so how do we judge well here's how not to do is an interesting little

1693
01:44:23,490 --> 01:44:27,650
story behind this but i just read this so scott adams i get delivered in

1694
01:44:27,650 --> 01:44:30,970
your countries but in the united states the popular comic

1695
01:44:30,980 --> 01:44:34,780
so scott adams did series and can skeptic and i'll give you a little story

1696
01:44:34,780 --> 01:44:35,930
behind but

1697
01:44:35,950 --> 01:44:39,800
so can the skeptic is saying i don't know the content of this and below

1698
01:44:39,850 --> 01:44:42,790
and so the i think it's a doctor

1699
01:44:42,810 --> 01:44:48,240
delivered anyway the guys who the characters as it to charcoal drawing which i think

1700
01:44:48,240 --> 01:44:53,210
is smaller and and can skeptics as nice try fraud that's a long way from

1701
01:44:53,210 --> 01:44:55,770
in trying to be reading a tangerine

1702
01:44:55,880 --> 01:45:02,780
so the story behind this is in nineteen ninety five government decided to declassify the

1703
01:45:02,780 --> 01:45:06,470
research because the cold war was over classified some of it

1704
01:45:06,490 --> 01:45:08,990
and the helen mentioned

1705
01:45:09,010 --> 01:45:12,410
i was on the team with ray hyman who is one well-known skeptic and we

1706
01:45:12,410 --> 01:45:15,410
were to do an evaluation of the research for congress and so we did this

1707
01:45:15,410 --> 01:45:20,750
evaluation and we published well we weren't allowed actually published results at first and then

1708
01:45:20,750 --> 01:45:26,340
somebody leads them to ABC news so we're on TV for about a week we're

1709
01:45:26,340 --> 01:45:31,730
on larry king live on CNN news here these TV shows and so scott adams

1710
01:45:31,730 --> 01:45:34,810
saw this and you wanted to know what was going on so called up and

1711
01:45:34,810 --> 01:45:40,070
may was the project director and then when i had lunch and then describe what's

1712
01:45:40,070 --> 01:45:44,010
going on in the series i can scale

1713
01:45:44,030 --> 01:45:48,070
so the point here is we can't use subjective probability of a match there's just

1714
01:45:48,070 --> 01:45:49,860
too much room for personal bias

1715
01:45:49,930 --> 01:45:54,600
so can the skeptics can pretty much dismissed whatever is done

1716
01:45:54,600 --> 01:45:56,240
infinite dimensional case

1717
01:45:56,360 --> 01:45:58,510
come to no surprise

1718
01:45:58,530 --> 01:46:03,200
then why should we care about this all the more to men with invasion is

1719
01:46:03,210 --> 01:46:08,220
the application to hierarchical bayes modelling which is

1720
01:46:08,230 --> 01:46:11,460
not quite the standard way of doing bayes modelling

1721
01:46:11,480 --> 01:46:17,320
it allows you to model situations where you have related but not

1722
01:46:17,330 --> 01:46:20,880
identical models for different situations example if your model

1723
01:46:20,900 --> 01:46:25,140
the outcome prediction in different hospitals these models might be slightly different

1724
01:46:25,150 --> 01:46:29,620
but also have a lot in common situation they can apply hierarchical bayes models

1725
01:46:29,640 --> 01:46:35,000
and then this is one of the core of the whole lecture usually processes

1726
01:46:35,010 --> 01:46:37,970
and then i will and its applications

1727
01:46:37,990 --> 01:46:42,440
and a couple more comments on nonparametric bayesian model

1728
01:46:47,930 --> 01:46:49,860
so this sort of two

1729
01:46:49,910 --> 01:46:55,350
get people want to to think about bayesian approaches to machine learning

1730
01:46:55,360 --> 01:47:00,600
so probability theory is considered a branch of mathematics

1731
01:47:00,610 --> 01:47:04,120
so you start with axioms and theorems are theorems

1732
01:47:04,140 --> 01:47:06,620
to prove things to be true or not

1733
01:47:06,640 --> 01:47:13,520
and statistics and statistical machine learning thirty attempts to apply probability theory

1734
01:47:13,600 --> 01:47:16,140
to solving problems in the real world

1735
01:47:16,160 --> 01:47:22,560
examples i effectiveness of medication so this is the typical problems statisticians want to address

1736
01:47:22,570 --> 01:47:30,010
is a significant improvement if you apply medication not in machine learning my text classification

1737
01:47:30,020 --> 01:47:34,850
you want to design a medical expert system where you want to incorporate the

1738
01:47:34,860 --> 01:47:39,450
the expert medical experts knowledge into a system and then you have to do inference

1739
01:47:39,450 --> 01:47:40,860
in the system about

1740
01:47:40,870 --> 01:47:44,200
disease is sometimes and so on

1741
01:47:44,520 --> 01:47:48,890
there are different approaches to applying probability theory to problems in the real world and

1742
01:47:48,910 --> 01:47:50,310
useful way

1743
01:47:50,680 --> 01:47:56,190
the classical way ways the frequentist way frequentist statistics and the bayesian statistics

1744
01:47:56,210 --> 01:47:58,490
in statistical learning theory

1745
01:47:58,510 --> 01:48:00,840
and there are couple more

1746
01:48:00,890 --> 01:48:05,690
so all of them seem to be doing something useful in their own right and

1747
01:48:06,050 --> 01:48:09,040
it's probably not very good idea to

1748
01:48:09,050 --> 01:48:11,350
try to start a discussion about which one is the

1749
01:48:11,370 --> 01:48:14,600
right one which is the wrong one there

1750
01:48:16,410 --> 01:48:23,190
situations where each of them can be very useful and but today we will take

1751
01:48:23,190 --> 01:48:24,450
a bayesian

1752
01:48:24,460 --> 01:48:30,330
o point of view on all of this

1753
01:48:30,340 --> 01:48:35,200
OK if that i was really started very far back

1754
01:48:35,210 --> 01:48:37,180
warming up a little bit cell

1755
01:48:38,430 --> 01:48:44,440
because if you haven't done bayesian statistics and what you might remember all these simple

1756
01:48:44,490 --> 01:48:49,680
bruce anymore so i will be with the review them briefly so the joint distribution

1757
01:48:49,680 --> 01:48:55,190
of x and y simply means the probability that that he observed that random access

1758
01:48:55,190 --> 01:48:58,670
in state x and y is in state y

1759
01:48:58,690 --> 01:49:03,510
then there is the definition of conditional distributions

1760
01:49:03,580 --> 01:49:05,120
p of y

1761
01:49:05,140 --> 01:49:06,320
given x

1762
01:49:06,340 --> 01:49:10,510
it is defined as the joint u of x and y

1763
01:49:10,530 --> 01:49:17,370
divided by p of x was just give experts normalizes this expression is the proper

1764
01:49:17,440 --> 01:49:20,740
probability the t in terms of y

1765
01:49:20,830 --> 01:49:23,720
this improves

1766
01:49:25,050 --> 01:49:30,530
then there's is the product decomposition if you have distance of two random variables you

1767
01:49:30,530 --> 01:49:35,320
can always decompose them in the probability distribution of one of them and then in

1768
01:49:35,360 --> 01:49:38,660
terms of distribution second one given the first one

1769
01:49:38,700 --> 01:49:42,460
and of course you can also change the order and get to

1770
01:49:42,790 --> 01:49:45,660
another decomposition

1771
01:49:45,680 --> 01:49:51,320
and this can be extended to repeatedly apply this rule we obtain the chain rule

1772
01:49:51,320 --> 01:49:57,610
of probability so any high dimensional probability distribution x one to x and can be

1773
01:49:57,620 --> 01:50:01,620
decomposed in any order by starting for example with x one

1774
01:50:01,730 --> 01:50:06,460
then you multiply it by the probability of x two given x one x three

1775
01:50:06,460 --> 01:50:09,870
given all the previous appear in the formula

1776
01:50:09,890 --> 01:50:13,350
and the last term is x given all

1777
01:50:13,400 --> 01:50:22,110
his predecessor this these are one dimensional probability densities and this is very general and

1778
01:50:22,110 --> 01:50:26,530
of course one hopes and that one can use exploit some independencies in the domain

1779
01:50:26,530 --> 01:50:31,570
for example x three is independent of x one given x two something of that

1780
01:50:31,580 --> 01:50:35,970
sort and this also the basis for invasion

1781
01:50:35,990 --> 01:50:44,290
network which are essentially implementation of the decomposition in in terms of the graphical representation

1782
01:50:44,310 --> 01:50:51,800
but as we have always who was also very simple PX given y is just

1783
01:50:51,800 --> 01:50:58,590
the definition of conditional probability distribution and then into the product decomposition up here and

1784
01:50:58,590 --> 01:51:00,730
you can through p of x

1785
01:51:00,840 --> 01:51:04,670
the enwise PY x two x divided by p of y

1786
01:51:04,690 --> 01:51:09,870
so all of this is a very simple very basic in bayesian modeling

1787
01:51:09,890 --> 01:51:15,520
formulas all the time and i think the last one which is relevant is the

1788
01:51:15,700 --> 01:51:22,760
marginalisation so if you know the joint probability distribution and you want to calculate the

1789
01:51:23,660 --> 01:51:26,700
one of the variables involved after integrate

1790
01:51:26,870 --> 01:51:30,040
this distribution over all the other animals

1791
01:51:30,050 --> 01:51:31,380
in the distribution

1792
01:51:31,400 --> 01:51:37,770
this marks solution mean this calculation gives you all the headaches if you applied to

1793
01:51:37,970 --> 01:51:39,250
the given problem

1794
01:51:39,270 --> 01:51:44,780
so so these are the basic rules which we use all the time the statistics

1795
01:51:46,970 --> 01:51:49,170
we simply apply these rules in

1796
01:51:50,350 --> 01:51:52,900
statistical sense so

1797
01:51:52,900 --> 01:52:00,520
same set of solutions that is given the upon daily mode of thought there exists

1798
01:52:00,520 --> 01:52:08,520
a unique density of new such that of the of quantized equals

1799
01:52:08,570 --> 01:52:11,380
the density said that

1800
01:52:11,860 --> 01:52:20,760
what uh purified is not defined in unique way so this equality holds up to

1801
01:52:20,760 --> 01:52:22,910
another measure such

1802
01:52:24,220 --> 01:52:33,380
OK and I would like to mention in passing a wide it's interesting to interests

1803
01:52:33,440 --> 01:52:35,060
in conveys dimensions

1804
01:52:35,440 --> 01:52:41,000
so that by construction quantizer subset

1805
01:52:41,420 --> 01:52:43,660
very dense and probability mass

1806
01:52:44,760 --> 01:52:51,180
for a fixed volume they maximize the probability mass while equal entry for a fixed

1807
01:52:51,180 --> 01:53:00,110
probability mass that minimize the volume and solar and they and tools of special interest

1808
01:53:00,110 --> 01:53:05,160
for investigating the structure of distributions and that's why are

1809
01:53:05,600 --> 01:53:15,380
and they are used in many unsupervised learning applications such that anomaly detection cluster analysis

1810
01:53:15,380 --> 01:53:20,680
probably many other applications

1811
01:53:20,690 --> 01:53:26,320
so now we're back to the 1 class is you and so this is the

1812
01:53:26,320 --> 01:53:29,680
algorithm exchange and the question

1813
01:53:29,770 --> 01:53:30,270
it is

1814
01:53:31,290 --> 01:53:35,860
what's the link between the acceptance region

1815
01:53:35,890 --> 01:53:43,220
computed by the algorithm and quantize or equivalently the density of

1816
01:53:48,620 --> 01:53:54,030
In the paper from coffin called

1817
01:53:54,760 --> 01:54:04,240
it was suggested that the algorithm might be for this problem but also I think

1818
01:54:04,240 --> 01:54:10,460
so far there has been no proof stating that this algorithm is effectively consistent for

1819
01:54:10,740 --> 01:54:17,680
this task and so that's the main from the main contribution of the work that

1820
01:54:17,680 --> 01:54:26,070
I'm going to presented now we have all of the all the of the all

1821
01:54:26,410 --> 01:54:39,620
all is name of the the of this program of and

1822
01:54:42,360 --> 01:54:53,140
so the main contribution here is to say that under certain assumptions and in some

1823
01:54:53,140 --> 01:55:01,840
contexts and want us as helium actually is actually consistent for estimating that set so

1824
01:55:01,840 --> 01:55:08,770
this context is characterized by the use of a fixed

1825
01:55:08,790 --> 01:55:16,060
the regularization parameter and that is whatever the number of training data points we will

1826
01:55:16,060 --> 01:55:21,910
work with a fixed values for the parameters and so this

1827
01:55:22,000 --> 01:55:30,480
this might be not in the spirit of standard liberalization since when we do regularization

1828
01:55:30,680 --> 01:55:33,740
the parameter

1829
01:55:34,220 --> 01:55:42,100
is usually a vanishing turns a syntactically but here we will work with the constant

1830
01:55:43,670 --> 01:55:52,270
that is 1 of the main particularity of the set up but in addition we

1831
01:55:52,270 --> 01:55:55,430
will consider the Gaussian kernel

1832
01:55:55,480 --> 01:56:03,810
and with a well-calibrated Bandwidth signal we consider a bandwidth that decrease starts at a

1833
01:56:03,810 --> 01:56:19,260
certain speed synthetically right so that know the very top can begin was sort of

1834
01:56:19,270 --> 01:56:25,500
introduction and uh so the talk is structured as follows in the 1st part I

1835
01:56:25,500 --> 01:56:32,270
will present the main result together with necessary notation for expressing the result

1836
01:56:32,620 --> 01:56:39,720
and also I will presented what I call the the picture of the whole machinery

1837
01:56:40,680 --> 01:56:45,840
so in in order to be viewed the inclusion of the main reason why the

1838
01:56:45,840 --> 01:56:48,260
main results actually holds

1839
01:56:49,110 --> 01:56:56,860
and afterwards so that the next 4 parts will be dedicated to the periphery proof

1840
01:56:57,100 --> 01:57:02,760
of that so that the

1841
01:57:03,010 --> 01:57:10,670
so uh unpublished use from the patients so

1842
01:57:11,180 --> 01:57:21,760
exploit accession that the data sample drawn from unknown probability distribution P the probability has

1843
01:57:21,760 --> 01:57:28,190
compact support well as support included in the reference subset subsets X which is assumed

1844
01:57:28,210 --> 01:57:34,440
to be compact so the accessories including on the

1845
01:57:34,910 --> 01:57:43,260
and the so really keep in mind that exist combined because it's strong assumption and

1846
01:57:43,540 --> 01:57:48,140
the result that I'm going to present are only valid for compact subset

1847
01:57:48,980 --> 01:57:57,740
than that the Gaussian kernel in here know that we used normalized to Gaussian kernel

1848
01:57:58,000 --> 01:58:03,900
so this this is the standard along with this constant that depends on sigma and

1849
01:58:03,900 --> 01:58:09,510
the thing is if we take the integral on audio of these functions with respect

1850
01:58:09,510 --> 01:58:11,130
to the 1st viable

1851
01:58:11,820 --> 01:58:15,740
that the interval equals 1 in

1852
01:58:15,980 --> 01:58:22,490
and page sigma is the associated Gaussians tests

1853
01:58:23,050 --> 01:58:33,070
entities and build with the normal denoted like that of denote by s sigma hat

1854
01:58:33,090 --> 01:58:39,310
the output of the one class SVM algorithms when used with the Gaussian kernel with

1855
01:58:39,510 --> 01:58:40,330
with sigma

1856
01:58:40,600 --> 01:58:46,820
and finally we use this notation

1857
01:58:46,880 --> 01:58:49,740
policy now that 2

1858
01:58:49,740 --> 01:58:51,760
this is something create

1859
01:58:52,440 --> 01:58:58,060
so this is the theoretical versions of this asymptotic there

1860
01:58:58,400 --> 01:59:03,920
3 of the emperical as risk so we just replace them by recalled comes here

1861
01:59:03,920 --> 01:59:12,300
so who who in this class is thinking of preparing a phd degree and who

1862
01:59:12,300 --> 01:59:19,580
in this class has already completed should degree so that's not something semantic many most

1863
01:59:19,580 --> 01:59:23,880
of you are actually preparing his it's so i'm assuming actually for most of you

1864
01:59:24,080 --> 01:59:29,040
know this material must must have been easier how many of you are working on

1865
01:59:29,070 --> 01:59:37,840
machine learning already gave so how many of you thought to know this

1866
01:59:38,380 --> 01:59:43,220
this introduction was easy to understand

1867
01:59:43,230 --> 01:59:52,080
who hasn't this is only the beginning of the intersection so how many of you

1868
01:59:52,080 --> 01:59:56,580
had heard before of kernel methods

1869
01:59:56,580 --> 02:00:01,310
how many of you have worked on the electrics on many of you have worked

1870
02:00:01,310 --> 02:00:10,420
on decision trees many of you have worked on monday to talk about it

1871
02:00:10,580 --> 02:00:13,750
how many of you had heard about our fitting

1872
02:00:19,730 --> 02:00:25,400
OK so the the development of of it so how many of you understands after

1873
02:00:25,400 --> 02:00:35,600
this lecture what is a risk functional i would like to see how many of

1874
02:00:35,600 --> 02:00:41,350
you understands what is a linear decision boundary

1875
02:00:41,360 --> 02:00:45,280
and uh

1876
02:00:45,340 --> 02:00:50,610
thank you

1877
02:00:50,620 --> 02:00:55,380
OK so let's go back very quickly then

1878
02:00:55,460 --> 02:00:59,840
and see how well we're doing here

1879
02:00:59,860 --> 02:01:03,160
so how many of you understands what on these axes

1880
02:01:03,180 --> 02:01:09,960
the index is the x and y here because of vote for the liberal understand

1881
02:01:10,960 --> 02:01:12,720
looking at this

1882
02:01:12,750 --> 02:01:15,710
i'm going on this year

1883
02:01:15,770 --> 02:01:20,750
looking at this matrix which is the data matrix

1884
02:01:20,770 --> 02:01:23,060
OK you have two dimensions

1885
02:01:23,140 --> 02:01:28,360
the number of features child the numbering of of input components and the number of

1886
02:01:28,360 --> 02:01:33,700
examples and so on on the axis of of these plots i'm showing the number

1887
02:01:33,710 --> 02:01:39,400
of features or inputs and the number of examples of this is the aspect ratio

1888
02:01:39,440 --> 02:01:43,400
of the data of the of the data and the aspect ratio of the data

1889
02:01:43,400 --> 02:01:48,890
is very important how many examples you have relative to the number of features that

1890
02:01:48,910 --> 02:01:54,820
plainly playing a lot with that in this class in india actually trying to take

1891
02:01:54,820 --> 02:02:01,680
these input matrix and shrink in that direction here discard features the features that are

1892
02:02:01,680 --> 02:02:08,080
not important because there are other cases in which we might want to discard examples

1893
02:02:08,080 --> 02:02:13,000
usually you don't want to discard examples right because examples are very useful in the

1894
02:02:13,000 --> 02:02:16,730
more examples you have the better you can train but in some cases you might

1895
02:02:16,730 --> 02:02:21,680
want to group examples because if your matrix is very sparse and and only few

1896
02:02:21,680 --> 02:02:27,680
features at the time are active then one example alone might not carry enough information

1897
02:02:27,680 --> 02:02:33,420
so might want to smooth things an average examples that's something people do but in

1898
02:02:33,420 --> 02:02:38,840
this class will be working mostly in that dimension how to compress data by removing

1899
02:02:38,840 --> 02:02:41,420
features that are useless or redundant

1900
02:02:43,200 --> 02:02:53,460
so how many of you understands this this graph here what it does so most

1901
02:02:53,460 --> 02:02:57,380
of you have understood that these are the inputs inputs so weighted

1902
02:02:57,440 --> 02:03:02,320
by some weights and to compute the decision function

1903
02:03:02,330 --> 02:03:07,520
and it's not which is this

1904
02:03:07,520 --> 02:03:12,560
so and is number of features in the conventions that i'm i'm using and is

1905
02:03:12,560 --> 02:03:19,060
going to be always number of features and and the number of examples

1906
02:03:19,080 --> 02:03:22,110
so it's very important to notice that because in the

1907
02:03:22,140 --> 02:03:27,480
percent from case you see that the sum runs over and so here's began because

1908
02:03:27,480 --> 02:03:32,420
we have transformed first inputs into new features so we have the original matrix which

1909
02:03:32,420 --> 02:03:37,130
have this small and here and then we doing some transform and you know we're

1910
02:03:37,130 --> 02:03:41,020
going to increase the number of features or to decrease it

1911
02:03:41,060 --> 02:03:47,170
in the case of the kernel methods actually usually increasing the number of features so

1912
02:03:47,180 --> 02:03:50,300
what happens in the case of the kernel method is that the sum now runs

1913
02:03:50,300 --> 02:03:55,960
over and but because of the kernel trick that i've shown you before you can

1914
02:03:55,960 --> 02:03:58,780
go between the two representations

1915
02:03:58,830 --> 02:04:07,040
you can transform the percent into a kernel methods and vice versa and the reason

1916
02:04:07,040 --> 02:04:13,480
why this is interesting is that then you can have few implicit feature spaces that

1917
02:04:13,480 --> 02:04:16,670
are of infinite dimension

1918
02:04:16,680 --> 02:04:18,860
why would you want that's well

1919
02:04:18,900 --> 02:04:27,190
the you want and a lot of expressive power so if you plunge your original

1920
02:04:27,190 --> 02:04:31,650
input into the very big feature space then you're going to be able to draw

1921
02:04:31,650 --> 02:04:37,210
decision boundaries that are very complex in your original input space

1922
02:04:37,220 --> 02:04:42,080
whereas in the feature space it it could be a linear decision function so that's

1923
02:04:42,080 --> 02:04:46,820
that's the main idea the main trick you gonna be taking original feature space and

1924
02:04:46,820 --> 02:04:50,100
for example the first people working on that they just make products of the input

1925
02:04:50,100 --> 02:04:55,140
features so one that when he invented the percent had this idea

1926
02:04:55,190 --> 02:05:01,090
well let's take random let's take random features this some feature the fund under the

1927
02:05:02,000 --> 02:05:08,260
and next take an arbitrary billion function and compute some feature at at random from

1928
02:05:08,260 --> 02:05:13,000
some of the original features and that many many times and by using these random

1929
02:05:13,000 --> 02:05:19,310
building functions he expanded his or original feature space into huge feature space and then

1930
02:05:19,310 --> 02:05:22,500
he said well not that had this huge feature space i'm just going to be

1931
02:05:22,500 --> 02:05:27,140
using a linear decision function and because he plunged his original they find a huge

1932
02:05:27,140 --> 02:05:33,000
feature space then he could make a perfect separation of the training examples with just

1933
02:05:33,000 --> 02:05:37,500
a simple linear separation it's a linear separation but there's hyper plan in in a

1934
02:05:37,500 --> 02:05:41,550
very very high dimensional space and you have big n now began can be you

1935
02:05:41,550 --> 02:05:46,850
know hundreds of thousands of of these features that are computed with these random building

1936
02:05:48,220 --> 02:05:53,430
now if you did that to address the problem that people would say well linear

1937
02:05:53,480 --> 02:05:58,420
decision functions are to simple they they can't express the complexity of the data there

1938
02:05:58,420 --> 02:06:03,900
are many datasets but are not linearly sufferable that is if you take the training

1939
02:06:03,900 --> 02:06:09,400
data you can't make is zero error separation you can't make an analyst separation that

1940
02:06:09,400 --> 02:06:13,880
will put all the examples of one class on one side all the examples of

1941
02:06:13,880 --> 02:06:18,860
the other class on the other side with a simple plane he said well no

1942
02:06:18,870 --> 02:06:23,800
problem i'm going to use these random building functions plunged original space into much bigger

1943
02:06:23,800 --> 02:06:28,630
i one

1944
02:06:28,690 --> 02:06:36,590
so i'm going to talk about this tool which he developed as

1945
02:06:39,420 --> 02:06:44,570
this recording

1946
02:06:44,600 --> 02:06:48,420
this one

1947
02:06:55,070 --> 02:06:59,000
so i try to stay

1948
02:06:59,000 --> 02:07:00,850
close to this mica that are

1949
02:07:00,860 --> 02:07:08,700
so i talked about this tool literally we've been developing at accenture as some of

1950
02:07:08,700 --> 02:07:13,060
you may know or some of you may not know the accenture consulting company

1951
02:07:13,070 --> 02:07:16,880
and most of our projects if not all of them

1952
02:07:16,920 --> 02:07:18,020
start by

1953
02:07:18,030 --> 02:07:20,280
requirements documentation which is

1954
02:07:20,290 --> 02:07:22,350
pretty important part of the process

1955
02:07:25,250 --> 02:07:26,420
so we've

1956
02:07:26,430 --> 02:07:31,080
what we've done is that we building tool which has got some very light-weight semantic

1957
02:07:31,080 --> 02:07:33,970
analysis and we're are moving deeper into

1958
02:07:33,990 --> 02:07:36,710
using ontologies so

1959
02:07:36,720 --> 02:07:40,050
so much of the things in the show this talk is going to be divided

1960
02:07:40,050 --> 02:07:44,970
into two two parts the first is the lexical and syntactic and syntactic analysis part

1961
02:07:45,020 --> 02:07:48,830
which is what already being developed and is currently being used by many of our

1962
02:07:48,830 --> 02:07:54,830
teams and then the semantic analysis part is what the talk about is what getting

1963
02:07:54,830 --> 02:07:59,250
into the reprobates with these but not many teams have used it yet

1964
02:08:02,190 --> 02:08:04,030
two stocks

1965
02:08:05,300 --> 02:08:15,680
to start the it like i said you know most of our projects started requirements

1966
02:08:15,680 --> 02:08:18,940
probably the for most software companies to

1967
02:08:18,960 --> 02:08:22,570
and it's by the number is really just do not picked up a couple of

1968
02:08:22,570 --> 02:08:26,580
numbers from a couple of studies it's been shown that if you get the requirements

1969
02:08:27,740 --> 02:08:31,220
it it can lead to lot of problems i'm not going to go into these

1970
02:08:31,220 --> 02:08:36,060
numbers but it's worth mentioning that you like we are moving more and more and

1971
02:08:36,060 --> 02:08:41,900
were distributed development model so so that put even more pressure on getting the requirements

1972
02:08:41,920 --> 02:08:43,330
right to begin with

1973
02:08:43,330 --> 02:08:45,520
i don't know if

1974
02:08:45,540 --> 02:08:47,680
people have seen this happen before

1975
02:08:48,810 --> 02:08:51,110
i'm sure some of you have seen it variant of

1976
02:08:53,050 --> 02:08:54,710
getting requirements wrong

1977
02:08:54,730 --> 02:08:56,770
the number of

1978
02:08:56,790 --> 02:08:58,710
factors which cause it

1979
02:08:58,730 --> 02:09:02,020
but the it is that in all stakeholders don't know what they want

1980
02:09:02,060 --> 02:09:07,420
another part is that the analyst may not understand what the stakeholder wanted and like

1981
02:09:07,420 --> 02:09:11,330
that can cause all kinds of problems and finally note

1982
02:09:11,340 --> 02:09:16,890
people may misunderstand what the analyst talk to that could cause even more problems so

1983
02:09:16,890 --> 02:09:21,840
obviously we cannot solve all the problems but that this particular tool what we are

1984
02:09:21,840 --> 02:09:22,710
trying to do

1985
02:09:22,730 --> 02:09:24,670
is that they're trying to

1986
02:09:24,680 --> 02:09:27,110
please make sure that

1987
02:09:27,120 --> 02:09:32,140
the people who write the requirements that i as clear and precise way as possible

1988
02:09:32,370 --> 02:09:37,620
so that some of these problems are elected

1989
02:09:39,740 --> 02:09:44,310
requirements pretty important part of the process

1990
02:09:44,330 --> 02:09:48,610
the many different people with a different skill sets who interact with it

1991
02:09:48,620 --> 02:09:49,610
so far

1992
02:09:49,650 --> 02:09:54,640
this is a really important point because get into it into the middle the next

1993
02:09:54,640 --> 02:09:58,950
light but it's important to understand that the different kind of people who are dealing

1994
02:09:58,950 --> 02:10:02,330
with a low you go to the stakeholders who are

1995
02:10:02,340 --> 02:10:07,770
many business people there are some technical people but ultimately the people were signing of

1996
02:10:07,770 --> 02:10:12,830
the requirements are pretty high up in the business and they're not necessarily technical so

1997
02:10:12,830 --> 02:10:16,980
you want to write requirements in every way they can understand them because ultimately has

1998
02:10:16,990 --> 02:10:22,360
the responsibility to sign and then the requirements team

1999
02:10:24,060 --> 02:10:26,180
first interfaces usually

2000
02:10:26,200 --> 02:10:27,790
business people

2001
02:10:27,800 --> 02:10:29,580
business analysts who are again

2002
02:10:29,610 --> 02:10:33,140
some of them may be technical but most of them are not

2003
02:10:34,240 --> 02:10:36,620
it's very hard to get them to

2004
02:10:36,640 --> 02:10:39,200
do anything which is farmers so they

2005
02:10:39,260 --> 02:10:40,290
if stick to

2006
02:10:40,300 --> 02:10:41,620
writing in english

2007
02:10:41,630 --> 02:10:45,620
on and then you want the technical side of the picture you want the design

2008
02:10:45,620 --> 02:10:51,090
team which are typically architects or designers who requirements document and

2009
02:10:51,100 --> 02:10:55,850
i try to create designs in architecture from them and then finally got the development

2010
02:10:55,850 --> 02:10:59,840
team who use the requirements as well as

2011
02:10:59,870 --> 02:11:03,150
the designs to actually create this office

2012
02:11:09,740 --> 02:11:10,980
so this

2013
02:11:10,990 --> 02:11:16,890
what working on is that in like number of problems for requirements can cause a

2014
02:11:16,890 --> 02:11:20,190
number of problems and others

2015
02:11:20,220 --> 02:11:25,500
a number of people have created a different kind of documentation best practices to help

2016
02:11:25,500 --> 02:11:30,360
avoid these problems and by many other best practices i just listed a few of

2017
02:11:30,360 --> 02:11:32,820
them and can try to organize them into

2018
02:11:33,250 --> 02:11:34,870
different categories

2019
02:11:34,880 --> 02:11:39,920
so lexicalized free the level you know that things like

2020
02:11:40,180 --> 02:11:43,800
they resemble things that don't use big words such as you know i don't see

2021
02:11:43,800 --> 02:11:46,000
a system will be easy to use are

2022
02:11:46,010 --> 02:11:49,120
flexible or so on and so forth

2023
02:11:49,130 --> 02:11:53,620
why this seemingly trivial i mean you'd be surprised home is

2024
02:11:53,630 --> 02:11:55,350
such requirements exists

2025
02:11:55,400 --> 02:12:01,990
which cause problems later on then that things like using consistent terminology making sure that

2026
02:12:02,000 --> 02:12:04,500
you refer the same system using the same name

2027
02:12:04,540 --> 02:12:07,530
again in the requirements document is about a thousand

2028
02:12:07,550 --> 02:12:08,660
pages are

2029
02:12:08,670 --> 02:12:13,930
you want to achieve what are the requirements some of these things start causing problems

2030
02:12:13,940 --> 02:12:19,740
then that things like make sure that each requirement the a single sentence and its

2031
02:12:21,310 --> 02:12:27,360
so and try to write requirements in in standard a syntax some things like you

2032
02:12:27,360 --> 02:12:31,410
know make sure that you clearly see who's doing what in this particular case using

2033
02:12:31,410 --> 02:12:32,400
the system

2034
02:12:32,410 --> 02:12:36,490
is the person who's doing something shall generate report

2035
02:12:36,510 --> 02:12:39,550
so instead of saying things like reporting generated

2036
02:12:39,560 --> 02:12:42,860
it's better to articulate who exactly the personal

2037
02:12:42,880 --> 02:12:43,990
the system is

2038
02:12:44,010 --> 02:12:46,430
doing a particular function

2039
02:12:46,450 --> 02:12:49,810
similarly and then there's

2040
02:12:49,820 --> 02:12:55,050
domain specific or semantic relationships like this comes more you like

2041
02:12:55,070 --> 02:12:59,690
this may be applicable during design or even for the business analysts like you know

2042
02:12:59,700 --> 02:13:01,100
knowing things like

2043
02:13:01,110 --> 02:13:03,800
including the security may include the performance

2044
02:13:03,800 --> 02:13:08,220
the main don't affect the kind of hard choices you make so that bunch of

2045
02:13:08,220 --> 02:13:10,680
dependency the the domain which

2046
02:13:10,690 --> 02:13:14,780
at least our assertion is that if you go into more than using ontologies you'll

2047
02:13:14,790 --> 02:13:19,760
be able to help either the business analyst on the stakeholder or the designers made

2048
02:13:19,790 --> 02:13:25,750
more intelligent choices and then there might be some enterprise knowledge where you may know

2049
02:13:25,750 --> 02:13:29,280
that you know certain platforms don't support a certain kind of technology

2050
02:13:30,180 --> 02:13:32,730
so it's much better to be able to look at this as early on as

2051
02:13:32,730 --> 02:13:36,910
possible so i'll talk a little bit about it on how we are trying to

2052
02:13:38,560 --> 02:13:41,100
get people who create rules and capture them

2053
02:13:41,110 --> 02:13:42,600
so that you know we can check

2054
02:13:42,610 --> 02:13:44,730
beginning to find these problems that

2055
02:13:44,740 --> 02:13:46,200
at the requirements stage

2056
02:13:50,530 --> 02:13:55,610
there's been a lot of work on requirements analysis and software engineering

2057
02:13:58,800 --> 02:13:59,680
they just

2058
02:13:59,690 --> 02:14:02,450
i mean if you look at it you were two options

2059
02:14:02,610 --> 02:14:07,050
one is that you try to get people to write requirements in the former location

2060
02:14:07,060 --> 02:14:10,430
and then you know

2061
02:14:10,450 --> 02:14:14,050
but then one two and the former location it becomes easier to analyse four different

2062
02:14:14,050 --> 02:14:15,410
kinds of properties

2063
02:14:15,420 --> 02:14:17,870
but the problem is that

