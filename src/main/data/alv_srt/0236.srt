1
00:00:00,000 --> 00:00:00,980
talking about

2
00:00:01,630 --> 00:00:05,400
and here are more details about this slide lock market right so now you can

3
00:00:12,860 --> 00:00:16,030
i don't know it's possible

4
00:00:20,050 --> 00:00:21,440
so it so

5
00:00:21,460 --> 00:00:24,800
let's talk more about the viral marketing right now we can ask what are the

6
00:00:24,820 --> 00:00:29,780
product features that make a particular product being invited to come anywhere and so on

7
00:00:29,840 --> 00:00:34,710
so on this dataset of sixteen million recommendations we had for product that we had

8
00:00:34,710 --> 00:00:40,340
books we had a hundred thousand books fifty thousand images four hundred thousand music series

9
00:00:40,340 --> 00:00:43,520
and then twenty six thousand dvds like

10
00:00:44,880 --> 00:00:50,030
and that's half a million products then we have the number of customers the number

11
00:00:50,030 --> 00:00:55,070
of recommendations the number of edges in the social networks and then the number of

12
00:00:55,070 --> 00:00:58,820
people that bought then got discount the number of people that

13
00:00:58,920 --> 00:01:02,590
both then got no discount what you can see for example is the very

14
00:01:02,610 --> 00:01:03,500
met so

15
00:01:03,500 --> 00:01:04,630
i have to do

16
00:01:05,690 --> 00:01:07,400
the numbers of are high

17
00:01:07,690 --> 00:01:08,920
light alone

18
00:01:08,960 --> 00:01:10,650
right and

19
00:01:10,670 --> 00:01:12,820
what i want i want to say is for example here you can see the

20
00:01:13,250 --> 00:01:19,030
average DVD shop made like ten recommendations wired for books there are about two recommendations

21
00:01:19,030 --> 00:01:23,260
per person writes that you can also see that this our social networks are extremely

22
00:01:23,260 --> 00:01:28,730
sparse right there are almost like these right so i have i have

23
00:01:28,730 --> 00:01:32,730
eight hundred thousand nodes and OK million edges here the number of edges is smaller

24
00:01:32,730 --> 00:01:34,260
than the number of nodes

25
00:01:35,130 --> 00:01:39,130
and then seems to me that this contract from books a lot of people got

26
00:01:39,130 --> 00:01:41,710
discounts and few few people

27
00:01:41,800 --> 00:01:46,590
made the purchase and order to discover why the the situation is reversed so you

28
00:01:46,590 --> 00:01:47,630
can ask

29
00:01:47,750 --> 00:01:50,880
what is going on how does the

30
00:01:50,920 --> 00:01:52,650
product every influence

31
00:01:54,190 --> 00:01:57,800
the purchases so here if you look at this

32
00:01:57,820 --> 00:02:02,710
more more categories again i have the for-profit degrees have the number of people that

33
00:02:02,730 --> 00:02:08,000
purchase and got discount and this would be put them forward commanded the problem so

34
00:02:08,250 --> 00:02:13,130
you you seem accommodation you buy a product then you for the comments to friends

35
00:02:13,130 --> 00:02:17,000
and what you can see for example is that people are much more likely to

36
00:02:17,230 --> 00:02:21,630
comment did they just purchased two of their friends that they

37
00:02:21,690 --> 00:02:28,400
two two tool a commendable music CD and so on

38
00:02:30,420 --> 00:02:33,460
more interesting things about like that so

39
00:02:33,610 --> 00:02:37,210
we have we have that

40
00:02:37,840 --> 00:02:40,650
so we had like four million people but then it turns out that less than

41
00:02:40,650 --> 00:02:46,720
fifty thousand people responsible for four two point five million recommendations out of sixty million

42
00:02:46,720 --> 00:02:51,550
recommendations right so as very small fraction of people is it is it possible for

43
00:02:51,550 --> 00:02:55,030
one of the all recommendation and also

44
00:02:55,050 --> 00:02:59,190
as as i'm showing before this seems to be very interesting

45
00:02:59,190 --> 00:03:01,130
i think to command for example

46
00:03:01,130 --> 00:03:08,720
there are there for this japanese animation cartoons time right the the the sixth the

47
00:03:08,720 --> 00:03:14,630
recommendation success was thirty percent right so every recommendation resulted in a purchase

48
00:03:14,670 --> 00:03:16,550
which is extremely high

49
00:03:16,550 --> 00:03:18,280
while for four

50
00:03:18,300 --> 00:03:19,690
the baseline b

51
00:03:19,710 --> 00:03:23,340
two or three percent of the recommendations with the resulting pictures

52
00:03:23,880 --> 00:03:26,690
and so if you look at the

53
00:03:26,730 --> 00:03:31,210
i did you how well connected is the network the giant component is usually very

54
00:03:31,210 --> 00:03:33,960
small so the largest connected component of these networks

55
00:03:33,980 --> 00:03:36,380
we saw earlier in the morning this

56
00:03:36,440 --> 00:03:38,190
the networks

57
00:03:38,210 --> 00:03:39,130
the most of

58
00:03:39,170 --> 00:03:44,610
most of most real networks have this largest connected component that has the most of

59
00:03:44,610 --> 00:03:50,190
the nodes in it in the recommendation network species very small

60
00:03:53,800 --> 00:03:56,250
so this is for david is now we can go

61
00:03:56,250 --> 00:04:00,670
and those simple regression experience so we take every product

62
00:04:00,670 --> 00:04:02,200
so if i give you

63
00:04:02,250 --> 00:04:05,560
value for x on the validation of the boat

64
00:04:05,600 --> 00:04:10,470
when the you can evaluate pointwise there's no problem one can evaluate point what is

65
00:04:10,470 --> 00:04:11,880
because we put the

66
00:04:11,900 --> 00:04:15,710
joint distribution of x on y we don't that the conditional distribution of x and

67
00:04:16,740 --> 00:04:20,040
but you can rewrite also the weight here

68
00:04:20,080 --> 00:04:22,260
as marginal likelihood

69
00:04:22,330 --> 00:04:23,800
time ratio

70
00:04:23,850 --> 00:04:26,390
which is the kind of discrepancy between

71
00:04:26,440 --> 00:04:28,490
the target distribution of inquiry

72
00:04:29,420 --> 00:04:31,070
the LTI

73
00:04:31,190 --> 00:04:34,850
this is something we're going to call the importance weight

74
00:04:34,900 --> 00:04:40,610
what i'm telling you is that once you've got basically

75
00:04:40,620 --> 00:04:42,510
this important for question

76
00:04:42,580 --> 00:04:45,030
you can rely very simply

77
00:04:45,080 --> 00:04:47,860
the marginal likelihood as

78
00:04:47,880 --> 00:04:49,180
the integral

79
00:04:49,190 --> 00:04:52,990
the expectation of importance weights

80
00:04:53,030 --> 00:04:53,930
and two

81
00:04:53,950 --> 00:04:57,370
or what but

82
00:05:02,390 --> 00:05:05,300
so for example

83
00:05:05,720 --> 00:05:08,140
no it's not that

84
00:05:08,160 --> 00:05:13,710
what you so you can do it with the questions

85
00:05:13,820 --> 00:05:23,050
now know it's not going to be actually

86
00:05:23,100 --> 00:05:27,120
so i think that we just like forget about that and that's what i mean

87
00:05:27,150 --> 00:05:31,100
the approximation of equation are just

88
00:05:31,140 --> 00:05:34,060
there there's the colour approximation

89
00:05:34,060 --> 00:05:36,430
OK so this equation

90
00:05:36,460 --> 00:05:38,430
well is straightforward because

91
00:05:38,440 --> 00:05:42,120
whatever simply everything that the marginal likelihood p

92
00:05:42,170 --> 00:05:43,350
of y

93
00:05:43,370 --> 00:05:45,130
is the integral

94
00:05:45,140 --> 00:05:50,850
of the joint distribution of x on y integrated with x

95
00:05:50,890 --> 00:05:53,660
on the using my poetry which is that

96
00:05:53,740 --> 00:05:56,780
a is called a divided by b b

97
00:05:56,780 --> 00:05:59,690
on this expression that nothing

98
00:05:59,890 --> 00:06:01,920
we also have an equation

99
00:06:04,930 --> 00:06:09,250
obviously we manage basically what good is we managed to rewrite p

100
00:06:09,260 --> 00:06:10,610
i simply

101
00:06:11,790 --> 00:06:15,160
the function of this way in the computer and

102
00:06:15,220 --> 00:06:18,820
whenever i know x find importance sampling distribution

103
00:06:18,820 --> 00:06:20,300
so if you want to go

104
00:06:20,310 --> 00:06:21,620
why the poor

105
00:06:22,040 --> 00:06:26,390
that is what what that's ever get it maybe once

106
00:06:26,450 --> 00:06:29,000
so you're either with the distribution of in

107
00:06:30,630 --> 00:06:33,110
of x y divided by p of life

108
00:06:33,500 --> 00:06:36,800
no you're right that

109
00:06:36,850 --> 00:06:40,470
e of x y p of x y divided by two

110
00:06:40,530 --> 00:06:42,390
multiply by q

111
00:06:42,820 --> 00:06:46,610
on this thing is fine because the

112
00:06:47,630 --> 00:06:50,660
these include in a part of you that in that infinite

113
00:06:50,940 --> 00:06:52,810
so this is what find

114
00:06:52,850 --> 00:06:56,740
i do this at the numeric on the denominator on

115
00:06:56,740 --> 00:06:57,920
final expression

116
00:06:57,940 --> 00:07:00,640
the for is wonderful from them until

117
00:07:00,660 --> 00:07:02,390
basically formula

118
00:07:02,470 --> 00:07:04,800
it was used to arrive

119
00:07:07,310 --> 00:07:12,110
as the kind of expectation of w way we normalize

120
00:07:12,800 --> 00:07:14,280
so theoretically

121
00:07:14,300 --> 00:07:16,990
the thing is basically

122
00:07:17,040 --> 00:07:22,150
what we call in the that's the actual point which are the right

123
00:07:22,210 --> 00:07:24,200
can have if you see

124
00:07:24,210 --> 00:07:28,800
what the derivative just basically alright point for nothing

125
00:07:29,780 --> 00:07:32,060
so we this expression

126
00:07:32,070 --> 00:07:35,260
on the way this expression is going be the keep

127
00:07:35,280 --> 00:07:37,130
already the key

128
00:07:37,220 --> 00:07:39,850
so what do we do

129
00:07:39,850 --> 00:07:41,580
well known

130
00:07:41,630 --> 00:07:43,250
everything ever written

131
00:07:43,250 --> 00:07:45,620
there was no more out of the time

132
00:07:45,710 --> 00:07:49,040
i was just rewriting t as a function of

133
00:07:49,080 --> 00:07:52,600
i know what i'm telling you is that remember that picked q

134
00:07:52,650 --> 00:07:54,770
so that it is helpful

135
00:07:55,080 --> 00:08:00,130
so i picked you as an arbitrary point distribution i think he was adopted

136
00:08:00,180 --> 00:08:03,380
but for what think and the realization from you

137
00:08:03,470 --> 00:08:04,750
in particular

138
00:08:04,750 --> 00:08:07,370
this means that i can approximate two

139
00:08:08,690 --> 00:08:14,440
the bombardment the telephone thing i was quite before i would describe the beginning that

140
00:08:14,440 --> 00:08:15,650
is what you're going to do

141
00:08:15,700 --> 00:08:20,110
you're going to fall and that people are one the viable according to

142
00:08:20,120 --> 00:08:25,300
all you're going to build the multicellular alternation

143
00:08:25,310 --> 00:08:27,550
very simple

144
00:08:29,080 --> 00:08:30,720
i know what you're going to do

145
00:08:30,740 --> 00:08:33,820
we're going to do something and try to

146
00:08:33,870 --> 00:08:35,790
well wherever

147
00:08:35,830 --> 00:08:38,260
we have few

148
00:08:38,310 --> 00:08:42,300
in the problem something i don't want to be described his life

149
00:08:42,310 --> 00:08:45,660
i'm going to substitute few it will become approximation

150
00:08:45,680 --> 00:08:46,620
very simple

151
00:08:46,920 --> 00:08:50,060
remember that i written and the marginal likelihood

152
00:08:50,080 --> 00:08:51,570
i think the goal

153
00:08:52,220 --> 00:08:54,600
this in part of weight the two

154
00:08:54,600 --> 00:08:58,360
well what i'm doing to get some of the multicolored may be used i

155
00:08:58,390 --> 00:09:01,210
i substitute to q

156
00:09:01,260 --> 00:09:03,560
these on pure measure

157
00:09:03,610 --> 00:09:09,450
basically everything in big or not of w right back onto commercial street but a

158
00:09:09,450 --> 00:09:12,740
lot and i have been following question four

159
00:09:12,740 --> 00:09:14,080
the marginal likelihood

160
00:09:14,160 --> 00:09:16,000
very simple

161
00:09:16,000 --> 00:09:17,880
o thing

162
00:09:17,900 --> 00:09:20,610
this estimate is not bad actually

163
00:09:20,610 --> 00:09:24,580
at least that they that the property that is owned by the name

164
00:09:24,590 --> 00:09:26,640
of the quantity of interest

165
00:09:26,670 --> 00:09:30,640
basically you can also

166
00:09:31,340 --> 00:09:33,140
the question of the lion

167
00:09:33,240 --> 00:09:36,260
of why you want to fit in

168
00:09:38,290 --> 00:09:40,260
it's not especially for me

169
00:09:40,280 --> 00:09:43,570
well what i want to do what i'm doing with the current equation

170
00:09:43,620 --> 00:09:45,440
i always want

171
00:09:45,450 --> 00:09:48,490
i mean my my system is one that but i want to do it in

172
00:09:48,490 --> 00:09:53,180
a sort of more likely that it massive i can be used for what we're

173
00:09:53,220 --> 00:09:55,040
going so

174
00:09:56,280 --> 00:09:58,390
all could utilize

175
00:09:58,450 --> 00:10:00,750
all could i select two

176
00:10:00,790 --> 00:10:04,090
so basically mean by the violence

177
00:10:04,100 --> 00:10:05,550
of the weight

178
00:10:06,590 --> 00:10:09,770
why do you insist

179
00:10:09,990 --> 00:10:12,710
this article two

180
00:10:12,760 --> 00:10:16,050
the possibility distribution are you get the answers that they

181
00:10:16,640 --> 00:10:22,930
did this the important information that minimize the violence because in these devices with zero

182
00:10:22,990 --> 00:10:24,490
it's basically

183
00:10:24,660 --> 00:10:26,950
doesn't tell us much because

184
00:10:27,160 --> 00:10:28,610
why we use that

185
00:10:28,610 --> 00:10:31,590
well if we use q in the first place

186
00:10:31,610 --> 00:10:34,590
instead of the because we do not

187
00:10:34,590 --> 00:10:36,830
i'm going to work with you

188
00:10:36,850 --> 00:10:38,980
on the new concepts

189
00:10:38,990 --> 00:10:42,000
and that is the concept

190
00:10:42,140 --> 00:10:44,460
what we call

191
00:10:44,600 --> 00:10:47,510
electric field we spend the whole lecture

192
00:10:47,560 --> 00:10:50,640
on electric field

193
00:10:50,690 --> 00:10:52,670
i have a

194
00:10:52,710 --> 00:10:54,170
a charge

195
00:10:54,210 --> 00:10:55,460
i just

196
00:10:57,920 --> 00:11:00,920
capital q and plus

197
00:11:00,930 --> 00:11:03,060
at a particular location

198
00:11:03,140 --> 00:11:05,890
and at another location

199
00:11:05,920 --> 00:11:07,490
i have another charge

200
00:11:07,490 --> 00:11:11,560
the i think that might test charge

201
00:11:11,560 --> 00:11:14,200
and there's a separation between the two

202
00:11:14,210 --> 00:11:16,280
these are

203
00:11:17,140 --> 00:11:20,350
the unit vector from capital q

204
00:11:20,420 --> 00:11:23,100
little q

205
00:11:23,160 --> 00:11:25,040
is this effect

206
00:11:25,060 --> 00:11:27,140
so now i know that the

207
00:11:27,180 --> 00:11:31,420
two charges if they positive let's suppose that q is positive

208
00:11:31,430 --> 00:11:33,620
they would repel each other

209
00:11:34,480 --> 00:11:36,120
q is negative

210
00:11:36,180 --> 00:11:38,450
they were attracted to

211
00:11:38,540 --> 00:11:41,180
that this force

212
00:11:41,180 --> 00:11:43,140
we have

213
00:11:43,150 --> 00:11:45,430
and last time we

214
00:11:45,480 --> 00:11:47,590
introduce girls law

215
00:11:47,620 --> 00:11:49,700
that always equals q

216
00:11:49,830 --> 00:11:53,420
capital q times school of calls

217
00:11:53,460 --> 00:11:55,680
divided by our square

218
00:11:55,700 --> 00:11:58,180
in the direction of our rules

219
00:11:58,340 --> 00:12:00,200
two have the same sign

220
00:12:00,240 --> 00:12:02,810
so this direction have opposite sign

221
00:12:02,820 --> 00:12:05,100
it's city idiotic

222
00:12:05,120 --> 00:12:07,170
and now i need to do

223
00:12:07,180 --> 00:12:09,840
the idea of electric field

224
00:12:09,880 --> 00:12:13,570
which we write simple capital e

225
00:12:16,700 --> 00:12:21,400
at that location b where i have my test charge little q

226
00:12:21,450 --> 00:12:23,320
at that location

227
00:12:23,340 --> 00:12:24,790
simply to force

228
00:12:24,870 --> 00:12:29,980
that fast charge experienced divided by the test charge

229
00:12:29,980 --> 00:12:32,560
so i eliminated tests

230
00:12:32,590 --> 00:12:34,870
so i get something that looks quite similar

231
00:12:34,920 --> 00:12:39,570
but it doesn't have the little q anymore

232
00:12:39,600 --> 00:12:44,520
and it is also effective

233
00:12:44,540 --> 00:12:46,760
and by convention

234
00:12:46,770 --> 00:12:48,880
we choose the

235
00:12:51,730 --> 00:12:56,230
such that if this is a positive test charge

236
00:12:56,260 --> 00:12:58,670
then we say he field

237
00:12:58,700 --> 00:12:59,880
is away

238
00:12:59,890 --> 00:13:01,940
from q q is positive

239
00:13:02,000 --> 00:13:03,760
if q is negative

240
00:13:03,820 --> 00:13:08,850
the force is in your direction and therefore easily direction so we adopt the convention

241
00:13:08,890 --> 00:13:14,660
the field is always in the direction that the forces on the positive charge

242
00:13:14,700 --> 00:13:16,720
what you have gained now is that

243
00:13:16,730 --> 00:13:19,320
you you've taken out the little q

244
00:13:19,330 --> 00:13:20,750
in other words

245
00:13:21,630 --> 00:13:22,950
four here

246
00:13:22,950 --> 00:13:24,470
depends on the q

247
00:13:24,510 --> 00:13:27,140
the electric field is not really electric field

248
00:13:27,190 --> 00:13:28,660
is the representation

249
00:13:28,670 --> 00:13:34,850
four what happens around the chart plastic you could be very complicated charge configuration

250
00:13:34,880 --> 00:13:37,420
an electric field tells you something about

251
00:13:37,470 --> 00:13:39,080
that charge configuration

252
00:13:39,100 --> 00:13:40,820
the uniform electric field

253
00:13:40,830 --> 00:13:44,050
you can see is newton's divided by coolmore

254
00:13:44,170 --> 00:13:45,920
and as i unit

255
00:13:48,440 --> 00:13:50,230
normally we won't even indicate

256
00:13:50,760 --> 00:13:53,070
the unit just leave that

257
00:13:53,070 --> 00:13:54,880
as it is

258
00:13:55,860 --> 00:13:58,270
we have graphical representations

259
00:13:59,790 --> 00:14:01,790
the electric field

260
00:14:01,830 --> 00:14:04,230
electric field is a vector

261
00:14:04,260 --> 00:14:06,040
so you expect

262
00:14:07,770 --> 00:14:10,570
and i have here an example

263
00:14:10,580 --> 00:14:12,000
of a

264
00:14:12,010 --> 00:14:14,230
the charge

265
00:14:14,260 --> 00:14:16,820
plus three

266
00:14:16,850 --> 00:14:21,300
by convention these arrows pointing away from the charge

267
00:14:21,350 --> 00:14:25,820
in the same direction that the positive test charge would experience the force

268
00:14:25,880 --> 00:14:27,520
you notice that

269
00:14:27,570 --> 00:14:29,260
very close to the

270
00:14:30,910 --> 00:14:34,380
the errors are larger than for the way that that sort of

271
00:14:34,420 --> 00:14:39,510
represents is trying to represent the interests are square relationship

272
00:14:39,570 --> 00:14:41,890
which can all be very quantities

273
00:14:41,950 --> 00:14:46,250
the basic idea is of course spherically symmetric if this is the point charge

274
00:14:46,300 --> 00:14:48,770
the basic idea is UCB

275
00:14:52,930 --> 00:14:56,660
the direction of the arrow tells you in which direction the force would be if

276
00:14:56,660 --> 00:14:58,330
it is the positive charge

277
00:14:58,380 --> 00:15:00,870
and the length of the vector gives you an idea

278
00:15:00,930 --> 00:15:05,210
of the magnitude and here i have another charge minus one

279
00:15:05,220 --> 00:15:06,680
it doesn't matter whether it is

280
00:15:06,690 --> 00:15:08,200
mine is one called more

281
00:15:08,210 --> 00:15:09,860
my micro cool

282
00:15:09,870 --> 00:15:11,700
just its relative

283
00:15:11,710 --> 00:15:15,620
representation and you see now that the e field vectors

284
00:15:15,660 --> 00:15:18,710
by reversing direction they pointing towards

285
00:15:18,770 --> 00:15:22,410
the is charged by convention and then you go further out

286
00:15:22,450 --> 00:15:23,840
they are small

287
00:15:23,860 --> 00:15:26,380
you have to go all the way to infinity of course

288
00:15:26,400 --> 00:15:27,430
for the

289
00:15:27,440 --> 00:15:28,910
field to become

290
00:15:30,620 --> 00:15:33,110
one of our square field falls off

291
00:15:33,120 --> 00:15:35,570
you have to be infinitely far away

292
00:15:35,640 --> 00:15:38,860
for you to not experience at least in principle

293
00:15:41,300 --> 00:15:42,630
fact from the

294
00:15:42,660 --> 00:15:44,950
from the charge

295
00:15:44,960 --> 00:15:47,160
what do we do now when we have more than one

296
00:15:48,660 --> 00:15:51,210
well if we have several charges

297
00:15:51,260 --> 00:15:52,840
we have

298
00:15:52,860 --> 00:15:54,760
q one

299
00:15:54,820 --> 00:15:57,280
and here we have

300
00:15:57,300 --> 00:15:58,690
q two

301
00:15:58,800 --> 00:16:00,390
here we have

302
00:16:00,440 --> 00:16:01,530
q three

303
00:16:01,580 --> 00:16:03,710
let's say you have

304
00:16:03,720 --> 00:16:06,000
q of i i charges

305
00:16:06,010 --> 00:16:08,870
and now we want to know what is the electric field

306
00:16:08,880 --> 00:16:11,750
at some point

307
00:16:11,800 --> 00:16:13,160
so it's independent

308
00:16:14,570 --> 00:16:18,070
the test charge that i want to think of it if you want to as

309
00:16:19,210 --> 00:16:21,280
the force per unit charge

310
00:16:21,300 --> 00:16:23,820
you've divided out the charge

311
00:16:23,860 --> 00:16:26,210
so now

312
00:16:26,220 --> 00:16:30,160
i can say what is the field due to q one alone

313
00:16:30,160 --> 00:16:36,040
you can the subdifferential is just the interval minus one one right for a euclidean norm for

314
00:16:36,040 --> 00:16:42,260
example a Euclidean norm would be subdifferent differentiable if X is not zero at

315
00:16:42,260 --> 00:16:47,660
the origin it's not differentiable but there is a sub a set of subgradients defined

316
00:16:47,660 --> 00:16:56,150
like this basically any vector with norm less than one is a subgradient

317
00:16:56,150 --> 00:17:02,780
at the origin and that follows from the Cauchy-Schwarz inequality so the  there's

318
00:17:02,780 --> 00:17:10,640
a calculus for subgradients that's similar to the calculus for just gradients and it turns

319
00:17:10,640 --> 00:17:17,860
out that  finding one  subgradient for an a non-differentiable function is usually

320
00:17:17,860 --> 00:17:23,020
very  quite easy if you can evaluate a non-differentiable function then

321
00:17:23,020 --> 00:17:29,980
you also usually  obtain at least one subgradient the question of computing all the subgradients

322
00:17:30,020 --> 00:17:34,560
or the entire subdifferential can be very difficult but fortunately you don't need at for

323
00:17:34,560 --> 00:17:40,780
algorithms algorithms for non-differentiable  convex optimization only require or ask from the user that

324
00:17:40,780 --> 00:17:45,440
they  provide one subgradient at each iteration you're not interested in the set

325
00:17:45,440 --> 00:17:52,570
of all possible subgradients and also nice property is that convex functions are usually

326
00:17:52,580 --> 00:17:57,660
subdifferentiable at all their points in their domain and the only exception that can

327
00:17:57,660 --> 00:18:03,220
happen  the only points where it might be nom subdifferentiable meaning it

328
00:18:03,220 --> 00:18:07,500
doesn't have a subgradient would be at the boundary of the domain for example if

329
00:18:07,500 --> 00:18:12,360
you take the negative of a  square root that's a convex function of X but

330
00:18:12,360 --> 00:18:17,800
at X is zero there is no subgradient because the only tangent you might make

331
00:18:17,800 --> 00:18:25,780
at that point is vertical and so you cannot find an vector G that satisfies this

332
00:18:25,780 --> 00:18:29,500
but that can only happen at the boundary of the domain of the function so you

333
00:18:29,500 --> 00:18:34,630
don't have to worry about this so what are some of these calculus rules so

334
00:18:34,650 --> 00:18:40,200
so much simple so F if F has a non-negative combination of two convex functions F

335
00:18:40,200 --> 00:18:47,080
one and F two then you can just took find the subgradient of F you find

336
00:18:47,080 --> 00:18:51,040
a subgradient of F one a subgradient of F two and then you combine them

337
00:18:51,040 --> 00:19:00,000
in the same way you would with gradients there's a similar rule for affine transformations point-wise maximum

338
00:19:00,140 --> 00:19:04,920
you've seen this is always convex if the functions F I are convex well it's also very easy

339
00:19:04,920 --> 00:19:10,300
to find a subgradient and the method is  this you to evaluate F of

340
00:19:10,300 --> 00:19:16,180
X at point X you have to determine the maximum of these functions well once you

341
00:19:16,180 --> 00:19:21,260
have the  function that's  suppose F 1 is the maximum then you

342
00:19:21,260 --> 00:19:25,320
can pick any subgradient of F one at that point and that's a subgradient

343
00:19:25,360 --> 00:19:31,270
of F of X and  this is something we'll need later probably

344
00:19:31,270 --> 00:19:36,010
so this should be F star of X that's as conjugate this was our if you

345
00:19:36,010 --> 00:19:42,180
remember from yesterday the definition of a conjugate off a function F Y if this

346
00:19:42,180 --> 00:19:46,200
is F star  the conjugate  of X so there is a similar rule for this

347
00:19:46,200 --> 00:19:52,420
maximization to compute a subgradient of this conjugate you find the value of Y

348
00:19:52,520 --> 00:20:00,260
for which this is maximized and then  you differentiate this function and that's very

349
00:20:00,260 --> 00:20:08,860
easy because the f you differentiated this function respect to X so

350
00:20:08,900 --> 00:20:14,300
the gradient is  basically Y so the subgradient of the conjugates at a vector

351
00:20:14,300 --> 00:20:19,800
X  is the vector Y that maximizes the argument here in the definition of

352
00:20:19,800 --> 00:20:30,900
the conjugate and if there are multiple maximizers any maximizer they're all subgradients

353
00:20:30,900 --> 00:20:36,110
okay so then the subgradient method is just similar to the gradient method

354
00:20:36,110 --> 00:20:42,200
so at each iteration you determine one subgradient of the function at

355
00:20:42,220 --> 00:20:49,040
your point if a step in that direction and you X so it's very simple

356
00:20:49,040 --> 00:20:55,140
and it's an extension of the gradient method to non-differentiable functions there are some common choices

357
00:20:55,140 --> 00:21:02,040
of step sizes that people use so you can use T K constants small enough constants another choice

358
00:21:02,040 --> 00:21:06,560
would be instead of making T constant you make the norm of the step constant

359
00:21:06,620 --> 00:21:12,860
so you choose this to be constant and another one that is usually needed and we'll see

360
00:21:12,860 --> 00:21:21,320
why is a diminishing step size so you let T Ks go to zero as K increases but

361
00:21:21,360 --> 00:21:26,700
you don't decrease them too quickly so you could let them go to zero but

362
00:21:26,700 --> 00:21:31,700
in such a way that the sum goes to infinity we'll see why this is

363
00:21:31,700 --> 00:21:38,960
needed and then these are some typical  convergence results for the subgradient method you need

364
00:21:38,960 --> 00:21:42,840
one  assumption on the functions that that they're lip lipschitz continuous so that's a very weak

365
00:21:42,840 --> 00:21:47,120
assumtion although as a function can be very non-differentiable

366
00:21:47,500 --> 00:21:54,440
and then you see with  for example a fixed step size T the function doesn't

367
00:21:54,440 --> 00:22:02,060
necessarily converge but as the iterates go on you converge to something that's within a

368
00:22:02,060 --> 00:22:06,980
certain distance from the optimum and the distance depends on the step size T so

369
00:22:06,980 --> 00:22:10,290
once we gather all these rules of thumb how do we combine them into a

370
00:22:10,290 --> 00:22:11,790
single prediction rule

371
00:22:11,800 --> 00:22:13,790
that will be very highly accurate

372
00:22:13,810 --> 00:22:18,220
so here we just do something simple we just take a majority vote or weighted

373
00:22:18,220 --> 00:22:20,440
majority vote of the rules of stuff

374
00:22:23,110 --> 00:22:27,610
given the new example we evaluate all the rules of each one makes a prediction

375
00:22:27,610 --> 00:22:29,550
of what the correct labels to be

376
00:22:29,600 --> 00:22:32,640
and then we just take a majority vote of those predictions

377
00:22:35,730 --> 00:22:37,390
OK so

378
00:22:37,400 --> 00:22:38,850
boosting is

379
00:22:38,850 --> 00:22:44,030
a general method of converting these rough rules of thumb into a highly accurate prediction

380
00:22:45,000 --> 00:22:48,020
and also i will be talking about it also has

381
00:22:48,030 --> 00:22:53,100
a more technical definition which is important to keep in mind to boosting starts out

382
00:22:53,100 --> 00:22:57,860
with the assumption really any learning algorithm starts out with an assumption that you're always

383
00:22:57,860 --> 00:23:01,660
assuming something about your data otherwise learning is impossible

384
00:23:01,720 --> 00:23:06,280
so boosting we start out with what's called the weak learning assumption we assume that

385
00:23:06,280 --> 00:23:09,490
we've been given a weak learning algorithm

386
00:23:09,540 --> 00:23:14,280
which can consistently find these weak classifiers or the what i've been calling these rules

387
00:23:14,280 --> 00:23:15,290
of thumb

388
00:23:15,300 --> 00:23:19,060
which are at least a little bit better than random guessing

389
00:23:19,090 --> 00:23:22,280
now if you just talking about a two class problem most of the talk lockerbie

390
00:23:22,280 --> 00:23:24,440
be talking about two class problems

391
00:23:24,470 --> 00:23:26,120
and you guessed randomly

392
00:23:26,150 --> 00:23:28,800
you'll be right exactly half the time

393
00:23:28,810 --> 00:23:32,980
so the weak learning assumption says that the accuracy of these classifiers to be a

394
00:23:32,980 --> 00:23:36,650
little bit better than fifty percent of maybe fifty five percent

395
00:23:36,690 --> 00:23:42,640
something that's a little bit better than random guessing and given that assumption and given

396
00:23:42,650 --> 00:23:48,840
enough data whatever that means a boosting algorithm is an algorithm that can provably

397
00:23:48,850 --> 00:23:51,370
construct a single classifiers

398
00:23:51,390 --> 00:23:55,790
with very high accuracy with accuracy say ninety nine percent

399
00:23:56,660 --> 00:24:00,770
that's why it's called boosting because your boosting the accuracy from fifty five percent to

400
00:24:00,770 --> 00:24:04,960
ninety nine percent

401
00:24:04,980 --> 00:24:07,870
OK so here's what i'm going to cover the tutorial that was by way of

402
00:24:09,160 --> 00:24:13,220
so i'm first going to give a very brief background and boosting that would be

403
00:24:13,240 --> 00:24:14,610
rather short

404
00:24:14,620 --> 00:24:19,140
and then being part of the tutorial will be in three parts the first part

405
00:24:19,140 --> 00:24:21,480
and going to talk about the basic

406
00:24:21,540 --> 00:24:28,640
adaboost algorithm adaboost was the first practical boosting algorithm and its core theory focusing on

407
00:24:28,660 --> 00:24:31,540
analyzing the era of the algorithm

408
00:24:33,060 --> 00:24:34,750
the second main part

409
00:24:34,770 --> 00:24:38,480
it is on other ways of understanding boosting so there are many many ways of

410
00:24:38,480 --> 00:24:42,110
thinking about boosting that have been developed over the years and going to be talking

411
00:24:42,110 --> 00:24:44,220
about three of those

412
00:24:44,230 --> 00:24:50,250
and the last part will be focused on experiments applications and extensions

413
00:24:50,420 --> 00:24:55,360
and i know it's a big group but needless to say you know please do

414
00:24:55,360 --> 00:24:58,660
ask questions especially clarifying questions

415
00:24:58,660 --> 00:25:03,490
OK so brief background

416
00:25:03,500 --> 00:25:08,530
OK so boosting has its roots in the theoretical machine learning model called the

417
00:25:08,540 --> 00:25:10,670
learning model pack stands for

418
00:25:10,690 --> 00:25:15,440
probably approximately correct it was introduced by valiant

419
00:25:15,550 --> 00:25:16,890
back in

420
00:25:16,900 --> 00:25:18,840
the eighties early eighties

421
00:25:18,890 --> 00:25:21,150
and in this pack model

422
00:25:21,160 --> 00:25:24,690
the learning algorithm gets random examples from some

423
00:25:24,720 --> 00:25:27,610
i non an arbitrary distribution

424
00:25:27,660 --> 00:25:29,420
and the stronger

425
00:25:29,420 --> 00:25:31,930
pac learning our model

426
00:25:31,960 --> 00:25:38,060
a strong PAC learning algorithm is one so that for any distribution over the examples

427
00:25:38,080 --> 00:25:41,080
any distribution generating the data that is

428
00:25:41,090 --> 00:25:43,560
with high probability given

429
00:25:43,580 --> 00:25:48,980
enough examples were enough means polynomially many examples and polynomial time

430
00:25:48,990 --> 00:25:53,710
the algorithm can find classifier with arbitrarily small generalisation error

431
00:25:53,730 --> 00:25:59,500
generalisation error means the air on the true distribution which is generating the data

432
00:25:59,520 --> 00:26:01,520
OK so we can drive the air

433
00:26:01,530 --> 00:26:03,680
as small as we want

434
00:26:03,690 --> 00:26:09,370
and the weak PAC learning model for weak PAC learning algorithm rather is one which

435
00:26:09,380 --> 00:26:13,950
satisfies all the same requirements but the generalisation error only has to be

436
00:26:13,960 --> 00:26:16,250
a little bit better than random guessing

437
00:26:16,290 --> 00:26:19,770
OK so in this model you have to be able to drive the air below

438
00:26:19,790 --> 00:26:23,810
one percent and here you only have to drive the air below

439
00:26:23,820 --> 00:26:26,860
forty five percent

440
00:26:26,870 --> 00:26:31,000
and turns and valiant were the ones who first looked at

441
00:26:31,020 --> 00:26:32,500
weak learning

442
00:26:32,510 --> 00:26:37,460
the weak learning model and they asked us weak learnability imply strong learnability if you

443
00:26:37,460 --> 00:26:40,590
can learn in this model and this week model

444
00:26:40,610 --> 00:26:43,270
is there some way of boosting your accuracy

445
00:26:43,290 --> 00:26:45,220
so that you can also learn in this

446
00:26:45,250 --> 00:26:49,900
seemingly much stronger learning model

447
00:26:50,910 --> 00:26:55,060
the first provable boosting algorithm was one that i came up with

448
00:26:55,120 --> 00:27:01,300
it's for my phd thesis and you are throwing came up with a better algorithm

449
00:27:01,520 --> 00:27:05,830
one which is optimal in a certain sense it's called boost by majority algorithm

450
00:27:05,860 --> 00:27:09,930
and there were some early experiments with these algorithms

451
00:27:09,950 --> 00:27:13,240
but they had certain

452
00:27:13,280 --> 00:27:17,880
there were certain practical reasons why these algorithms really did not work so well in

453
00:27:17,880 --> 00:27:23,040
practice were difficult to use in practice i'll talk about that a little bit later

454
00:27:23,040 --> 00:27:24,710
so the first practical

455
00:27:24,720 --> 00:27:27,700
boosting algorithm is adaboost which i'll be focusing on

456
00:27:27,720 --> 00:27:32,190
and again i will talk about the practical advantages later

457
00:27:32,250 --> 00:27:36,310
and since then there's been a lot of work experimental work in theoretical work on

458
00:27:36,310 --> 00:27:39,070
boosting obviously not going to cover

459
00:27:39,120 --> 00:27:41,510
all of this but

460
00:27:41,530 --> 00:27:44,040
and this list is far from exhaustive

461
00:27:44,080 --> 00:27:45,740
but i'll try to

462
00:27:45,760 --> 00:27:48,800
cover cover good part of it

463
00:27:49,090 --> 00:27:52,710
OK so that was a little bit of historical background

464
00:27:52,720 --> 00:27:54,570
so what i want to do now

465
00:27:54,610 --> 00:27:56,630
is introduced adaboost

466
00:27:56,650 --> 00:27:59,420
and talk about and analyse its training air

467
00:27:59,430 --> 00:28:01,720
it's error rate on the training set

468
00:28:01,780 --> 00:28:05,610
and then talk about how to analyse its test air to air on a separate

469
00:28:05,610 --> 00:28:07,810
test set or its generalization there

470
00:28:07,830 --> 00:28:13,030
using something called the margins theory

471
00:28:13,040 --> 00:28:16,890
OK so i already introduced boosting at a high level what i want to do

472
00:28:17,670 --> 00:28:18,680
is go back

473
00:28:18,690 --> 00:28:24,030
and talk about it in more formal terms and introduce some mathematical notation

474
00:28:24,040 --> 00:28:27,040
so as with any learning algorithm

475
00:28:27,050 --> 00:28:30,180
of this kind we start out with the training set

476
00:28:30,200 --> 00:28:32,990
so there are m training examples

477
00:28:33,000 --> 00:28:37,860
each training example is a pair x i y i so

478
00:28:38,010 --> 00:28:41,700
site is the object that we're trying to

479
00:28:41,710 --> 00:28:44,120
classify so for instance it's

480
00:28:44,130 --> 00:28:47,770
the utterance that was said by the person in the example i gave at the

481
00:28:48,660 --> 00:28:54,400
and capital taxes the instance space is the space of all possible instances

482
00:28:54,410 --> 00:29:00,340
and why is the label so for instance it's the correct classification of one of

483
00:29:00,340 --> 00:29:01,950
those utterances

484
00:29:01,950 --> 00:29:03,560
the tree

485
00:29:03,570 --> 00:29:07,890
because again i emphasise this simple example so this might look like an easy job

486
00:29:07,890 --> 00:29:09,600
for this particular example

487
00:29:09,620 --> 00:29:16,150
the more complex sentences for longer sentences recovering these grammatical relations is far from trivial

488
00:29:16,170 --> 00:29:20,360
there's another example his object example so here we have another

489
00:29:20,380 --> 00:29:24,750
syntactic configuration which is shown in red here dominating for pope

490
00:29:25,920 --> 00:29:27,970
and then we can disregard

491
00:29:28,010 --> 00:29:30,270
the object relation in this case

492
00:29:30,980 --> 00:29:34,950
and is that there are other relations in these trees

493
00:29:34,990 --> 00:29:40,960
so that's perhaps one of the primary motivations that this idea of grammatical relations

494
00:29:41,060 --> 00:29:44,250
just to finish the section on background let me first talk about

495
00:29:44,270 --> 00:29:45,310
none of the

496
00:29:45,350 --> 00:29:48,890
the first models prior to machine learning for syntax and perhaps one of the simplest

497
00:29:48,890 --> 00:29:52,300
models many or if you will be familiar with these this is the idea of

498
00:29:52,300 --> 00:29:54,670
probabilistic context free grammars

499
00:29:54,720 --> 00:29:59,470
OK so you have cheese we generalize CRF cheese by adding a probability to each

500
00:29:59,470 --> 00:30:01,250
rule production

501
00:30:01,260 --> 00:30:07,100
and here's what probability one to this is the conditional probability given the expanding the

502
00:30:07,100 --> 00:30:08,310
symbol s

503
00:30:08,330 --> 00:30:12,820
i have seen these particular nonterminals for that and p four p

504
00:30:12,830 --> 00:30:14,370
OK so this essentially

505
00:30:14,470 --> 00:30:18,320
looks like a probability associated with each rule in the grammar

506
00:30:18,440 --> 00:30:21,940
the probability is we can take an entire parse tree

507
00:30:21,960 --> 00:30:25,300
and define the probability that tree as a product

508
00:30:25,310 --> 00:30:28,450
all of these probabilities

509
00:30:28,460 --> 00:30:30,440
so that's a very simple direct model

510
00:30:30,570 --> 00:30:34,300
we start to probabilities of these structures and we can use this to actually learn

511
00:30:34,300 --> 00:30:36,970
mappings from strings to be structures

512
00:30:37,020 --> 00:30:42,370
there's a problem in that pcfgs in this simple form as i've shown you here

513
00:30:42,390 --> 00:30:45,140
we have very small set of non terminals

514
00:30:45,310 --> 00:30:48,670
actually extremely poor models of language

515
00:30:48,730 --> 00:30:51,120
in a couple of sentences firstly

516
00:30:51,220 --> 00:30:52,930
so very badly

517
00:30:52,940 --> 00:30:59,380
and our ranking alternate histories particular input in terms probability and secondly that very poor

518
00:30:59,380 --> 00:31:02,760
in terms of assigning probability distributions to strings

519
00:31:02,820 --> 00:31:06,140
and the reason for that is that these nonterminals

520
00:31:06,150 --> 00:31:10,080
you could very little information about the surrounding context in the tree

521
00:31:10,110 --> 00:31:12,920
let me spell it quite poor models so the

522
00:31:12,930 --> 00:31:15,600
talk will come back to pcfgs

523
00:31:15,610 --> 00:31:17,010
and we'll see some tricks

524
00:31:17,030 --> 00:31:19,720
well by you can consider improve performance

525
00:31:19,730 --> 00:31:21,220
essentially by

526
00:31:21,230 --> 00:31:24,600
exploding the number of non terminals in the grammar that's one approach has been used

527
00:31:24,600 --> 00:31:27,590
quite fruitful

528
00:31:27,600 --> 00:31:29,450
OK so now let's

529
00:31:29,470 --> 00:31:30,520
talk about

530
00:31:31,420 --> 00:31:33,650
i've been using

531
00:31:33,700 --> 00:31:37,030
so i guess the key points here as follows

532
00:31:37,060 --> 00:31:40,930
first is the the general point of how we can use probabilistic weighted grammars in

533
00:31:40,930 --> 00:31:44,990
machine learning and specifically of describing how

534
00:31:45,000 --> 00:31:46,940
so models can be

535
00:31:46,970 --> 00:31:50,370
generalized to learning with these weighted grammars

536
00:31:50,390 --> 00:31:54,020
the second main point in this section is to

537
00:31:54,030 --> 00:31:56,330
the introduced tree adjoining grammars

538
00:31:56,340 --> 00:31:57,850
as an alternative

539
00:31:59,000 --> 00:32:02,390
the formalism to to if

540
00:32:02,400 --> 00:32:04,500
so true adjoining grammars are fairly well known

541
00:32:04,510 --> 00:32:10,180
in natural language processing community but probably much less familiar to the community here

542
00:32:10,190 --> 00:32:13,450
and for some advantages

543
00:32:13,490 --> 00:32:18,100
context free grammars in terms of how they parameterise the parsing problem so try to

544
00:32:18,470 --> 00:32:22,550
describe advanced use these grammars grammars bring

545
00:32:23,230 --> 00:32:23,940
so to sum

546
00:32:23,970 --> 00:32:26,530
so have so let me just briefly

547
00:32:26,760 --> 00:32:29,330
describe conditional random fields i guess

548
00:32:29,440 --> 00:32:32,130
pretty much all of you would have seen these but leads to

549
00:32:32,190 --> 00:32:35,610
defined notation on let's talk about conditional random fields

550
00:32:35,630 --> 00:32:38,800
OK so it's impossible to comply conditional random field two

551
00:32:39,190 --> 00:32:43,680
this sequence labeling so this problem of input x is a sequence of tokens for

552
00:32:43,680 --> 00:32:45,800
example sequence of words in a sentence

553
00:32:45,850 --> 00:32:51,020
and the output y is the sequence of underlying state OK so for example the

554
00:32:51,020 --> 00:32:54,100
output i might be a sequence of part of speech tags one part of speech

555
00:32:54,100 --> 00:32:56,650
for each word in the sentence

556
00:32:56,670 --> 00:32:58,600
so here's is a particular example

557
00:32:58,630 --> 00:33:00,630
we might have this input sentence

558
00:33:00,630 --> 00:33:03,810
and this might be one candidate output sequence we have now

559
00:33:03,880 --> 00:33:07,190
very determined positions and so on

560
00:33:08,710 --> 00:33:13,730
a crucial idea in conditional random fields really the building block for conditional random fields

561
00:33:13,900 --> 00:33:19,940
is that the feature vectors which are used to represent these sequence pairs

562
00:33:19,940 --> 00:33:24,250
so we have a feature vector mapping the following form so

563
00:33:24,340 --> 00:33:30,010
it's a feature vector and it takes as input an entire sentence x

564
00:33:30,110 --> 00:33:34,060
position in that sentence for example made we're looking for position

565
00:33:34,060 --> 00:33:39,710
so a prior is conjugate if basically it gets updated in a neat way so that the

566
00:33:39,710 --> 00:33:42,440
form of the posterior

567
00:33:42,480 --> 00:33:48,100
the normal distribution or exponential or whatever the form of the distribution doesn't change just the parameters

568
00:33:48,470 --> 00:33:51,080
of that distribution get updated

569
00:33:51,380 --> 00:33:55,710
so for one very simple example let's take a take the case of the poisson distribution

570
00:33:55,710 --> 00:33:59,420
for the data because it's a poisson distribution

571
00:33:59,480 --> 00:34:03,350
now we could take various possible priors for the parameter theta

572
00:34:03,380 --> 00:34:06,870
it's a non-negative real number so let's

573
00:34:06,900 --> 00:34:11,940
let's try various things but if we chose the the gamma prior gamma distrtibution which I've drawn

574
00:34:11,940 --> 00:34:17,000
there then and then go through the bayes

575
00:34:17,190 --> 00:34:22,040
go through bayes rules we find the posterior distribution is again of exactly the same form

576
00:34:22,270 --> 00:34:27,380
and what's happened is that the priors got updated the posterior simply by tweaking the parameters

577
00:34:27,380 --> 00:34:32,580
we started of with gamma alpha beta and now we have gamma alpha plus Y beta

578
00:34:32,580 --> 00:34:33,710
plus Y

579
00:34:33,710 --> 00:34:36,520
now think about now

580
00:34:36,540 --> 00:34:38,130
what would happen if you took the second observation

581
00:34:39,580 --> 00:34:43,960
OK I've told you about sequential acquisition of data well the prior the posterior after first observation

582
00:34:43,960 --> 00:34:47,870
is the prior for the next so you can see what happens is that we just get the the new

583
00:34:47,870 --> 00:34:53,000
observation gets added on and the beta parameter get gets added

584
00:34:53,750 --> 00:34:55,440
on again

585
00:34:56,030 --> 00:35:00,630
and that's a very nice idea because it tells us something about how to interpret prior

586
00:35:00,630 --> 00:35:05,370
distiributions in this case because you can see that effectively what would have

587
00:35:06,560 --> 00:35:10,380
is Alpha plus the sum of all the data and beta plus the number of

588
00:35:12,500 --> 00:35:15,920
so that's just like starting from zero in both

589
00:35:15,940 --> 00:35:18,670
zero total observations

590
00:35:18,690 --> 00:35:22,270
you know zero number of observations and then

591
00:35:22,310 --> 00:35:27,380
what we start is to imagine that there are a total are beta observations

592
00:35:27,400 --> 00:35:32,290
and their total is alpha so priors can then be thought of as as like

593
00:35:32,290 --> 00:35:35,810
prior data it's like date you had before you started observing anything

594
00:35:36,040 --> 00:35:38,670
and that's quite a useful trick if

595
00:35:38,870 --> 00:35:43,920
you try to elicit a prior from someone who is reluctant to give one that's

596
00:35:43,920 --> 00:35:48,880
quite a good way of pullout from him or her what the

597
00:35:50,150 --> 00:35:53,870
the kind of uncertainty they think about theta really is

598
00:35:53,920 --> 00:35:57,810
now this used to be a really important principle because you couldn't you couldn't do

599
00:35:57,810 --> 00:36:03,670
bayesian statistics except in this sort of simple situation it's it's become much less important

600
00:36:04,650 --> 00:36:09,460
there's still some temptation sometimes to use

601
00:36:09,520 --> 00:36:15,350
conditionally conjugate prior so in in a big big model

602
00:36:15,350 --> 00:36:20,870
you know a least conditional on some other parameters maybe the prior you're gonna use is

603
00:36:21,370 --> 00:36:26,770
is conjugate and that sort of thing can make computation a little bit neater

604
00:36:27,850 --> 00:36:30,980
but I would say you know really this is history and it's a mistake to

605
00:36:30,980 --> 00:36:35,540
let that kind of consideration persuade you to do you use a prior

606
00:36:35,540 --> 00:36:38,420
that doesn't reflect what you really think

607
00:36:38,440 --> 00:36:45,170
now we can't say much more about priors without distinguishing two different subspecies of

608
00:36:45,170 --> 00:36:48,150
bayesians and these the subjective ones and the objective ones

609
00:36:53,520 --> 00:36:56,210
the subjective ones

610
00:36:56,210 --> 00:37:01,270
are kind of ideally idealist living on a hill and the objective ones are the

611
00:37:01,270 --> 00:37:06,290
ones who have to get on with reality in some sense subjective ones take

612
00:37:06,290 --> 00:37:07,830
the view that

613
00:37:07,870 --> 00:37:12,350
providing we work hard to think about all our probabilities and write things down consistently

614
00:37:12,540 --> 00:37:18,150
OK and we've really done our best then and

615
00:37:18,190 --> 00:37:22,980
use bayes theorem properly then all the probabilities we come up with properly represent our

616
00:37:22,980 --> 00:37:24,940
personal degrees of belief in

617
00:37:24,960 --> 00:37:27,310
the uncertainties we still have

618
00:37:27,310 --> 00:37:32,150
OK very very clean and you know nobody else can really say anything about that if you really believe

619
00:37:33,600 --> 00:37:35,750
the sun won't rise tomorrow or that

620
00:37:36,560 --> 00:37:40,560
you know the euro will go up tomorrow or whatever it is

621
00:37:40,600 --> 00:37:45,560
you know that's your beliefs are your beliefs and that's fine

622
00:37:47,670 --> 00:37:54,420
the the objective view is that achieving that in complex problems is just too hard

623
00:37:54,440 --> 00:37:55,900
Ok you can be

624
00:37:55,940 --> 00:37:59,850
you could be fully coherent you properly understand your own uncertainties when there's a small number of

625
00:37:59,850 --> 00:38:02,630
variables or perhaps you can't when there's a lot

626
00:38:02,730 --> 00:38:10,170
so the objective basyesian takes the view you you'll be inevitably forced into simplifying things you gonna have to you know approximate

627
00:38:10,170 --> 00:38:15,230
your beliefs by something you something you can work with in particular you often

628
00:38:15,270 --> 00:38:20,080
have to assume lots of things are independent when you really don't know they are

629
00:38:20,790 --> 00:38:24,690
and their view kind of leads into saying that

630
00:38:24,690 --> 00:38:31,020
conditional probabilities don't represent ordinary judgments in the in the natural sense but simply quantifying the extent to which

631
00:38:32,330 --> 00:38:38,190
event logically determines another it's a sort of necessity about it rather than

632
00:38:38,230 --> 00:38:41,190
rather than the best guess

633
00:38:42,630 --> 00:38:47,310
so our emphasis now switches into choosing priors to have minimal impact on posterior inference rather then

634
00:38:47,310 --> 00:38:52,870
properly quantifying probabilities so it's a little coming from another different place

635
00:38:53,550 --> 00:38:57,830
I think for interest of time I might skip over this but there's been some effort to try

636
00:38:57,830 --> 00:39:03,790
and objectify the choice of priors in Objective bayesian inference

637
00:39:03,790 --> 00:39:05,480
to make analogy to

638
00:39:05,540 --> 00:39:08,620
compassion is on the detector

639
00:39:08,630 --> 00:39:09,750
of the text

640
00:39:09,760 --> 00:39:14,750
today i just want to spend some time the

641
00:39:14,770 --> 00:39:16,310
and that to be determined

642
00:39:19,310 --> 00:39:22,430
what do we to have a

643
00:39:23,090 --> 00:39:28,490
that are coming from that given time in in binary form can be used to

644
00:39:28,680 --> 00:39:31,380
analyse computer

645
00:39:31,400 --> 00:39:37,020
and in fact we have three type of digitization in you know

646
00:39:37,920 --> 00:39:42,130
the first one is a very simple one like you one bit ADC which is

647
00:39:42,130 --> 00:39:46,410
just to say whether we have got to heat which is the signal

648
00:39:46,560 --> 00:39:51,950
being higher than a given threshold so that very often what you will find the

649
00:39:51,960 --> 00:39:58,270
city contractor for instance we you just have the preamplifier shaper followed by discriminator and

650
00:39:58,270 --> 00:40:01,390
then you just one bit per channel

651
00:40:04,140 --> 00:40:10,670
thing is that sometimes you want to add up an captured measurements you wanted to

652
00:40:10,670 --> 00:40:11,950
know what is the

653
00:40:11,960 --> 00:40:18,200
for the amplitude of a given signal after the shaper

654
00:40:18,240 --> 00:40:21,540
all you want to make

655
00:40:21,570 --> 00:40:28,310
to to get seven point seven hundred and points on your signals which is just

656
00:40:28,310 --> 00:40:29,170
you are

657
00:40:29,460 --> 00:40:35,710
making like tossing dies you are just looking at the value of a signal at

658
00:40:35,710 --> 00:40:40,150
a given at a given frequency and this is done by a and there are

659
00:40:40,150 --> 00:40:45,790
several types and we them with the number of people and which can go from

660
00:40:46,120 --> 00:40:50,960
a few for two twenty four for the best

661
00:40:50,980 --> 00:40:53,400
best of them in terms of resolution

662
00:40:53,480 --> 00:40:59,160
things that we do is time measurements of the instances you have you can measure

663
00:40:59,160 --> 00:41:02,960
the time of flight of particularly in between two

664
00:41:02,960 --> 00:41:04,730
to detect although you can

665
00:41:04,730 --> 00:41:06,630
you can measure

666
00:41:06,650 --> 00:41:09,820
the duration of a signal

667
00:41:09,850 --> 00:41:11,380
of the

668
00:41:11,390 --> 00:41:16,200
now i might be able to time of a signal with respect to a class

669
00:41:17,280 --> 00:41:21,130
what we do a lot for instance with that see in it see we have

670
00:41:21,160 --> 00:41:26,250
collisions every twenty five nanoseconds so we have a clock at forty million and very

671
00:41:26,250 --> 00:41:30,230
often we make some time measurements with respect to the

672
00:41:30,260 --> 00:41:32,040
to the face of the clock

673
00:41:32,070 --> 00:41:35,980
and that the time to digital converter in fact

674
00:41:36,000 --> 00:41:38,040
start with that

675
00:41:38,050 --> 00:41:40,780
so the plan to convert i

676
00:41:41,030 --> 00:41:43,230
short introduction to then

677
00:41:43,250 --> 00:41:44,550
the first thing that we have

678
00:41:44,570 --> 00:41:49,420
do when we when we want to make precise timing measurements to have a good

679
00:41:49,510 --> 00:41:55,630
discriminator and there's this device which is making the conversion between that signal to a

680
00:41:55,630 --> 00:41:57,140
digital one

681
00:41:57,160 --> 00:41:59,290
and then i will go to different

682
00:41:59,320 --> 00:42:03,420
time type of TDC

683
00:42:06,380 --> 00:42:08,880
so again what we have to

684
00:42:08,910 --> 00:42:13,200
simulate time of flight time defenses and and

685
00:42:13,230 --> 00:42:17,080
and and before feeding it to the sea we have to have

686
00:42:18,630 --> 00:42:24,010
physical signal the analog signal transforming in a logic signal so in phase with it

687
00:42:24,290 --> 00:42:27,920
and this is this is all of the discriminant

688
00:42:27,940 --> 00:42:32,110
so the discriminant of an input images and you have an output which is did

689
00:42:32,110 --> 00:42:35,360
you and you want to get the output

690
00:42:35,380 --> 00:42:40,540
we've after the the input with a fixed delay

691
00:42:40,570 --> 00:42:43,950
and avoid any energy to on that

692
00:42:44,010 --> 00:42:47,830
as soon as you exceed especially in in fact

693
00:42:47,850 --> 00:42:52,940
discriminator is like again combined to us again on the fire

694
00:42:52,950 --> 00:42:57,320
so you've got

695
00:42:57,550 --> 00:43:07,910
so you've got this symbolic of an amplifier you put films footage and as soon

696
00:43:07,910 --> 00:43:11,410
as the signal will be i of this i found that you will have the

697
00:43:14,700 --> 00:43:19,170
which we see also you have good input here and after a while one of

698
00:43:19,170 --> 00:43:23,540
the output which is so this one is kick could be any type of amplitude

699
00:43:23,540 --> 00:43:26,930
as soon as it is i have and especially when you get get invited to

700
00:43:26,930 --> 00:43:29,060
attend bit

701
00:43:29,090 --> 00:43:33,160
so example

702
00:43:33,190 --> 00:43:38,100
of commercial device from format to to format devised just for you to get an

703
00:43:38,100 --> 00:43:43,040
idea of the kind performance you can get so you see in the first line

704
00:43:43,040 --> 00:43:48,030
opposition leader this one is quite fast and its two point five minutes

705
00:43:48,040 --> 00:43:53,840
in between the input and the output get something so it's the the

706
00:43:53,850 --> 00:43:57,980
the output is in this case it here which is one of the

707
00:43:57,990 --> 00:44:01,160
normalize the logic level

708
00:44:01,170 --> 00:44:05,100
and this and you have got very

709
00:44:05,990 --> 00:44:09,490
the latest fashions o which is also so good when you want to have a

710
00:44:10,580 --> 00:44:14,860
so when you want to have a good time resolution you have to be very

711
00:44:14,860 --> 00:44:21,650
careful use some form systems so very often will with almost city casino's finds opinion

712
00:44:21,660 --> 00:44:25,340
people i would prefer to play output

713
00:44:25,350 --> 00:44:29,300
we got the shape which you will get all with the same lifestyle and the

714
00:44:29,420 --> 00:44:33,810
amplitude is changing so now let's imagine that you've got

715
00:44:33,830 --> 00:44:37,960
and threshold somewhere on the single you see that

716
00:44:37,970 --> 00:44:43,270
because of this course compiling i've in time all this signal of starting at the

717
00:44:43,270 --> 00:44:48,660
same point what i will get the output of the comparator is something which is

718
00:44:49,210 --> 00:44:51,730
not the same phase

719
00:44:51,730 --> 00:44:53,500
and so what we call that

720
00:44:53,870 --> 00:44:56,220
time walk which is that just

721
00:44:56,230 --> 00:44:59,170
because of the amplitude of the signal

722
00:44:59,190 --> 00:45:01,870
you have phase of the output is changing

723
00:45:01,900 --> 00:45:09,560
and you can get quite a large number of instance about issue of it time

724
00:45:10,670 --> 00:45:15,220
you go from ten to be able to one you put fifty million one special

725
00:45:15,220 --> 00:45:18,830
you you will get the time walk which is close to also which is a

726
00:45:21,250 --> 00:45:22,550
so what do we do

727
00:45:23,840 --> 00:45:27,290
to to tied to get to get rid of that

728
00:45:27,300 --> 00:45:34,390
it a i will come to do that is to is to apply some tricks

729
00:45:35,240 --> 00:45:38,110
which is which is probably the

730
00:45:38,290 --> 00:45:40,540
this is the measurements

731
00:45:40,550 --> 00:45:45,350
this is the phase of the output signal which is called the mean time which

732
00:45:45,350 --> 00:45:46,980
is the fact that

733
00:45:47,000 --> 00:45:52,790
you want to have you will have a zero time single which is something which

734
00:45:52,790 --> 00:45:56,840
is extremely fast despite that if you change the amplitude of the signal u t

735
00:45:58,080 --> 00:46:04,480
some effects and and and the propagation delay which so you see your points one

736
00:46:04,490 --> 00:46:07,350
on this example one

737
00:46:07,600 --> 00:46:12,750
if x is extracted from the pressure of one one part uses that as soon

738
00:46:12,750 --> 00:46:14,400
as you alive

739
00:46:14,410 --> 00:46:19,420
the threshold by more than five million six OK you have always the same patient

740
00:46:19,730 --> 00:46:22,390
but you is lower than that

741
00:46:22,430 --> 00:46:29,090
the discriminator which can fire but then the population has is changed this this one

742
00:46:29,090 --> 00:46:29,960
is good

743
00:46:29,970 --> 00:46:32,420
what part of this one is b

744
00:46:33,020 --> 00:46:34,370
there's good and

745
00:46:34,410 --> 00:46:36,240
you see that you have

746
00:46:36,270 --> 00:46:38,170
even worse characteristics so

747
00:46:39,990 --> 00:46:46,520
difficult to do so we have to do to get to get this phase

748
00:46:47,530 --> 00:46:49,790
but it has to do is to use the

749
00:46:49,790 --> 00:46:51,820
put the bar next year

750
00:46:51,830 --> 00:46:52,900
the bar

751
00:46:54,140 --> 00:46:56,790
basically what you see

752
00:46:56,840 --> 00:46:59,850
is that these independent

753
00:47:03,850 --> 00:47:05,300
given that

754
00:47:05,300 --> 00:47:06,610
x has given

755
00:47:06,630 --> 00:47:09,920
given that this lead to be more technical

756
00:47:13,280 --> 00:47:21,330
i mean of

757
00:47:22,550 --> 00:47:28,530
this course is really for it

758
00:47:28,550 --> 00:47:33,790
so basically we have reliable x one

759
00:47:35,800 --> 00:47:37,180
x p

760
00:47:39,240 --> 00:47:43,340
and have the particular realization is vital for example x

761
00:47:43,390 --> 00:47:45,540
x the

762
00:47:46,810 --> 00:47:48,310
is that the range of his

763
00:47:48,320 --> 00:47:50,830
right so we have the p

764
00:47:51,950 --> 00:47:53,340
x a

765
00:47:53,350 --> 00:47:54,470
musical to

766
00:47:54,470 --> 00:47:56,040
x a

767
00:47:56,050 --> 00:48:00,160
x p

768
00:48:00,230 --> 00:48:01,510
is is equal to

769
00:48:03,760 --> 00:48:06,200
and xing

770
00:48:23,760 --> 00:48:27,420
not necessarily what

771
00:48:27,470 --> 00:48:38,530
if the x is not an unmet medical

772
00:48:38,530 --> 00:48:41,040
this can be anything

773
00:48:43,330 --> 00:48:47,840
this can be the world the probability

774
00:48:51,480 --> 00:48:54,260
bipartite graph

775
00:48:54,270 --> 00:48:56,920
it can be anything you

776
00:48:57,070 --> 00:49:00,000
but of course probabilities and everything zero

777
00:49:00,050 --> 00:49:04,900
these objects can be a

778
00:49:04,950 --> 00:49:07,810
so this is the case for every

779
00:49:07,820 --> 00:49:09,630
x a

780
00:49:09,690 --> 00:49:12,780
x and x

781
00:49:12,830 --> 00:49:15,850
if these holes

782
00:49:15,860 --> 00:49:17,800
we say that

783
00:49:20,520 --> 00:49:24,200
is conditionally independent of x being

784
00:49:24,250 --> 00:49:26,230
given x e

785
00:49:26,280 --> 00:49:29,430
these annotations for independence

786
00:49:30,050 --> 00:49:31,720
so in this case here

787
00:49:31,730 --> 00:49:33,450
we have one

788
00:49:35,140 --> 00:49:39,740
is independent of x b

789
00:49:39,780 --> 00:49:50,130
in this case here we have the vaccines at the end of the conditional

790
00:49:51,540 --> 00:50:00,540
these are exact function only on the on the on the sample space yes

791
00:50:03,340 --> 00:50:06,270
after that they become away

792
00:50:06,280 --> 00:50:09,300
five live

793
00:50:09,310 --> 00:50:15,900
public holiday you will be looking at the probability and they were very very

794
00:50:15,930 --> 00:50:17,120
hard work

795
00:50:17,230 --> 00:50:24,310
i'm not being paid to be here and they had

796
00:50:33,830 --> 00:50:39,980
that's what condition depends will see some examples of conditional independence

797
00:50:40,030 --> 00:50:41,910
and here you have the notation

798
00:50:41,970 --> 00:50:46,250
so x a independent big difference

799
00:50:46,270 --> 00:50:50,400
now let's look at some examples of conditional independence

800
00:50:51,340 --> 00:50:52,830
the weather tomorrow

801
00:50:52,830 --> 00:50:56,650
it is independent of the weather yesterday

802
00:50:56,670 --> 00:50:58,510
given the weather today

803
00:50:58,530 --> 00:51:03,320
that's an interesting example because of course the weather tomorrow is not independent from the

804
00:51:03,320 --> 00:51:04,900
weather yesterday

805
00:51:09,590 --> 00:51:13,480
but if i observed the weather today

806
00:51:13,500 --> 00:51:19,010
then this two things become independent because there is no information from the weather yesterday

807
00:51:19,100 --> 00:51:21,760
can be transferred to the weather tomorrow

808
00:51:21,780 --> 00:51:24,050
it's not completely

809
00:51:24,060 --> 00:51:27,260
president in the weather today

810
00:51:27,270 --> 00:51:30,260
so it's the conditional independence

811
00:51:30,270 --> 00:51:32,820
another condition independence statements

812
00:51:32,870 --> 00:51:34,680
my genome is

813
00:51:34,690 --> 00:51:38,230
independence from the ground my grandparents genome

814
00:51:38,340 --> 00:51:41,530
even my

815
00:51:41,540 --> 00:51:42,830
of course you

816
00:51:42,880 --> 00:51:45,950
the family of grand prize

817
00:51:46,940 --> 00:51:48,570
if you know you

818
00:51:50,850 --> 00:51:54,630
any information about grandparents one held

819
00:51:54,650 --> 00:51:57,550
twenty for we want

820
00:51:59,040 --> 00:52:05,580
this is another example of conditional independence

821
00:52:09,220 --> 00:52:13,940
well i'm assuming well i'm assuming here that all the genes

822
00:52:13,990 --> 00:52:18,720
that comes from your grandparents to you come to your

823
00:52:29,280 --> 00:52:32,200
in most cases in most cases

824
00:52:32,200 --> 00:52:35,810
even so that the system moves

825
00:52:35,840 --> 00:52:37,980
in one direction

826
00:52:38,020 --> 00:52:39,910
from one into another

827
00:52:39,950 --> 00:52:43,340
the alternative is branched chain

828
00:52:43,360 --> 00:52:46,180
british chain

829
00:52:46,180 --> 00:52:51,530
a different type of architecture in in that case you actually have something

830
00:52:51,560 --> 00:52:54,040
so this is not a graph this is actually the

831
00:52:54,050 --> 00:52:54,840
the main

832
00:52:54,850 --> 00:52:57,160
component of the backbone

833
00:52:57,200 --> 00:52:59,500
continuing to grow but growing in

834
00:52:59,550 --> 00:53:01,500
a plurality of pass

835
00:53:02,450 --> 00:53:05,350
you can imagine that one of these is going to pack much better than the

836
00:53:05,350 --> 00:53:10,790
other one is going to have a greater chance of forming such zones of crystallinity

837
00:53:10,870 --> 00:53:15,240
and clearly it's going to be the linear chain these branches stick out and they

838
00:53:16,220 --> 00:53:17,550
tight packing

839
00:53:17,610 --> 00:53:22,670
they prevent tight packing so the branched chain is hard to crystallize

840
00:53:22,710 --> 00:53:24,600
hard to crystallize

841
00:53:24,650 --> 00:53:30,730
and when it forms its amorphous structures it has more free volume

842
00:53:30,740 --> 00:53:37,490
more free volume and also you can think about the glass transition temperature

843
00:53:37,500 --> 00:53:40,970
tg how is that going to work so on so by

844
00:53:41,010 --> 00:53:42,040
looking at

845
00:53:42,570 --> 00:53:44,740
branching you end up with such

846
00:53:46,020 --> 00:53:47,640
there's one other one

847
00:53:47,710 --> 00:53:50,760
and as shown here

848
00:53:50,820 --> 00:53:52,390
that's too

849
00:53:52,390 --> 00:53:54,710
take several changes with

850
00:53:54,760 --> 00:53:57,710
linear or branched i'm just going to draw

851
00:53:57,750 --> 00:53:59,920
several straight chains here

852
00:54:00,820 --> 00:54:04,430
it's possible to link chains

853
00:54:05,540 --> 00:54:08,600
joining them with some covalent

854
00:54:10,390 --> 00:54:12,430
covalent bridges

855
00:54:12,440 --> 00:54:17,650
this is the covalent bridge this covalent bridge

856
00:54:17,660 --> 00:54:20,090
and it actually links one

857
00:54:20,150 --> 00:54:22,160
change to another

858
00:54:23,900 --> 00:54:28,790
imagine the mechanical properties of this material that otherwise there would be weak van der

859
00:54:28,790 --> 00:54:32,900
waals bonds and if we apply shear stress we can cause the material to plastic

860
00:54:32,900 --> 00:54:36,190
leader four but these are strong covalent bonds

861
00:54:36,200 --> 00:54:38,790
so what will happen is i can move this material

862
00:54:38,890 --> 00:54:43,120
sure enough to appoint but then i encountered the resistance of these bonds and this

863
00:54:43,120 --> 00:54:48,550
is i let go of the applied force it springs back so by cross linking

864
00:54:48,570 --> 00:54:53,050
by cross linking this is a crosslinked architecture

865
00:54:53,150 --> 00:54:54,740
by crosslinking

866
00:54:56,260 --> 00:54:57,760
in parts of

867
00:54:59,060 --> 00:55:03,810
elastic this city and this is the fact that the structure of a rubber and

868
00:55:03,810 --> 00:55:07,070
we call such polymers elastomers

869
00:55:07,090 --> 00:55:08,920
elastomers because they will

870
00:55:08,920 --> 00:55:10,980
it will spring back to shape

871
00:55:11,030 --> 00:55:15,190
and what have to happen here in order to allow for the cross we have

872
00:55:15,190 --> 00:55:17,850
to be able to form covalent bonds

873
00:55:17,860 --> 00:55:21,790
if we're going to form covalent bonds we have to break some bonds that are

874
00:55:21,790 --> 00:55:23,790
present in the back backbone

875
00:55:23,800 --> 00:55:25,270
so if i've got it

876
00:55:25,320 --> 00:55:26,340
if i got

877
00:55:26,350 --> 00:55:30,170
o bonds such as this in the backbone and then i come up with this

878
00:55:30,180 --> 00:55:32,290
our group that wants to

879
00:55:32,310 --> 00:55:34,140
somehow attached

880
00:55:34,170 --> 00:55:36,310
it's pretty clear that if i

881
00:55:36,330 --> 00:55:40,760
break one of these bonds in order to make the attachment i cut the change

882
00:55:40,760 --> 00:55:45,570
so by the it's axiomatic then if i'm going to form crosslink i need to

883
00:55:45,570 --> 00:55:50,670
find a set position along the backbone that has a double bond

884
00:55:50,670 --> 00:55:53,310
thereby i can break the double bond

885
00:55:53,330 --> 00:55:59,030
converted to a single bond link up with the crosslinking bridge and keep the integrity

886
00:55:59,030 --> 00:56:04,450
of the backbone while forming a new bond to an adjacent to change

887
00:56:05,400 --> 00:56:06,830
this is the

888
00:56:06,860 --> 00:56:09,240
this is the condition for which

889
00:56:09,250 --> 00:56:11,150
we formed elastomers

890
00:56:11,200 --> 00:56:13,610
and this is the basis for rubber

891
00:56:13,670 --> 00:56:15,410
this is the basis for forever

892
00:56:15,420 --> 00:56:17,690
and i think this next cartoon

893
00:56:17,730 --> 00:56:21,870
shows that here's here's poly i supreme

894
00:56:21,990 --> 00:56:23,900
and you can see there is

895
00:56:23,960 --> 00:56:30,900
double bond in the backbone and by presenting sulfur sulfur is capable as his oxygen

896
00:56:31,070 --> 00:56:36,000
of forming bridges we saw in the silicate in the bar is how the oxygen

897
00:56:36,000 --> 00:56:40,620
forms covalent bonds on either side thereby linking silicate units

898
00:56:40,670 --> 00:56:45,370
well far low oxygen if we believe mandalay have it should have similar properties

899
00:56:45,460 --> 00:56:47,340
so sulphurous forming

900
00:56:47,370 --> 00:56:53,910
bridging structures breaking these double bonds and now linking the one orange back along with

901
00:56:53,920 --> 00:56:55,860
the adjacent orange backbone

902
00:56:55,910 --> 00:57:01,800
so sulphur enables crosslinking and this in fact was discovered here in boston it was

903
00:57:05,500 --> 00:57:07,800
who was working in roxbury

904
00:57:07,860 --> 00:57:12,790
and i i was trying to stabilize rubber natural rubber is very sticky

905
00:57:12,840 --> 00:57:18,420
and so he founded by adding sulfur he could cause the the robert to lose

906
00:57:18,420 --> 00:57:20,490
its stickiness and thereby

907
00:57:20,500 --> 00:57:25,600
developing a synthetic rubber that had superior properties and that it was charles goodyear who

908
00:57:25,600 --> 00:57:30,650
came to boston saw this took the invention licence that it was good you're who

909
00:57:30,650 --> 00:57:33,710
accidentally spilled some of

910
00:57:33,740 --> 00:57:38,270
this crosslinked rubber onto a hot stove and they

911
00:57:38,790 --> 00:57:40,890
the thermal treatment

912
00:57:40,960 --> 00:57:46,060
gave a far superior rubber which gave birth to vulcan is asian and then

913
00:57:46,080 --> 00:57:47,720
from there it's all history

914
00:57:48,830 --> 00:57:51,270
automobile tires and whatnot

915
00:57:51,270 --> 00:57:52,830
there being the same as what went in

916
00:57:56,490 --> 00:57:57,580
so that's the game plan

917
00:57:59,940 --> 00:58:02,890
so we call source coding data compression

918
00:58:03,570 --> 00:58:05,820
and we want to understand how to do this

919
00:58:11,950 --> 00:58:13,600
in order to discuss compression

920
00:58:14,570 --> 00:58:15,690
it may be helpful to

921
00:58:16,100 --> 00:58:19,120
think about idealised sources all

922
00:58:21,520 --> 00:58:25,270
so you can think about compressing things that idealized and and simple

923
00:58:26,090 --> 00:58:30,120
end the first example i'm going to talk about is the bent coin

924
00:58:36,160 --> 00:58:37,380
and then going is going

925
00:58:39,060 --> 00:58:41,570
an outcome and we're going to get lots of articles

926
00:58:42,690 --> 00:58:44,930
now a lieutenant of about

927
00:58:45,540 --> 00:58:47,320
and alphabet has to

928
00:58:49,350 --> 00:58:51,770
character uncle fails called heads

929
00:58:52,900 --> 00:58:54,270
and the probabilities

930
00:58:58,210 --> 00:58:59,890
of those two outcomes

931
00:59:01,870 --> 00:59:03,320
ninety percent chance that tail

932
00:59:03,910 --> 00:59:05,370
and that ten percent chance

933
00:59:12,060 --> 00:59:14,690
my notation is of course random variable

934
00:59:17,370 --> 00:59:21,010
and the probability that x is tales is not quite nice so this is sort

935
00:59:21,080 --> 00:59:23,250
notation are used in the book

936
00:59:26,250 --> 00:59:27,310
random variables

937
00:59:29,120 --> 00:59:29,330
and the

938
00:59:30,070 --> 00:59:31,860
question we want to ask about this

939
00:59:32,260 --> 00:59:36,690
redundant sources redundant because it's got zeros and not very many ones

940
00:59:37,200 --> 00:59:39,050
we to understand better

941
00:59:40,120 --> 00:59:45,150
if we get a file made up say one thousand outcomes from this source

942
00:59:45,820 --> 00:59:47,790
so we talked about quite a thousand times

943
00:59:51,600 --> 00:59:54,420
that's that's a string outcomes have

944
00:59:55,150 --> 00:59:55,660
i found

945
00:59:56,370 --> 00:59:59,270
how should we go about compressing that's string about comes

946
01:00:00,220 --> 01:00:00,780
and finally

947
01:00:02,140 --> 01:00:06,370
what is the smallest possible compressed file size that we should be aiming for

948
01:00:09,830 --> 01:00:11,410
i would like to answer this question before

949
01:00:13,100 --> 01:00:16,210
random variable is just a simple example in general

950
01:00:17,620 --> 01:00:18,830
i use this notation

951
01:00:20,180 --> 01:00:20,850
on sunday

952
01:00:22,640 --> 01:00:24,880
will have a couple of a say ax

953
01:00:25,390 --> 01:00:26,420
is the things

954
01:00:28,350 --> 01:00:29,290
it's a random variable

955
01:00:29,640 --> 01:00:30,340
an alphabet

956
01:00:31,200 --> 01:00:31,340
and the

957
01:00:35,850 --> 01:00:36,610
a random variable

958
01:00:39,210 --> 01:00:41,020
the challenge with a lower case letter

959
01:00:42,310 --> 01:00:44,190
the set possible outcomes

960
01:00:56,680 --> 01:00:57,280
because of

961
01:01:03,310 --> 01:01:05,940
i where i is the number of know about

962
01:01:10,900 --> 01:01:12,290
o set a police

963
01:01:26,460 --> 01:01:27,840
made up people on it

964
01:01:29,970 --> 01:01:32,530
he i

965
01:01:33,090 --> 01:01:33,520
so what

966
01:01:40,320 --> 01:01:41,580
prove it i

967
01:01:44,710 --> 01:01:49,260
right hand these probabilities have the property that the sum of the years

968
01:01:51,990 --> 01:01:52,950
and all positive

969
01:01:57,370 --> 01:01:57,950
so that's

970
01:01:58,760 --> 01:02:01,120
general source of random outcomes

971
01:02:01,720 --> 01:02:03,640
which the bent coin is one example

972
01:02:06,600 --> 01:02:07,580
we're going to discuss

973
01:02:09,370 --> 01:02:11,410
the calling and some other examples

974
01:02:13,080 --> 01:02:16,320
i will discuss the examiner playing missus shannon's

975
01:02:19,110 --> 01:02:20,430
that's the right way

976
01:02:21,190 --> 01:02:22,830
to measure information content

977
01:02:23,280 --> 01:02:24,070
and the right way

978
01:02:24,720 --> 01:02:29,520
to say how much should we expect to able compress things is asked

979
01:02:30,870 --> 01:02:32,800
we define something called the shannon

980
01:02:33,300 --> 01:02:34,290
information content

981
01:02:41,180 --> 01:02:43,560
the sun information content of an outcome

982
01:02:51,390 --> 01:02:54,850
note about just a single outcome for this general ensemble

983
01:02:56,520 --> 01:02:58,770
well we get the random variable it turned out to be

984
01:02:59,530 --> 01:03:03,200
i i the shannon information content of the outcome is

985
01:03:03,900 --> 01:03:05,440
age should have the outcome

986
01:03:07,880 --> 01:03:08,610
level base to

987
01:03:09,860 --> 01:03:10,440
one over

988
01:03:11,640 --> 01:03:12,340
the probability

989
01:03:13,280 --> 01:03:14,040
the outcome

990
01:03:15,900 --> 01:03:16,940
i we measure

991
01:03:21,720 --> 01:03:24,260
well i would be shannon says

992
01:03:24,780 --> 01:03:25,620
that's the right way

993
01:03:27,750 --> 01:03:30,620
the information content when something happens you can say on

994
01:03:31,490 --> 01:03:33,650
i have gained much information

995
01:03:34,440 --> 01:03:38,350
and you that's a measure of information content by looking at the probability

996
01:03:39,120 --> 01:03:40,120
and taking the log

997
01:03:41,100 --> 01:03:41,860
one over it

998
01:03:42,010 --> 01:03:43,060
what does look like

999
01:03:44,000 --> 01:03:47,190
so if you tell me what the problem that the outcome was

1000
01:03:48,720 --> 01:03:50,040
shannon saying

1001
01:03:50,040 --> 01:03:54,830
plus the term we got if you remember from the of original application of of

1002
01:03:54,960 --> 01:03:57,210
the diameter inequality

1003
01:03:57,270 --> 01:04:03,270
so with confidence one minus delta the rademacher complexity is exactly measuring the amount of

1004
01:04:03,310 --> 01:04:08,080
sort of weakness we have well together with this rather small quantity here

1005
01:04:08,110 --> 01:04:10,790
in our estimate from the

1006
01:04:10,790 --> 01:04:14,900
so let's just go back and look at rademacher complexity and see what it is

1007
01:04:14,960 --> 01:04:18,670
this is the definition

1008
01:04:18,690 --> 01:04:21,380
it looks a bit scary but

1009
01:04:21,940 --> 01:04:23,750
it actually

1010
01:04:23,770 --> 01:04:27,870
has a very natural structure to it because what it's saying is

1011
01:04:27,880 --> 01:04:31,250
i'm going to see i'm going to pick a random plus minus one back to

1012
01:04:31,250 --> 01:04:34,190
think of it as classification pick a random

1013
01:04:34,710 --> 01:04:36,350
labeling of my

1014
01:04:36,350 --> 01:04:37,690
function class

1015
01:04:37,710 --> 01:04:43,000
and then i'm going to ask how well can my function class approximate that

1016
01:04:43,060 --> 01:04:46,020
correlate with that labeling

1017
01:04:46,770 --> 01:04:50,520
and i'm gonna average that over all possible labellings

1018
01:04:50,540 --> 01:04:52,060
clearly is

1019
01:04:52,080 --> 01:04:55,230
for most labelings i can get a good fit

1020
01:04:55,290 --> 01:04:56,850
and that's pretty

1021
01:04:56,980 --> 01:05:02,100
flexible class and it's likely that it's not going to do a good job i

1022
01:05:02,100 --> 01:05:04,020
mean it's like even overfit

1023
01:05:04,040 --> 01:05:05,400
your data

1024
01:05:05,400 --> 01:05:07,710
because you can pretty well fit anything

1025
01:05:07,730 --> 01:05:09,940
on the other hand if it's very small

1026
01:05:09,940 --> 01:05:14,630
on average this is actually a you know a small quantity that would indicate that

1027
01:05:14,630 --> 01:05:18,170
if we've got a good fit on the training data we must have really hit

1028
01:05:18,170 --> 01:05:20,150
on a real

1029
01:05:20,150 --> 01:05:24,330
the match with our function class and therefore will likely to

1030
01:05:24,370 --> 01:05:24,940
two two

1031
01:05:24,960 --> 01:05:26,810
generalize well

1032
01:05:34,020 --> 01:05:35,770
log likelihood

1033
01:05:35,770 --> 01:05:37,370
did you see

1034
01:05:43,690 --> 01:05:48,020
know the cost of loss functions really

1035
01:05:48,040 --> 01:05:55,370
you don't think so

1036
01:05:55,500 --> 01:05:57,520
i mean it seems reasonable

1037
01:06:03,960 --> 01:06:09,460
i see what you mean you're thinking because the labels being in the right place

1038
01:06:09,460 --> 01:06:13,020
you point

1039
01:06:13,040 --> 01:06:19,600
well if you think of the functions being why i times

1040
01:06:19,650 --> 01:06:21,580
f of x i

1041
01:06:21,630 --> 01:06:24,730
then multiplying by random plus minus one

1042
01:06:24,790 --> 01:06:28,750
is the same whether that's why there are not just make a difference

1043
01:06:28,870 --> 01:06:31,060
because you just cancel

1044
01:06:31,060 --> 01:06:33,560
you just permute the whole set of

1045
01:06:33,580 --> 01:06:36,580
so same

1046
01:06:46,560 --> 01:06:47,790
that's true

1047
01:06:47,850 --> 01:06:49,150
o point

1048
01:06:54,250 --> 01:06:57,480
yeah i agree with that but

1049
01:06:58,690 --> 01:07:00,600
not sure but i i d

1050
01:07:02,210 --> 01:07:03,210
good point

1051
01:07:03,230 --> 01:07:10,630
you should

1052
01:07:10,650 --> 01:07:16,850
take it offline i think that's a good question not sure

1053
01:07:17,250 --> 01:07:19,900
anyone else any thoughts on that

1054
01:07:19,920 --> 01:07:21,020
in size

1055
01:07:21,080 --> 01:07:24,250
so the question was it seems a bit strange that you've got

1056
01:07:24,330 --> 01:07:28,580
a loss function here which is close to zero zero one and

1057
01:07:28,650 --> 01:07:32,540
is measuring correlation with the loss function actually such a good thing

1058
01:07:32,580 --> 01:07:36,330
as a measure of complexity mean i kind of presented it is you're just measuring

1059
01:07:36,330 --> 01:07:38,580
how well the function class can

1060
01:07:38,580 --> 01:07:40,880
actually align with sigma i

1061
01:07:40,940 --> 01:07:42,000
i mean

1062
01:07:42,100 --> 01:07:44,380
should say that typically what you do is you actually

1063
01:07:45,230 --> 01:07:49,830
you see when we put the band together we first found the rademacher complexity of

1064
01:07:49,830 --> 01:07:51,750
a function class and then we

1065
01:07:51,810 --> 01:07:53,940
and on the loss function

1066
01:07:53,960 --> 01:07:58,600
as an extra ingredient in proof about how you can do that i think you

1067
01:07:58,600 --> 01:08:00,630
know probably were OK but

1068
01:08:00,630 --> 01:08:06,440
it's a good is to get inside much show in fact the correct

1069
01:08:10,290 --> 01:08:13,630
i mean the way the way the that is applied in in the support vector

1070
01:08:13,630 --> 01:08:16,190
cases actually through lipschitz

1071
01:08:16,210 --> 01:08:21,130
so uses a hinge loss and then you have the lipschitz

1072
01:08:21,190 --> 01:08:23,060
and there's a bound on the

1073
01:08:23,060 --> 01:08:26,460
if you apply a lipschitz function to class then

1074
01:08:26,850 --> 01:08:33,040
there's a simple bound for the rademacher complexity of the resulting class

1075
01:08:33,080 --> 01:08:34,130
OK so

1076
01:08:34,130 --> 01:08:39,710
anyway if we think of this is the function class and ignore the question then

1077
01:08:40,900 --> 01:08:43,230
i think it's a very natural measure of complexity

1078
01:08:44,480 --> 01:08:47,830
sort of measure how you fit ground was in fact you know some people use

1079
01:08:47,830 --> 01:08:51,400
that as a as a way of testing a very nice way of testing

1080
01:08:51,440 --> 01:08:56,210
how likely you are to perform well on the test set is just randomizer labels

1081
01:08:56,210 --> 01:08:58,630
we run your training and see how well you do

1082
01:08:58,690 --> 01:09:00,940
and if you do much worse than

1083
01:09:00,940 --> 01:09:03,520
you should be in a good situation

1084
01:09:04,210 --> 01:09:11,270
note that the rademacher complexity is distribution dependent because it has an expectation here over

1085
01:09:11,270 --> 01:09:12,730
the sample

1086
01:09:12,750 --> 01:09:16,520
so we're actually measuring somehow complexity on the sample

1087
01:09:16,580 --> 01:09:21,210
whereas the VC if you remember was sample independent it was sort of taking the

1088
01:09:21,230 --> 01:09:22,830
the worst case

1089
01:09:22,830 --> 01:09:27,980
growth function over all possible choices of inputs

1090
01:09:27,980 --> 01:09:31,750
so here there is a slight gain in that respect

1091
01:09:31,770 --> 01:09:35,100
but it appears it might be hard to compute

1092
01:09:36,940 --> 01:09:38,100
a very nice

1093
01:09:38,110 --> 01:09:40,290
the observation that you can further

1094
01:09:44,080 --> 01:09:49,250
further application and the diameter can give you something that we i call the empirical

1095
01:09:51,730 --> 01:09:57,600
which is the observed practice rademacher complexity on your particular samples so because we can

1096
01:09:57,600 --> 01:09:58,920
think of

1097
01:09:59,810 --> 01:10:04,440
as the expectation over the value the expected value over

1098
01:10:04,460 --> 01:10:07,020
the choice of the particular z

1099
01:10:07,020 --> 01:10:12,330
sequence of particular set of training examples if we take a sample

1100
01:10:12,380 --> 01:10:14,290
the particular sample we get

1101
01:10:14,290 --> 01:10:18,630
that gives us one observation and if we know that this quantity is concentrated we

1102
01:10:18,630 --> 01:10:22,830
know that observation it is with high probability close to the mean

1103
01:10:22,920 --> 01:10:23,710
and so

1104
01:10:23,750 --> 01:10:27,980
we can actually estimate mean from the samples of we take this is the empirical

1105
01:10:27,980 --> 01:10:31,670
estimate from the given sample that we actually observe

1106
01:10:31,690 --> 01:10:34,370
then we actually end up with this bound

1107
01:10:34,400 --> 01:10:38,710
where there some slightly larger constants here three into

1108
01:10:38,770 --> 01:10:40,850
coming in here

1109
01:10:40,850 --> 01:10:44,460
but we actually replace the rademacher complexity by empirical

1110
01:10:46,380 --> 01:10:48,480
so that's compared to here

1111
01:10:48,500 --> 01:10:53,790
you notice that this was the true rademacher complexity expected value this is its empirical

1112
01:10:53,790 --> 01:10:58,920
value so a second application of the time get

1113
01:10:58,920 --> 01:11:02,630
area this is one of the and some of effect size

1114
01:11:02,640 --> 01:11:08,000
it's simply a symbol of generalisation what we've done for the drops where f at

1115
01:11:08,000 --> 01:11:11,580
this stage was basically an indicator function of the set

1116
01:11:11,600 --> 01:11:12,600
so some

1117
01:11:12,620 --> 01:11:14,180
large numbers

1118
01:11:14,210 --> 01:11:17,330
it's by as all the violence basically

1119
01:11:17,340 --> 01:11:21,960
decrease as one of the and whatever basically being the dimension of the space doesn't

1120
01:11:21,960 --> 01:11:25,770
mean any bicycles of dimensionality on you have this awful limit or

1121
01:11:26,970 --> 01:11:31,670
that's basically what the calumet similarly what is now also about it is not only

1122
01:11:31,670 --> 01:11:35,810
you have an estimate of the quantity of interest which is the expectation here but

1123
01:11:35,820 --> 01:11:41,550
you can also approximate the value of this system quite easily using the sampled by

1124
01:11:41,550 --> 01:11:48,380
doing simply what multicellular approximation of the y so simply simple for once you basically

1125
01:11:48,380 --> 01:11:53,360
i dimensional target distribution so you have a distribution with a set of argument x

1126
01:11:53,360 --> 01:11:57,110
one x two where x one x two as continue to buy it if you're

1127
01:11:57,110 --> 01:12:01,940
interested in conducting an estimate of the marginal distribution of one of the guys basically

1128
01:12:01,940 --> 01:12:06,660
so x one the way you do you will become approximation very simple you discard

1129
01:12:06,680 --> 01:12:10,810
the song paul the company x of the sample that give you an approximation of

1130
01:12:10,810 --> 01:12:12,170
the marginal next

1131
01:12:12,180 --> 01:12:14,460
you can learn things that

1132
01:12:14,510 --> 01:12:17,440
so that's basically

1133
01:12:17,450 --> 01:12:21,390
in chile the multicolored principled come

1134
01:12:21,400 --> 01:12:24,870
you sample from places so deep-seated auditioned pi

1135
01:12:24,880 --> 01:12:29,930
OK you want to approximate that you essentially in an empirical measures which have the

1136
01:12:29,940 --> 01:12:35,880
property that essentially automatically you focus the kind of great problems of fourteen regional by

1137
01:12:35,880 --> 01:12:40,680
quality mass once you have these guys you can compute is lean toward you can

1138
01:12:40,680 --> 01:12:45,600
marginalize easily integrate easy some some valuable is very simple

1139
01:12:46,570 --> 01:12:51,160
so that's quite nice OK but you realize basically

1140
01:12:51,190 --> 01:12:54,390
on the abilities to song paul

1141
01:12:54,400 --> 01:12:55,310
fall on

1142
01:12:55,310 --> 01:12:57,490
the point is a solution of interest pi

1143
01:12:58,330 --> 01:13:03,240
so that's nice but think the question is if i give you pointed deception by

1144
01:13:03,240 --> 01:13:06,200
so let's say you complex bayesian model

1145
01:13:06,220 --> 01:13:11,650
the resulting target distribution is the posterior distribution of the roman five hundred tries given

1146
01:13:11,650 --> 01:13:12,820
the acceleration

1147
01:13:12,820 --> 01:13:17,470
all the sample from that OK on the answer is essentially

1148
01:13:17,490 --> 01:13:20,270
no general way of doing it

1149
01:13:21,160 --> 01:13:29,060
the distribution of interest is of stone outperforms ago show gaussian exponential of i don't

1150
01:13:29,060 --> 01:13:30,750
know ledge bases whatever

1151
01:13:30,800 --> 01:13:34,140
then basically you can sample from it exactly

1152
01:13:34,160 --> 01:13:40,220
OK but as soon as typically you're dealing with more complicated scenarios you're going to

1153
01:13:40,320 --> 01:13:46,240
have to come up with some kind of more sophisticated techniques on essentially all the

1154
01:13:46,240 --> 01:13:53,300
techniques i will discuss essentially based on the idea of proposing samples from

1155
01:13:53,710 --> 01:13:58,700
distribution which is easy to sample on tried to use some kind of mechanism so

1156
01:13:58,700 --> 01:13:59,990
as to move

1157
01:14:00,000 --> 01:14:05,200
your sample from solution of easy to sample from to the target distribution because that's

1158
01:14:05,200 --> 01:14:06,810
what we do

1159
01:14:09,800 --> 01:14:15,760
in any case if you're interested basically looking at least at some exact simulation techniques

1160
01:14:16,020 --> 01:14:18,110
to sample from the distribution

1161
01:14:18,120 --> 01:14:21,690
basically our i ask you i mean you should have a look at the blade

1162
01:14:21,690 --> 01:14:23,540
of avoid which is actually extremely good

1163
01:14:23,550 --> 01:14:24,970
but we will discuss

1164
01:14:25,070 --> 01:14:27,740
basically this type of technique here

1165
01:14:27,760 --> 01:14:31,360
OK everything is available to begin in matlab toolbox so

1166
01:14:31,360 --> 01:14:32,830
i don't know what

1167
01:14:32,850 --> 01:14:38,870
i'm gonna basically discusses the scenario where a moving target distribution which are quite complicated

1168
01:14:38,870 --> 01:14:41,930
on that at the time you dispel not for

1169
01:14:42,630 --> 01:14:46,120
on TV kelly typically

1170
01:14:46,140 --> 01:14:48,900
in all these scenarios

1171
01:14:48,930 --> 01:14:50,610
i'm going to deal with

1172
01:14:51,380 --> 01:14:54,080
i'm going to deal with the case where

1173
01:14:54,120 --> 01:14:56,190
i'm going to be important later on

1174
01:14:56,210 --> 01:14:58,990
where the distribution

1175
01:14:59,010 --> 01:15:00,450
i want

1176
01:15:00,470 --> 01:15:02,320
to sample from

1177
01:15:02,330 --> 01:15:06,140
he's known only to normalizing constant OK

1178
01:15:06,150 --> 01:15:08,430
so i'm going to deal primarily

1179
01:15:08,450 --> 01:15:14,750
with the chase essentially where say pi solicit pi x is equal to

1180
01:15:14,760 --> 01:15:21,320
a function gamma x divided by its normalizing constant is into integral of gamma of

1181
01:15:21,340 --> 01:15:23,850
the war integration domain OK

1182
01:15:23,850 --> 01:15:30,290
where gamma is known pointwise but it's normalizing constant is in the typical example would

1183
01:15:30,330 --> 01:15:33,080
say you're doing bayesian statistics

1184
01:15:33,090 --> 01:15:35,340
you know the ploy on any particularly

1185
01:15:35,360 --> 01:15:37,660
you know the likelihood analytically

1186
01:15:37,680 --> 01:15:40,000
but the post distribution

1187
01:15:40,050 --> 01:15:44,540
is known only up to a normalizing constant is proportional to the point the likelihood

1188
01:15:44,540 --> 01:15:48,990
that can be evaluated but you don't have access to be to the marginal likelihood

1189
01:15:48,990 --> 01:15:53,270
which is the integral of the likelihood of the point so typically will i want

1190
01:15:53,270 --> 01:15:58,210
to admit which are said that was made to simulate from pride but at all

1191
01:15:58,350 --> 01:16:01,900
need to know pi the normalizing constant

1192
01:16:01,900 --> 01:16:06,940
so the first online models general techniques to do that

1193
01:16:07,020 --> 01:16:13,080
to do exact simulation of such distribution is call rejection sampling was proposed by von

1194
01:16:15,580 --> 01:16:17,640
having during the los alamos project

1195
01:16:17,650 --> 01:16:18,600
OK so

1196
01:16:18,650 --> 01:16:20,130
is gonna go for it

1197
01:16:21,750 --> 01:16:26,150
regional sampling you want to sample from distribution by x

1198
01:16:26,160 --> 01:16:28,390
is proportional to

1199
01:16:28,400 --> 01:16:34,010
gamma x OK so you have access to an online version of the target distribution

1200
01:16:34,020 --> 01:16:36,960
gamma x but you typically don't know

1201
01:16:37,010 --> 01:16:41,050
the normalizing constant we shouldn't ignore of them out the end of the domain of

1202
01:16:41,050 --> 01:16:44,110
definition capital that's all they do

1203
01:16:44,490 --> 01:16:46,200
in addition sampling

1204
01:16:46,210 --> 01:16:48,160
well you say OK

1205
01:16:48,160 --> 01:16:49,470
what i'm going to do

1206
01:16:49,490 --> 01:16:50,600
i'm gonna

1207
01:16:50,600 --> 01:16:52,810
basically sample

1208
01:16:52,820 --> 01:16:56,100
i'm going to propose some candidates so important

1209
01:16:56,110 --> 01:16:57,790
and now i

1210
01:16:57,810 --> 01:17:03,080
essentially proposal poisson distribution on the same space which is going to be doing q

1211
01:17:03,100 --> 01:17:05,470
and i also assume that basically

1212
01:17:05,500 --> 01:17:10,320
she is known up to a normalizing constant well typically you know it exactly but

1213
01:17:10,370 --> 01:17:14,890
let's say for the sake of JIT that i only know q of ten articles

1214
01:17:14,890 --> 01:17:16,220
so you have five

1215
01:17:16,230 --> 01:17:19,650
you want to sample from pi you introduce the probability distribution q

1216
01:17:19,660 --> 01:17:24,050
on this version two has been picked so that it's easy to sample from OK

1217
01:17:24,060 --> 01:17:29,210
and then you want to introduce a kind of mechanism which essentially wants to propose

1218
01:17:29,220 --> 01:17:30,480
sample from q

1219
01:17:30,520 --> 01:17:35,690
is going to do some kind of some kind of feeling like featuring mechanism that's

1220
01:17:35,690 --> 01:17:39,310
going to pull you with sample from pi OK

1221
01:17:39,320 --> 01:17:41,960
so to do that you need to

1222
01:17:41,980 --> 01:17:44,590
introduce i into

1223
01:17:44,630 --> 01:17:52,900
essentially the assumption i make it essentially gamma two stop is basically bomb

1224
01:17:52,920 --> 01:17:56,270
so he obviously i could have put a prior over q is upper bounded OK

1225
01:17:56,570 --> 01:18:01,810
so essentially what this condition means way of is it implied that where if you've

1226
01:18:01,830 --> 01:18:04,080
got mass and the initial

1227
01:18:04,090 --> 01:18:07,480
distribution unit to have mass and the probability distribution

1228
01:18:07,510 --> 01:18:12,150
OK that makes sense but is also make sure that essentially you need to have

1229
01:18:12,150 --> 01:18:13,450
a probability distribution

1230
01:18:13,520 --> 01:18:19,760
which essentially as feature tails than the tails of the target distributions OK this is

1231
01:18:19,870 --> 01:18:25,140
essentially what this condition is so you want basically the public distribution of this figure

1232
01:18:26,070 --> 01:18:29,060
so this is the condition we introduce on two

1233
01:18:29,080 --> 01:18:33,590
just don't it easy to set up this is the same to make

1234
01:18:33,610 --> 01:18:35,390
OK now basically

1235
01:18:35,410 --> 01:18:38,070
you might not be able to compute exactly

1236
01:18:38,080 --> 01:18:41,150
the step between them and to start

1237
01:18:41,160 --> 01:18:42,650
this is given by c

1238
01:18:42,650 --> 01:18:44,430
so let's consid

1239
01:18:44,460 --> 01:18:48,150
but let's say that even if you don't want to compute these guys exactly you

1240
01:18:48,150 --> 01:18:54,930
not all

1241
01:19:43,400 --> 01:19:45,930
one there

1242
01:20:05,050 --> 01:20:07,220
over here

1243
01:20:07,340 --> 01:20:09,570
we are

1244
01:21:18,900 --> 01:21:41,740
all o

1245
01:22:34,040 --> 01:22:40,140
did all

1246
01:23:26,010 --> 01:23:29,930
i mean

1247
01:23:29,930 --> 01:23:33,480
so again general strategy

1248
01:23:33,490 --> 01:23:36,710
well if you want to estimate y given x

1249
01:23:36,840 --> 01:23:38,700
now we have to do is

1250
01:23:38,710 --> 01:23:41,590
you pick some topics and why

1251
01:23:41,600 --> 01:23:45,020
some joint sufficient statistic

1252
01:23:45,070 --> 01:23:47,980
get condition multinomial ones

1253
01:23:48,030 --> 01:23:51,140
we get a multiclass casting process

1254
01:23:51,190 --> 01:24:04,090
if you pick wine are get regression

1255
01:24:04,180 --> 01:24:07,610
in this case we get possible recreation

1256
01:24:07,660 --> 01:24:10,760
and this will give us sequences

1257
01:24:10,860 --> 01:24:15,920
and then the problem is convex we might not be able to solve it fully

1258
01:24:15,930 --> 01:24:17,160
because it might be

1259
01:24:17,170 --> 01:24:19,100
expensive to high dimensional

1260
01:24:19,140 --> 01:24:23,990
but we can find nice approximate solutions for it

1261
01:24:24,300 --> 01:24:28,100
so let's have very very simple example

1262
01:24:28,140 --> 01:24:30,140
multiclass classification

1263
01:24:30,150 --> 01:24:35,880
so i could for instance because people as we saw how do you pick you

1264
01:24:35,890 --> 01:24:39,990
for fixing y what does it look like how does it depend on y

1265
01:24:40,000 --> 01:24:43,410
one option is you could pick for fixed and y to be

1266
01:24:45,000 --> 01:24:48,100
tensor product with the y

1267
01:24:49,370 --> 01:24:52,350
in the morning i was talking about multiclass

1268
01:24:52,400 --> 01:24:59,090
but just distribution of and different domains discrete distribution was just picking EY

1269
01:24:59,100 --> 01:25:04,380
to be sufficient statistics i'm just blowing fourth example

1270
01:25:04,430 --> 01:25:07,130
by one hundred dimensions i have here

1271
01:25:07,170 --> 01:25:11,270
just like replicating this for fixed as many times

1272
01:25:11,290 --> 01:25:15,530
many times and if y for instance happens to be three

1273
01:25:15,550 --> 01:25:18,030
and the first two entries will be zero

1274
01:25:18,040 --> 01:25:21,850
the third two will befall fixed and then all the subsequent entries will be here

1275
01:25:23,460 --> 01:25:24,470
and if

1276
01:25:24,520 --> 01:25:30,590
why one in the first century before fixed and everything else would vanish

1277
01:25:32,220 --> 01:25:36,390
now in this case taking the inner product is quite easy

1278
01:25:36,440 --> 01:25:38,770
this is just kind of fixing prime

1279
01:25:38,780 --> 01:25:40,530
delta y prime

1280
01:25:40,610 --> 01:25:44,690
weatherwise to strangers to sit

1281
01:25:44,740 --> 01:25:47,680
there might be reasons why this is a bad idea

1282
01:25:47,690 --> 01:25:50,780
but computational is actually very good one

1283
01:25:50,830 --> 01:25:54,750
is it allows me to store a few parameters

1284
01:25:54,800 --> 01:25:56,700
that's the great advantage over it

1285
01:25:56,960 --> 01:26:02,000
and of course am assuming that there is no real interaction no real correlation between

1286
01:26:02,090 --> 01:26:04,940
different ways

1287
01:26:04,990 --> 01:26:07,560
and that may not always be true

1288
01:26:07,570 --> 01:26:11,930
the case that get the data

1289
01:26:11,940 --> 01:26:14,410
it's not songwriting from one to n

1290
01:26:14,420 --> 01:26:21,890
well the wires going from one to n alpha i y for fixed on y

1291
01:26:21,900 --> 01:26:28,340
so it's like having one set of parameters for the class

1292
01:26:28,360 --> 01:26:34,030
optimisation problem the big mess but it's convex

1293
01:26:36,130 --> 01:26:40,370
well to example some dots here somewhat so there

1294
01:26:40,410 --> 01:26:43,360
and this is what you get if you want to estimate p of y given

1295
01:26:44,540 --> 01:26:48,600
OK you have to go through them as a little bit annoying but it's not

1296
01:26:48,600 --> 01:26:50,320
really deep

1297
01:26:50,340 --> 01:26:52,360
i can do that quite easily

1298
01:26:52,410 --> 01:26:54,790
OK it works for noisy data

1299
01:26:54,810 --> 01:27:00,250
these will be probability point four o point six so you've probably seen similar slides

1300
01:27:00,250 --> 01:27:05,450
from bernard he was talking about the margin of support vector machines and you know

1301
01:27:06,200 --> 01:27:08,430
what this corresponds to are

1302
01:27:08,460 --> 01:27:10,660
certain threshold probability

1303
01:27:10,710 --> 01:27:14,460
at the moment i'm just keeping that the level of intuition

1304
01:27:14,480 --> 01:27:19,660
election turnout this is not just an intuition but you can actually make

1305
01:27:19,750 --> 01:27:25,470
considerably closer map between those terms

1306
01:27:26,630 --> 01:27:31,960
summing up a little bit before we go to the next set of slides

1307
01:27:32,590 --> 01:27:35,400
what we got from the the hammersley clifford theorem

1308
01:27:35,410 --> 01:27:38,440
are decomposition results

1309
01:27:38,490 --> 01:27:40,750
we got this connection between

1310
01:27:42,180 --> 01:27:44,580
sufficient statistics

1311
01:27:45,450 --> 01:27:50,420
we saw that for the normal distribution is actually a direct mapping between the graphs

1312
01:27:50,650 --> 01:27:53,720
and sparse matrices

1313
01:27:53,800 --> 01:27:59,190
and full conditional distributions we looked at log partition functions expectation through tears and casting

1314
01:28:01,170 --> 01:28:05,950
generalized across tricks and we looked at one first example of where this can actually

1315
01:28:05,950 --> 01:28:07,850
be is

1316
01:28:07,870 --> 01:28:08,900
so now

1317
01:28:08,910 --> 01:28:10,310
we have a look at

1318
01:28:10,350 --> 01:28:13,800
other applications

1319
01:28:29,810 --> 01:28:32,190
now we look at the novelty detection

1320
01:28:32,240 --> 01:28:36,030
good classification again

1321
01:28:36,080 --> 01:28:41,300
and regression

1322
01:28:43,130 --> 01:28:45,020
density estimation

1323
01:28:45,030 --> 01:28:48,320
well the MAP estimate

1324
01:28:48,340 --> 01:28:51,900
so this is what you would usually somebody going back from the conditioning to an

1325
01:28:51,900 --> 01:28:53,250
unconditional models

1326
01:28:53,300 --> 01:28:56,520
switching is a little bit

1327
01:28:56,540 --> 01:29:01,040
but not if you cover novelty detection

1328
01:29:02,020 --> 01:29:05,160
so you're all experts in all detection by now

1329
01:29:05,210 --> 01:29:07,640
after this you know it even better

1330
01:29:07,660 --> 01:29:10,030
maybe it will confuse it completely

1331
01:29:10,050 --> 01:29:13,660
well if i were to do density estimation

1332
01:29:13,680 --> 01:29:15,880
well what they do is they would just

1333
01:29:15,890 --> 01:29:20,220
so for instance the maximum a posteriori estimation problem

1334
01:29:20,230 --> 01:29:22,000
the negative log likelihood

1335
01:29:22,020 --> 01:29:23,300
that's from

1336
01:29:23,320 --> 01:29:27,260
up to constant the negative log prior

1337
01:29:28,010 --> 01:29:33,590
now comes the message from nice concentration of measure results

1338
01:29:33,910 --> 01:29:35,820
so this actually

1339
01:29:35,830 --> 01:29:37,780
some results of the

1340
01:29:37,960 --> 01:29:41,870
got with out and that play quite nicely in this context

1341
01:29:41,890 --> 01:29:44,100
but there is one big problem

1342
01:29:44,150 --> 01:29:49,470
his g of data may be really painful to compute

1343
01:29:49,490 --> 01:29:52,930
because you know you need to normalize it this entire domain

1344
01:29:52,940 --> 01:29:54,760
so if

1345
01:29:54,770 --> 01:29:57,970
say the axes are over the main of images

1346
01:29:57,980 --> 01:30:02,390
then well gee theta would mean you would have to integrate over all possible images

1347
01:30:02,390 --> 01:30:03,720
that you could

1348
01:30:03,770 --> 01:30:05,400
could ever encounter

1349
01:30:05,650 --> 01:30:08,160
so if you look in RGB space

1350
01:30:08,200 --> 01:30:09,830
then you have

1351
01:30:09,840 --> 01:30:12,840
while twenty four bits per pixel

1352
01:30:12,850 --> 01:30:17,450
you take a five megapixel the captured image

1353
01:30:17,460 --> 01:30:18,930
so you have

1354
01:30:18,980 --> 01:30:22,950
between two to the twenty four times five million

1355
01:30:22,950 --> 01:30:27,400
this is not a useless model obviously i wouldn't be telling you otherwise it will

1356
01:30:27,400 --> 01:30:28,810
be very powerful

1357
01:30:28,860 --> 01:30:31,320
for proving that we can do better than n log n

1358
01:30:31,340 --> 01:30:34,970
but as writing down and out of you're going to implement something this tree is

1359
01:30:34,970 --> 01:30:35,970
not so

1360
01:30:36,020 --> 01:30:38,040
so useful

1361
01:30:38,050 --> 01:30:40,410
even if you had a decision tree computer

1362
01:30:40,490 --> 01:30:41,840
whatever the case

1363
01:30:42,030 --> 01:30:49,130
OK let's prove this theorem the decision trees in some sense model

1364
01:30:49,660 --> 01:30:51,770
comparison sorting algorithms

1365
01:30:53,730 --> 01:30:54,960
which we call

1366
01:30:54,980 --> 01:30:57,940
just comparison sorts

1367
01:31:03,810 --> 01:31:06,680
this is the transformation

1368
01:31:06,770 --> 01:31:09,340
and we're going to build one tree

1369
01:31:09,350 --> 01:31:11,270
for each value of n

1370
01:31:11,280 --> 01:31:15,850
also mentioned that can be decision trees depend on

1371
01:31:15,870 --> 01:31:23,150
the algorithm hopefully does well depends on n but it works for all values and

1372
01:31:23,150 --> 01:31:29,580
we're just going to think of the algorithm as splitting into two

1373
01:31:29,680 --> 01:31:31,950
of course the left subtree

1374
01:31:31,990 --> 01:31:36,210
and the right subtree whenever it makes a comparison

1375
01:31:40,400 --> 01:31:46,300
if we take a comparison sort

1376
01:31:46,430 --> 01:31:48,320
like can merge sort

1377
01:31:48,400 --> 01:31:52,480
he does lots of stuff it does index arithmetic it this recursion whatever at some

1378
01:31:52,480 --> 01:31:54,790
point it makes a comparison and then we say OK

1379
01:31:54,840 --> 01:31:56,100
there's too much too

1380
01:31:56,120 --> 01:31:59,160
has the other is what the algorithm would do

1381
01:31:59,210 --> 01:32:02,650
if the comparison came out less than or equal to and this is what the

1382
01:32:02,650 --> 01:32:07,180
algorithm would do the comparison came out greater than

1383
01:32:07,190 --> 01:32:08,390
so you can

1384
01:32:08,400 --> 01:32:11,550
build the tree in this way

1385
01:32:11,590 --> 01:32:17,120
so in some sense what histories is doing is listing all possible executions of this

1386
01:32:18,140 --> 01:32:23,010
considering what happened for all possible values of the comparison

1387
01:32:27,090 --> 01:32:37,830
so we'll call these all possible

1388
01:32:37,840 --> 01:32:40,750
instruction traces

1389
01:32:40,760 --> 01:32:44,190
you write down all the instructions that are executed

1390
01:32:44,950 --> 01:32:46,600
this algorithm

1391
01:32:46,650 --> 01:32:49,030
for all possible inputs

1392
01:32:49,080 --> 01:32:51,360
input array a one to and

1393
01:32:51,380 --> 01:32:53,220
see what all the comparisons are

1394
01:32:53,270 --> 01:32:57,750
how they could come out and how what the algorithm does the and you get

1395
01:32:57,770 --> 01:32:59,090
a tree

1396
01:32:59,160 --> 01:33:06,110
how big of attributes

1397
01:33:18,560 --> 01:33:23,280
as a function of n

1398
01:33:31,780 --> 01:33:34,780
right if we if it's going to be able to sort every possible list of

1399
01:33:34,780 --> 01:33:35,920
length n

1400
01:33:35,940 --> 01:33:41,230
at the leaves i have to have all the permutations of those elements

1401
01:33:41,950 --> 01:33:46,830
that's a lot there's alot of permutations on n elements and factorial

1402
01:33:46,850 --> 01:33:50,120
and factorial is exponential is really big

1403
01:33:50,190 --> 01:33:52,530
so this tree is huge

1404
01:33:52,530 --> 01:33:55,970
it's going to be exponential in the input size and this is why

1405
01:33:55,980 --> 01:33:58,990
we don't write algorithms down normally is the decision tree

1406
01:33:59,010 --> 01:34:01,570
even though it's some cases maybe we could

1407
01:34:01,590 --> 01:34:02,790
it's not very

1408
01:34:02,800 --> 01:34:07,490
it's not a very compact representation these algorithms and you write them down in pseudocode

1409
01:34:07,490 --> 01:34:09,560
they have constant length

1410
01:34:09,570 --> 01:34:14,170
has very succinct representation of this algorithm here the link depends on n and it

1411
01:34:14,170 --> 01:34:18,040
depends exponentially on which is not useful

1412
01:34:18,050 --> 01:34:19,800
if you want to implement the algorithm

1413
01:34:19,810 --> 01:34:23,070
writing down the other would take a long time

1414
01:34:23,090 --> 01:34:25,400
OK but nonetheless we can use this is the tool

1415
01:34:25,410 --> 01:34:30,860
to analyse this comparison sorting out we have all these any algorithm can be transformed

1416
01:34:30,860 --> 01:34:33,350
in this way into a decision tree

1417
01:34:33,360 --> 01:34:35,050
and now we have this

1418
01:34:35,060 --> 01:34:40,630
observation that the number of leaves in this decision tree has to be really big

1419
01:34:40,640 --> 01:34:44,220
OK so

1420
01:34:44,220 --> 01:34:48,390
well let me talk about leaves and in second

1421
01:34:48,440 --> 01:34:56,560
before we get to leave let's talk about the depth of the tree

1422
01:34:56,570 --> 01:35:05,560
so this decision tree represents all possible executions of the algorithm if by looking at

1423
01:35:05,590 --> 01:35:08,780
particular execution should corresponds to some

1424
01:35:08,790 --> 01:35:11,350
root to leaf path in the tree

1425
01:35:11,380 --> 01:35:15,810
the running time with the number of comparisons made by that execution is just the

1426
01:35:15,810 --> 01:35:18,590
length of the

1427
01:35:18,610 --> 01:35:26,780
and therefore the worst case running time

1428
01:35:26,810 --> 01:35:32,520
over all possible inputs of length n

1429
01:35:32,530 --> 01:35:34,260
is going to be

1430
01:35:34,270 --> 01:35:43,550
and minus one could be

1431
01:35:43,560 --> 01:35:45,490
the president decision tree

1432
01:35:45,490 --> 01:35:49,650
but as a function of the decision tree

1433
01:35:50,840 --> 01:35:54,370
the longest track which is called the height of the tree

1434
01:35:59,150 --> 01:36:02,050
OK so this is what we want to measure we want to claim that the

1435
01:36:02,050 --> 01:36:05,240
height of the tree has to be at least n log n

1436
01:36:05,240 --> 01:36:07,830
with no megan front

1437
01:36:07,850 --> 01:36:10,060
that's all for

1438
01:36:10,070 --> 01:36:21,000
and the only thing really is that the number of leaves in the tree has

1439
01:36:21,000 --> 01:36:23,600
to be has to be an factorial

1440
01:36:32,910 --> 01:36:37,240
this is a lower bound

1441
01:36:40,960 --> 01:36:44,200
decision tree sorting

1442
01:36:44,210 --> 01:36:59,530
and the bound says that if you have any decision tree

1443
01:36:59,540 --> 01:37:01,520
that sorts

1444
01:37:01,540 --> 01:37:04,240
an elements

1445
01:37:04,250 --> 01:37:09,280
then it's height has to be at least n log n

1446
01:37:09,290 --> 01:37:12,020
up to constant factors

1447
01:37:12,080 --> 01:37:24,920
so that's the theorem are going to prove the theorem

1448
01:37:32,390 --> 01:37:35,630
we're going to use the the number of leaves in the tree

1449
01:37:35,770 --> 01:37:38,710
must be

1450
01:37:39,950 --> 01:37:41,350
and factorial

1451
01:37:41,370 --> 01:37:43,640
because there and factorial

1452
01:37:43,660 --> 01:37:47,340
permutations of the input all of them could happen

1453
01:37:47,360 --> 01:37:50,640
and so for this album to be correct it has to detect every one of

1454
01:37:50,640 --> 01:37:52,250
those permutations

1455
01:37:52,250 --> 01:37:56,760
fans were talking about wave function since we're talking about the properties of waves we

1456
01:37:56,760 --> 01:38:01,190
don't only have constructive interference we can also imagine a case where we would have

1457
01:38:01,210 --> 01:38:07,020
destructive interference just like see destructive interference with water waves with light ways we can

1458
01:38:07,020 --> 01:38:10,070
also see destructive interference with orbitals

1459
01:38:10,140 --> 01:38:13,310
so let's think about what that would look like so in this case we would

1460
01:38:13,310 --> 01:38:19,230
have one a and one b and instead we would subtract one from the other

1461
01:38:19,240 --> 01:38:22,550
and what we would see is that instead of having additional

1462
01:38:22,570 --> 01:38:27,270
more weight function in the middle here we're actually cancelled out the wavefunction

1463
01:38:27,280 --> 01:38:29,150
and we end up with node

1464
01:38:29,150 --> 01:38:32,870
so we can also name this orbital and the orbital we're going to call thing

1465
01:38:32,890 --> 01:38:35,150
no one as star

1466
01:38:35,200 --> 01:38:40,420
so if we name this orbital this is an bonding molecular orbitals so we had

1467
01:38:40,420 --> 01:38:43,340
bonding and we're talking about inside bonding

1468
01:38:43,380 --> 01:38:45,190
when we talk about

1469
01:38:45,250 --> 01:38:46,550
in two bonding

1470
01:38:46,560 --> 01:38:53,260
essentially we're taking one as a and our subtracting one b

1471
01:38:53,270 --> 01:38:57,920
what we end up with again is sigma one has an important thing

1472
01:38:57,980 --> 01:39:01,450
to remember is to write the star here so any time you see a star

1473
01:39:01,460 --> 01:39:06,490
that means an antibonding orbital

1474
01:39:06,560 --> 01:39:10,190
again we can look at this in terms of thinking about a picture this way

1475
01:39:10,190 --> 01:39:13,460
in terms of drawing the wavefunction out on an axis

1476
01:39:13,550 --> 01:39:18,450
so we have one a and we're trying this as having a positive amplitude but

1477
01:39:18,450 --> 01:39:24,630
since we have destructive interference we're going to draw one be as having the opposite

1478
01:39:24,630 --> 01:39:28,710
sign so we have a plus and minus in terms of science so that to

1479
01:39:28,710 --> 01:39:31,910
make it very easy to picture that this is being cancelled out in the middle

1480
01:39:32,250 --> 01:39:37,200
if we overlay what the actual molecular orbital is on top of that what you

1481
01:39:37,200 --> 01:39:42,470
see is that in this country and cancelling out the function entirely

1482
01:39:42,470 --> 01:39:47,300
so this is the one at star sigma one star robert all and what you

1483
01:39:47,300 --> 01:39:53,850
have in the centre here the node right in the centre between the two nuclei

1484
01:39:53,870 --> 01:39:58,120
so again if we look at this in terms of its physical interpretation of probability

1485
01:39:58,120 --> 01:40:03,440
density what we need to do is square the wave function itself we square

1486
01:40:03,460 --> 01:40:05,400
sigma one star

1487
01:40:05,420 --> 01:40:08,230
we let the amplitude all positive now

1488
01:40:08,240 --> 01:40:11,190
but again we still have this node right in the middle

1489
01:40:11,190 --> 01:40:14,480
so if we talk about the probability density and we write that and it's going

1490
01:40:14,480 --> 01:40:17,170
to be sigma one star square

1491
01:40:17,220 --> 01:40:21,240
so now we're talking about one and a minus one sp

1492
01:40:21,260 --> 01:40:23,460
all of that being square

1493
01:40:23,540 --> 01:40:26,200
and again if we write out what all the terms are

1494
01:40:26,200 --> 01:40:30,470
we again have one as a squared plus one be square

1495
01:40:30,480 --> 01:40:36,340
but now what we're doing is we're actually subtracting the interference term software subtracting the

1496
01:40:36,340 --> 01:40:43,550
interference term what we have here now is destructive interference

1497
01:40:43,570 --> 01:40:46,780
so let's think about the energy of interaction here

1498
01:40:46,800 --> 01:40:52,030
when we were talking about constructive interference we had more electron density in between the

1499
01:40:52,030 --> 01:40:53,140
two nuclei

1500
01:40:53,180 --> 01:40:55,200
so that lower the energy

1501
01:40:55,240 --> 01:41:01,120
of the molecular orbitals bonding orbitals are down here but we think about where antibonding

1502
01:41:01,120 --> 01:41:01,830
orbital should be

1503
01:41:02,240 --> 01:41:04,600
it should be higher energy

1504
01:41:04,690 --> 01:41:09,430
it's increased compared to the atomic orbitals so we would label

1505
01:41:09,440 --> 01:41:12,430
are bonding orbital higher energy

1506
01:41:12,450 --> 01:41:15,880
then are one f atomic orbitals

1507
01:41:15,890 --> 01:41:20,010
so let's think a little bit about what this means for symbolic entropy any time

1508
01:41:20,010 --> 01:41:24,920
we see the star sigma one star that's an antibonding orbital

1509
01:41:25,030 --> 01:41:28,650
when we talk about that basically what we're saying and you can see that because

1510
01:41:28,650 --> 01:41:34,250
of that negative interference we actually have less electron density between the nuclei

1511
01:41:34,300 --> 01:41:37,620
then we did when they were two separate atoms so you can see that this

1512
01:41:37,620 --> 01:41:39,550
is non-binding this is

1513
01:41:39,760 --> 01:41:45,250
even worse the non-binding inside bonding because we're actually getting rid of electron density between

1514
01:41:45,250 --> 01:41:48,640
the two nuclei and we know that the electron density between the nuclei

1515
01:41:48,640 --> 01:41:52,050
that holds two atoms together in the bond

1516
01:41:52,070 --> 01:41:54,700
so what i want to point out is that it creates an effect that is

1517
01:41:54,700 --> 01:41:59,810
exactly opposite of of bond you might have thought before we started talking about molecular

1518
01:41:59,810 --> 01:42:05,100
orbital theory that non-binding was the opposite of bonding it's not inside bonding is the

1519
01:42:05,100 --> 01:42:08,780
opposite of bonding and antibonding is not non-binding

1520
01:42:08,870 --> 01:42:13,150
we can see that if we just look at this picture here here's bonding and

1521
01:42:13,150 --> 01:42:18,810
here is non-binding in bonding is even higher in energy than non bonding

1522
01:42:18,910 --> 01:42:21,740
and the other thing to point out that the energy

1523
01:42:21,750 --> 01:42:24,500
that an antibonding orbital is raised by

1524
01:42:24,510 --> 01:42:28,470
is the same amount as the bonding orbital was lowered by

1525
01:42:28,470 --> 01:42:33,440
so anytime i draw these molecular orbitals i do my best not always perfect perfectly

1526
01:42:33,450 --> 01:42:38,280
trying to make this energy difference is exactly the same for the entire bonding orbital

1527
01:42:38,280 --> 01:42:42,370
being raised first the bonding orbitals being lower so those should be

1528
01:42:42,390 --> 01:42:46,530
raise the lower by the same energy

1529
01:42:46,550 --> 01:42:53,190
so now let's take a look at some of these are questionable

1530
01:42:53,190 --> 01:42:58,640
well they don't want to go into the higher energy situation

1531
01:42:58,680 --> 01:43:01,440
so we'll start to look at molecules and we'll see

1532
01:43:01,470 --> 01:43:05,840
if we take two of atoms and that we feel in a molecular orbital and

1533
01:43:05,840 --> 01:43:09,500
it turns out that they have more antibonding orbitals bonding

1534
01:43:09,530 --> 01:43:12,400
that's a diatomic molecule will never see

1535
01:43:12,520 --> 01:43:16,470
so it helps this project will we see this for example h two which were

1536
01:43:16,470 --> 01:43:17,740
going to be about to do c

1537
01:43:18,180 --> 01:43:23,560
is stabilized because more bonding and antibonding solar project yes there's a bunch of that's

1538
01:43:23,560 --> 01:43:25,730
a really good question

1539
01:43:25,740 --> 01:43:27,120
right so

1540
01:43:27,120 --> 01:43:30,300
let's let's go ahead and do this and take a look at some of the

1541
01:43:30,300 --> 01:43:35,230
actual atoms that we can think about and think about them in more in molecules

1542
01:43:35,230 --> 01:43:39,440
so our simplest case that we started talking about with molecular hydrogen i want to

1543
01:43:39,440 --> 01:43:43,560
well let's go back to the basic that you can walk on any space as

1544
01:43:43,560 --> 01:43:46,540
soon as you can the twenty points so one of the

1545
01:43:46,610 --> 01:43:47,670
and the

1546
01:43:47,690 --> 01:43:50,750
the nodes of the graph so what we need

1547
01:43:50,770 --> 01:43:53,380
is the way in the system at the right

1548
01:43:53,560 --> 01:44:00,060
into the space and we need to count between any two two

1549
01:44:00,690 --> 01:44:02,420
i will not

1550
01:44:03,290 --> 01:44:04,340
but i do

1551
01:44:05,940 --> 01:44:08,520
so so the question is

1552
01:44:08,540 --> 01:44:11,710
how can make can also the rise

1553
01:44:11,770 --> 01:44:16,000
so just to match the following here

1554
01:44:16,020 --> 01:44:18,730
all the previous contextual the input

1555
01:44:19,110 --> 01:44:20,460
find that

1556
01:44:20,480 --> 01:44:25,690
OK is not support because strings is just not given rise

1557
01:44:27,440 --> 01:44:31,330
so there can be one of the protein that is the story

1558
01:44:31,340 --> 01:44:35,110
in i sense as soon as you give me metrics

1559
01:44:35,110 --> 01:44:41,670
all size one hundred by one thousand positive semidefinite is defined by count

1560
01:44:41,670 --> 01:44:46,250
OK because any counties and totally defined by experts

1561
01:44:46,270 --> 01:44:49,960
so in a sense from the visual effects you can make an account of the

1562
01:44:49,960 --> 01:44:51,840
nodes of graph is yes

1563
01:44:51,860 --> 01:44:53,940
give me any positive geometries

1564
01:44:53,960 --> 01:44:57,400
positive semidefinite matrices it's going to be about him

1565
01:44:57,420 --> 01:45:00,650
but what missing here is what i would like to is to be inferred from

1566
01:45:00,650 --> 01:45:06,940
the ground scientific inference would be well if another function of an underground i want

1567
01:45:06,940 --> 01:45:07,790
to infer the russian

1568
01:45:07,830 --> 01:45:12,110
another not intuitively if it's not too far away and the graph like the function

1569
01:45:12,110 --> 01:45:17,090
to to pass the positron so actually how to the knowledge of the that projects

1570
01:45:17,170 --> 01:45:19,380
into the canal

1571
01:45:19,380 --> 01:45:23,980
and what you will be able to see the size that's

1572
01:45:24,090 --> 01:45:26,340
they can be first

1573
01:45:26,520 --> 01:45:29,230
so that's what

1574
01:45:29,250 --> 01:45:32,090
the point is that is not one that so the mother

1575
01:45:32,110 --> 01:45:37,560
fortunately the distance to say well i photographed

1576
01:45:38,170 --> 01:45:42,060
so from the ROC curve is the right distance between his

1577
01:45:42,650 --> 01:45:44,190
the length of the shortest paths

1578
01:45:44,210 --> 01:45:49,610
this is not a distance because between it allows me for my right to

1579
01:45:49,670 --> 01:45:51,400
a distance

1580
01:45:51,420 --> 01:45:53,290
in the process

1581
01:45:53,400 --> 01:45:57,610
they have the potential find the distance between the gulf

1582
01:45:57,630 --> 01:45:59,500
and then can

1583
01:45:59,520 --> 01:46:05,360
so this would be the geometric approach and was doesn't work

1584
01:46:05,360 --> 01:46:10,670
in in fact you might think well approaches which is also part the

1585
01:46:10,670 --> 01:46:13,690
this approach to do is to the other way around

1586
01:46:13,690 --> 01:46:19,340
say well instead of looking at the point at this point that the RKHS and

1587
01:46:19,490 --> 01:46:23,310
define RKHS functions by defining with what they call a small portion of the

1588
01:46:23,330 --> 01:46:27,980
because it would be easier if any given graph the function from the graph the

1589
01:46:27,980 --> 01:46:31,610
real numbers i can myself if i call

1590
01:46:31,710 --> 01:46:34,270
small steps right in front of

1591
01:46:34,340 --> 01:46:36,340
says i i would be able

1592
01:46:36,340 --> 01:46:39,110
to see what to prove RKHS

1593
01:46:39,290 --> 01:46:41,150
twenty was kernel

1594
01:46:41,170 --> 01:46:45,000
which is it is that these are the more

1595
01:46:45,020 --> 01:46:46,440
and finally

1596
01:46:46,440 --> 01:46:48,590
question that which is

1597
01:46:48,610 --> 01:46:50,900
if you really want to make you know

1598
01:46:50,980 --> 01:46:55,730
you like to on our and you might say as well

1599
01:46:55,880 --> 01:47:00,830
i really want to make the semantic notion that we can do another the way

1600
01:47:00,830 --> 01:47:05,590
here is twenty proved that in fact solve some equation

1601
01:47:05,630 --> 01:47:09,810
or after the end of the differential question

1602
01:47:09,900 --> 01:47:15,060
and it short about the admission come

1603
01:47:15,090 --> 01:47:19,730
the result of documents which is the bad news is they

1604
01:47:19,750 --> 01:47:20,960
the question is

1605
01:47:20,960 --> 01:47:22,810
nine squared distances

1606
01:47:22,810 --> 01:47:28,210
are there specific needs to be said by the show the length of the shortest

1607
01:47:28,210 --> 01:47:31,400
path between two points is not permitted it

1608
01:47:31,420 --> 01:47:32,590
so as usual

1609
01:47:32,610 --> 01:47:34,460
perhaps you don't care

1610
01:47:34,480 --> 01:47:38,230
is that in practice you can get that

1611
01:47:39,360 --> 01:47:44,230
there is something i'm not sure about so don't which is present same way instead

1612
01:47:44,230 --> 01:47:50,400
of taking a financial motive for instance i can think function of the distance to

1613
01:47:50,400 --> 01:47:52,070
my knowledge

1614
01:47:52,090 --> 01:48:00,340
well i don't know my original noisy way to find function five this the

1615
01:48:00,360 --> 01:48:03,980
shorter distances was that

1616
01:48:03,980 --> 01:48:09,270
in the cases where you can find trails find is because it tells you i

1617
01:48:09,380 --> 01:48:11,670
think it is

1618
01:48:11,690 --> 01:48:17,290
so i would like know just to give you some phase also what is a

1619
01:48:17,340 --> 01:48:20,400
good solution here

1620
01:48:20,420 --> 01:48:21,980
let's forget about that

1621
01:48:21,980 --> 01:48:24,810
you don't need to say OK if you give me the right

1622
01:48:25,500 --> 01:48:30,790
instead of taking initial sparse let's go the right around the acacias and try to

1623
01:48:30,790 --> 01:48:36,030
hello from arizona state university to that

1624
01:48:36,100 --> 01:48:39,330
so that

1625
01:48:39,350 --> 01:48:42,660
what about what is the most of

1626
01:48:42,670 --> 01:48:44,490
it possible

1627
01:48:44,510 --> 01:48:45,520
this is

1628
01:48:45,530 --> 01:48:46,680
in the

1629
01:48:49,180 --> 01:48:51,110
well so what

1630
01:48:51,120 --> 01:48:55,510
so that the two explain

1631
01:48:55,520 --> 01:48:58,390
the support all that

1632
01:48:58,410 --> 01:49:01,470
thank you

1633
01:49:04,370 --> 01:49:09,980
this was the first to measure this time

1634
01:49:10,000 --> 01:49:17,280
OK with the stuff of trying to make a reason as to why i might

1635
01:49:17,280 --> 01:49:22,520
be in this group of people were talking to you about machine learning

1636
01:49:22,660 --> 01:49:26,760
and partly because i was looking at my CV and the number of machine learning

1637
01:49:26,760 --> 01:49:31,220
ICML paper i had in the last year or so has been zero if you

1638
01:49:31,220 --> 01:49:36,850
look at probably lost ten years it still zero so so i was trying to

1639
01:49:36,850 --> 01:49:41,350
figure out why is it that i would be interested in talking about this and

1640
01:49:41,350 --> 01:49:43,820
you know i should give you some reasons

1641
01:49:43,960 --> 01:49:47,370
that i believe in as to why you should listen to me and i was

1642
01:49:47,370 --> 01:49:51,840
thinking about why do we learn it and you know why do we learn of

1643
01:49:51,840 --> 01:49:57,740
these types of different knowledge are if you are anything like me it's probably because

1644
01:49:57,740 --> 01:50:03,740
you want to act in the world and improve your performance and so ultimately you

1645
01:50:05,180 --> 01:50:11,350
to be able to act and planning is intimately connected to that so

1646
01:50:11,400 --> 01:50:16,150
i will talk to you about so in a way i guess the whole reason

1647
01:50:16,150 --> 01:50:17,270
for learning

1648
01:50:17,310 --> 01:50:21,750
is to the planet and then to action in the world i'm sure somebody else

1649
01:50:21,750 --> 01:50:25,640
it's say a it's a different reason for learning but know today and tomorrow users

1650
01:50:25,640 --> 01:50:32,710
into so so i'm going to talk to you about learning and planning basically

1651
01:50:32,750 --> 01:50:39,160
the fourth of interesting work getting has been done and getting done in that intersection

1652
01:50:39,670 --> 01:50:44,200
so there's a bunch of acknowledgements here and i want read them model but you

1653
01:50:44,200 --> 01:50:46,660
know you can look at them up

1654
01:50:46,710 --> 01:50:51,840
let me see what are the aims of these lectures i think belfast i guess

1655
01:50:51,850 --> 01:50:56,180
it difficult to give you a feel for the exciting work that has been done

1656
01:50:56,180 --> 01:50:58,040
and he's being done

1657
01:50:58,080 --> 01:51:02,560
in the intersection of planning and learning over the past several years and at least

1658
01:51:02,560 --> 01:51:08,850
twenty years OK i should say up front that i'm not aiming to give you

1659
01:51:09,250 --> 01:51:14,750
a large saturday of a of every possible people that has been in this area

1660
01:51:14,800 --> 01:51:19,730
i have done that in fact they they put in a magazine that you can

1661
01:51:19,730 --> 01:51:22,670
get to you know from my from the

1662
01:51:22,690 --> 01:51:24,690
if you go to this particular

1663
01:51:24,740 --> 01:51:27,680
so space machine learning summer school

1664
01:51:27,690 --> 01:51:32,820
so this page from my page which has the people which you whose main the

1665
01:51:32,820 --> 01:51:37,020
main interest there is to give you a large

1666
01:51:37,040 --> 01:51:40,880
so we have all the kinds of work that's been done so you could look

1667
01:51:40,880 --> 01:51:45,470
at that there the additional resources which are much more of a summary i what

1668
01:51:45,470 --> 01:51:50,530
i would like to do is sort of an opinionated

1669
01:51:50,550 --> 01:51:55,380
sampling of the area to give you an idea of where i t plus interesting

1670
01:51:55,380 --> 01:51:59,340
stuff has been done with a view to seeing how much of that is likely

1671
01:51:59,350 --> 01:52:04,410
to be more and more relevant as new work is being done in this area

1672
01:52:07,290 --> 01:52:08,910
so the two ways to

1673
01:52:08,930 --> 01:52:14,460
view these lectures one of course as i said as an application area of machine

1674
01:52:14,460 --> 01:52:19,190
learning you know so as i said i think has applications go i guess planning

1675
01:52:19,220 --> 01:52:24,620
is right up there with the whole reason to do learning decision monasteries for learning

1676
01:52:24,910 --> 01:52:29,110
is to be able to improve your performance in acting in the real world

1677
01:52:29,130 --> 01:52:33,810
OK so in the end you view that your that way

1678
01:52:33,820 --> 01:52:38,110
in a sense i need to tell you mostly

1679
01:52:38,130 --> 01:52:43,070
about you know what sort of machine learning work has been done on the article

1680
01:52:43,070 --> 01:52:48,880
may not matter planning community of you know they have done work on improving the

1681
01:52:48,880 --> 01:52:51,730
performance of the plan also

1682
01:52:52,680 --> 01:52:57,650
the main knowledge itself you know also that two different pieces and then there's also

1683
01:52:57,650 --> 01:52:59,700
learning strategies which sort of

1684
01:52:59,750 --> 01:53:04,680
combine both learning the domain knowledge and improving the performance of the talk about this

1685
01:53:04,680 --> 01:53:09,450
more but to understand any of these i have to give you some very short

1686
01:53:09,450 --> 01:53:11,100
brief introduction to

1687
01:53:11,120 --> 01:53:14,440
the planning if i mean if you want to improve somebody want to figure out

1688
01:53:14,440 --> 01:53:17,100
where they are currently so i'll give you

1689
01:53:17,110 --> 01:53:22,800
like i guess one third after this talk properly is going to be just a

1690
01:53:22,800 --> 01:53:28,590
quick overview of real automated planning is mostly in as much as it is useful

1691
01:53:28,590 --> 01:53:30,870
to understand

1692
01:53:30,920 --> 01:53:33,730
the rest of the pictures which is you know how to improve the performance and

1693
01:53:33,730 --> 01:53:36,250
how to learn them manage one

1694
01:53:36,260 --> 01:53:40,670
OK so that's one way of looking at it another way of looking at these

1695
01:53:40,670 --> 01:53:44,170
lectures is the following which is that i was looking at the

1696
01:53:44,180 --> 01:53:49,850
a set of lectures that's been given in the last several days and i'm sure

1697
01:53:49,860 --> 01:53:50,760
will be given

1698
01:53:50,820 --> 01:53:56,230
some more days until the end of this week i and the reflect the fact

1699
01:53:56,230 --> 01:54:00,380
that machine learning as the discipline of it has been

1700
01:54:00,390 --> 01:54:06,670
doing certain types of has been focusing on certain types of machine learning algorithms and

1701
01:54:06,670 --> 01:54:09,680
if you look at the sorts of organisms that have been looked at the sorts

1702
01:54:09,680 --> 01:54:15,380
of problems that are being looked at currently and look in and compared what happens

1703
01:54:15,380 --> 01:54:21,030
in my lectures about the intersection of planning and learning is it two important things

1704
01:54:21,030 --> 01:54:24,500
one is much of the work that's been done in the intersection of planning and

1705
01:54:24,500 --> 01:54:30,400
learning has been sort of unabashedly knowledge-based OK you start with

1706
01:54:30,450 --> 01:54:36,520
quite a large amount of background knowledge and learning in the context of it now

1707
01:54:36,570 --> 01:54:39,920
there has been a tradition of this sort of you know learning in the context

1708
01:54:39,920 --> 01:54:44,550
of prior knowledge in machine learning and it's coming back into work but if you

1709
01:54:44,550 --> 01:54:49,320
look at much of the work that is an outsider looking in machine learning i

1710
01:54:49,320 --> 01:54:54,360
see that you know often times it looks almost as if we are apologizing that

1711
01:54:54,360 --> 01:54:57,470
we are using background knowledge about and if you want to use it you have

1712
01:54:57,470 --> 01:54:59,770
to convert into some sort of the kernel and then

1713
01:54:59,790 --> 01:55:00,710
let it too

1714
01:55:00,710 --> 01:55:02,530
but unlike and sparse coding model

1715
01:55:02,990 --> 01:55:05,090
here if i show you this new image

1716
01:55:05,690 --> 01:55:09,990
i can quickly tell you almost instantly tell you what are the features that make up this image

1717
01:55:10,400 --> 01:55:10,990
so i can do

1718
01:55:11,550 --> 01:55:13,010
recognition very quickly

1719
01:55:14,510 --> 01:55:19,030
and then you can represent again this new image in terms of its conditional conditional probability

1720
01:55:20,300 --> 01:55:21,300
in terms of learning

1721
01:55:21,820 --> 01:55:24,590
you know it's the same story as as in any type of

1722
01:55:25,030 --> 01:55:25,780
graphical model

1723
01:55:26,740 --> 01:55:28,970
if you give me a set of ideas images

1724
01:55:30,130 --> 01:55:32,110
then it turns out that if i wanna look and they

1725
01:55:32,720 --> 01:55:37,590
log-likelihood object if i can try to maximize the joint probability of observing all of these images

1726
01:55:38,150 --> 01:55:42,380
there is a regularization term typically people use i could be aligned could be able to

1727
01:55:43,900 --> 01:55:47,690
and then there's all the math behind these models not too difficult just a little

1728
01:55:47,690 --> 01:55:52,450
bit of algebra here but again you have the difference between these two expectations

1729
01:55:55,010 --> 01:55:55,300
you know

1730
01:55:55,900 --> 01:55:57,880
the first expectation is easy to compute

1731
01:55:59,650 --> 01:56:02,400
and it turns out that is that compute the reason why it's easy to compute

1732
01:56:02,860 --> 01:56:05,570
it is because when i look at the conditional probability

1733
01:56:06,240 --> 01:56:10,320
that's conditional probabilities is computed and then has to do with a particular structure

1734
01:56:10,860 --> 01:56:13,260
bipartite structure of of a

1735
01:56:13,490 --> 01:56:14,030
this model

1736
01:56:14,490 --> 01:56:18,360
this term is difficult to compute because you have to enumerate all possible images

1737
01:56:20,710 --> 01:56:23,360
and again that's spaces that space is exponential so

1738
01:56:23,800 --> 01:56:25,650
it's difficult to compute in one of

1739
01:56:26,170 --> 01:56:32,130
the most dominant approaches to compute these expectations is is is using markov chain monte carlo methods

1740
01:56:32,820 --> 01:56:34,090
right and it turns out

1741
01:56:34,570 --> 01:56:39,320
you know these insincere methods are fairly easy to to using these models factors before

1742
01:56:39,320 --> 01:56:42,920
this kind of model takes really five lines of my matlab code

1743
01:56:43,380 --> 01:56:45,510
five lines of arco to actually coded up

1744
01:56:47,130 --> 01:56:50,920
you know in terms of computing these expectations you can essentially do something that's called

1745
01:56:50,920 --> 01:56:55,360
deep sampling right it's a very commonly used markov chain monte carlo algorithm

1746
01:56:55,970 --> 01:56:57,650
and the way it works is that you start

1747
01:56:58,170 --> 01:57:00,820
at some configuration you started the data point

1748
01:57:01,340 --> 01:57:02,360
and then you just repeat

1749
01:57:02,800 --> 01:57:06,950
you compute the probability of observed data given the latent variables and then you can compute

1750
01:57:07,490 --> 01:57:09,470
the conditional probability of latent variables given

1751
01:57:11,920 --> 01:57:17,260
given the reconstructed data so just doing alternating gibbs sampling it's very easy to do you compute the hidden features

1752
01:57:17,720 --> 01:57:22,380
sample the hidden features given the data then reconstruct the data given the hidden features a new precedent

1753
01:57:23,150 --> 01:57:27,110
and that's exactly what i was showing you where when you saw these samples generating

1754
01:57:27,300 --> 01:57:31,940
right generate different sounds can generating different airplanes that's exactly the procedure that i was

1755
01:57:32,940 --> 01:57:33,860
and again

1756
01:57:34,280 --> 01:57:35,900
it's it's very easy to code

1757
01:57:38,340 --> 01:57:42,970
if you're looking at maximum likelihood learning for these models right you need to estimate

1758
01:57:42,970 --> 01:57:45,360
this expectation this is essentially how looks like right

1759
01:57:46,210 --> 01:57:46,530
and then

1760
01:57:47,210 --> 01:57:50,630
you know what the theory is values that's if you go all the way up

1761
01:57:50,650 --> 01:57:52,880
to infinity you'll get the true expectations

1762
01:57:54,970 --> 01:57:57,990
the markov chain will converge set eventually end

1763
01:57:59,260 --> 01:58:03,840
you'll get thee thee unbiased estimate of of these expectations

1764
01:58:04,360 --> 01:58:05,440
now in practice

1765
01:58:05,970 --> 01:58:11,510
people actually use something it's called contrastive divergence probably heard the term contrastive divergence um

1766
01:58:12,740 --> 01:58:15,820
and what it essentially does is you start with the training data

1767
01:58:16,300 --> 01:58:19,150
you update all the hidden units all the hidden variables

1768
01:58:19,690 --> 01:58:23,030
you update all the visible variables against you reconstruct the data

1769
01:58:23,550 --> 01:58:27,170
and then you update all the hidden variables again and then you just update the parameters by

1770
01:58:27,800 --> 01:58:30,690
by looking at the difference between these two expectations which you can compute

1771
01:58:31,510 --> 01:58:32,940
so you know it's kind of

1772
01:58:33,440 --> 01:58:36,920
an interesting algorithm it basically says instead running a markov chain all the way up

1773
01:58:36,920 --> 01:58:39,450
to infinity just running for one step that's all it says

1774
01:58:40,340 --> 01:58:43,090
and it turns out to be very useful learning algorithms very quick

1775
01:58:43,590 --> 01:58:45,530
it's biased but you can learn

1776
01:58:48,220 --> 01:58:48,900
good features

1777
01:58:50,470 --> 01:58:51,440
using this algorithm

1778
01:58:52,670 --> 01:58:57,010
now you can extend the beauty of these models is that you can extend these

1779
01:58:57,010 --> 01:59:01,650
multimodal all kinds of distributions right so for example if i if i'm interested in

1780
01:59:01,650 --> 01:59:02,510
modeling images

1781
01:59:03,130 --> 01:59:05,340
and i can think of images has been

1782
01:59:06,490 --> 01:59:08,820
as vectors in real valued space

1783
01:59:09,970 --> 01:59:12,400
and then with a little bit of modification of of

1784
01:59:12,990 --> 01:59:15,550
my energy functional my probability here right

1785
01:59:15,970 --> 01:59:17,920
again you have pairwise you have you

1786
01:59:18,450 --> 01:59:19,990
pairwise terms you have know terms

1787
01:59:20,440 --> 01:59:21,340
it turns out that just

1788
01:59:22,150 --> 01:59:25,900
this simple modification in the code you can actually learn meaningful features from images

