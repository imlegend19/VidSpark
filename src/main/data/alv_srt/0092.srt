1
00:00:00,000 --> 00:00:06,230
so we have to be really careful but in this case it doesn't matter

2
00:00:06,250 --> 00:00:10,790
any questions about the substitution method that was the same example three times in the

3
00:00:10,790 --> 00:00:13,500
end it turned out we got the right answer

4
00:00:13,500 --> 00:00:16,290
but we still need to know the answer in order to find out which is

5
00:00:16,290 --> 00:00:17,600
a bit of a pain

6
00:00:17,660 --> 00:00:19,560
so it would be nicer to just

7
00:00:19,580 --> 00:00:22,290
figure out the answer by some procedure and that will be

8
00:00:22,310 --> 00:00:26,520
the next two techniques we talk about

9
00:00:27,980 --> 00:00:31,390
how would you improve lower bound i haven't tried it for this

10
00:00:31,410 --> 00:00:34,940
recurrence but you should be able to do exactly the same form

11
00:00:34,960 --> 00:00:37,940
i have argued that human is greater than equal to

12
00:00:37,960 --> 00:00:43,060
c one times t m squared minus two times and

13
00:00:43,080 --> 00:00:49,060
i didn't check whether that particular form work but i think it does

14
00:00:49,060 --> 00:00:50,770
so try it

15
00:00:51,790 --> 00:00:55,460
these other methods will give you in some sense upper and lower bounds

16
00:00:55,460 --> 00:00:56,810
a little bit careful

17
00:00:56,830 --> 00:01:00,460
but to really check things you pretty much have to do the substitution method

18
00:01:00,480 --> 00:01:05,180
and you'll get some practice with that usually we only care about upper bounds so

19
00:01:05,180 --> 00:01:08,980
big i mean for like this is what will focus on but occasionally we lower

20
00:01:10,520 --> 00:01:12,890
so it's nice to know that you have the right answer by giving a matching

21
00:01:12,890 --> 00:01:14,120
lower bound

22
00:01:14,140 --> 00:01:22,140
so the next method we'll will talk about

23
00:01:23,930 --> 00:01:25,750
the recursion tree method

24
00:01:25,750 --> 00:01:30,040
and it's a particular way of of adding up recurrence

25
00:01:30,060 --> 00:01:34,080
and it's the it's my favorite way it's particularly

26
00:01:34,520 --> 00:01:39,270
it is usually just works that's the great thing about it provides intuition for free

27
00:01:39,290 --> 00:01:41,500
it tells you what the answer is pretty much

28
00:01:41,500 --> 00:01:45,200
it's slightly non rigorous this is a bit of a pain so you have to

29
00:01:45,200 --> 00:01:48,640
be really careful when you apply otherwise you might get the wrong answer

30
00:01:48,660 --> 00:01:52,230
because it involves dot dot dots are favourite

31
00:01:52,230 --> 00:01:56,460
three characters have but that that so is a little bit

32
00:01:56,480 --> 00:01:57,600
rigorous so

33
00:01:57,600 --> 00:01:58,560
be careful

34
00:01:58,580 --> 00:02:02,160
technically what you should do is to find out what the answer is with recursion

35
00:02:02,160 --> 00:02:05,770
tree method and prove that it's actually write with the substitution

36
00:02:06,770 --> 00:02:10,430
usually that's not necessary but you should at least have in your mind that

37
00:02:10,460 --> 00:02:12,120
is required to rigorously

38
00:02:12,140 --> 00:02:15,600
and probably the first you requested yourself you should do it that way when you

39
00:02:15,600 --> 00:02:19,290
really understand the recursion tree method you can be a little bit more

40
00:02:19,290 --> 00:02:22,210
sloppy you really sure you have the right answer so

41
00:02:23,000 --> 00:02:26,120
do an example we saw recursion tree is very briefly

42
00:02:26,140 --> 00:02:33,390
last time with our sort as the intuition y with n log n

43
00:02:33,480 --> 00:02:37,460
and the sixth i mean if you took an example like the one we just

44
00:02:37,460 --> 00:02:40,120
do with the recursion tree method is dead simple

45
00:02:40,120 --> 00:02:42,210
so let's just to make her life harder

46
00:02:42,230 --> 00:02:46,750
let's do more complicated recursion so here we imagine we have some algorithm starts the

47
00:02:46,750 --> 00:02:51,350
problem size and it recursively solve the problem of size n over four then recursively

48
00:02:51,350 --> 00:02:55,230
solve the problem size and over two dozen square work on the side

49
00:02:55,250 --> 00:02:59,060
with without non and non recursive work

50
00:02:59,060 --> 00:03:03,310
so what is that i mean that's a bit less obvious i would say

51
00:03:03,370 --> 00:03:07,060
so what we're going to do is

52
00:03:07,060 --> 00:03:10,770
drop picture

53
00:03:10,790 --> 00:03:14,000
i've had which is going to expand the recursion

54
00:03:14,020 --> 00:03:15,710
in tree form

55
00:03:15,810 --> 00:03:27,830
and then just and everything out

56
00:03:27,910 --> 00:03:31,500
so we want the general picture

57
00:03:31,520 --> 00:03:35,160
and the general principle in

58
00:03:35,180 --> 00:03:38,810
recursion tree method is we just draw this is the picture we say well to

59
00:03:38,810 --> 00:03:39,870
you then

60
00:03:42,930 --> 00:03:46,440
the sum of n square TNN over four

61
00:03:47,020 --> 00:03:49,810
and over two

62
00:03:49,910 --> 00:03:54,160
so i mean this is we're we're writing a someone to write it that way

63
00:03:54,180 --> 00:03:57,410
OK this is going to be a tree

64
00:03:57,410 --> 00:04:01,560
and it's going to be retrieved by expand by recursively expanding each of these two

65
00:04:01,580 --> 00:04:06,850
lee so i start by rick expanded to this i keep expanding expanding expanding everything

66
00:04:07,120 --> 00:04:09,040
so let's go one more step

67
00:04:09,060 --> 00:04:14,330
we have this square here over four

68
00:04:14,410 --> 00:04:19,750
you an over two weeks spent one more time this is going to be in

69
00:04:19,750 --> 00:04:22,020
is actually good

70
00:04:22,080 --> 00:04:27,700
the basic on the basis of this proposition independence most you're looking at the two

71
00:04:27,720 --> 00:04:29,120
hundred and its

72
00:04:29,180 --> 00:04:33,470
and because of optically if the URI then through it in looking at things and

73
00:04:33,470 --> 00:04:36,410
this transform from two set of posts in something

74
00:04:36,430 --> 00:04:40,870
and when i look at the real digit or something you dream of digital to

75
00:04:40,910 --> 00:04:45,950
another letter or arabic letters whatever they follow the same transformation

76
00:04:46,000 --> 00:04:48,520
and in the first place you have and

77
00:04:48,580 --> 00:04:50,500
they're going to be on the same

78
00:04:51,140 --> 00:04:54,000
and is still going to describe the domain

79
00:04:54,000 --> 00:04:57,120
the thing the the examples you dreamed a lot more

80
00:04:58,040 --> 00:05:01,140
the examples you actually of from the facts

81
00:05:01,160 --> 00:05:03,790
the fact that you can use exemplars you dream

82
00:05:03,790 --> 00:05:06,040
is quite amusing

83
00:05:06,100 --> 00:05:09,970
so we called them unicorns because nothing you dream this for

84
00:05:10,000 --> 00:05:14,810
helping children understand what the truth is

85
00:05:15,040 --> 00:05:24,910
how important it is

86
00:05:35,850 --> 00:05:42,020
the funny thing is nothing really bad happens but

87
00:05:42,060 --> 00:05:43,310
if you know

88
00:05:43,330 --> 00:05:44,790
the points you have

89
00:05:44,850 --> 00:05:48,410
actually examples of the two classes the opposite you like to do

90
00:05:48,500 --> 00:05:51,750
you would like to put them squarely into one class or the although not in

91
00:05:54,470 --> 00:06:20,620
if you have a few of them is not going to bother you with all

92
00:06:20,720 --> 00:06:23,950
if you only have that you could be in trouble again

93
00:06:25,220 --> 00:06:26,700
we almost always

94
00:06:26,700 --> 00:06:29,160
we don't know so much

95
00:06:29,810 --> 00:06:33,350
we we had this concept met some experiments showed that

96
00:06:33,350 --> 00:06:36,270
well the concept that the quite powerful

97
00:06:36,290 --> 00:06:39,870
we understand that just do repositories tradition spaces

98
00:06:39,870 --> 00:06:43,680
and we understand this relation with the motion of bodies priori and that's about it

99
00:06:43,680 --> 00:06:45,520
for now

100
00:06:45,540 --> 00:06:48,410
the amazing thing is that this is

101
00:06:48,470 --> 00:06:52,020
quite innovative provocative ideas so

102
00:06:52,040 --> 00:06:58,040
now i would say

103
00:07:15,980 --> 00:07:18,200
if you have a semi supervised the

104
00:07:18,200 --> 00:07:22,000
you have an additional set of examples of the liabilities

105
00:07:22,000 --> 00:07:25,180
you know there are five or eight

106
00:07:25,220 --> 00:07:27,160
the examples of the class

107
00:07:27,180 --> 00:07:29,770
the class is seeking the point is

108
00:07:29,810 --> 00:07:32,220
but you don't know the label

109
00:07:32,390 --> 00:07:35,910
and that means that intuitively what you want to do with this example

110
00:07:35,930 --> 00:07:39,620
is not to place them in the middle of your separation

111
00:07:39,640 --> 00:07:45,330
but place them squarely into the red side of the inside

112
00:07:45,680 --> 00:07:49,970
the a level exam the quite close to the test examples here

113
00:07:49,970 --> 00:07:51,830
see the green one

114
00:07:51,850 --> 00:07:56,620
there is some relations between transitions and since the right

115
00:07:59,000 --> 00:08:04,080
what this tries to do is try to find a good separation between the

116
00:08:04,080 --> 00:08:06,040
i love the example you want to

117
00:08:06,040 --> 00:08:08,180
classifiers for us

118
00:08:08,200 --> 00:08:10,310
green right

119
00:08:10,350 --> 00:08:14,160
so you will have to them nicely

120
00:08:14,220 --> 00:08:15,660
it's the opposite way

121
00:08:15,680 --> 00:08:19,350
so you could consider you have a full set of examples

122
00:08:19,370 --> 00:08:22,120
from training examples of one class

123
00:08:22,180 --> 00:08:24,160
training examples of the other class

124
00:08:24,640 --> 00:08:27,330
the training examples all of them

125
00:08:27,370 --> 00:08:31,620
the best patterns which are the one that you're going to test this one

126
00:08:31,700 --> 00:08:34,970
the one for which she won answers

127
00:08:35,020 --> 00:08:38,350
the unlabelled examples of the classes

128
00:08:39,330 --> 00:08:40,750
the class of interest

129
00:08:40,770 --> 00:08:43,160
another five hundred states

130
00:08:43,180 --> 00:08:46,890
and the universe some examples of how should belong to the class

131
00:08:46,910 --> 00:08:49,390
but help you define the geometry

132
00:08:49,450 --> 00:08:53,620
of the problem in the high dimensional space

133
00:08:53,680 --> 00:08:56,390
and in fact that it is the way we implemented if we try to play

134
00:08:56,390 --> 00:08:58,600
with these

135
00:08:58,750 --> 00:09:06,660
it behaves pretty much the way i tell you just don't have the numbers here

136
00:09:07,580 --> 00:09:13,290
i'm going to go to another idea letting it called as the employees for purely

137
00:09:13,290 --> 00:09:23,700
local reasons

138
00:09:23,720 --> 00:09:29,830
the wonder

139
00:09:29,870 --> 00:09:35,500
explain the learning algorithm for maximum number of contradiction that some approximations

140
00:09:35,500 --> 00:09:37,020
because counting

141
00:09:37,040 --> 00:09:39,250
born that contradiction no

142
00:09:39,290 --> 00:09:42,700
so what we said is that we took as the cost

143
00:09:42,720 --> 00:09:46,000
transitivity in because we trust them

144
00:09:46,040 --> 00:09:48,250
that the norm of the w square

145
00:09:48,290 --> 00:09:50,200
but that is the

146
00:09:50,200 --> 00:09:53,040
the soft margin down

147
00:09:53,080 --> 00:09:57,920
that is the transitive them physically was saying that we would like

148
00:09:58,700 --> 00:10:01,060
testing but tends to be far away

149
00:10:01,080 --> 00:10:02,920
from the hyperplane

150
00:10:03,000 --> 00:10:05,980
we basically the testing about things we want them to be squarely

151
00:10:06,040 --> 00:10:08,090
in one class of the order

152
00:10:08,130 --> 00:10:13,800
and we added another ten cost function which say the universe poland should be

153
00:10:13,850 --> 00:10:15,210
as much as possible

154
00:10:15,220 --> 00:10:19,220
it in the kind of highway between the two

155
00:10:20,670 --> 00:10:23,120
close to the separating hyperplane

156
00:10:23,170 --> 00:10:24,830
for each of these terms

157
00:10:24,840 --> 00:10:28,630
as of course politicians so hyperparameters

158
00:10:28,630 --> 00:10:33,520
and then we may cross validation experiments to find the right way to do it

159
00:10:33,600 --> 00:10:36,370
and optimize optimisation is just

160
00:10:36,380 --> 00:10:37,830
i is

161
00:10:37,880 --> 00:10:39,720
he is

162
00:10:39,770 --> 00:10:43,210
the most difficult part is not the universal which is convex

163
00:10:43,220 --> 00:10:50,300
it's the semi supervised because it makes nonconvexity

164
00:10:50,350 --> 00:10:53,740
the thing is that if i go to explain this in the there's going mad

165
00:10:53,740 --> 00:11:07,840
then to be long

166
00:11:07,850 --> 00:11:11,210
it's been applied to all the elements i just don't have the mind images and

167
00:11:11,480 --> 00:11:12,710
it is here

168
00:11:13,840 --> 00:11:15,670
it was the talking

169
00:11:15,850 --> 00:11:21,520
apply to all of them and place some speech recognition problem some fixed classifications with

170
00:11:21,520 --> 00:11:24,560
the same kind of results

171
00:11:24,620 --> 00:11:28,750
so you can dream up official text mining syntax together

172
00:11:28,800 --> 00:11:32,010
they belong to the class because the not complete nonsense

173
00:11:32,090 --> 00:11:34,630
but still this nonsense text that you

174
00:11:34,670 --> 00:11:35,760
i understand

175
00:11:35,770 --> 00:11:37,590
meaningful text

176
00:11:37,620 --> 00:11:40,920
you can do the same thing from some speech recognition problem

177
00:11:40,970 --> 00:11:44,580
this is not the did you think it this is known to work in many

178
00:11:52,300 --> 00:11:53,790
yes there is

179
00:11:54,740 --> 00:11:55,680
i think you should

180
00:11:56,890 --> 00:12:00,010
if you go to

181
00:12:00,040 --> 00:12:08,290
do you can show you now

182
00:12:08,300 --> 00:12:14,270
maybe i should tell you to likely to the key this

183
00:12:14,290 --> 00:12:18,130
there is a cyclical universe university

184
00:12:18,180 --> 00:12:23,210
that has been written by fabian scenes including and he was end

185
00:12:23,300 --> 00:12:26,040
the two years ago

186
00:12:26,040 --> 00:12:30,490
but when binds to apply to serve surface it does so

187
00:12:30,500 --> 00:12:32,400
with carbon down

188
00:12:32,440 --> 00:12:37,330
the oxygen and and there's a really good reason for why that is the case

189
00:12:37,330 --> 00:12:41,530
then we'll talk about that when we talk about molecular orbitals

190
00:12:42,150 --> 00:12:45,300
what you see here is really just the top

191
00:12:45,350 --> 00:12:50,190
of the CEO molecules using the oxygen and of the CEO

192
00:12:50,430 --> 00:12:53,230
all right well

193
00:12:53,250 --> 00:12:54,850
this image here

194
00:12:54,860 --> 00:13:00,020
it was taken by something called a scanning tunneling microscope

195
00:13:01,560 --> 00:13:02,780
i'm sorry to say

196
00:13:02,790 --> 00:13:06,960
but this is actually invented just before you were born

197
00:13:06,970 --> 00:13:13,470
and it was invented was actually work done by who's got affected by the again

198
00:13:14,530 --> 00:13:18,230
and they are the nobel prize for their work

199
00:13:18,280 --> 00:13:22,760
and the way the technique works is the following

200
00:13:22,770 --> 00:13:24,580
let me the

201
00:13:24,600 --> 00:13:27,650
four life here so there

202
00:13:27,680 --> 00:13:32,500
i'm going to do some more work

203
00:13:38,190 --> 00:13:43,320
what you do to to do this is that you take a tungsten wires

204
00:13:43,330 --> 00:13:48,260
and you do a little chemistry with the tungsten wire down to is finite set

205
00:13:48,260 --> 00:13:49,720
as you can

206
00:13:49,740 --> 00:13:51,220
so you want

207
00:13:51,270 --> 00:13:56,390
but it in some potassium hydroxide and current with the will can

208
00:13:58,010 --> 00:14:02,060
and what you find is a predefined

209
00:14:02,140 --> 00:14:05,920
at the end of this tungsten wires

210
00:14:05,930 --> 00:14:07,450
but what you do

211
00:14:07,470 --> 00:14:10,020
if you take that tungsten wire

212
00:14:10,150 --> 00:14:12,130
and you can actually

213
00:14:12,150 --> 00:14:16,460
two a ph so electric crystal

214
00:14:16,480 --> 00:14:20,550
so i piezoelectric crystal and let me

215
00:14:20,600 --> 00:14:22,710
put it this way here

216
00:14:26,370 --> 00:14:28,320
is some material

217
00:14:28,440 --> 00:14:33,750
if you put of voltage across that it expands a little bit

218
00:14:33,760 --> 00:14:35,180
a little bit means

219
00:14:35,180 --> 00:14:36,930
and twenty years from

220
00:14:38,430 --> 00:14:40,480
and so on now

221
00:14:40,590 --> 00:14:44,290
you would have done some wire to the material you have some

222
00:14:44,330 --> 00:14:47,240
control on the angstrom scales

223
00:14:47,330 --> 00:14:50,040
that's what happens

224
00:14:50,040 --> 00:14:53,080
in order to move the active around

225
00:14:53,940 --> 00:14:54,830
what you do

226
00:14:54,850 --> 00:14:56,720
if you take this test

227
00:14:56,720 --> 00:15:03,060
and you bring it say within five angstroms of the top of the socio

228
00:15:03,950 --> 00:15:06,650
i plan for this year

229
00:15:07,660 --> 00:15:11,060
my carbon here and here's my oxygen

230
00:15:11,770 --> 00:15:13,370
well i'm going to do

231
00:15:13,390 --> 00:15:16,370
it is i'm going to bring this this

232
00:15:16,450 --> 00:15:19,590
say within five storms here

233
00:15:21,570 --> 00:15:22,570
oxygen and

234
00:15:22,600 --> 00:15:25,860
of the CEO molecules

235
00:15:25,880 --> 00:15:28,080
OK and that

236
00:15:28,080 --> 00:15:29,760
i'm going to put it

237
00:15:29,780 --> 00:15:34,340
a negative voltage and this constant here

238
00:15:34,390 --> 00:15:39,450
and i'm going to take the city of molecule in the attic flat surface and

239
00:15:39,450 --> 00:15:41,310
ground it

240
00:15:41,360 --> 00:15:43,740
now in this country

241
00:15:43,790 --> 00:15:49,260
there are course of electrons instances of that all the metal has the characteristic were

242
00:15:49,260 --> 00:15:54,000
some of those electrons strongly bound to the nucleus

243
00:15:54,010 --> 00:15:55,170
are actually

244
00:15:56,110 --> 00:15:59,270
they actually run around the solid

245
00:16:02,200 --> 00:16:05,010
this is a high energy state

246
00:16:05,010 --> 00:16:12,000
in that electrons are negatively charged and their environment the negative potential that's rather high

247
00:16:12,000 --> 00:16:13,540
energy state

248
00:16:13,590 --> 00:16:15,500
for those electrons

249
00:16:16,950 --> 00:16:18,370
if i were to draw

250
00:16:18,380 --> 00:16:20,850
energy level by william here

251
00:16:20,870 --> 00:16:22,830
this is energy

252
00:16:22,830 --> 00:16:27,120
and i'm going to represent those electrons and the tungsten

253
00:16:27,140 --> 00:16:28,560
going to be at some

254
00:16:29,160 --> 00:16:30,310
energy level

255
00:16:30,370 --> 00:16:33,910
this is the electrons in the tungsten

256
00:16:34,230 --> 00:16:39,920
however there are also some electrons down here in are at ground potential so these

257
00:16:39,920 --> 00:16:43,080
electrons that are more energy

258
00:16:43,080 --> 00:16:45,050
so i'm going to

259
00:16:45,100 --> 00:16:52,250
draw energy level representing that the electrons and the planet right so this score years

260
00:16:52,920 --> 00:16:57,770
distance right this is the distance this of the electrons at the time spent over

261
00:16:57,770 --> 00:17:01,350
here the electrons and the planned

262
00:17:03,510 --> 00:17:06,370
these electrons and the constant

263
00:17:06,390 --> 00:17:07,860
i would like to to do

264
00:17:07,880 --> 00:17:12,870
it is to be here on the planet is that the lower energy state

265
00:17:12,890 --> 00:17:14,830
but the problem is

266
00:17:14,860 --> 00:17:18,620
there's a gap right here there's no material

267
00:17:18,630 --> 00:17:23,820
between the tip and the oxygen and the CEO molecule

268
00:17:23,850 --> 00:17:28,160
there's if you do it in a few areas of the accu here there's vacuum

269
00:17:28,160 --> 00:17:29,900
few here

270
00:17:30,060 --> 00:17:34,140
so for the electrons to go from here to here

271
00:17:34,180 --> 00:17:37,020
well there's a very large areas

272
00:17:37,020 --> 00:17:43,280
this is a very high energy state of an electron in between this guy here

273
00:17:43,290 --> 00:17:45,220
so i'm going to represent

274
00:17:45,260 --> 00:17:47,450
the energy change

275
00:17:47,480 --> 00:17:53,550
as electrons flow from the tungsten tip to the tongues to the land surface and

276
00:17:53,550 --> 00:17:55,630
is the area like this

277
00:17:55,690 --> 00:17:58,090
right very large periods

278
00:17:58,100 --> 00:18:04,950
now you've seen these kinds of i call the reaction coordinates in chemistry in high

279
00:18:06,080 --> 00:18:08,690
that is you SC

280
00:18:08,720 --> 00:18:13,130
situations where we might here we're doing energy level diagram

281
00:18:13,160 --> 00:18:14,370
this is the energy

282
00:18:14,390 --> 00:18:19,380
of the reactants out here this the energy of the product

283
00:18:19,390 --> 00:18:21,330
right and in between

284
00:18:21,340 --> 00:18:25,110
is the activation energy barrier that we call

285
00:18:27,170 --> 00:18:28,340
in other words

286
00:18:28,360 --> 00:18:31,800
you have to put energy into the system

287
00:18:31,800 --> 00:18:36,350
in order to get the reaction to go put energy into the system

288
00:18:36,400 --> 00:18:39,670
before you get any energy out

289
00:18:39,680 --> 00:18:45,590
right that's what happens in a chemical reaction point energy stop before we make the

290
00:18:45,590 --> 00:18:46,590
and the speed to

291
00:18:48,500 --> 00:18:50,770
as with the beginning of the last talk i was

292
00:18:51,640 --> 00:18:52,440
i noted that

293
00:18:53,160 --> 00:18:58,820
the masters to make the talk accessible to a broad audience yet when i get here i find that fourteen

294
00:18:59,250 --> 00:19:01,920
experts very mathematically technically skilled

295
00:19:02,480 --> 00:19:04,750
who probably know more about this field and i did so

296
00:19:05,210 --> 00:19:05,940
what i'm gonna do

297
00:19:06,660 --> 00:19:10,160
is given my broad general talk that do it very very quickly

298
00:19:10,930 --> 00:19:12,610
and hopefully that will sort of engaging

299
00:19:14,740 --> 00:19:18,970
i got materials so perhaps if you could stop me about ten minutes before the

300
00:19:18,970 --> 00:19:21,040
end will just stop there and then sort

301
00:19:22,790 --> 00:19:23,750
deal with questions i

302
00:19:27,080 --> 00:19:28,350
big right hand

303
00:19:30,230 --> 00:19:31,070
what i'm going to do

304
00:19:31,520 --> 00:19:33,900
today is then taken through

305
00:19:36,510 --> 00:19:42,000
an approach to try and understand biological systems and in particular the brain my perspective

306
00:19:42,390 --> 00:19:44,320
comes from neuroscience and using

307
00:19:45,890 --> 00:19:51,300
statistical and information theoretic approaches to understand the structure and function of the brain

308
00:19:53,670 --> 00:19:57,360
i should also say that you're not going to learn anything from listening to me

309
00:19:57,360 --> 00:20:00,320
so you don't really have to remember very much in the sense that

310
00:20:00,770 --> 00:20:02,160
the idea of the ambition

311
00:20:02,630 --> 00:20:04,780
it's a develop a general framework that

312
00:20:05,890 --> 00:20:10,640
provides a number of contacts for many things which are only understood both in terms

313
00:20:10,720 --> 00:20:16,240
of neural computations andin terms in machine learning so i repeat the idea here is

314
00:20:16,240 --> 00:20:18,170
really it to try contextualize

315
00:20:18,600 --> 00:20:21,770
hand unify at a lot of existing work

316
00:20:23,010 --> 00:20:24,180
the approach take

317
00:20:24,760 --> 00:20:30,860
borrows heavily from the notion that be helmholtz machine from geoffrey hinton peter diane and

318
00:20:30,880 --> 00:20:33,130
appeals time and time again today

319
00:20:34,010 --> 00:20:35,730
the writings helmholtz here

320
00:20:36,560 --> 00:20:38,930
in essence what i'm gonna be talking about is

321
00:20:39,290 --> 00:20:41,750
a perspective on brain function

322
00:20:43,160 --> 00:20:43,790
that the

323
00:20:45,790 --> 00:20:52,760
the brain has making inferences about the causes of its sensory inputs and this is my articulated by helmholtz

324
00:20:53,280 --> 00:20:57,400
in this class of objects are always imagined as being present in the field of vision

325
00:20:57,950 --> 00:20:58,830
doesn't have to be there

326
00:20:59,310 --> 00:21:02,280
in order to produce the same impression on the nervous system

327
00:21:04,290 --> 00:21:04,610
i'm going

328
00:21:05,350 --> 00:21:07,480
uses these advances

329
00:21:07,950 --> 00:21:13,360
brought the table by people like jeff and d from statistical physics which refinement formalize notion

330
00:21:13,870 --> 00:21:18,280
in terms of a variational bayes and the minimization of variational free energy

331
00:21:18,730 --> 00:21:23,400
and just provide u with a few examples depending on how much time

332
00:21:23,900 --> 00:21:27,590
so we have to show the sorts of things that we are all that can

333
00:21:27,590 --> 00:21:30,260
be simulated on the this general approach

334
00:21:32,390 --> 00:21:36,120
i normally start of this talk for a general audience by asking them

335
00:21:36,550 --> 00:21:37,290
to think about

336
00:21:37,950 --> 00:21:40,940
in particular question which underlies the motivation

337
00:21:41,450 --> 00:21:43,330
for this this approach

338
00:21:44,190 --> 00:21:50,590
and the question is what's the difference between a biological self organizing system and non-biological

339
00:21:50,910 --> 00:21:52,700
self organizing system and in particular

340
00:21:53,710 --> 00:21:55,630
let's hear about what's the difference between

341
00:21:56,080 --> 00:21:56,890
a snowflake

342
00:21:57,290 --> 00:21:59,210
and the bird in the sense that they're both

343
00:22:02,300 --> 00:22:05,630
thermodynamic systems that both shallots cells in the structure

344
00:22:07,440 --> 00:22:10,250
they are characteristically different in one aspect

345
00:22:11,510 --> 00:22:14,540
and i normally ask the audience just to try and think about what is a

346
00:22:14,540 --> 00:22:19,620
crucial difference between a biological system and a biological system that we have any answers

347
00:22:30,700 --> 00:22:31,670
for me there is you

348
00:22:32,100 --> 00:22:33,110
yeah am

349
00:22:33,300 --> 00:22:36,390
i'm assuming that there is a strong agent shot boundary

350
00:22:37,820 --> 00:22:45,180
because the answer is is actually on on the slightly amsterdam's great before anyway so it's somewhat arbitrary question at

351
00:22:45,690 --> 00:22:46,890
having thought about it

352
00:22:47,970 --> 00:22:50,720
there one only ends up with with thee

353
00:22:51,400 --> 00:22:55,850
conclusion that the only thing that distinguishes a biological from a non biological system

354
00:22:56,510 --> 00:22:58,920
is that the biological system can physically move

355
00:22:59,610 --> 00:23:00,080
two ri

356
00:23:00,670 --> 00:23:08,190
configure its relationship to the environment or to change the samples of the outcomes from the environment

357
00:23:10,480 --> 00:23:11,310
and they could be

358
00:23:12,790 --> 00:23:18,550
yeah well phototropic behavior i think would be a nice example of physically moving to resample

359
00:23:19,080 --> 00:23:20,680
those aspects to be violent

360
00:23:22,650 --> 00:23:23,430
and is like

361
00:23:28,360 --> 00:23:31,920
or even a single cell organisms so showing showing chemotaxis

362
00:23:32,510 --> 00:23:33,400
so yes so

363
00:23:34,580 --> 00:23:38,330
changing one's configuration over very slow time are very very quickly

364
00:23:39,040 --> 00:23:39,580
in a way

365
00:23:40,120 --> 00:23:44,900
that is a function of the system in all the cells selective sampling importance

366
00:23:46,070 --> 00:23:49,390
you know of symmetry example would be that would deploy its

367
00:23:51,660 --> 00:23:53,770
leaves in order to get a sample

368
00:23:55,540 --> 00:23:57,210
light from the sun for example

369
00:24:03,530 --> 00:24:08,750
yeah i will be very particular to each sort of integrity sort biological system absolutely yeah but

370
00:24:11,800 --> 00:24:12,980
she you so

371
00:24:18,520 --> 00:24:22,780
yes so there wouldn't be would be an example a non biological systems so what's

372
00:24:22,780 --> 00:24:25,340
the quite essential difference the water doesn't actually

373
00:24:25,750 --> 00:24:26,570
move uphill

374
00:24:27,660 --> 00:24:29,850
maintain it sort of a

375
00:24:29,890 --> 00:24:31,410
the states that could occupy

376
00:24:32,040 --> 00:24:36,980
so the idea here is that the biological systems will try to avoid

377
00:24:37,490 --> 00:24:41,360
crossing phase boundaries or more particularly the try to retain

378
00:24:41,900 --> 00:24:42,750
the physical

379
00:24:43,300 --> 00:24:50,220
physiological or any sort of state you can imagine within particular bounds so put another way in so doing

380
00:24:50,740 --> 00:24:58,360
they will avoid particular boundaries in particular phase transitions so that i don't die auditory doesn't desiccate

381
00:24:59,540 --> 00:25:02,520
although the water doesn't drip into the drainage system

382
00:25:03,450 --> 00:25:05,310
so the idea is that they simply move

383
00:25:05,720 --> 00:25:08,810
in order to keep the physical states within particular bounds

384
00:25:09,290 --> 00:25:14,210
also put expressed mathematically in terms the shannon entropy that will try to minimize

385
00:25:14,700 --> 00:25:15,520
the shannon entropy

386
00:25:16,090 --> 00:25:19,040
all minimize dispersion states that they could occupy

387
00:25:19,690 --> 00:25:22,640
that's basically what i'm going to be using to motivate the survey

388
00:25:23,280 --> 00:25:24,550
the minimization of free

389
00:25:25,370 --> 00:25:31,140
so support very simply always saying is that biological systems physically move

390
00:25:31,140 --> 00:25:33,580
particular there may be a face

391
00:25:35,710 --> 00:25:41,760
then there is also the work here by civilian collaborators in which they try to

392
00:25:41,760 --> 00:25:46,760
model the object of relationships justin segmentations of the take an image this segment this

393
00:25:46,760 --> 00:25:52,640
image from this is segment they try to the create set of possible object labels

394
00:25:52,640 --> 00:25:58,500
that may exist there have been some object appearance models of the on examples of

395
00:25:58,540 --> 00:25:59,790
now there

396
00:25:59,810 --> 00:26:04,020
possible labels that my account for the different features of cells in each segment

397
00:26:04,020 --> 00:26:07,710
and then they will have a kind of a CNF on of it in order

398
00:26:07,740 --> 00:26:11,120
to decide what are the final labels

399
00:26:11,160 --> 00:26:15,430
and the way they do it here is the one that they try to model

400
00:26:15,430 --> 00:26:19,130
the full joint high so what they're going to do is to model

401
00:26:19,200 --> 00:26:24,390
the joint just separate of all possible pairwise comparisons at all possible locations

402
00:26:24,400 --> 00:26:27,040
so that's just an upper approximation

403
00:26:27,060 --> 00:26:32,990
but if they show that also improves performance is in this case

404
00:26:33,020 --> 00:26:36,820
they are going to have this prior you they all the labels all the segments

405
00:26:36,820 --> 00:26:41,870
and they are just modelling by dino all possible pairwise comparisons all the possible level

406
00:26:45,000 --> 00:26:48,140
this is another piece of work in which

407
00:26:48,160 --> 00:26:51,640
another thing that is very interesting is that most of the time in images what

408
00:26:51,640 --> 00:26:55,960
is important in order to learn consistent of relationships is not a relationship between this

409
00:26:56,200 --> 00:27:01,010
object is the fact that you have a monitor chair but the relationship between staff

410
00:27:01,030 --> 00:27:06,680
and objects stuff is what makes most of the scene led the ground grass sky

411
00:27:06,690 --> 00:27:08,120
buildings walls

412
00:27:08,130 --> 00:27:12,060
does this stuff is more or less what it buys more like ninety percent of

413
00:27:12,060 --> 00:27:15,840
the image and the discrete objects that appear from time to time

414
00:27:15,870 --> 00:27:19,580
so in this piece of what they try to do is to model stuff

415
00:27:19,580 --> 00:27:24,560
and stuff in order to give context information for which of disrespect to be on

416
00:27:24,560 --> 00:27:25,400
the image

417
00:27:26,370 --> 00:27:31,530
so what is interesting is that they don't use labels in order to train

418
00:27:31,560 --> 00:27:35,370
but this stuff is so they dont have training data for so you know this

419
00:27:35,370 --> 00:27:38,890
is sky or graphs and so on what they do is they

420
00:27:38,910 --> 00:27:44,330
segmentation of the image they like a bunch of segments with a computer features from

421
00:27:44,330 --> 00:27:47,400
the segments and they they apply k means for instance to do you get a

422
00:27:47,400 --> 00:27:50,520
bunch of cluster centers for

423
00:27:51,640 --> 00:27:52,500
and then they

424
00:27:52,660 --> 00:27:55,550
they use each cluster centre as one label and they

425
00:27:55,610 --> 00:28:00,710
analyse what context of relationships between objects and those clusters into labels

426
00:28:00,730 --> 00:28:05,500
so here for instance they learn the relationship between buildings and cars but there is

427
00:28:05,500 --> 00:28:10,600
not an explicit representation the something some building is just when you have looks like

428
00:28:11,280 --> 00:28:15,440
then cars generally appear on top of it and when you have actually look like

429
00:28:15,440 --> 00:28:19,230
this then cast nearby by but not on top

430
00:28:19,630 --> 00:28:23,250
there are all the things that you can do with scenes

431
00:28:23,260 --> 00:28:25,640
not just trying to model the

432
00:28:25,640 --> 00:28:29,470
consist of relationships in order to improve the detection of objects

433
00:28:29,490 --> 00:28:33,600
you can also try to answer all the questions like

434
00:28:33,640 --> 00:28:37,640
what is happening on the image who is doing what

435
00:28:37,650 --> 00:28:41,760
so in this case in this image so this is work by william fyfe

436
00:28:41,770 --> 00:28:45,990
in which a what they try to do is given an image they try to

437
00:28:45,990 --> 00:28:51,140
give you information about the five there are no people here on a boat and

438
00:28:51,140 --> 00:28:55,020
after this in the background but also that is an event going on these people

439
00:28:55,020 --> 00:28:57,420
is role the scene

440
00:28:57,470 --> 00:29:01,260
that is the lake so that all these three themes here that interact with each

441
00:29:02,020 --> 00:29:07,420
the goal is not anymore to just labelled obvious personal image but after provide you

442
00:29:07,420 --> 00:29:11,100
some additional information about what is going on the picture

443
00:29:12,740 --> 00:29:14,870
this elephant

444
00:29:14,880 --> 00:29:18,890
so this is different examples of different events that they have in the database like

445
00:29:19,140 --> 00:29:23,770
polo players so here again the objects that our

446
00:29:23,790 --> 00:29:27,850
participate in this event and then the scene description of the what of this event

447
00:29:27,850 --> 00:29:29,170
is taking place

448
00:29:29,180 --> 00:29:32,390
and the same thing here

449
00:29:32,410 --> 00:29:35,870
then there is an old family of models that i'm not going to talk about

450
00:29:36,280 --> 00:29:40,520
which grammars these are very important class of models and it's almost a shame only

451
00:29:40,520 --> 00:29:43,940
going to vote with lies to them is to be almost an entire tutorial by

452
00:29:43,940 --> 00:29:50,430
itself and and these models started very long time ago from the seventies and they

453
00:29:50,430 --> 00:29:54,640
have been there family awards trying to develop grammars for vision

454
00:29:54,710 --> 00:29:57,340
and basically the idea is that

455
00:29:57,360 --> 00:30:01,660
again you are going to the composer seem to have this hierarchy of different interpretations

456
00:30:01,670 --> 00:30:05,420
of an image but they are just not grammars so not only do are going

457
00:30:05,420 --> 00:30:08,910
to have a rules like and and or this is present then this should be

458
00:30:09,020 --> 00:30:12,130
compose instead of elements and so on and so on

459
00:30:13,040 --> 00:30:17,870
here are some works by some senses group in which they use grammars to represent

460
00:30:17,870 --> 00:30:21,960
not just scenes but also objects that you can use this for almost anything

461
00:30:21,970 --> 00:30:25,780
it's a very rich set of models and i'm not going to talk about them

462
00:30:25,780 --> 00:30:31,310
here but they basically pursue the same goals what have been discriminative

463
00:30:31,340 --> 00:30:34,570
OK so all that was about

464
00:30:34,630 --> 00:30:38,570
object recognition but all the things that you can do with scenes in particular decomposition

465
00:30:38,570 --> 00:30:42,720
of the three d structure and the something very important that people are starting to

466
00:30:42,720 --> 00:30:48,140
do very recently seriously and so one of the first observations that humans we have

467
00:30:48,140 --> 00:30:49,840
really wired for three d

468
00:30:51,230 --> 00:30:53,070
you know here i have two eyes

469
00:30:53,090 --> 00:30:56,590
they are in front of my face i could have one i in front and

470
00:30:56,750 --> 00:30:59,740
the my back in this way you will see the entire we still feel i

471
00:30:59,740 --> 00:31:03,370
will see everything around me but now i decided to give up half of my

472
00:31:03,370 --> 00:31:06,890
visual feel and i put my my eyes here nearby

473
00:31:06,890 --> 00:31:10,190
you seem exactly the same thing so that i can see

474
00:31:11,190 --> 00:31:15,430
so that means that it is really important for for many many animals and they

475
00:31:15,430 --> 00:31:18,990
are willing to give up half the was of feel and you know you don't

476
00:31:18,990 --> 00:31:21,140
know what's going to become in front or back

477
00:31:21,140 --> 00:31:23,070
but you get to see into the

478
00:31:23,090 --> 00:31:26,460
so that means that he has been a really really important for us

479
00:31:26,480 --> 00:31:30,510
and in fact the is again something that we get to use all the time

480
00:31:30,520 --> 00:31:31,960
we can not shut down

481
00:31:32,010 --> 00:31:35,060
so here is an example of one of the solution

482
00:31:35,070 --> 00:31:39,170
in which there are these two lines both mention as early the same length

483
00:31:39,230 --> 00:31:42,400
and yet you see there must be in different is really hard to believe that

484
00:31:42,400 --> 00:31:44,310
they are actually the same length

485
00:31:44,390 --> 00:31:47,390
but this is because when to see this picture to reasoning through the about it

486
00:31:47,600 --> 00:31:52,240
and you can shut down the fact that this is a three d picture

487
00:31:52,250 --> 00:31:56,390
of course is that the picture larger projected on the screen and these lines happen

488
00:31:56,400 --> 00:32:00,060
to measure the same thing but you can't shut down during processing

489
00:32:00,090 --> 00:32:05,410
so it's very very powerful this is another example by a little bit and ridership

490
00:32:05,490 --> 00:32:06,460
in which

491
00:32:06,470 --> 00:32:09,380
these two table tops are identical

492
00:32:09,390 --> 00:32:10,480
they are one

493
00:32:10,490 --> 00:32:14,260
just the rotation of the and i did this just by adjusting for the show

494
00:32:14,340 --> 00:32:19,120
so this stub is nothing from his original driving just the figure and i rotated

495
00:32:19,340 --> 00:32:22,140
for the shop and put it on top so is identical is the very same

496
00:32:22,140 --> 00:32:28,130
one although who knows what windows does with objects not but that

497
00:32:28,980 --> 00:32:30,670
but the truth is that

498
00:32:30,680 --> 00:32:33,860
you you can of really all i through the perception here

499
00:32:33,870 --> 00:32:37,430
i you can see these two you can actually build them all and you can

500
00:32:37,480 --> 00:32:40,600
be the object you know is the same object you know exactly what you have

501
00:32:40,600 --> 00:32:45,240
done you could have better priors about what this shape should be and yet you

502
00:32:45,240 --> 00:32:46,510
could see differently

503
00:32:46,520 --> 00:32:50,240
so here all you general bayesian enlisted just fall apart

504
00:32:50,260 --> 00:32:53,380
during the this is not the kind of prior that you are you are built

505
00:32:53,390 --> 00:32:55,550
to encode

506
00:32:57,390 --> 00:33:00,600
and also the something that people have been doing for some time

507
00:33:02,280 --> 00:33:04,370
so here is an example of

508
00:33:04,890 --> 00:33:08,570
you seem very simple city it can also already be very powerful

509
00:33:08,600 --> 00:33:12,840
and so in this work this is war by in scotland and ireland u

510
00:33:12,890 --> 00:33:16,180
the only thing they try to do is to identify with the right online of

511
00:33:16,180 --> 00:33:18,830
possible sets that we can assign volume two

512
00:33:18,850 --> 00:33:21,720
now it's simply taking the powerset doesn't work

513
00:33:21,740 --> 00:33:25,660
because it's complicated and so we have to figure out which sets we would like

514
00:33:25,660 --> 00:33:27,450
to we would like to

515
00:33:27,510 --> 00:33:30,200
to be able to stand volume two

516
00:33:30,220 --> 00:33:32,240
and in that case so

517
00:33:32,260 --> 00:33:34,120
the original way that

518
00:33:34,120 --> 00:33:38,370
this is spread sigma algebra was defined on the real line was people started

519
00:33:38,450 --> 00:33:41,410
by taking all the half open intervals

520
00:33:41,430 --> 00:33:44,830
and said OK that's certainly something and tell us something that we certainly want to

521
00:33:44,830 --> 00:33:47,320
be able to assign one two

522
00:33:47,330 --> 00:33:51,050
otherwise our theory would make might make much sense because basically just take the length

523
00:33:51,050 --> 00:33:54,180
of the interval and that's all

524
00:33:54,200 --> 00:33:55,990
OK and then then you say

525
00:33:56,470 --> 00:34:00,820
it defines the signature of the smaller sigma which contains all these half open sets

526
00:34:00,870 --> 00:34:02,470
of these have open into about

527
00:34:02,470 --> 00:34:06,260
and on that you define and measure so every set in that interval is mapped

528
00:34:06,330 --> 00:34:07,470
a number

529
00:34:07,470 --> 00:34:11,740
we still have to figure out how to find measure that would be like the

530
00:34:11,740 --> 00:34:15,680
original historic examples here

531
00:34:15,850 --> 00:34:26,160
i want to do

532
00:34:32,390 --> 00:34:35,970
exactly so usually if we if we work in this way

533
00:34:36,010 --> 00:34:40,850
then then we don't have a problem basically it's more problem that you have in

534
00:34:40,850 --> 00:34:45,700
improves when you have such maneuver claim OK akayev's is this sigma algebra comes out

535
00:34:45,700 --> 00:34:49,430
somewhere and i have to prove the smallest one containing all these sets

536
00:34:49,760 --> 00:34:51,970
they can be really really difficult

537
00:34:52,870 --> 00:34:58,660
sigma of

538
00:34:58,660 --> 00:35:00,010
well because

539
00:35:04,510 --> 00:35:07,680
what does this

540
00:35:07,740 --> 00:35:12,580
it does not include all subsets well what if we want to greater one of

541
00:35:12,580 --> 00:35:20,160
the sets were OK so the question is what does not measure was set in

542
00:35:20,160 --> 00:35:22,350
this sigma algebra look like on the real line

543
00:35:26,780 --> 00:35:30,220
basically the the the sets that that are not measurable

544
00:35:30,280 --> 00:35:34,640
in this in this for a sigma algebra on the real line r ridiculously complicated

545
00:35:34,700 --> 00:35:37,120
actually for those of you who are

546
00:35:37,410 --> 00:35:40,010
who are familiar with the axiom of choice

547
00:35:42,450 --> 00:35:45,530
if you want

548
00:35:45,550 --> 00:35:48,220
a set four is set to be non measurable

549
00:35:48,260 --> 00:35:54,430
it is basically constructing an unmeasurable set in the signature requires the axiom of choice

550
00:35:55,580 --> 00:35:58,300
are actually in

551
00:35:58,330 --> 00:36:04,240
that's not corrupted but

552
00:36:04,260 --> 00:36:07,720
so is that is not measurable in the technology but if it has if there

553
00:36:07,720 --> 00:36:10,600
is no model of set theory in which it does not require the axiom of

554
00:36:10,600 --> 00:36:12,410
choice to construct

555
00:36:12,430 --> 00:36:15,160
does it help

556
00:36:15,180 --> 00:36:20,490
it's great have no OK basically basically and he said

557
00:36:20,510 --> 00:36:22,050
just as

558
00:36:22,080 --> 00:36:25,820
and he said that we ever will encounter that we can

559
00:36:25,830 --> 00:36:30,830
meaningfully integrate or want to integrate or any application within be the signal that is

560
00:36:30,850 --> 00:36:34,850
why it has emerged as as the standard sigma algebra everybody uses

561
00:36:35,120 --> 00:36:39,370
so in the beginning i don't think that that was clear just took some time

562
00:36:39,370 --> 00:36:42,080
to figure out first of all that all the

563
00:36:42,100 --> 00:36:46,350
the different assumptions that we can make about what sigma what sigma would be reasonable

564
00:36:46,620 --> 00:36:50,100
seem to lead to the same signal or do lead to the same technology

565
00:36:50,140 --> 00:36:51,820
and that really

566
00:36:51,930 --> 00:36:54,600
turns out to be a meaningful series

567
00:36:54,680 --> 00:36:57,390
that gives you all the power in the twenty

568
00:37:09,930 --> 00:37:14,160
but you can see they are still there still in a signature like this right

569
00:37:14,160 --> 00:37:17,280
for example if you want to construct half open

570
00:37:17,390 --> 00:37:21,490
the open the open interval would be in their own right

571
00:37:21,510 --> 00:37:23,280
and since

572
00:37:23,300 --> 00:37:25,450
individual points are closed sets

573
00:37:25,470 --> 00:37:26,490
the close

574
00:37:26,510 --> 00:37:29,910
the two endpoints are also in the individual individually and they can just take the

575
00:37:31,100 --> 00:37:34,300
of the interval one of the points and you can do the same for more

576
00:37:34,300 --> 00:37:35,830
complicated sets

577
00:37:35,890 --> 00:37:41,120
as long as you only need a countable number of those operations

578
00:37:41,180 --> 00:37:44,120
and that's why the sets become so complicated if you if you don't want them

579
00:37:44,120 --> 00:37:46,850
to be in there then you have to invent some

580
00:37:46,870 --> 00:37:51,160
some algorithm which does uncountably many steps to construct them from sets that we usually

581
00:37:54,830 --> 00:38:04,530
any more questions

582
00:38:04,550 --> 00:38:05,700
good so

583
00:38:05,740 --> 00:38:06,890
now after

584
00:38:06,890 --> 00:38:10,510
i've been speaking for forty five minutes i can finally tell you what to measure

585
00:38:12,410 --> 00:38:13,720
so if we

586
00:38:13,720 --> 00:38:18,120
if sigma algebras given so that's the domain of our right we want to measure

587
00:38:18,120 --> 00:38:19,620
to be a function that maps

588
00:38:19,660 --> 00:38:24,280
the sigmod but to the real line r positive like this

589
00:38:26,240 --> 00:38:29,140
we know that the definition so that the

590
00:38:29,200 --> 00:38:33,720
any any such function which does that which maps the sigmod approaches to the to

591
00:38:33,720 --> 00:38:34,720
the real line

592
00:38:34,740 --> 00:38:36,800
with these properties here

593
00:38:36,850 --> 00:38:40,720
is called to measure the properties of first of all the empty set always has

594
00:38:40,720 --> 00:38:41,820
measure zero

595
00:38:41,830 --> 00:38:45,220
that integrating over an empty set

596
00:38:45,240 --> 00:38:50,490
and then we need this this property here of additivity or pairwise disjoint sets so

597
00:38:50,490 --> 00:38:53,550
if the sets here are pairwise disjoint

598
00:38:54,600 --> 00:38:57,950
the measure of the union is the sum of the measures

599
00:38:58,850 --> 00:39:01,050
and you you were

600
00:39:01,490 --> 00:39:05,990
you will notice that these are not all the properties we looked at four measures

601
00:39:06,030 --> 00:39:10,320
but if if you have not already seen all of this and if you think

602
00:39:10,320 --> 00:39:14,140
it's interesting then you can quickly check at home that this is that this is

603
00:39:15,030 --> 00:39:16,930
implies all the other properties

604
00:39:17,240 --> 00:39:21,550
OK so this is the measure in general with these two properties here

605
00:39:21,600 --> 00:39:23,370
and then if

606
00:39:23,390 --> 00:39:28,830
four for the measure to be a probability measure we just the only additional requirement

607
00:39:28,830 --> 00:39:31,240
we make is that it's not like so

608
00:39:31,260 --> 00:39:33,950
the probability of the whole space

609
00:39:33,970 --> 00:39:35,470
here is one

610
00:39:35,490 --> 00:39:38,700
the probability that if we take a random draw that it is somewhere in our

611
00:39:38,700 --> 00:39:40,470
sample space that's one

612
00:39:40,530 --> 00:39:43,010
it's always

613
00:39:43,010 --> 00:39:45,260
OK and the two

614
00:39:45,300 --> 00:39:47,870
the two most important examples of measure

615
00:39:47,950 --> 00:39:49,450
that we

616
00:39:49,490 --> 00:39:52,120
we use in in everyday math

617
00:39:52,120 --> 00:39:56,390
are the lib back measure which i already mentioned that this measure which assigns

618
00:39:56,390 --> 00:39:58,780
the d dimensional volume two set

619
00:40:02,530 --> 00:40:07,410
and integration with this measure basically is a generalisation of the semantic

620
00:40:07,410 --> 00:40:19,010
all of them is one of the things that have been

621
00:40:19,010 --> 00:40:27,350
seems to be the same so so so so

622
00:40:27,370 --> 00:40:32,320
so question of how do you test the convergence rate and the different ways of

623
00:40:32,320 --> 00:40:36,240
testing for convergence one is you can look at two different iterations to see if

624
00:40:36,240 --> 00:40:40,240
he has changed a lot and if it hasn't changed much winter operations you say

625
00:40:40,240 --> 00:40:45,470
is more this converged of something that's done maybe slightly more often is look at

626
00:40:45,470 --> 00:40:48,550
the value j of theatre and j theatre

627
00:40:48,560 --> 00:40:52,510
so the opposite a is the quantity you're trying to minimize is not changing much

628
00:40:52,510 --> 00:40:53,780
anymore than you might

629
00:40:53,820 --> 00:40:58,530
being kind to believe it converges so these are sort of heuristics standard rules of

630
00:40:58,530 --> 00:41:04,220
thumb that are often used to decide if it into service composition

631
00:41:09,390 --> 00:41:10,530
you know it's true

632
00:41:13,260 --> 00:41:16,580
one one

633
00:41:16,600 --> 00:41:19,930
twenty two

634
00:41:23,680 --> 00:41:27,870
so what we do

635
00:41:27,890 --> 00:41:33,430
this see that there are some questions

636
00:41:33,450 --> 00:41:37,910
you know how going send looking pre sixty around changes in the direction of steepest

637
00:41:39,560 --> 00:41:44,370
so it actually turns out not the second part of it turns out that if

638
00:41:44,370 --> 00:41:45,370
you are

639
00:41:45,390 --> 00:41:50,050
if you stand on the hell and if you are

640
00:41:50,080 --> 00:41:53,530
turns out that we compute the gradient of the function we compute the derivative of

641
00:41:53,530 --> 00:41:57,530
a function then it just turns out that that is indeed the direction of steepest

642
00:41:57,530 --> 00:42:00,910
ascent on but it is point that you would never want to go in the

643
00:42:00,910 --> 00:42:05,820
opposite direction because the opposite direction would actually be the direction of steepest ascent right

644
00:42:06,490 --> 00:42:08,410
so so it turns out

645
00:42:08,480 --> 00:42:12,760
new hope made it seem to me that he talk more about this on on

646
00:42:12,780 --> 00:42:16,370
the intersection of those entries on to so we take the derivative of the function

647
00:42:16,370 --> 00:42:18,160
and the derivative of a function

648
00:42:18,220 --> 00:42:22,450
so it turns out to just give you the direction of steepest descent on

649
00:42:22,970 --> 00:42:27,580
and so you don't explicitly you know look of the agency's degrees around users just

650
00:42:27,580 --> 00:42:31,990
compute the derivative that turns out to be the direction of steepest descent

651
00:42:33,740 --> 00:42:39,010
i mean it is this is us that using talk more about this i

652
00:43:02,930 --> 00:43:08,870
let go ahead and give this algorithm on a specific names so this album here

653
00:43:08,870 --> 00:43:11,240
is actually called

654
00:43:11,260 --> 00:43:16,660
on back street to

655
00:43:16,680 --> 00:43:22,660
and the term batches and greater term the term that refers to the fact that

656
00:43:23,300 --> 00:43:27,330
on every step again this and you're going to look at the entire training set

657
00:43:27,330 --> 00:43:34,080
is going to your performer some over your or m training examples of

658
00:43:34,100 --> 00:43:38,140
so so so back to understand afterwards very well so i i use it very

659
00:43:38,140 --> 00:43:43,200
often on and it turns the sometimes if you have a really really large training

660
00:43:43,200 --> 00:43:48,370
sets so imagine that it's having forty seven holes is from portland oregon training said

661
00:43:48,370 --> 00:43:53,390
he had to the US census database of with US census size devices can often

662
00:43:53,390 --> 00:43:56,680
have hundreds of thousands of millions of examples

663
00:43:58,100 --> 00:43:59,450
so if m is

664
00:43:59,470 --> 00:44:01,010
you know a few million

665
00:44:02,240 --> 00:44:05,850
if you run in batch gradient descent then this means that to perform every step

666
00:44:05,850 --> 00:44:07,060
appeared in descent

667
00:44:07,120 --> 00:44:11,950
you need to perform some from j equals one two million which is that's that's

668
00:44:11,950 --> 00:44:15,930
sort a lot of training examples for your computer programs have to look at before

669
00:44:15,930 --> 00:44:19,910
you can even take you know one step down hole in the function j theta

670
00:44:21,300 --> 00:44:26,220
it turns out that when you when you have very large training sets on she

671
00:44:26,220 --> 00:44:29,410
is she right down an alternative

672
00:44:29,410 --> 00:44:35,140
that's whole stochastic gradient descent

673
00:44:35,260 --> 00:44:42,050
sometimes also called incremental gradient descent

674
00:44:42,060 --> 00:44:45,080
but you have to follow

675
00:44:45,080 --> 00:44:52,530
you will repeat until convergence

676
00:44:52,550 --> 00:44:56,200
and what the rate for g equals one to m

677
00:45:00,910 --> 00:45:04,970
and will perform one of these is the gradient descent updates

678
00:45:05,030 --> 00:45:08,470
using just the j th training example

679
00:45:33,330 --> 00:45:39,200
and as usual this is really you perform the update all the parameters theta as

680
00:45:39,200 --> 00:45:44,320
you performed as a natural

681
00:45:44,330 --> 00:45:48,970
meaning before i indexes and the parameter vector c just before the study of all

682
00:45:48,970 --> 00:45:51,870
the parameters simultaneously

683
00:45:51,890 --> 00:45:55,280
on the advantage of this algorithm is that

684
00:45:56,680 --> 00:46:00,200
in order to pull in order to to to start learning in order to start

685
00:46:00,200 --> 00:46:02,410
modifying the parameters on

686
00:46:02,410 --> 00:46:05,660
you only need to look at your first training examples is looking for first training

687
00:46:06,510 --> 00:46:11,030
and perform an update using the derivative of the error with respect to just the

688
00:46:11,030 --> 00:46:16,220
first example and then you look is sickening training example perform another

689
00:46:16,280 --> 00:46:19,260
he said that he perhaps much much more quickly

690
00:46:19,260 --> 00:46:21,450
without needing to

691
00:46:21,490 --> 00:46:28,050
t scan over your entire US census data point started out this

692
00:46:30,530 --> 00:46:31,350
the c

693
00:46:31,350 --> 00:46:36,640
for large datasets of stochastic gradient descent is often much faster

694
00:46:36,800 --> 00:46:38,470
and so on

695
00:46:38,510 --> 00:46:44,030
what happens to stochastic gradient descent is that it was actually converge to the global

696
00:46:44,030 --> 00:46:45,510
minimum exactly

697
00:46:45,530 --> 00:46:46,820
but on

698
00:46:48,600 --> 00:46:51,870
these are the continuity of function

699
00:46:51,890 --> 00:46:56,450
then as you run stochastic gradient descent is you sort of tend to wander around

700
00:46:56,470 --> 00:46:59,560
you may actually end up going up occasionally

701
00:46:59,580 --> 00:47:01,260
parameters also the

702
00:47:01,260 --> 00:47:04,230
of them on the web would be happy to talk to you if you if

703
00:47:04,230 --> 00:47:05,270
you need it

704
00:47:05,420 --> 00:47:09,080
and we are blessed with three

705
00:47:11,130 --> 00:47:13,400
who took this causes freshmen

706
00:47:13,440 --> 00:47:17,920
who act as what are called peer tutors no run session sunday evening from eight

707
00:47:17,930 --> 00:47:22,090
to ten PM is the current plan will announce the rooms for these things on

708
00:47:22,090 --> 00:47:24,890
the website probably by email to you as well

709
00:47:24,910 --> 00:47:27,980
so let me introduce to you know how

710
00:47:28,020 --> 00:47:30,590
OK and recline

711
00:47:30,640 --> 00:47:32,040
and justin kemp

712
00:47:32,050 --> 00:47:34,780
so there will be a big help to do so there's plenty of

713
00:47:34,830 --> 00:47:38,160
plenty of personal help so use it

714
00:47:38,630 --> 00:47:43,550
these are the dates we're going to have exams at ten lectures that exam nine

715
00:47:43,550 --> 00:47:48,770
lectures exam nine lectures example actually if you check you'll find the the and also

716
00:47:48,770 --> 00:47:51,750
you get fifty points for participation in the wiki

717
00:47:51,760 --> 00:47:56,780
and the total of six hundred fifty points that's what your grammar exams based on

718
00:47:57,200 --> 00:48:02,020
actually this doesn't cover nine lectures that are covered on the example of the previous

719
00:48:02,020 --> 00:48:04,960
wednesday part of the lecture is going to be a guest lecturers it's going to

720
00:48:04,960 --> 00:48:06,600
be here just that day

721
00:48:06,610 --> 00:48:10,640
so we're putting exam offered lonely cover the previous material not that that's a big

722
00:48:11,460 --> 00:48:13,750
OK this master grade

723
00:48:13,780 --> 00:48:19,320
is biased that is based on this your total score here how six hundred fifty

724
00:48:19,320 --> 00:48:23,590
points but if you're nearer cut off and you are very good about turning in

725
00:48:23,590 --> 00:48:27,530
your problem sets and so on then we boosted up here we don't great problem

726
00:48:28,550 --> 00:48:30,880
but if it's worthwhile to do them

727
00:48:30,890 --> 00:48:33,920
right and they might make a difference

728
00:48:36,900 --> 00:48:40,280
where are we going to this one of the goals of our fresh and organic

729
00:48:40,280 --> 00:48:44,140
chemistry in fact if you click that your powerpoint you get taken to that site

730
00:48:44,230 --> 00:48:47,320
but it's right on the website you'll see it anyhow

731
00:48:47,370 --> 00:48:50,560
first of all is to learn the crucial facts

732
00:48:50,580 --> 00:48:55,050
and vocabulary of organic chemistry products what we think we're here for

733
00:48:55,090 --> 00:48:58,060
and to develop the theoretical intuition

734
00:48:58,100 --> 00:49:00,280
about how bonding works

735
00:49:00,330 --> 00:49:03,970
this is the goal for the primary goal of the first half of the fall

736
00:49:03,970 --> 00:49:07,440
semester is to learn how funding works really

737
00:49:08,100 --> 00:49:11,410
and that relates that the molecular structure

738
00:49:11,460 --> 00:49:14,320
and also how bonding changes

739
00:49:14,370 --> 00:49:16,740
and that of course is reactivity

740
00:49:16,790 --> 00:49:20,080
but under the line there are a lot of other things that you we do

741
00:49:20,080 --> 00:49:25,050
in freshman organic chemistry that arguably just important just as important

742
00:49:25,060 --> 00:49:30,240
like to make the scientific transition from school to university

743
00:49:30,340 --> 00:49:34,390
in school they try to teach you what people know in the university you try

744
00:49:34,390 --> 00:49:36,240
to develop new knowledge

745
00:49:36,260 --> 00:49:39,890
so you need a different mindset for that and we hope this course helps you

746
00:49:39,900 --> 00:49:40,980
develop that

747
00:49:41,040 --> 00:49:46,190
so learned from organic chemistry which is really in my view of marble science

748
00:49:46,240 --> 00:49:51,370
how to be creative scientist so here's here's the creative scientist by anybody's major louis

749
00:49:52,690 --> 00:49:57,320
and in the eighteen eighties he said this in french but in english it says

750
00:49:57,400 --> 00:50:00,560
going to be astonished by something

751
00:50:00,570 --> 00:50:04,610
is the mines first step toward discovery

752
00:50:04,620 --> 00:50:06,390
another way of putting that is

753
00:50:06,530 --> 00:50:09,000
the characteristic comment

754
00:50:09,050 --> 00:50:12,360
on making a real discovery is not eureka

755
00:50:14,130 --> 00:50:16,330
that's funny

756
00:50:16,410 --> 00:50:21,960
so that's what you really have to learn you learn enough about how chemistry works

757
00:50:21,960 --> 00:50:24,260
in form this picture in your mind

758
00:50:24,370 --> 00:50:25,490
that when

759
00:50:25,500 --> 00:50:29,990
something happens that doesn't fit you know to be astonished so that you can discover

760
00:50:31,000 --> 00:50:35,970
that's exactly what pastor did and we'll talk about that in the course

761
00:50:36,030 --> 00:50:39,860
and even perhaps more important to develop good taste

762
00:50:39,910 --> 00:50:43,600
so that you can distinguish sense from nonsense

763
00:50:43,650 --> 00:50:47,580
there's certainly more non-sense floating around themselves

764
00:50:47,600 --> 00:50:50,350
and being able to tell the difference for the way you do it is to

765
00:50:50,350 --> 00:50:54,480
develop good taste by looking at a lot of good examples

766
00:50:54,490 --> 00:50:57,840
and then you're aware of how crummy the bad examples are so we're going to

767
00:50:57,840 --> 00:51:00,830
try to emphasise the examples

768
00:51:00,850 --> 00:51:03,250
and have fun

769
00:51:05,280 --> 00:51:08,950
so in a as we go along if you have questions break in you know

770
00:51:08,980 --> 00:51:12,020
this much more as we go along no

771
00:51:13,350 --> 00:51:18,510
the class really is mostly about theory

772
00:51:18,520 --> 00:51:22,230
although we describe the basis for the theory and spend a lot of time trying

773
00:51:22,230 --> 00:51:24,450
to make it real

774
00:51:24,460 --> 00:51:29,890
but we require chemistry one twenty six l the lab this is the only chemistry

775
00:51:29,890 --> 00:51:34,720
course that requires you to take a lab simultaneously so i hope you're all rolling

776
00:51:34,720 --> 00:51:36,730
in that because there will be a certain

777
00:51:36,780 --> 00:51:39,850
they that you want to be able to take it it's just one afternoon a

778
00:51:40,600 --> 00:51:42,500
three hours or whatever it is

779
00:51:42,520 --> 00:51:46,150
but if you want to get your first choice of line soon

780
00:51:46,170 --> 00:51:49,700
well you'll be accommodated but it's just more convenient if you get

781
00:51:49,780 --> 00:51:51,880
given range earlier but why

782
00:51:51,890 --> 00:51:56,170
because lab answers the really big questions

783
00:51:56,170 --> 00:52:00,510
let's define the

784
00:52:00,560 --> 00:52:03,040
partitioning based r

785
00:52:03,060 --> 00:52:10,400
portfolio election

786
00:52:11,780 --> 00:52:13,630
we have two integer

787
00:52:13,640 --> 00:52:15,500
one is here

788
00:52:15,540 --> 00:52:20,170
the length of the nearby which is taken in holland

789
00:52:21,260 --> 00:52:24,030
and the other one is that in here

790
00:52:24,040 --> 00:52:27,460
we should really be indexed

791
00:52:27,530 --> 00:52:31,220
if a sequence of partitions

792
00:52:31,270 --> 00:52:37,610
for example the partition and you can imagine that the that

793
00:52:37,660 --> 00:52:39,740
e g

794
00:52:43,740 --> 00:52:46,540
cycle and go the miners

795
00:52:46,590 --> 00:52:50,580
for example

796
00:52:59,550 --> 00:53:01,370
and we

797
00:53:01,380 --> 00:53:04,120
each such partition we

798
00:53:05,200 --> 00:53:09,510
i have a partition or a quantizer of the

799
00:53:09,560 --> 00:53:11,000
the return vector

800
00:53:11,230 --> 00:53:14,470
the imaginary back

801
00:53:18,300 --> 00:53:20,500
let g be the

802
00:53:20,510 --> 00:53:22,400
one quantizer a bit

803
00:53:22,430 --> 00:53:27,920
corresponds to the partition pn

804
00:53:27,940 --> 00:53:36,520
and then just because there and and like a sequence of one thousand value

805
00:53:36,530 --> 00:53:43,270
just use it using these short vision

806
00:53:43,280 --> 00:53:48,760
OK i imagine that we we are

807
00:53:51,510 --> 00:53:54,190
n minus one

808
00:53:54,210 --> 00:54:00,130
and we would like to construct a portfolio that or

809
00:54:00,140 --> 00:54:04,680
in order to make the ricci v and

810
00:54:06,770 --> 00:54:07,410
so we

811
00:54:07,420 --> 00:54:11,770
park read the segment of nearby

812
00:54:13,490 --> 00:54:15,400
look at what if he

813
00:54:15,410 --> 00:54:21,140
but until then we look at the vector today and yesterday

814
00:54:21,190 --> 00:54:25,300
and make the quantization

815
00:54:25,340 --> 00:54:29,380
and the search for similarities is in the box

816
00:54:29,440 --> 00:54:31,950
similarity in

817
00:54:31,960 --> 00:54:37,070
at the time you i'm i'm one means

818
00:54:37,080 --> 00:54:39,010
that quantifies the

819
00:54:39,020 --> 00:54:46,060
that's one of these goodman is equal to the quantized version of the fact that

820
00:54:46,080 --> 00:54:50,430
if we find but i minus one

821
00:54:50,480 --> 00:54:53,170
then OK then we are happy

822
00:54:53,180 --> 00:54:56,450
and expect that

823
00:54:57,880 --> 00:55:01,760
the very next return that are

824
00:55:01,840 --> 00:55:06,110
the time for

825
00:55:06,160 --> 00:55:08,520
they i

826
00:55:08,530 --> 00:55:10,910
if behaviour will be

827
00:55:10,920 --> 00:55:17,850
that is the goal is similar to the behaviour of the two more recent

828
00:55:17,860 --> 00:55:19,460
so what

829
00:55:19,470 --> 00:55:21,410
chevy do

830
00:55:21,460 --> 00:55:23,370
we are here

831
00:55:23,400 --> 00:55:25,530
this is an minus one

832
00:55:25,540 --> 00:55:28,150
go back in the past

833
00:55:28,160 --> 00:55:34,950
search for i mean i'm i'm one that the segment is similar to the that

834
00:55:34,960 --> 00:55:38,660
five bond matching similarity

835
00:55:38,700 --> 00:55:41,810
then becomes the very next to the

836
00:55:41,900 --> 00:55:44,420
and and then again in similar

837
00:55:44,470 --> 00:55:49,280
and we get a stop sequence of return back

838
00:55:49,330 --> 00:55:51,930
this is an obvious way of thinking that

839
00:55:51,980 --> 00:55:57,260
if i don't do anything but i have a history

840
00:55:57,700 --> 00:56:03,190
possibly said then more for the actual current situation

841
00:56:03,230 --> 00:56:06,010
i look for similarities in the past

842
00:56:06,020 --> 00:56:13,730
because of the very next the return vector or the after the similar

843
00:56:13,740 --> 00:56:17,520
it is the the set of mass makes here

844
00:56:17,570 --> 00:56:23,290
i minus one and become the very next the in

845
00:56:25,000 --> 00:56:29,190
what is it here

846
00:56:29,300 --> 00:56:31,170
this is the song

847
00:56:31,180 --> 00:56:36,120
maybe i can divide it by the size of g and

848
00:56:36,130 --> 00:56:37,770
and it would be

849
00:56:37,810 --> 00:56:39,970
an estimated or

850
00:56:40,020 --> 00:56:44,790
all of the regression function

851
00:56:44,840 --> 00:56:50,810
but it is better to explain it as the as i did before

852
00:56:50,820 --> 00:56:54,470
so i look for similarities

853
00:56:54,550 --> 00:56:57,260
both the very next week there learn

854
00:56:57,310 --> 00:57:00,110
and for the three vector for these

855
00:57:00,180 --> 00:57:03,960
stop sequence of the return vector or

856
00:57:04,010 --> 00:57:06,970
these are dynamic portfolio that or

857
00:57:06,990 --> 00:57:11,230
which performed best along the

858
00:57:13,020 --> 00:57:18,100
and this seems to be the definition of the portfolio vector

859
00:57:18,120 --> 00:57:20,900
depending on the eye

860
00:57:20,910 --> 00:57:25,750
and depending on who are parameter k and l

861
00:57:25,800 --> 00:57:26,670
i have two

862
00:57:26,690 --> 00:57:30,480
because of the choice of the and

863
00:57:30,570 --> 00:57:32,310
if accidentally

864
00:57:32,320 --> 00:57:35,030
if you sorry

865
00:57:35,040 --> 00:57:36,630
this g and

866
00:57:36,680 --> 00:57:37,950
is that

867
00:57:38,770 --> 00:57:42,950
just to the uniform portfolio

868
00:57:43,000 --> 00:57:47,690
so for each feature for a pair of integers k and l

869
00:57:47,700 --> 00:57:53,380
i introduced the portfolio vector for

870
00:58:02,050 --> 00:58:09,980
sadly i minus one

871
00:58:28,880 --> 00:58:30,370
the point

872
00:58:30,410 --> 00:58:33,600
we don't have any conditions on it

873
00:58:33,670 --> 00:58:34,660
the other

874
00:58:35,410 --> 00:58:36,190
this is the

875
00:58:36,210 --> 00:58:41,180
i mean at university it's a good question maybe a i i did the two

876
00:58:41,760 --> 00:58:45,780
because he mentioned

877
00:58:45,900 --> 00:58:51,670
concept of universality

878
00:58:51,720 --> 00:58:57,100
are all these nonparametric in problem

879
00:58:57,150 --> 00:59:00,590
classification regression estimation

880
00:59:00,590 --> 00:59:02,780
one month is going to

881
00:59:02,800 --> 00:59:05,570
one month the second one

882
00:59:05,570 --> 00:59:07,360
and the reading of the rules

883
00:59:07,380 --> 00:59:08,440
rather than

884
00:59:08,470 --> 00:59:12,670
the that you can read them semantically

885
00:59:13,570 --> 00:59:18,240
if the numerator if all of the formula in here k satisfiable

886
00:59:18,260 --> 00:59:21,690
in one of these guys is k satisfiable

887
00:59:21,690 --> 00:59:24,260
case class of kripke frames

888
00:59:24,320 --> 00:59:27,760
what is k satisfiable mean it means there is some

889
00:59:28,990 --> 00:59:31,720
with that contains the world makes the truth

890
00:59:31,760 --> 00:59:32,840
some models

891
00:59:32,900 --> 00:59:37,070
it contains the world makes it true it's double existential notion

892
00:59:37,110 --> 00:59:40,220
there exists some model there exists the world

893
00:59:40,260 --> 00:59:41,800
the same is true

894
00:59:41,800 --> 00:59:44,780
at well what does it mean for us it to be true at the world

895
00:59:44,920 --> 00:59:48,610
means every formula in the same is true

896
00:59:48,690 --> 00:59:51,220
in the multiset sorry

897
00:59:51,280 --> 00:59:53,420
OK so what's a k tableau

898
00:59:53,420 --> 00:59:55,510
it's an inverted tree

899
00:59:55,550 --> 00:59:59,750
with the root note so k tableau for set y you give me some say

900
01:00:00,470 --> 01:00:04,150
and the first thing i do is i work with the internet of y

901
01:00:04,150 --> 01:00:08,860
they are preserved logical equivalence this is slightly bigger just means i can use three

902
01:00:08,860 --> 01:00:10,300
rules rather than

903
01:00:10,320 --> 01:00:12,780
ten or whatever it is

904
01:00:12,840 --> 01:00:14,530
the root note is nnf y

905
01:00:15,820 --> 01:00:20,670
the children are all obtained from their parent node by instantiating a rule of inference

906
01:00:20,670 --> 01:00:23,510
so what you do is you pattern match this guy

907
01:00:23,530 --> 01:00:28,170
with these rules the numerator these rules and say is any one of them

908
01:00:28,220 --> 01:00:31,490
a pattern match and if it is then you extend the tableau

909
01:00:31,510 --> 01:00:35,360
by the bottom line of given example in a minute

910
01:00:35,420 --> 01:00:39,460
and take a k tableau is close so you're going to get some sort of

911
01:00:39,530 --> 01:00:40,420
three years

912
01:00:40,570 --> 01:00:43,590
it's closed if all of the things across the board

913
01:00:43,630 --> 01:00:45,150
i'm not with across

914
01:00:45,170 --> 01:00:50,820
in other words the last rule in every branch is that it's true

915
01:00:50,860 --> 01:00:51,940
on there

916
01:00:55,780 --> 01:00:59,030
here's an example

917
01:00:59,050 --> 01:01:03,150
so he is an instance of k

918
01:01:03,170 --> 01:01:04,700
it's been negated

919
01:01:04,720 --> 01:01:08,900
i showed you on the previous slide that when you push the negation in you

920
01:01:08,900 --> 01:01:10,990
get something like this

921
01:01:11,010 --> 01:01:15,630
so this is we're going to begin this is the end y

922
01:01:15,670 --> 01:01:19,030
and now i look for rule is applicable

923
01:01:19,860 --> 01:01:22,110
is that all applicable

924
01:01:22,220 --> 01:01:23,990
what am i looking for

925
01:01:23,990 --> 01:01:28,300
i'm looking for some atom and its negation and some other stuff

926
01:01:28,360 --> 01:01:31,840
uh well now i've just got one big formula here

927
01:01:31,860 --> 01:01:34,920
but is one formula sitting here

928
01:01:34,940 --> 01:01:36,670
there are no free atoms

929
01:01:36,690 --> 01:01:38,630
so that is not applicable

930
01:01:38,690 --> 01:01:41,820
that one is because i've got a conjunction

931
01:01:41,840 --> 01:01:43,570
and x is empty

932
01:01:45,760 --> 01:01:48,740
just to the conjunction into a semicolon

933
01:01:48,740 --> 01:01:50,340
now i've got two

934
01:01:50,360 --> 01:01:52,630
formulating the multiset

935
01:01:52,670 --> 01:01:53,780
it's still not

936
01:01:53,800 --> 01:01:55,690
atomic contradiction

937
01:01:55,720 --> 01:01:56,780
is now

938
01:01:56,880 --> 01:02:01,320
got there is a conjunction because this thing is a conjunction right parentheses can be

939
01:02:02,690 --> 01:02:05,740
so i just turned that into a same

940
01:02:05,760 --> 01:02:08,630
there is still no atomic contradiction

941
01:02:08,630 --> 01:02:10,940
there are no conjunctions left

942
01:02:10,970 --> 01:02:15,570
there are no disjunctions because this disjunction is protected by the box

943
01:02:15,610 --> 01:02:17,570
so i've got an instance

944
01:02:17,590 --> 01:02:19,690
of this

945
01:02:19,720 --> 01:02:22,460
so what's the diamond formula it's this guy

946
01:02:22,460 --> 01:02:25,490
so if i is equal to not p

947
01:02:25,550 --> 01:02:29,400
what the boxes in x it's these guys

948
01:02:29,440 --> 01:02:32,880
and what's state is indeed

949
01:02:32,920 --> 01:02:34,200
what do i do

950
01:02:34,220 --> 01:02:38,780
i removed the boxes from all of these guys so i copied this there

951
01:02:38,840 --> 01:02:41,380
i copied this them that box

952
01:02:41,400 --> 01:02:46,700
i removed the diamonds and copy subformula down

953
01:02:46,720 --> 01:02:50,740
have i got a contradiction well known as the p norm but that's the p

954
01:02:50,740 --> 01:02:54,300
one they differ so it's got an instance of that because that's is that have

955
01:02:54,300 --> 01:02:56,460
to be the same

956
01:02:56,460 --> 01:02:57,630
you have two of them

957
01:02:57,640 --> 01:03:04,210
it looks like a little plastic plates

958
01:03:04,260 --> 01:03:05,570
this is that plate

959
01:03:09,720 --> 01:03:12,120
and it is cut in such a way

960
01:03:12,180 --> 01:03:15,490
i will take this off because we don't need this

961
01:03:17,080 --> 01:03:20,600
and this plate is cut in such a way you can come in many cases

962
01:03:20,790 --> 01:03:22,590
by this large sheets

963
01:03:22,630 --> 01:03:24,160
cut in such a way

964
01:03:24,170 --> 01:03:25,880
that if light

965
01:03:25,940 --> 01:03:28,130
unpolarized light

966
01:03:28,200 --> 01:03:30,570
goes through plate

967
01:03:30,610 --> 01:03:33,450
that is hundred percent right what comes out

968
01:03:33,460 --> 01:03:35,390
in this direction

969
01:03:35,410 --> 01:03:38,420
and i believe that all these plates have been cut that way

970
01:03:38,460 --> 01:03:41,490
but i cannot be a hundred percent certain

971
01:03:41,580 --> 01:03:45,190
i can find that it's possible that some of them like this but i don't

972
01:03:45,190 --> 01:03:46,010
think so

973
01:03:46,060 --> 01:03:49,310
i think all of them will be like this

974
01:03:49,420 --> 01:03:52,960
so that means if light comes from the back

975
01:03:53,010 --> 01:03:54,990
and reaches you

976
01:03:55,850 --> 01:03:58,990
after it has gone through this play

977
01:03:59,080 --> 01:04:00,300
all the light

978
01:04:00,350 --> 01:04:04,820
going to come to you this

979
01:04:04,830 --> 01:04:09,010
that's one way we can make linearly polarized light and that's the first thing

980
01:04:09,030 --> 01:04:12,420
we're going to look at that there is another way that we can make a

981
01:04:12,430 --> 01:04:15,010
linearly polarized light which have also

982
01:04:15,070 --> 01:04:16,810
discuss and demonstrate

983
01:04:16,820 --> 01:04:19,210
and that is by reflecting light

984
01:04:19,220 --> 01:04:22,330
of glass of water

985
01:04:22,340 --> 01:04:24,360
and there's a third way

986
01:04:24,440 --> 01:04:26,320
more difficult

987
01:04:26,360 --> 01:04:29,860
that is if we have a fine dust particles

988
01:04:29,920 --> 01:04:34,340
and we shine the light to these fine dust particles the light will be thrown

989
01:04:34,340 --> 01:04:40,230
in all directions it's not nice reflection like you have mirror recall that scattering

990
01:04:40,250 --> 01:04:41,860
i'll show it to you

991
01:04:41,870 --> 01:04:43,420
that's also away

992
01:04:43,460 --> 01:04:44,580
and you can make

993
01:04:44,630 --> 01:04:47,480
polarized light

994
01:04:47,610 --> 01:04:50,280
but that's first now turned to these

995
01:04:50,290 --> 01:04:52,790
polarizes which were invented by

996
01:04:52,860 --> 01:04:54,210
edwin land who

997
01:04:54,260 --> 01:04:55,190
founded the

998
01:04:55,200 --> 01:04:56,730
polaroid company

999
01:04:56,820 --> 01:05:01,720
although the idea that you can make polarized light out of unpolarized light

1000
01:05:01,780 --> 01:05:03,080
it was already known

1001
01:05:03,080 --> 01:05:04,790
in the seventeenth century

1002
01:05:04,840 --> 01:05:07,370
but they didn't have these fancy mice

1003
01:05:07,430 --> 01:05:11,390
edwin land plates

1004
01:05:11,410 --> 01:05:13,320
if this plate

1005
01:05:13,390 --> 01:05:18,290
we are what we call an ideal polarizer

1006
01:05:24,160 --> 01:05:26,780
i you right

1007
01:05:26,850 --> 01:05:31,700
then it would mean that if you shine unpolarized light on it

1008
01:05:31,830 --> 01:05:34,560
that fifty percent comes through

1009
01:05:34,600 --> 01:05:36,320
so the light

1010
01:05:36,380 --> 01:05:38,200
is we do

1011
01:05:38,240 --> 01:05:39,460
in half

1012
01:05:39,460 --> 01:05:41,850
but everything that comes through

1013
01:05:41,860 --> 01:05:43,990
this is the uniform

1014
01:05:44,000 --> 01:05:46,850
so certain light intensity goes in

1015
01:05:46,910 --> 01:05:49,560
half of that comes out but it's all in your

1016
01:05:51,890 --> 01:05:55,010
that's what we call an do for in reality

1017
01:05:55,030 --> 01:05:57,960
you never get fifty percent you can always something less

1018
01:05:57,970 --> 01:06:01,010
think of it as being an ideal polarizing

1019
01:06:02,990 --> 01:06:05,780
get picture of what these plates

1020
01:06:05,790 --> 01:06:07,830
will do

1021
01:06:07,860 --> 01:06:09,560
so what i wanted to do now

1022
01:06:09,570 --> 01:06:15,700
this take one of these plates close one eye and halted play from the one

1023
01:06:15,740 --> 01:06:18,500
and hold it like this

1024
01:06:18,560 --> 01:06:21,460
all the light that you will see now

1025
01:06:21,540 --> 01:06:24,730
is linearly polarized in this direction

1026
01:06:24,790 --> 01:06:28,540
this is an exciting experiment no because you can see it

1027
01:06:28,550 --> 01:06:31,590
i can not see linear polarized radiation

1028
01:06:31,640 --> 01:06:34,580
animals can some animals can all be scanned

1029
01:06:34,630 --> 01:06:36,840
you can see the linearly polarized light

1030
01:06:36,840 --> 01:06:38,370
they can only which direction

1031
01:06:38,380 --> 01:06:41,460
but at least i want you to look through it

1032
01:06:41,710 --> 01:06:46,470
imagine if you had everything is now polarized on percent in this direction

1033
01:06:46,530 --> 01:06:49,390
and then i want you to put the

1034
01:06:49,420 --> 01:06:50,670
played like so

1035
01:06:50,680 --> 01:06:54,490
now all the light that comes through is no polarized in this direction

1036
01:06:54,500 --> 01:06:55,580
and i want you to

1037
01:06:55,680 --> 01:06:57,330
played like this

1038
01:06:57,410 --> 01:07:05,090
and all the light that he sees polarized now in that direction

1039
01:07:05,140 --> 01:07:08,460
in fact some people can see

1040
01:07:08,510 --> 01:07:11,010
but i've actually afforestation that's the difference

1041
01:07:11,070 --> 01:07:13,680
takes a tremendous amount of training

1042
01:07:13,700 --> 01:07:19,960
most of you can not

1043
01:07:19,960 --> 01:07:21,680
how can it be

1044
01:07:21,690 --> 01:07:24,490
that this polarized light

1045
01:07:24,500 --> 01:07:27,250
going through this piece of plastic

1046
01:07:27,340 --> 01:07:31,060
everything comes out like this that is boggling

1047
01:07:31,070 --> 01:07:34,370
to imagine what's going to happen and i agree with you that it's not easy

1048
01:07:35,750 --> 01:07:37,860
that's why most physicists will say

1049
01:07:37,870 --> 01:07:44,320
we don't understand light all that well but we can predict what will happen

1050
01:07:44,340 --> 01:07:46,240
if i go back

1051
01:07:46,290 --> 01:07:48,030
two my

1052
01:07:49,790 --> 01:07:55,350
when the spring was oscillating like this

1053
01:07:56,450 --> 01:07:58,280
that has a spring

1054
01:07:58,290 --> 01:08:01,240
what i was shaking is up and down

1055
01:08:01,260 --> 01:08:03,530
and new wave went like this

1056
01:08:03,610 --> 01:08:07,720
imagine that i would make stuff like this

1057
01:08:07,730 --> 01:08:08,840
this award

1058
01:08:08,840 --> 01:08:13,300
was essentially the reason so belief propagation passes the entire distribution is the message

1059
01:08:13,300 --> 01:08:18,840
rise expectation propagation will only pass onto the distribution certain expectations distribution allows you to

1060
01:08:18,840 --> 01:08:20,840
a very compact messages

1061
01:08:20,860 --> 01:08:24,740
you get to work

1062
01:08:24,760 --> 01:08:31,360
there's hardly any i mean there's some for belief propagation which would be a special

1063
01:08:31,360 --> 01:08:35,960
case but for the general case of all expectation propagation organs there's hardly any results

1064
01:08:35,960 --> 01:08:37,530
on so

1065
01:08:37,530 --> 01:08:41,470
what i did was i i took the exact mean of the posterior distribution

1066
01:08:41,490 --> 01:08:44,470
so the mean of the the green colour of the blue curve

1067
01:08:44,510 --> 01:08:45,840
and then i used

1068
01:08:45,840 --> 01:08:49,420
the mean of the gas in the came out to measure the absolute difference

1069
01:08:50,150 --> 01:08:51,420
and the same for the

1070
01:08:51,440 --> 01:08:54,130
o thing in both cases

1071
01:09:00,300 --> 01:09:03,650
so even if if you use

1072
01:09:06,420 --> 01:09:07,490
good zero

1073
01:09:10,690 --> 01:09:11,860
that's right

1074
01:09:11,860 --> 01:09:15,780
so you know cases OK right so this so using this metric is somehow cheating

1075
01:09:15,780 --> 01:09:20,190
because it's measuring something that goes in capture

1076
01:09:20,210 --> 01:09:22,820
and yes i agree

1077
01:09:22,860 --> 01:09:24,010
the have

1078
01:09:24,030 --> 01:09:27,070
this is the first position

1079
01:09:28,920 --> 01:09:33,490
the woman exactly that's right

1080
01:09:33,510 --> 01:09:35,840
so measuring air

1081
01:09:37,010 --> 01:09:42,990
that's correct

1082
01:09:43,010 --> 01:09:47,550
i mean will very cases where the posterior mean is actually the answer to the

1083
01:09:47,550 --> 01:09:51,820
problems of example if i want to predict for a test point where would appear

1084
01:09:51,880 --> 01:09:54,740
what is what is the expected location of a new point

1085
01:09:54,760 --> 01:09:58,690
and the posterior mean x would be the answer to that question

1086
01:10:02,920 --> 01:10:04,490
so so

1087
01:10:04,550 --> 01:10:07,840
earlier this morning increases talk someone asked about censoring

1088
01:10:07,860 --> 01:10:09,820
so i thought why we do something

1089
01:10:09,900 --> 01:10:12,530
so person here

1090
01:10:16,030 --> 01:10:18,420
we can we can write down a censoring problem which is very similar to the

1091
01:10:18,420 --> 01:10:22,190
court problem so here i've written down simple centering problem or

1092
01:10:22,190 --> 01:10:23,050
i have

1093
01:10:23,050 --> 01:10:25,470
the next that i want to estimate from y

1094
01:10:27,170 --> 01:10:29,280
x again has discussed in prior

1095
01:10:29,300 --> 01:10:32,920
and i a bunch of wise which are discussed in observations with his axe

1096
01:10:32,920 --> 01:10:37,280
but some of the wiser sensors and the scenting process that i picked

1097
01:10:37,340 --> 01:10:41,420
is one where you just know that the magnitude of y is bigger than some

1098
01:10:41,420 --> 01:10:44,960
threshold seen otherwise has magnitude ten

1099
01:10:44,960 --> 01:10:50,740
or more for example censoring process my my whatever device i'm using can measure beyond

1100
01:10:50,800 --> 01:10:53,690
some threshold just says all international that's it

1101
01:10:55,190 --> 01:10:58,490
but that sort of sensing problem we can solve using EP as well

1102
01:10:58,610 --> 01:11:01,280
what's the first thing we do when you want to solve the problem

1103
01:11:01,300 --> 01:11:05,690
right on the fact that right so

1104
01:11:05,740 --> 01:11:10,280
thank you for this problem will be

1105
01:11:10,300 --> 01:11:14,760
a bunch of wise and suppose we have a special centred y

1106
01:11:14,780 --> 01:11:16,760
we are writers

1107
01:11:16,780 --> 01:11:19,110
y four parente

1108
01:11:19,110 --> 01:11:22,710
this special censored observations

1109
01:11:22,730 --> 01:11:23,940
OK so

1110
01:11:23,990 --> 01:11:25,820
these factors

1111
01:11:25,840 --> 01:11:30,990
will be ordinary gasoline factors connected to x

1112
01:11:31,010 --> 01:11:34,110
OK and for the centered observation of this

1113
01:11:34,150 --> 01:11:35,760
special factor

1114
01:11:35,780 --> 01:11:37,420
who's formula

1115
01:11:37,440 --> 01:11:39,070
over here

1116
01:11:39,090 --> 01:11:40,650
so this factor says

1117
01:11:40,670 --> 01:11:42,880
this is just the probability of observing

1118
01:11:42,900 --> 01:11:43,860
a y

1119
01:11:43,860 --> 01:11:46,320
which is between minus infinity

1120
01:11:46,340 --> 01:11:52,110
here is the probability of observing y whose magnitude bigger than t

1121
01:11:52,130 --> 01:11:54,550
so here get zero minus t

1122
01:11:55,900 --> 01:11:59,090
the probability of getting observation

1123
01:12:01,490 --> 01:12:04,460
some disintegrating the the likelihood

1124
01:12:04,470 --> 01:12:07,420
because you like that over the region

1125
01:12:11,320 --> 01:12:15,820
now will be the exact posterior for for acts in this problem

1126
01:12:17,110 --> 01:12:20,900
as if i have a bunch of these censored observations will get similar things will

1127
01:12:20,900 --> 01:12:26,110
happen before i have a bunch of these features the centre observations because some

1128
01:12:26,130 --> 01:12:30,510
and if i'm multiplying together all have to the these terms showing each of the

1129
01:12:30,530 --> 01:12:34,190
terms in this sum by the way is very simple because this term for example

1130
01:12:34,210 --> 01:12:36,360
is just a cumulative garcia

1131
01:12:37,130 --> 01:12:38,780
so this term b

1132
01:12:38,800 --> 01:12:43,130
the problem is that you have to solve two to this kind of gaussians

1133
01:12:43,170 --> 01:12:48,800
so if we want to estimate approximate the posterior for acts in this problem we

1134
01:12:48,800 --> 01:12:51,880
can run expectation propagation essentially what happens

1135
01:12:51,900 --> 01:12:54,380
when we visit each of these

1136
01:12:54,420 --> 01:12:56,570
calcium factors

1137
01:12:56,590 --> 01:12:59,070
you will find their approximations themselves

1138
01:12:59,070 --> 01:13:03,360
because there already and so when you want to formalise this find should yourself as

1139
01:13:03,360 --> 01:13:08,740
the approximation and the only thing that we need to wear something interesting happens

1140
01:13:08,740 --> 01:13:11,030
it is on these censored observations

1141
01:13:11,050 --> 01:13:13,570
where we're is going to get

1142
01:13:17,230 --> 01:13:19,900
the following problem we're going to have this observation

1143
01:13:19,920 --> 01:13:21,360
we're going to have this

1144
01:13:21,360 --> 01:13:23,820
context term

1145
01:13:23,840 --> 01:13:25,090
you know i x

1146
01:13:25,760 --> 01:13:28,730
in this case it will be easier to be the product of all the other

1147
01:13:28,730 --> 01:13:30,110
graphs in terms

1148
01:13:30,110 --> 01:13:34,780
my context for one censored observations and then only all i need to do is

1149
01:13:34,780 --> 01:13:37,170
to the project that

1150
01:13:37,210 --> 01:13:39,880
divide that by q not iax

1151
01:13:39,900 --> 01:13:41,130
and that will be my

1152
01:13:41,130 --> 01:13:47,490
approximation for that censored factor

1153
01:13:50,510 --> 01:13:53,260
this is the moment of this product

1154
01:13:53,280 --> 01:13:55,130
our our can be worked out

1155
01:13:55,150 --> 01:13:59,940
so basically you have discussed TV of time the grass in the work of

1156
01:13:59,940 --> 01:14:03,740
and i'll show later on a strict for working on moments of this type which

1157
01:14:03,740 --> 01:14:06,650
arise quite a lot in t

1158
01:14:06,740 --> 01:14:08,420
so all that

1159
01:14:08,460 --> 01:14:11,300
that shows how you might use this censoring this

1160
01:14:11,320 --> 01:14:13,730
this framework

1161
01:14:15,880 --> 01:14:18,920
so let's move on to some other problems

1162
01:14:18,940 --> 01:14:20,630
now illustrate how you can apply

1163
01:14:20,670 --> 01:14:23,970
those problems as well alright so

1164
01:14:24,030 --> 01:14:26,550
here's the problem that arises a lot in machine learning

1165
01:14:26,570 --> 01:14:29,760
you want to use the tracking problem right you have

1166
01:14:29,760 --> 01:14:34,360
if an object which is moving around i observe the object at certain spots

1167
01:14:34,380 --> 01:14:36,280
at certain times

1168
01:14:36,820 --> 01:14:40,820
and my measurements noisy so the object is moving around central location

1169
01:14:40,820 --> 01:14:42,280
which is given by x

1170
01:14:42,280 --> 01:14:43,990
and as an instance in time

1171
01:14:44,010 --> 01:14:48,470
i observe a noisy estimate its position so the y one which is noisy version

1172
01:14:48,470 --> 01:14:55,190
of x one y two is noisy version of x two and so on

1173
01:14:55,240 --> 01:14:59,530
so what is the factor graph that sort of problem well we have supposed that

1174
01:15:00,110 --> 01:15:04,420
object is undergoing some simple markov chains around the market very common assumptions

1175
01:15:04,670 --> 01:15:09,400
so for example this equation for the dynamics which turns into that

1176
01:15:09,420 --> 01:15:11,670
familiar linear chain photograph

1177
01:15:11,730 --> 01:15:19,130
and our observation just we have got noise tracks so we have this thing off

1178
01:15:19,150 --> 01:15:20,630
and the

1179
01:15:20,650 --> 01:15:23,990
estimation problem is that we want to estimate where the object you want to estimate

1180
01:15:23,990 --> 01:15:27,820
x given twice

1181
01:15:27,840 --> 01:15:33,030
OK so

1182
01:15:33,610 --> 01:15:34,650
what i'm going to do

1183
01:15:34,670 --> 01:15:36,970
so first of all

1184
01:15:39,050 --> 01:15:42,240
you might recognise this is familiar problems you might see on this is the common

1185
01:15:42,240 --> 01:15:46,990
for problem you know with your signal processing book and star rating calling filtering equations

1186
01:15:47,440 --> 01:15:49,690
but i would suggest don't do that

1187
01:15:49,710 --> 01:15:53,090
because we are going to give you is much more general so you can apply

1188
01:15:53,090 --> 01:15:53,960
not only to

1189
01:15:53,960 --> 01:15:57,630
you got some problems but nonlinear problems well i'll show you how you can write

1190
01:15:57,630 --> 01:16:01,400
on nonlinear problem so put away your signal processing book for while i just think

1191
01:16:01,400 --> 01:16:05,450
in terms of graphical models and so we just have a graphical model when applied

1192
01:16:05,450 --> 01:16:08,160
general purpose graph modeling techniques this problem

1193
01:16:08,160 --> 01:16:12,860
now we assume that it is orthogonal to all expansion

1194
01:16:12,870 --> 01:16:18,150
and also that this function is strictly monotonic therefore if we plug our general functions

1195
01:16:18,150 --> 01:16:19,880
so we set our function can be

1196
01:16:19,890 --> 01:16:21,650
decomposed into their

1197
01:16:21,670 --> 01:16:24,280
part in the span of the training points in the from part

1198
01:16:25,540 --> 01:16:27,460
we can rewrite this

1199
01:16:27,480 --> 01:16:29,210
just using pythagoras

1200
01:16:29,220 --> 01:16:30,820
this quantity

1201
01:16:30,830 --> 01:16:35,030
but now we since we know that there is only a strictly monotonic and also

1202
01:16:35,030 --> 01:16:37,670
the square root is strictly monotonic

1203
01:16:38,060 --> 01:16:41,050
then we know that if we throw away this part

1204
01:16:41,100 --> 01:16:44,630
we actually might get smaller but we're not going to get bigger

1205
01:16:44,650 --> 01:16:48,220
so we can if anything we can we can save something in terms of the

1206
01:16:49,560 --> 01:16:52,180
by throwing away his of part

1207
01:16:52,200 --> 01:16:57,770
so that means the objective function two parts the first part of the first part

1208
01:16:57,770 --> 01:17:00,670
it didn't matter whether we have enough problems part or not

1209
01:17:00,690 --> 01:17:04,950
the second part we actually have to pay more if we also know that is

1210
01:17:04,950 --> 01:17:10,650
nonzero the minimizer of term must have must be zero

1211
01:17:10,660 --> 01:17:15,830
so therefore the minimizer so before we started with this fall

1212
01:17:15,850 --> 01:17:19,750
we started with this general form of functions we showed that actually this part should

1213
01:17:19,750 --> 01:17:23,840
be zero for the minimizer therefore the minimizer has this kind of expansion which is

1214
01:17:23,840 --> 01:17:27,360
exactly what we wanted to prove

1215
01:17:27,370 --> 01:17:33,500
and there are two different applications of this theorem one is support vector classifier but

1216
01:17:33,500 --> 01:17:36,140
i'm not going to talk about this now because i alex small i will tell

1217
01:17:36,140 --> 01:17:40,380
you in detail another example

1218
01:17:40,390 --> 01:17:43,750
are maximally both posteriori so you only estimates

1219
01:17:43,760 --> 01:17:46,460
in a bayesian setting or kernel PCA

1220
01:17:46,470 --> 01:17:51,140
which i also don't have time to talk about all these things can be viewed

1221
01:17:51,140 --> 01:17:55,650
as special cases of the representer theorem

1222
01:17:57,830 --> 01:18:01,830
which brings me to the conclusion of my three lecture so far

1223
01:18:01,840 --> 01:18:07,720
so it was all about kernels and i hope that i managed to well i

1224
01:18:07,720 --> 01:18:13,260
started with the informal notion of thinking of the kernel as a similarity measure for

1225
01:18:13,260 --> 01:18:14,400
the data

1226
01:18:14,410 --> 01:18:16,440
so we started then

1227
01:18:16,450 --> 01:18:18,210
on the blackboard reconstructed

1228
01:18:18,220 --> 01:18:22,560
a hilbert space such that the dot product in that space is equal to the

1229
01:18:22,560 --> 01:18:28,420
kernel we constructed a representation of the data in the table space such that the

1230
01:18:28,420 --> 01:18:32,890
kernel is equal to the dot product in that space so therefore in a sense

1231
01:18:32,890 --> 01:18:34,610
we have constructed a

1232
01:18:34,630 --> 01:18:36,880
a linear representation of the data

1233
01:18:36,900 --> 01:18:41,960
so the kernel also corresponds to a representation of the data in finally due to

1234
01:18:41,960 --> 01:18:48,370
the representer theorem representer theorem i told you that you have a fairly general optimisation

1235
01:18:50,310 --> 01:18:56,170
then the solution of the optimisation of the function is an expansion in terms of

1236
01:18:56,190 --> 01:19:00,530
the kernel so in that sense the kernel also parameterizes the

1237
01:19:00,550 --> 01:19:05,520
the function space that we use for learning hypothesis space is sometimes called so i

1238
01:19:05,520 --> 01:19:08,010
mentioned before i answering some questions that

1239
01:19:08,030 --> 01:19:12,260
in machine learning is always crucial not only to have training data but also to

1240
01:19:12,260 --> 01:19:15,820
make hypothesis about what solutions we will admit

1241
01:19:15,840 --> 01:19:17,200
so in this case

1242
01:19:17,280 --> 01:19:21,080
due to the representer theorem you can see the set of possible solutions is actually

1243
01:19:21,080 --> 01:19:25,070
is a set of expansions of kernel sitting on centered on training points so the

1244
01:19:25,070 --> 01:19:32,590
kernel some whole represents a way of dealing with all these three issues at the

1245
01:19:32,590 --> 01:19:33,730
same time unified way

1246
01:19:34,120 --> 01:19:38,890
so that's that's quite nice from a mathematical point of view maybe that's why is

1247
01:19:38,890 --> 01:19:43,240
also very popular field to work in an interview different problems in

1248
01:19:43,250 --> 01:19:46,950
and from a practical point of view of course and i could show you that

1249
01:19:46,950 --> 01:19:49,590
but from a practical point of view the reason why people wouldn't girls is that

1250
01:19:50,150 --> 01:19:51,800
they can they can

1251
01:19:51,810 --> 01:19:55,090
people we did a lot of different geometric algorithms and

1252
01:19:55,100 --> 01:19:56,760
construct interesting non-linear

1253
01:19:56,880 --> 01:20:02,120
methods for data analysis and i only show you a few examples one was the

1254
01:20:02,530 --> 01:20:04,340
thousand windows-based classifier

1255
01:20:04,350 --> 01:20:10,860
i briefly mentioned the possibility of doing two sample tests with kernels of also mention

1256
01:20:10,880 --> 01:20:15,600
support vector machines you will see them some something about them tomorrow i mentioned kernel

1257
01:20:15,600 --> 01:20:18,070
PCA and i think we don't even have time to

1258
01:20:18,080 --> 01:20:21,970
talk about kernel PCA but i guess you can imagine how it's going to work

1259
01:20:21,970 --> 01:20:23,680
now that you know the country

1260
01:20:23,690 --> 01:20:27,860
so there's a lot of different algorithms so comes are

1261
01:20:29,300 --> 01:20:33,550
not only from a theoretical point of view but also from a practical point of

1262
01:20:33,550 --> 01:20:38,210
view and probably going to see some some examples in different pictures also references i

1263
01:20:38,210 --> 01:20:38,460
i don't

1264
01:20:40,630 --> 01:20:42,070
shortly say ten to the ten

1265
01:20:43,980 --> 01:20:45,180
bits memory

1266
01:20:48,570 --> 01:20:51,970
um that's if you've got one gig memory because they have more

1267
01:20:52,950 --> 01:20:57,590
but i don't want to be unnecessarily unfair because what they just sitting there being completely unused

1268
01:20:58,270 --> 01:20:59,950
what about in the city itself

1269
01:21:02,330 --> 01:21:05,460
maybe is another million or so devices in a single

1270
01:21:06,770 --> 01:21:07,370
ceepee u

1271
01:21:09,580 --> 01:21:13,440
so you could say i yeah the pigeon is being the greatest got more devices

1272
01:21:14,060 --> 01:21:18,800
but the pigeons devices are slow so we taking into account the clock rate

1273
01:21:25,600 --> 01:21:25,950
i'd say

1274
01:21:27,380 --> 01:21:29,680
the clock rate is something like a thousand megahertz

1275
01:21:30,540 --> 01:21:31,650
which means that's

1276
01:21:32,950 --> 01:21:34,130
we're in the ballpark of

1277
01:21:35,510 --> 01:21:36,400
ten fifteen

1278
01:21:36,860 --> 01:21:38,120
or ten from nineteen

1279
01:21:45,130 --> 01:21:45,650
the second

1280
01:21:46,700 --> 01:21:50,310
i just checked what the latest news on supercomputers is in

1281
01:21:50,760 --> 01:21:53,950
that's a super computers with lots and lots of superiors in terms of memory

1282
01:21:54,510 --> 01:21:55,130
and they can do

1283
01:21:56,110 --> 01:21:57,440
ten to sixteen

1284
01:21:58,220 --> 01:21:59,220
floating point

1285
01:22:01,870 --> 01:22:02,520
per second

1286
01:22:03,980 --> 01:22:09,020
so this is two different ways the avg quantifying how much you've got a grey

1287
01:22:09,340 --> 01:22:12,060
services in terms of the hardware

1288
01:22:15,280 --> 01:22:21,060
how many hardware device operations you've got a second and this is in terms of the output what an entire

1289
01:22:23,960 --> 01:22:24,360
this isn't

1290
01:22:24,800 --> 01:22:25,220
a single

1291
01:22:26,350 --> 01:22:27,150
craig anymore

1292
01:22:28,140 --> 01:22:28,420
this is

1293
01:22:29,420 --> 01:22:31,840
today's top soup supercomputer

1294
01:22:35,290 --> 01:22:38,250
and crank sixteen floating point operations per second

1295
01:22:39,100 --> 01:22:39,910
what about the pigeon

1296
01:22:40,510 --> 01:22:44,000
well clock rate corresponds to attend millisecond

1297
01:22:46,740 --> 01:22:51,070
i'd say roughly because a single neuron can fire a hundred times a second if you push it

1298
01:22:53,920 --> 01:22:58,330
you could argue that some bits of the hardware maybe have a faster clock time

1299
01:22:58,330 --> 01:23:03,590
maybe you know an advocate the idea synergies can do some very rapid interesting things

1300
01:23:04,380 --> 01:23:05,930
let's just run with this number anyway

1301
01:23:06,900 --> 01:23:09,640
that's means you've got ten to thirteen operations

1302
01:23:11,630 --> 01:23:12,400
per second

1303
01:23:14,180 --> 01:23:16,340
if you take the neurons has ure

1304
01:23:17,020 --> 01:23:18,050
operating devices

1305
01:23:20,250 --> 01:23:21,400
ten to the sixteenth

1306
01:23:23,150 --> 01:23:24,350
operations per second

1307
01:23:27,190 --> 01:23:29,500
if you view elementary transistor like

1308
01:23:32,110 --> 01:23:33,990
as being sorry these sign-ups is

1309
01:23:37,460 --> 01:23:39,670
so in terms of operations per second

1310
01:23:40,540 --> 01:23:43,600
device operations per second the craig got more

1311
01:23:44,000 --> 01:23:44,660
going it

1312
01:23:45,760 --> 01:23:49,570
they pigeon actual output floating operations per second

1313
01:23:49,970 --> 01:23:53,020
if you take the signups is doing floating point operations

1314
01:23:55,170 --> 01:23:59,500
the same level as a supercomputer in a single page brain so

1315
01:24:01,040 --> 01:24:03,590
amazing computational ability in a pigeon brain

1316
01:24:04,260 --> 01:24:05,820
but it's not actually bigger

1317
01:24:06,190 --> 01:24:08,560
in terms of these numbers than a crate yes

1318
01:24:11,380 --> 01:24:16,570
okay the question is what's the size of the words so you're saying for these floating point operations

1319
01:24:18,530 --> 01:24:20,910
he is a floating point operation craig so

1320
01:24:21,380 --> 01:24:24,910
probably that's some sort of operation along the lines of the thirty two bit

1321
01:24:26,050 --> 01:24:27,960
in getting multiplied by another thirty two

1322
01:24:28,650 --> 01:24:30,090
the interior or something like that

1323
01:24:30,600 --> 01:24:32,680
it and doing it absolutely perfectly

1324
01:24:35,070 --> 01:24:37,020
if you're lucky is probably doing

1325
01:24:37,900 --> 01:24:39,880
and maybe two of orbits

1326
01:24:42,870 --> 01:24:43,630
per operation

1327
01:24:44,310 --> 01:24:47,880
i guess i'd be surprised if on on a timescale just ten millisecond

1328
01:24:48,290 --> 01:24:51,570
really getting anything more than two of bit because it's all dominated by class on

1329
01:24:51,570 --> 01:24:56,550
noise weather channels are open or not you've only got small integer numbers of channels

1330
01:24:58,880 --> 01:25:00,770
so yes the word sizes all

1331
01:25:01,310 --> 01:25:04,410
the precision of these operations is bigger for the craig

1332
01:25:05,810 --> 01:25:07,410
i'm all for today supercomputer

1333
01:25:08,160 --> 01:25:10,600
then for the patient there in the same ballpark

1334
01:25:12,230 --> 01:25:15,550
and that's not the case that the pigeon has far more resources at its disposal

1335
01:25:18,280 --> 01:25:23,130
maybe the pigeon is far better this image recognizing task because it organizes hardware

1336
01:25:33,420 --> 01:25:37,430
are essentially just a standard serial computers wired up slightly

1337
01:25:38,050 --> 01:25:38,430
different way

1338
01:25:38,990 --> 01:25:39,890
they have a sleepy u

1339
01:25:41,580 --> 01:25:42,970
all also to be used

1340
01:25:45,710 --> 01:25:47,490
if you want to store new memory

1341
01:25:49,900 --> 01:25:55,420
without losing interview old memories in a standard computer you have to have some new virgin hardware

1342
01:26:02,120 --> 01:26:06,080
so there is no connection at all between all memories and numerically just put them

1343
01:26:06,080 --> 01:26:07,770
in another place and you need to know

1344
01:26:08,330 --> 01:26:08,880
well they are

1345
01:26:11,490 --> 01:26:13,660
meanwhile pigeon brains have um

1346
01:26:17,830 --> 01:26:19,090
they have high conductivity

1347
01:26:23,620 --> 01:26:26,760
the typical neuron is probably connected to about a thousand

1348
01:26:27,840 --> 01:26:28,500
other neurons

1349
01:26:29,040 --> 01:26:30,970
whereas the typical transistor

1350
01:26:31,450 --> 01:26:32,070
and craig

1351
01:26:32,500 --> 01:26:33,940
is probably connected

1352
01:26:37,580 --> 01:26:39,260
roughly three or ten

1353
01:26:39,950 --> 01:26:40,780
other transistors

1354
01:26:46,710 --> 01:26:48,740
the computation however is being done

1355
01:26:50,010 --> 01:26:51,020
not we understand it

1356
01:26:51,690 --> 01:26:53,050
is definitely distributed

1357
01:26:57,410 --> 01:26:59,150
pigeon learn some new memory

1358
01:26:59,150 --> 01:27:04,030
restaurant process and then the guy wisely chooses his own table

1359
01:27:06,480 --> 01:27:08,900
the real point is used to meeting and make sure

1360
01:27:09,410 --> 01:27:13,540
as a density is your evolving over time just as the number of pixels so

1361
01:27:13,600 --> 01:27:18,340
can change and particle methods can be used to approximate this type of evolving

1362
01:27:18,420 --> 01:27:20,880
this it is

1363
01:27:20,940 --> 01:27:22,940
to do nonparametric inference

1364
01:27:23,180 --> 01:27:28,490
tom griffiths also did some work on nonparametric inference i forgot to include

1365
01:27:29,140 --> 01:27:32,910
let's come to static models

1366
01:27:33,350 --> 01:27:37,170
and those are the most we tend to actually use a lot

1367
01:27:37,460 --> 01:27:39,720
things like both machines for example

1368
01:27:39,800 --> 01:27:43,720
now in the static model the state space is not growing over time it's not

1369
01:27:43,720 --> 01:27:47,170
changing over time you state space is fixed

1370
01:27:47,420 --> 01:27:52,050
there's no index and the state space is defined

1371
01:27:52,120 --> 01:27:53,110
just on

1372
01:27:53,130 --> 01:27:56,010
kspace x

1373
01:27:56,010 --> 01:27:57,240
the space

1374
01:27:57,260 --> 01:27:59,260
it's not like what we had before

1375
01:27:59,270 --> 01:28:01,220
which was the state space there was growing

1376
01:28:01,240 --> 01:28:03,230
over time

1377
01:28:03,240 --> 01:28:07,150
however we might want to have the distribution is changing over time

1378
01:28:07,160 --> 01:28:09,030
even though the state spaces

1379
01:28:09,070 --> 01:28:11,610
thanks to get better samples from it

1380
01:28:11,650 --> 01:28:16,490
so one example is if you have a boltzmann machine

1381
01:28:17,290 --> 01:28:19,530
so two hierarchical levels

1382
01:28:19,550 --> 01:28:25,200
if you really like deep belief networks i believe this and tutorial on the topic

1383
01:28:25,940 --> 01:28:29,390
next door might be more related nonetheless

1384
01:28:30,020 --> 01:28:34,870
this improves the actually to apply to these architectures and people like you and me

1385
01:28:35,160 --> 01:28:39,380
marry and ruslan have actually applied

1386
01:28:43,110 --> 01:28:44,410
so we started

1387
01:28:44,440 --> 01:28:48,080
the way we do this we start with the simpler distribution

1388
01:28:48,090 --> 01:28:50,190
and in this case it's a tree

1389
01:28:51,210 --> 01:28:55,620
and in the simplest region it's possible to draw samples of this guy

1390
01:28:55,630 --> 01:28:59,680
exactly the same the forward backward algorithm so we can get exact samples we don't

1391
01:28:59,680 --> 01:29:02,660
get approximate samples and now if we had a way

1392
01:29:02,670 --> 01:29:06,600
of migrating those samples to become samples of this

1393
01:29:06,640 --> 01:29:09,640
and i remind you that in an undirected graphical model

1394
01:29:09,650 --> 01:29:13,680
every time we have an age what we're doing is we're adding constraint

1395
01:29:13,690 --> 01:29:17,870
by adding the age forcing x two to agree with tax-free three

1396
01:29:17,880 --> 01:29:22,490
and in fact that's why these problems are hard because each h either requires agreement

1397
01:29:22,490 --> 01:29:27,800
took our disagreement and this obviously needs to a lot of contradictions and these systems

1398
01:29:28,490 --> 01:29:34,290
known as frustrated systems

1399
01:29:34,300 --> 01:29:38,110
but if you have the constraints one at the time moving the samples the way

1400
01:29:38,110 --> 01:29:41,840
we heard in the samples as we are adding constraints were able to move the

1401
01:29:41,840 --> 01:29:44,220
samples to these regions

1402
01:29:44,490 --> 01:29:48,360
we we might end up with some of the the right target distribution

1403
01:29:48,410 --> 01:29:50,570
that's the sort of intuition for it

1404
01:29:51,390 --> 01:29:53,740
after is in blue that's in green

1405
01:29:53,780 --> 01:29:58,530
the only difference thing now is the distribution of static the sequences artificial

1406
01:29:58,570 --> 01:30:02,340
what sequence to construct its arbitrary

1407
01:30:02,360 --> 01:30:07,140
you can construct a sequence by hand different sequences of possible could have any of

1408
01:30:07,280 --> 01:30:09,490
the edges or i could add edges

1409
01:30:09,490 --> 01:30:12,890
and in fact because we can construct a sequence you can actually take a look

1410
01:30:12,890 --> 01:30:15,180
ahead and if you don't like what you got

1411
01:30:15,200 --> 01:30:21,260
if you go back and all sorts of games start appearing for the station

1412
01:30:21,260 --> 01:30:26,450
so one example of this is just sequential bayesian estimation when you have a posterior

1413
01:30:26,450 --> 01:30:30,030
and you keep getting more data you start with some data in the data keeps

1414
01:30:30,990 --> 01:30:35,280
so in this case is just a graphical model again we are adding more potential

1415
01:30:35,280 --> 01:30:37,800
it's very similar to what we had before

1416
01:30:37,820 --> 01:30:41,590
another examples annealing where we might have a distribution

1417
01:30:41,620 --> 01:30:45,780
and we have some sort of power the power goes up this thing gets more

1418
01:30:45,780 --> 01:30:50,640
and more concentrated sort of samples concentrate on the peaks of the distribution

1419
01:30:50,640 --> 01:30:55,260
well we might want to do tempering where we start with an easy

1420
01:30:55,280 --> 01:30:58,510
to sample distribution mu

1421
01:30:58,530 --> 01:31:01,780
and we start with a schedule that makes mu beta one

1422
01:31:01,840 --> 01:31:04,910
let's active so we said it initially to one

1423
01:31:04,930 --> 01:31:07,800
and then we decrease eta and so the samples

1424
01:31:07,820 --> 01:31:11,660
but we had from you which was the easy to some distribution becomes samples of

1425
01:31:12,550 --> 01:31:15,470
in the limit

1426
01:31:15,720 --> 01:31:20,340
we can do rare event simulation which is a big problem especially in high dimensions

1427
01:31:20,340 --> 01:31:23,820
because in high dimensions a lot of sense becomes really

1428
01:31:23,890 --> 01:31:25,620
exponentially small

1429
01:31:25,860 --> 01:31:29,160
so if we have to estimate the probability of an event and this problem is

1430
01:31:29,160 --> 01:31:32,050
very small like tail probability

1431
01:31:33,570 --> 01:31:36,490
so the target distribution and the

1432
01:31:37,140 --> 01:31:39,180
subject is

1433
01:31:39,200 --> 01:31:42,610
the final for this very tiny set

1434
01:31:42,640 --> 01:31:47,220
we can again construct a sequence that first we sample from from the whole state

1435
01:31:47,220 --> 01:31:48,860
space and then we just

1436
01:31:50,800 --> 01:31:53,760
we're we sampling from until eventually we

1437
01:31:53,760 --> 01:31:55,820
and our sampling from the right

1438
01:31:55,840 --> 01:31:59,700
target distribution so if you have a system where you're adding more and more and

1439
01:31:59,700 --> 01:32:00,970
more constraints

1440
01:32:00,990 --> 01:32:03,860
and you're trying to sample events are very rare

1441
01:32:03,860 --> 01:32:07,740
like rewards in for reinforcement learning for example if there were if there's very

1442
01:32:08,030 --> 01:32:10,200
logprob motivating that reward

1443
01:32:10,220 --> 01:32:12,950
you might want to construct a sequence u

1444
01:32:13,070 --> 01:32:14,340
zoom in

1445
01:32:14,610 --> 01:32:16,950
in important areas

1446
01:32:17,010 --> 01:32:19,570
and this is true of many other problems in computer science

1447
01:32:19,590 --> 01:32:24,050
in science you might find some constraints always work like this you are closest one

1448
01:32:24,050 --> 01:32:25,110
at the time

1449
01:32:25,140 --> 01:32:29,620
so you have think constraints you could stochastic version of these methods for sat

1450
01:32:29,640 --> 01:32:32,530
and it's been used in other problems were

1451
01:32:32,530 --> 01:32:36,780
a study by computer science theories

1452
01:32:36,800 --> 01:32:38,340
OK so

1453
01:32:38,360 --> 01:32:43,030
the derivation of the moral and are not that many people work on this problem

1454
01:32:43,030 --> 01:32:48,930
was the really hard problem two to considered as one good solution

1455
01:32:48,950 --> 01:32:52,530
one very reasonable solution to come up with the following

1456
01:32:52,530 --> 01:32:56,050
even though we care about the topic distribution that static

1457
01:32:57,320 --> 01:33:02,590
we can do is we can create a joint distribution by expanding

1458
01:33:02,610 --> 01:33:03,950
the state space

1459
01:33:03,970 --> 01:33:06,340
so we create a much larger distribution

1460
01:33:06,430 --> 01:33:09,740
there's always to do that but

1461
01:33:09,800 --> 01:33:12,740
they tend not to be as efficient as this

1462
01:33:12,740 --> 01:33:14,470
now for various reasons

1463
01:33:14,470 --> 01:33:20,090
but we construct is extended distribution so when i take the

1464
01:33:20,120 --> 01:33:22,260
one marginalize

1465
01:33:22,590 --> 01:33:25,840
and actually this should be over x one to n minus one college at the

1466
01:33:26,700 --> 01:33:31,530
when i marginalized from one to n minus one i still get the right target

1467
01:33:31,530 --> 01:33:35,570
distribution of what

1468
01:33:35,590 --> 01:33:39,590
again the derivation is the same as the one that i just scribble before

1469
01:33:39,610 --> 01:33:42,200
the importance weights are

1470
01:33:42,220 --> 01:33:45,120
the unnormalized function divided by the proposal

1471
01:33:45,180 --> 01:33:49,700
and in these methods the proposal will typically be an MCMC kernel so we're going

1472
01:33:49,700 --> 01:33:53,660
to propose by just doing one step of gets the one step of metropolis source

1473
01:33:53,990 --> 01:33:56,910
in one go your favourite samples

1474
01:33:57,510 --> 01:33:59,840
so i'm going to use k

1475
01:33:59,860 --> 01:34:01,030
instead of q

1476
01:34:01,080 --> 01:34:03,010
it's the same thing

1477
01:34:03,430 --> 01:34:05,780
and again the same derivation

1478
01:34:05,800 --> 01:34:10,390
and importance weights we end up with now have this extra matrix the matrix

1479
01:34:10,450 --> 01:34:12,820
that's exactly what we had before

1480
01:34:12,840 --> 01:34:19,360
except that there is this extra l matrix

1481
01:34:19,410 --> 01:34:20,930
and so the algorithm

1482
01:34:20,950 --> 01:34:24,990
this again is and the same type of sequential monte carlo we had before

1483
01:34:25,070 --> 01:34:28,550
well for the boltzmann machine we would have had all the samples drawn from the

1484
01:34:28,550 --> 01:34:30,820
tree at the first step

1485
01:34:30,840 --> 01:34:35,220
and then we would have moved to those samples by applying one gibbs iterations through

1486
01:34:35,220 --> 01:34:38,020
is the probability of the intersection of a and b

1487
01:34:38,030 --> 01:34:40,710
divided by the probability of b

1488
01:34:40,730 --> 01:34:42,880
that's just the definition

1489
01:34:42,900 --> 01:34:46,700
and a and b are of course just labels i can turn this around and

1490
01:34:46,700 --> 01:34:49,300
right for the probability of b

1491
01:34:49,300 --> 01:34:50,680
given a

1492
01:34:50,710 --> 01:34:55,390
is the probability of the intersection of being a divided by the probability

1493
01:34:58,020 --> 01:35:01,510
but then if you think about the way what does it mean a intersect to

1494
01:35:01,520 --> 01:35:05,850
be that that intersection of subsets a intersects b

1495
01:35:05,870 --> 01:35:10,520
is the same as b intersect a we denote the same subset

1496
01:35:10,620 --> 01:35:15,620
so therefore the probability of those two things also must be equal

1497
01:35:15,630 --> 01:35:20,340
so if i simply solve each of these two equations for the probability of a

1498
01:35:20,340 --> 01:35:25,140
intersect b here the probability b intersect a and i said them equal

1499
01:35:25,160 --> 01:35:26,660
then you get an equation

1500
01:35:26,710 --> 01:35:28,870
which i can then be right in this form

1501
01:35:28,880 --> 01:35:32,060
it gives them the probability of a given b

1502
01:35:32,080 --> 01:35:35,450
is equal to the probability of b given a

1503
01:35:35,480 --> 01:35:40,800
times the probability of a divided by the probability of b that is based there

1504
01:35:40,800 --> 01:35:42,930
which is very important theorem

1505
01:35:42,950 --> 01:35:46,630
in comes up all the time in the analysis

1506
01:35:46,640 --> 01:35:48,460
it was published in

1507
01:35:48,470 --> 01:35:52,700
actually a couple years after bayes died in nineteen sixty three

1508
01:35:52,710 --> 01:35:56,190
and i'm sure you can google for this and find it on online and reprinted

1509
01:35:56,190 --> 01:35:59,630
many places it's interesting actually to take a look at this to see how much

1510
01:35:59,650 --> 01:36:04,600
the english language has changed in two hundred fifty years it's absolutely unreadable see if

1511
01:36:04,600 --> 01:36:08,160
you read it and you can even realise he's talking about probability theory i think

1512
01:36:08,160 --> 01:36:10,890
that's that's a challenge

1513
01:36:10,940 --> 01:36:14,160
but it's certainly very important there

1514
01:36:14,170 --> 01:36:17,860
now certainly go back to this

1515
01:36:17,870 --> 01:36:22,800
the probability of b that appears in the denominator very often

1516
01:36:22,800 --> 01:36:24,950
it's useful to rewrite that

1517
01:36:24,970 --> 01:36:26,830
in a slightly different form

1518
01:36:26,840 --> 01:36:29,680
and so that's what i want to talk about now is is a way of

1519
01:36:29,680 --> 01:36:30,850
rewriting that

1520
01:36:30,870 --> 01:36:34,660
probability of the of the smaller subset which is going to go

1521
01:36:34,710 --> 01:36:38,390
for the condition of the conditional probability

1522
01:36:39,530 --> 01:36:43,600
i have two ways of explaining one with a picture one the some formulas

1523
01:36:43,620 --> 01:36:46,450
so let me start just with the picture suppose the

1524
01:36:46,950 --> 01:36:49,840
rectangle represents the sample space

1525
01:36:49,920 --> 01:36:53,440
and then the red curve represents b

1526
01:36:53,460 --> 01:36:56,930
so i'm i'm trying to find the probability of a given b so i'm going

1527
01:36:56,930 --> 01:37:01,930
to suppose that that i'm going to impose the condition b on the sample space

1528
01:37:03,060 --> 01:37:05,990
in certain circumstances it can be possible to

1529
01:37:07,550 --> 01:37:10,370
the initial sample space s in two

1530
01:37:10,380 --> 01:37:12,570
disjoint pieces

1531
01:37:12,580 --> 01:37:16,490
so is that suppose i've done that and and i will label those pieces a

1532
01:37:16,490 --> 01:37:19,350
sub ISO a with an index

1533
01:37:19,370 --> 01:37:23,700
and so these are the different a one a two a three and so forth

1534
01:37:23,740 --> 01:37:26,200
and then in the places where those

1535
01:37:26,210 --> 01:37:28,030
pieces which are all disjoint

1536
01:37:28,040 --> 01:37:30,370
where they intersect with b

1537
01:37:30,400 --> 01:37:31,850
i would say a b

1538
01:37:31,850 --> 01:37:35,240
intersect a that's what the yellow guy represents

1539
01:37:35,260 --> 01:37:37,780
now so there would have to be

1540
01:37:37,800 --> 01:37:41,810
intersect AIP intersect at j and so forth

1541
01:37:41,820 --> 01:37:47,350
remember what the third kolmogorov axioms sense it says that if subsets are disjoint

1542
01:37:47,380 --> 01:37:50,110
then the probability is simply add

1543
01:37:50,130 --> 01:37:54,040
the statement of the law that i'm trying to derive the following it says that

1544
01:37:54,040 --> 01:37:55,590
the probability of b

1545
01:37:55,600 --> 01:37:59,650
is simply the sum of the probabilities of these individual pieces

1546
01:37:59,680 --> 01:38:02,490
b intersect AIP intersect

1547
01:38:02,510 --> 01:38:03,740
what that on there

1548
01:38:03,760 --> 01:38:07,600
and so forth if you simply sum the probabilities correspond to those

1549
01:38:08,070 --> 01:38:10,460
pieces you get the probability of b

1550
01:38:10,530 --> 01:38:11,840
that's the graphical

1551
01:38:11,860 --> 01:38:13,300
derivation of

1552
01:38:13,320 --> 01:38:16,260
the theorem of the formula that i'm going to write down

1553
01:38:16,270 --> 01:38:20,710
now in formulas here it is again you simply say that b is the union

1554
01:38:20,720 --> 01:38:23,190
of all these small chunks be

1555
01:38:23,210 --> 01:38:24,960
intersect a i

1556
01:38:25,010 --> 01:38:29,600
and so therefore the probability of b is equal to the sum of the corresponding

1557
01:38:29,600 --> 01:38:33,180
probabilities because they're all disjoint

1558
01:38:33,200 --> 01:38:38,090
and if you think back to the definition of conditional probability you see

1559
01:38:38,110 --> 01:38:41,170
the probability of b intersect AI

1560
01:38:41,220 --> 01:38:44,820
that's simply what appears in the numerator of the

1561
01:38:44,830 --> 01:38:50,240
definition of conditional probability that simply equal by definition to the probability of b

1562
01:38:50,260 --> 01:38:54,330
given a i times the probability of a i

1563
01:38:54,370 --> 01:38:59,370
right so there is that is that my formula for the probability of b

1564
01:38:59,390 --> 01:39:03,380
that's called the law of total probability and if i put that then in the

1565
01:39:04,520 --> 01:39:07,370
of bayes theorem were before i had p

1566
01:39:07,390 --> 01:39:08,920
that gives me the

1567
01:39:08,940 --> 01:39:12,410
the form of bayes theorem that is actually useful right so that the

1568
01:39:12,430 --> 01:39:18,370
the version of the of bayes theorem which we actually apply

1569
01:39:18,410 --> 01:39:20,130
OK so let's do an example

1570
01:39:20,140 --> 01:39:24,770
with that and by by tradition the first example that always gets shown with bayes

1571
01:39:24,770 --> 01:39:28,150
theorem has to do with the medical test this tradition

1572
01:39:28,170 --> 01:39:30,130
so i suppose you have some

1573
01:39:30,130 --> 01:39:33,740
some disease i guess to be topical this year i could have chosen swine flu

1574
01:39:33,740 --> 01:39:36,210
or something i suppose you have some

1575
01:39:36,220 --> 01:39:40,280
the disease and that in some group of people

1576
01:39:40,300 --> 01:39:44,670
suppose the probability to have the disease is one in a thousand

1577
01:39:44,700 --> 01:39:47,630
o point o o one and so therefore

1578
01:39:47,630 --> 01:39:51,530
the probability not to have the disease has to be o point nine nine nine

1579
01:39:51,680 --> 01:39:55,280
some of those two numbers has to be equal to one

1580
01:39:55,290 --> 01:39:59,720
right now suppose you have a test which can test whether a person is a

1581
01:39:59,720 --> 01:40:03,310
carrier of the disease but this test isn't perfect

1582
01:40:03,430 --> 01:40:05,260
it is simply a plus

1583
01:40:05,260 --> 01:40:07,830
of positive or negative results

1584
01:40:07,870 --> 01:40:11,040
but the test isn't perfect suppose the probability

1585
01:40:11,050 --> 01:40:14,690
to get a positive result given that you have the disease

1586
01:40:14,780 --> 01:40:16,920
is ninety eight percent

1587
01:40:16,930 --> 01:40:20,600
and so then passed the probability of getting a negative result

1588
01:40:20,600 --> 01:40:22,550
even though given you have the disease

1589
01:40:22,570 --> 01:40:23,860
is two percent

1590
01:40:23,880 --> 01:40:27,540
some of those two numbers has to be equal to one by construction

1591
01:40:30,210 --> 01:40:33,510
if you go to your doctor and you say well how reliable is this test

1592
01:40:34,160 --> 01:40:36,090
hearts ninety percent reliable

1593
01:40:36,090 --> 01:40:39,280
well then already know your doctors in very good doctor because that

1594
01:40:39,290 --> 01:40:44,360
that alone is insufficient to characterize the reliability of the test

1595
01:40:44,380 --> 01:40:45,820
you can't simply say

1596
01:40:45,820 --> 01:40:47,970
what is the probability say together

1597
01:40:48,030 --> 01:40:51,430
incorrect negative result you also have to specify

1598
01:40:51,450 --> 01:40:54,200
what is the probability to get a positive result

1599
01:40:54,220 --> 01:40:58,520
if the person didn't have the disease that's that's separate number that's not derivable from

1600
01:40:58,520 --> 01:41:03,300
those so suppose in this case that happened to be three percent

1601
01:41:03,310 --> 01:41:04,440
right then

1602
01:41:04,460 --> 01:41:06,760
the probability to get negative result

1603
01:41:06,770 --> 01:41:10,960
given the person doesn't have the disease is ninety seven percent those two numbers have

1604
01:41:10,960 --> 01:41:13,360
to add up to one

1605
01:41:13,410 --> 01:41:16,280
OK so in any case it looks like it's fairly reliable

1606
01:41:16,320 --> 01:41:20,360
test you know maybe the doctor is a ninety percent ninety seven percent and it

1607
01:41:20,370 --> 01:41:22,030
looks like a pretty good test

1608
01:41:22,040 --> 01:41:27,460
so suppose you take this test and you come back with positive results

1609
01:41:27,480 --> 01:41:29,310
how worried should you be

1610
01:41:29,320 --> 01:41:31,360
do you have the disease

1611
01:41:31,410 --> 01:41:34,440
OK so if you look at the numbers you realize that what is the number

1612
01:41:34,440 --> 01:41:35,650
that you really want

1613
01:41:35,700 --> 01:41:39,110
you want to know the probability to have the disease

1614
01:41:39,110 --> 01:41:42,040
given that you have a positive test result

1615
01:41:42,050 --> 01:41:44,640
that's not the same as any other numbers up there

1616
01:41:44,660 --> 01:41:48,590
but we can get that using based there

1617
01:41:48,590 --> 01:41:50,180
so what we want

1618
01:41:50,200 --> 01:41:54,370
is the probability to have the disease given the positive test results

1619
01:41:54,370 --> 01:41:58,270
that is equal to the probability to have a positive test result

1620
01:41:58,290 --> 01:42:00,180
given that you have the disease

1621
01:42:01,190 --> 01:42:04,830
the probability overall what's called the prior probability

1622
01:42:04,830 --> 01:42:08,350
they have the disease that's before you took any test

1623
01:42:08,360 --> 01:42:13,020
and then in the denominator here we have basically what was coming in through

1624
01:42:13,140 --> 01:42:18,630
the law of total probability this is the probability of a positive test result rewritten

1625
01:42:18,630 --> 01:42:23,460
using the law of total probability that's the part probability of class given the disease

1626
01:42:23,470 --> 01:42:28,370
times the probability of the disease plus the probability of a positive test result

1627
01:42:28,370 --> 01:42:30,180
given that you don't have the disease

1628
01:42:30,210 --> 01:42:32,020
times the prior probability

1629
01:42:32,020 --> 01:42:33,840
not to have the disease

1630
01:42:33,860 --> 01:42:36,350
OK so we can simply plug in the numbers

1631
01:42:36,390 --> 01:42:37,470
from the previous

1632
01:42:38,790 --> 01:42:40,270
and work it out

1633
01:42:40,270 --> 01:42:44,680
then it has to simulations of the ranking civil twelve policies and you have to

1634
01:42:44,680 --> 01:42:48,610
simulate that as well and we have used in the same amount of space as

1635
01:42:48,610 --> 01:42:50,610
the comparative law

1636
01:42:50,910 --> 01:42:53,950
so i just want to point out injured

1637
01:42:58,240 --> 01:43:01,700
the simplest algorithm will be when the well

1638
01:43:01,720 --> 01:43:05,940
so they really have discovered by the current best policy would best means lowest number

1639
01:43:05,940 --> 01:43:08,700
of hits in the window of say three hundred

1640
01:43:08,710 --> 01:43:10,200
this works well

1641
01:43:10,210 --> 01:43:11,820
including a little bit

1642
01:43:13,160 --> 01:43:17,040
it's hard to the window size and it takes a lot of additional space

1643
01:43:17,060 --> 01:43:19,210
so what we did is we use

1644
01:43:20,020 --> 01:43:21,630
four online learning

1645
01:43:22,620 --> 01:43:27,030
we have one where one weight per expert policy

1646
01:43:27,040 --> 01:43:30,230
we're WIC estimate earned

1647
01:43:30,280 --> 01:43:35,270
relative performance of policy i believe that the current policy it's best if the numbers

1648
01:43:35,270 --> 01:43:39,920
closed is point nine it's a pretty good policy if it's point zero one it's

1649
01:43:39,920 --> 01:43:42,970
about policy

1650
01:43:42,990 --> 01:43:46,360
with respect to the recent data set

1651
01:43:46,400 --> 01:43:47,650
we need to do

1652
01:43:47,680 --> 01:43:49,650
updates of these weights

1653
01:43:49,660 --> 01:43:53,510
one is going to be the last updated twenty e to the minus loss with

1654
01:43:53,560 --> 01:43:55,870
all this stuff i did this in the first lecture

1655
01:43:55,880 --> 01:44:00,070
and then a new classifier is called the share updates which are very crucial to

1656
01:44:00,070 --> 01:44:02,730
prevent the curse of the multiplicative updates

1657
01:44:02,740 --> 01:44:08,060
it's quite interesting this of these two groups so you to fix them

1658
01:44:09,840 --> 01:44:13,720
the first update is one of those updates e to the minus loss update you

1659
01:44:13,720 --> 01:44:18,040
can also write it is always tend to be been miss it's a number between

1660
01:44:18,040 --> 01:44:24,090
zero one the majority of the world's simplest version of the the second of july

1661
01:44:24,100 --> 01:44:26,690
OK that by itself

1662
01:44:26,730 --> 01:44:28,850
it's very dangerous data is

1663
01:44:29,730 --> 01:44:32,980
very dangerous because if one policy

1664
01:44:32,990 --> 01:44:34,290
isabelle bad or

1665
01:44:34,340 --> 01:44:37,440
it has a lot of missus it's which goes down to zero

1666
01:44:37,450 --> 01:44:41,610
so when the same policy gets good later on

1667
01:44:41,610 --> 01:44:42,870
you can recover

1668
01:44:42,880 --> 01:44:46,440
so the multiplicative update is a very dangerous thing because is very fast it's too

1669
01:44:48,350 --> 01:44:52,530
so what you need to do is you need to do what's called share update

1670
01:44:52,540 --> 01:44:56,730
we were from california we share so

1671
01:44:57,370 --> 01:45:00,000
so you

1672
01:45:00,010 --> 01:45:02,660
take a little bit of the weight

1673
01:45:03,750 --> 01:45:05,600
so you take your weight vector to be

1674
01:45:05,610 --> 01:45:08,600
the weight vector you just got but you also take a little of

1675
01:45:08,610 --> 01:45:13,200
the weight given to the past average weight and this has an amazing effects that

1676
01:45:13,200 --> 01:45:18,610
i will elaborate on now in the moment it put it lets the algorithms recover

1677
01:45:18,610 --> 01:45:20,270
into all kinds of things and i

1678
01:45:20,360 --> 01:45:25,010
we can theoretically motivated this actually even though

1679
01:45:25,020 --> 01:45:28,010
the catching applications much richer

1680
01:45:28,020 --> 01:45:32,260
and full of clue jason the theory doesn't apply but in the clean context we

1681
01:45:32,260 --> 01:45:36,520
can what it is

1682
01:45:36,540 --> 01:45:39,450
these are the probabilities

1683
01:45:39,470 --> 01:45:43,220
and you can see that it really does shifting these are the probability of one

1684
01:45:43,220 --> 01:45:47,340
of the runs of the algorithms on one of the datasets you can see that

1685
01:45:47,340 --> 01:45:52,500
some of the weights very low some algorithms but some weights the good ones actually

1686
01:45:52,500 --> 01:45:57,270
if you go back and look at the slides

1687
01:45:57,310 --> 01:46:01,540
showed the current best response to look at these colours that are often used here

1688
01:46:01,560 --> 01:46:03,850
a lot of purple

1689
01:46:03,860 --> 01:46:09,060
and the way and these out cities online algorithms the kind of

1690
01:46:09,140 --> 01:46:13,660
shift between you know he is the herbalist weight is pretty high the right one

1691
01:46:13,660 --> 01:46:14,990
does the right thing

1692
01:46:15,000 --> 01:46:16,710
it's kind of like cool

1693
01:46:16,730 --> 01:46:18,090
to watch

1694
01:46:18,100 --> 01:46:20,730
so now i'm going to give you a little bit more

1695
01:46:20,820 --> 01:46:24,250
information online out shifting expert on them

1696
01:46:24,270 --> 01:46:27,090
OK remember with the expert framework

1697
01:46:27,110 --> 01:46:29,710
they want to predict the weather

1698
01:46:29,720 --> 01:46:31,850
prediction true label loss

1699
01:46:31,860 --> 01:46:34,250
we had updates

1700
01:46:34,260 --> 01:46:37,490
this was summarizing the predictions of the experts

1701
01:46:37,500 --> 01:46:39,040
we had updates

1702
01:46:39,060 --> 01:46:40,540
this was the the

1703
01:46:40,560 --> 01:46:41,520
first of all

1704
01:46:41,530 --> 01:46:44,910
good prediction to make predictions labels incur loss

1705
01:46:44,930 --> 01:46:49,950
no still no statistical assumptions on the

1706
01:46:49,990 --> 01:46:54,960
on the data and you want to do as well as the best expert

1707
01:46:54,970 --> 01:46:57,720
this is the total number of the total

1708
01:46:57,990 --> 01:47:00,910
x that is the kind of about one

1709
01:47:00,990 --> 01:47:04,760
i don't really the loss the best expert

1710
01:47:04,770 --> 01:47:09,240
they update was simple weighted average prediction and it was

1711
01:47:09,280 --> 01:47:11,350
e to the minus loss

1712
01:47:12,030 --> 01:47:15,590
weighted majority was an example generalized but you all

1713
01:47:15,630 --> 01:47:18,640
i talked about all this

1714
01:47:18,660 --> 01:47:24,280
good but now what happens is i mean the shifting setting i have expert seven

1715
01:47:24,290 --> 01:47:27,470
might be good for a while and then the data shifts

1716
01:47:27,520 --> 01:47:29,450
exp at twenty is good for one

1717
01:47:29,460 --> 01:47:34,750
then the policy for is good then policy fifty one is going missing

1718
01:47:34,790 --> 01:47:39,490
so if i use these kind of multiplicative updates at this point

1719
01:47:39,490 --> 01:47:40,900
the weight of the two

1720
01:47:40,910 --> 01:47:44,840
exp at twenty might be zero why because expert twenty might have

1721
01:47:44,860 --> 01:47:48,250
stream amount missus you've lost in this area

1722
01:47:48,260 --> 01:47:52,890
so i need to do something to help it recover

1723
01:47:52,950 --> 01:47:56,320
and that's what share does so we want to do this

1724
01:47:56,330 --> 01:48:01,070
well as the best compartment expert what's the loss of this commodity for total loss

1725
01:48:01,070 --> 01:48:03,490
in each of these segments

1726
01:48:03,500 --> 01:48:06,830
lots of data two well it does not recover fast enough

1727
01:48:06,840 --> 01:48:08,370
he has to fix

1728
01:48:08,640 --> 01:48:11,870
due to its that

1729
01:48:11,880 --> 01:48:14,060
right so that's what explain

1730
01:48:14,070 --> 01:48:19,660
two of these the first there is this multiplicative updates either minus loss

1731
01:48:19,690 --> 01:48:23,680
that homes in very fast it's i call it the short-term memory it homes in

1732
01:48:23,690 --> 01:48:26,330
very fast to the best policy

1733
01:48:26,340 --> 01:48:29,360
if policy is good

1734
01:48:30,320 --> 01:48:31,490
its weight

1735
01:48:31,520 --> 01:48:35,820
is going to be not multiply by these factors and all the other ones are

1736
01:48:35,820 --> 01:48:39,750
multiplied by these bad fact by these factors and wiped out and it's going to

1737
01:48:39,750 --> 01:48:45,370
be brought the good way is going to be brought extremely fast lots

1738
01:48:45,460 --> 01:48:50,200
the second thing you want to do is you want to take

1739
01:48:50,240 --> 01:48:51,970
the current weight vector

1740
01:48:57,310 --> 01:48:58,040
this is

1741
01:48:58,060 --> 01:49:00,500
and makes it

1742
01:49:00,560 --> 01:49:03,470
they can make sure the previously factor

1743
01:49:03,490 --> 01:49:06,310
and the simplest way to do it would be to take

1744
01:49:06,360 --> 01:49:08,560
imagine one percent

1745
01:49:08,610 --> 01:49:10,490
ninety percent

1746
01:49:10,540 --> 01:49:12,560
you take the weight that just got

1747
01:49:12,600 --> 01:49:16,030
is isn't and using the even minus last update

1748
01:49:16,080 --> 01:49:16,900
and one

1749
01:49:16,930 --> 01:49:20,180
say you take a uniform distribution

1750
01:49:20,230 --> 01:49:21,980
well in

1751
01:49:21,990 --> 01:49:24,190
so that means

1752
01:49:24,230 --> 01:49:27,890
no wait goes down to zero because you always one percent is always

1753
01:49:27,910 --> 01:49:28,590
you know

1754
01:49:28,860 --> 01:49:30,950
mixed back to the uniform

1755
01:49:31,110 --> 01:49:32,400
fancy one will be

1756
01:49:32,430 --> 01:49:35,500
it is ninety nine percent times the current weight vector

1757
01:49:36,680 --> 01:49:38,230
one percent

1758
01:49:38,910 --> 01:49:41,390
you take past average weight vector

1759
01:49:41,410 --> 01:49:44,400
or some kind of king of the past

1760
01:49:44,410 --> 01:49:45,730
so the thing

1761
01:49:45,750 --> 01:49:47,960
so all the old ones come back in

1762
01:49:47,980 --> 01:49:49,750
so this is a particular simple

1763
01:49:49,770 --> 01:49:50,660
where you

1764
01:49:51,700 --> 01:49:56,110
nine you take this update and then you take ninety nine percent

1765
01:49:56,160 --> 01:50:01,220
the weight vector is ninety nine percent time this displays plus one percent times to

1766
01:50:01,220 --> 01:50:02,560
pass average

1767
01:50:02,580 --> 01:50:04,770
everybody understand what i'm doing

1768
01:50:04,790 --> 01:50:08,070
so you want to mix in the past all the vectors and there an extremely

1769
01:50:08,070 --> 01:50:10,330
interesting thing is it was

1770
01:50:13,880 --> 01:50:19,570
so we did some additional expenses joint work with the scheme

1771
01:50:20,650 --> 01:50:24,340
so my had had fourteen thousand trials

1772
01:50:24,340 --> 01:50:28,320
i had twenty thousand expected huge amounts of experts

1773
01:50:28,340 --> 01:50:30,600
and i did six shifts

1774
01:50:30,610 --> 01:50:32,100
in this segment

1775
01:50:32,120 --> 01:50:36,130
the first expert was the best then the second was the best person with the

1776
01:50:36,130 --> 01:50:42,380
best second best third person person express second was the best

1777
01:50:42,380 --> 01:50:44,950
so let me show you

1778
01:50:46,230 --> 01:50:47,810
it's the total loss

1779
01:50:47,830 --> 01:50:52,740
this is the total loss of an arbitrary expert over time

1780
01:50:52,990 --> 01:50:55,200
great goes up like this you square loss

1781
01:50:55,210 --> 01:51:00,150
this is the total loss of best expert

1782
01:51:00,160 --> 01:51:02,460
if you would know what the best expert is

1783
01:51:02,470 --> 01:51:04,660
there's some or noise rate

1784
01:51:04,660 --> 01:51:09,390
the idea is quite similar but the difference here is now that can not k

1785
01:51:09,390 --> 01:51:12,030
nearest neighbors so risking his neighbours are

1786
01:51:12,100 --> 01:51:15,920
used for the reference but at some neighborhoods

1787
01:51:15,970 --> 01:51:17,200
OK so again

1788
01:51:17,240 --> 01:51:20,580
points that have distance less than epsilon

1789
01:51:20,620 --> 01:51:21,350
to that

1790
01:51:21,560 --> 01:51:24,270
point of interest

1791
01:51:24,270 --> 01:51:26,550
and the idea of

1792
01:51:26,680 --> 01:51:31,500
this local outlier correlation because also to test multiple resolutions multiple

1793
01:51:31,520 --> 01:51:33,190
well values for epsilon

1794
01:51:33,210 --> 01:51:35,810
which are here called granularities

1795
01:51:35,820 --> 01:51:37,800
and this is

1796
01:51:37,810 --> 01:51:42,060
especially important to get rid of any input

1797
01:51:42,150 --> 01:51:45,830
so the model is based on that sunday one point and then the local density

1798
01:51:45,830 --> 01:51:48,040
of points just the number of objects

1799
01:51:48,040 --> 01:51:51,710
in this have some neighbor which is very similar to this distance based approach with

1800
01:51:51,710 --> 01:51:53,640
epsilon p

1801
01:51:53,950 --> 01:52:00,810
but now it computes the average density of of neighborhoods here

1802
01:52:00,820 --> 01:52:02,520
if you take a closer look

1803
01:52:02,540 --> 01:52:07,170
it's very complex here but it's not really so this is the density the average

1804
01:52:07,170 --> 01:52:09,100
density of the point p

1805
01:52:09,140 --> 01:52:12,630
so absent is there is the radius of the neighborhood

1806
01:52:12,630 --> 01:52:17,920
and i find is parameter which is smaller than one two zero and one

1807
01:52:17,950 --> 01:52:19,230
and what

1808
01:52:19,930 --> 01:52:25,190
the according the local correlation because actually does is

1809
01:52:25,240 --> 01:52:27,840
we examine the

1810
01:52:29,990 --> 01:52:31,460
o point p itself

1811
01:52:31,480 --> 01:52:33,580
of with radius absolute

1812
01:52:33,630 --> 01:52:34,910
and and for the neighbours

1813
01:52:36,630 --> 01:52:41,300
decrease the radius of the of the neighborhood which we which we can't buy

1814
01:52:41,300 --> 01:52:42,790
this factor

1815
01:52:42,840 --> 01:52:46,620
OK so maybe we could do this

1816
01:52:46,640 --> 01:52:53,400
picture here so take point p year which is on an inspection and well

1817
01:52:53,420 --> 01:52:56,570
we this is that the have some neighborhoods

1818
01:52:56,600 --> 01:52:58,520
of p

1819
01:53:00,720 --> 01:53:04,300
we count the number of points in the steps and they which is the density

1820
01:53:04,300 --> 01:53:05,350
around p

1821
01:53:05,350 --> 01:53:10,310
and this is compared to all neighbors in this neighbourhood the density of all neighbours

1822
01:53:10,310 --> 01:53:13,520
in this neighbourhood but for the the density of the neighbours

1823
01:53:14,300 --> 01:53:17,820
consider a smaller range

1824
01:53:18,020 --> 01:53:21,850
i five times this power should be absolute sorry

1825
01:53:21,900 --> 01:53:27,880
other times that's case so the density around the neighborhood the neighbours

1826
01:53:27,890 --> 01:53:29,890
this is a

1827
01:53:29,910 --> 01:53:31,680
reference it is smaller

1828
01:53:31,720 --> 01:53:33,170
OK this is the

1829
01:53:33,180 --> 01:53:34,550
most important

1830
01:53:34,550 --> 01:53:35,450
o point here

1831
01:53:35,520 --> 01:53:41,890
OK so and came back here based on this on this density we can derive

1832
01:53:41,890 --> 01:53:46,190
a so-called multi granularity deviation factor which is

1833
01:53:46,280 --> 01:53:48,460
and an outlier score

1834
01:53:48,480 --> 01:53:50,630
which is basically here

1835
01:53:50,630 --> 01:53:53,380
one minus the cardinality of

1836
01:53:53,390 --> 01:53:54,710
the neighbours

1837
01:53:56,840 --> 01:54:01,840
we've hired by sorry divided by the density

1838
01:54:01,890 --> 01:54:04,200
of name

1839
01:54:05,770 --> 01:54:07,660
for this muddy deviations

1840
01:54:09,920 --> 01:54:16,250
we can compute the normalized standard deviation of all densities of all points

1841
01:54:16,270 --> 01:54:17,900
which is

1842
01:54:19,370 --> 01:54:20,580
denoted here by

1843
01:54:21,880 --> 01:54:25,440
and the half and

1844
01:54:26,120 --> 01:54:29,780
it's very interesting because this is the kind of

1845
01:54:29,790 --> 01:54:34,880
relationship to the statistical based approaches statistical based approaches where we have these tests with

1846
01:54:35,120 --> 01:54:40,850
the gaussian distributions a very common test is points that are three times the standard

1847
01:54:40,850 --> 01:54:43,790
deviation apart from the from the mean

1848
01:54:43,840 --> 01:54:49,400
are considered outliers and we can apply these three sigma rule also here

1849
01:54:49,430 --> 01:54:52,320
so all points

1850
01:54:52,350 --> 01:54:58,520
that have a multi granularity deviation factor of more than three times the standard deviation

1851
01:54:58,520 --> 01:55:00,600
can be considered

1852
01:55:02,450 --> 01:55:09,420
points that have ndf zero equals zero are definitely within

1853
01:55:09,430 --> 01:55:10,710
so this is

1854
01:55:10,720 --> 01:55:17,270
nice feature and there's another features of the parameters epsilon alpha are determined automatically

1855
01:55:17,280 --> 01:55:23,590
to use don't has a doesn't to specify this in fact all possible values for

1856
01:55:23,590 --> 01:55:25,380
one test

1857
01:55:25,390 --> 01:55:27,580
all of possible

1858
01:55:27,750 --> 01:55:33,190
and the output

1859
01:55:33,220 --> 01:55:36,620
is a very nice display for each point

1860
01:55:36,780 --> 01:55:39,290
for each point you get

1861
01:55:39,310 --> 01:55:43,060
visualisation like this just

1862
01:55:43,110 --> 01:55:47,300
plots the cardinality of the that neighborhood

1863
01:55:47,380 --> 01:55:54,330
of each point on the density around these each point with varying epsilon varying radius

1864
01:55:55,270 --> 01:55:59,740
and for example for for the point here but this is the micro cluster point

1865
01:55:59,750 --> 01:56:00,580
is this

1866
01:56:00,630 --> 01:56:02,610
this red point here

1867
01:56:02,640 --> 01:56:03,700
you see that

1868
01:56:04,570 --> 01:56:06,350
his that this so it this

1869
01:56:06,360 --> 01:56:08,390
distance special eps along

1870
01:56:09,160 --> 01:56:10,560
happen something

1871
01:56:13,000 --> 01:56:14,280
what happens well

1872
01:56:14,280 --> 01:56:18,740
the distance gets large enough in order to make this point an outlier

1873
01:56:18,770 --> 01:56:22,910
here for cluster point in this in this direction here

1874
01:56:22,970 --> 01:56:27,270
over large spectrum of epsilon nothing happens that means

1875
01:56:27,280 --> 01:56:29,160
over all the spectrum

1876
01:56:29,210 --> 01:56:32,570
this point remains in layer no outlier

1877
01:56:32,580 --> 01:56:35,500
and for this point over here

1878
01:56:35,520 --> 01:56:37,490
you see that several

1879
01:56:37,550 --> 01:56:39,160
at some threshold

1880
01:56:39,190 --> 01:56:43,180
so you have these bombs in this in this course

1881
01:56:43,190 --> 01:56:44,380
which indicates that

1882
01:56:45,950 --> 01:56:52,790
well use this point all over all these these values point is it's not

1883
01:56:52,790 --> 01:56:56,600
so there's a nice feature that you get for each point kind of graphical

1884
01:56:58,000 --> 01:57:00,820
is it and also not

1885
01:57:00,830 --> 01:57:01,970
and which

1886
01:57:04,860 --> 01:57:10,610
shortly a sketch of the algorithm so the exact solutions rather expensive because you have

1887
01:57:10,610 --> 01:57:15,050
to compute all these values for all possible absence values of course

1888
01:57:15,070 --> 01:57:17,300
not very nice so

1889
01:57:17,320 --> 01:57:19,380
the office came up with the

1890
01:57:19,380 --> 01:57:24,960
all draw something from the distribution

1891
01:57:24,980 --> 01:57:29,010
so this is interesting and now calling self distribution

1892
01:57:29,050 --> 01:57:33,700
and remember this will be different for each stock

1893
01:57:33,770 --> 01:57:38,350
it will return me some random value from either the gaussians or

1894
01:57:38,370 --> 01:57:41,930
the normal distribution

1895
01:57:41,990 --> 01:57:46,430
with different volatility for the stocks because that was also selected randomly

1896
01:57:46,490 --> 01:57:48,800
plus some market by us

1897
01:57:48,800 --> 01:57:51,820
saying well the market on average will go up a little bit or go down

1898
01:57:51,830 --> 01:57:54,020
a little bit

1899
01:57:56,070 --> 01:57:56,920
and then

1900
01:57:56,940 --> 01:57:57,920
all set

1901
01:57:57,930 --> 01:58:02,170
the new price if you will self that price to sell stock price

1902
01:58:02,180 --> 01:58:05,760
times one plus the base move

1903
01:58:05,770 --> 01:58:09,820
so notice with this says the base movie is

1904
01:58:12,480 --> 01:58:16,200
then the price doesn't change so that makes sense

1905
01:58:16,240 --> 01:58:18,310
interesting question

1906
01:58:19,220 --> 01:58:22,340
do you think

1907
01:58:22,370 --> 01:58:23,580
i said

1908
01:58:23,600 --> 01:58:27,410
self-doubt prices times one plus the base move

1909
01:58:27,430 --> 01:58:32,010
rather than just adding the base moved to the stock price of the stock

1910
01:58:32,010 --> 01:58:34,400
again the first time i coded this i

1911
01:58:34,460 --> 01:58:38,350
i had in addition there instead of multiplication

1912
01:58:38,350 --> 01:58:44,680
what would the ramification of in addition there b

1913
01:58:44,700 --> 01:58:47,800
that would say how much the stock changed

1914
01:58:47,850 --> 01:58:52,900
is independent of its current price

1915
01:58:52,960 --> 01:58:54,590
and when i ran that

1916
01:58:54,590 --> 01:58:56,620
it just i got we're results

1917
01:58:56,640 --> 01:58:58,800
because we know that google

1918
01:58:58,840 --> 01:59:01,830
price to take three hundred

1919
01:59:01,870 --> 01:59:06,160
is much more likely to move by ten points in today

1920
01:59:06,220 --> 01:59:08,120
then the stock priced at

1921
01:59:08,120 --> 01:59:11,760
fifty cents

1922
01:59:11,760 --> 01:59:15,090
so in fact it is the case if you look at data

1923
01:59:15,100 --> 01:59:17,820
and by the way that's the way i ended up setting a lot of these

1924
01:59:17,820 --> 01:59:24,700
parameters and playing with it was comparing what my simulation said to historical stock data

1925
01:59:24,760 --> 01:59:29,350
and indeed it is the case that the price of the stock ten the move

1926
01:59:29,350 --> 01:59:33,590
the amount of move tends to be proportional to the price of the stock

1927
01:59:33,620 --> 01:59:35,990
expensive stocks move more

1928
01:59:36,090 --> 01:59:43,120
interestingly enough the percentage moves are not much difference between cheap stocks inexpensive stocks

1929
01:59:43,160 --> 01:59:44,620
and that's why

1930
01:59:44,620 --> 01:59:46,730
i ten ended up using

1931
01:59:46,780 --> 01:59:50,880
a multiplicative factor rather than additive factor

1932
01:59:50,970 --> 02:00:03,590
this is again a general lessons

1933
02:00:03,590 --> 02:00:07,510
as you build these kinds of simulations or anything like this

1934
02:00:07,570 --> 02:00:10,340
you need to think through

1935
02:00:10,390 --> 02:00:16,680
whether things should be multiplicative or additive because you get very different results typically

1936
02:00:18,600 --> 02:00:22,230
is what you want to do with the amount of change is proportional to the

1937
02:00:22,230 --> 02:00:25,310
current size

1938
02:00:25,330 --> 02:00:26,370
whether it's

1939
02:00:26,380 --> 02:00:28,590
price or anything else an additive

1940
02:00:28,600 --> 02:00:30,410
if the change is

1941
02:00:31,680 --> 02:00:34,360
of the current value typically

1942
02:00:34,360 --> 02:00:40,560
this is i think the general way to think about it

1943
02:00:41,520 --> 02:00:43,360
you'll see this other kind of

1944
02:00:43,370 --> 02:00:44,840
the chilean thing

1945
02:00:44,850 --> 02:00:48,290
so i now set the price

1946
02:00:48,310 --> 02:00:52,530
and then i've got this test here

1947
02:00:52,540 --> 02:00:54,730
if mo

1948
02:00:54,790 --> 02:00:59,070
most stands for momentum

1949
02:00:59,130 --> 02:01:00,110
i now

1950
02:01:00,120 --> 02:01:02,520
exploring the question

1951
02:01:02,610 --> 02:01:05,610
of whether or not

1952
02:01:05,690 --> 02:01:09,460
stock prices are indeed memoryless

1953
02:01:09,510 --> 02:01:12,170
or stock changes

1954
02:01:12,190 --> 02:01:19,550
and they fancy word for that is possible

1955
02:01:22,740 --> 02:01:24,010
people often

1956
02:01:24,010 --> 02:01:27,680
model things as possible processes

1957
02:01:27,700 --> 02:01:28,950
which is to say

1958
02:01:28,970 --> 02:01:31,990
processes in which past behavior

1959
02:01:31,990 --> 02:01:33,160
has no

1960
02:01:33,160 --> 02:01:35,550
impact on future behavior

1961
02:01:35,570 --> 02:01:37,530
it's memoryless

1962
02:01:37,550 --> 02:01:39,340
and in fact that's what

1963
02:01:39,390 --> 02:01:42,720
the efficient market hypothesis

1964
02:01:42,720 --> 02:01:45,390
reports to say

1965
02:01:45,470 --> 02:01:47,260
it says that

1966
02:01:47,320 --> 02:01:50,410
since all the information is in the current price

1967
02:01:50,410 --> 02:01:53,740
you don't have to worry about whether went up or down yesterday

1968
02:01:53,800 --> 02:01:59,010
to decide what it's going to do today

1969
02:01:59,070 --> 02:02:01,360
there are people who don't believe that

1970
02:02:01,360 --> 02:02:05,990
and instead argue that there is this notion called momentum

1971
02:02:05,990 --> 02:02:08,220
these are called momentum investors

1972
02:02:08,260 --> 02:02:09,010
they said

1973
02:02:09,010 --> 02:02:13,610
what's most likely to happen today is what happened yesterday

1974
02:02:13,620 --> 02:02:14,990
or more likely

1975
02:02:15,010 --> 02:02:18,470
if the stock went up yesterday it's more likely to go up today

1976
02:02:18,490 --> 02:02:21,800
and if it didn't go up yesterday

1977
02:02:21,820 --> 02:02:24,450
so i wasn't sure which

1978
02:02:24,470 --> 02:02:26,640
religion i was willing to believe in

1979
02:02:26,680 --> 02:02:27,800
if either

1980
02:02:27,840 --> 02:02:33,140
so i added a parameter called if you believe momentum

1981
02:02:33,160 --> 02:02:36,380
then you should change the price

1982
02:02:37,470 --> 02:02:42,320
and here i just did something taking gaussians times the last change

1983
02:02:42,360 --> 02:02:45,640
and in fact have

1984
02:02:45,680 --> 02:02:49,780
so if it went up yesterday it will more likely go up today example throwing

1985
02:02:50,720 --> 02:02:52,070
positive number

1986
02:02:52,090 --> 02:02:54,950
otherwise the negative numbers

1987
02:02:54,950 --> 02:02:58,200
notice that this is additive

1988
02:02:58,260 --> 02:02:59,840
because it's dealing with

1989
02:02:59,860 --> 02:03:02,320
yesterday's price

1990
02:03:02,360 --> 02:03:04,620
change with the change

1991
02:03:05,660 --> 02:03:10,140
so that's why we're dealing with that

1992
02:03:10,180 --> 02:03:12,590
now here's where i should have

1993
02:03:12,640 --> 02:03:16,820
put in this test that i had up here

1994
02:03:16,880 --> 02:03:19,340
get help from there

1995
02:03:19,360 --> 02:03:23,360
because what i wanna do it says itself that price is less than zero point

1996
02:03:23,360 --> 02:03:24,800
zero one

1997
02:03:24,800 --> 02:03:27,110
i'm going to set it to zero

1998
02:03:27,110 --> 02:03:32,800
just keep it there that doesn't solve the problem we to have before the right

1999
02:03:32,840 --> 02:03:35,390
then i'm going to

2000
02:03:38,200 --> 02:03:41,860
keep the last change for future use

2001
02:03:41,880 --> 02:03:45,700
OK people understand what's going on here

2002
02:03:45,760 --> 02:03:47,860
and then show history is just going

2003
02:03:47,950 --> 02:03:51,510
producer plot we've seen that a million times before

2004
02:03:51,550 --> 02:03:56,380
any questions about this

2005
02:03:56,390 --> 02:03:58,910
well my i have a question

2006
02:03:58,970 --> 02:04:01,450
it doesn't make any sense is going to work at all

2007
02:04:01,510 --> 02:04:04,660
so now let's test

2008
02:04:06,220 --> 02:04:08,510
i now have

2009
02:04:08,610 --> 02:04:11,260
this unit test program

2010
02:04:11,300 --> 02:04:14,890
called unit test stock

2011
02:04:14,890 --> 02:04:18,410
i originally did not make it to function i had in line

2012
02:04:18,430 --> 02:04:20,340
i realize that was really stupid

2013
02:04:20,360 --> 02:04:24,140
because i want to do a lot of times

2014
02:04:25,180 --> 02:04:29,090
it's got an internal procedure internal function

2015
02:04:29,200 --> 02:04:31,220
local to the unit test

2016
02:04:31,260 --> 02:04:33,740
that runs the simulation

2017
02:04:33,740 --> 02:04:36,630
in in a in the linear model

2018
02:04:36,640 --> 02:04:41,370
where we have infinitely many basis functions on the basis functions a little gaussians that

2019
02:04:41,370 --> 02:04:44,650
are centered on every possible point in the input space

2020
02:04:44,660 --> 02:04:50,340
OK so think we get the same answer as if we did a bayesian inference

2021
02:04:50,340 --> 02:04:52,220
on a linear model

2022
02:04:52,230 --> 02:04:54,100
where the feature space

2023
02:04:54,110 --> 02:05:01,230
contain these contain these galson points galson bonds and their as important distributed everywhere in

2024
02:05:01,230 --> 02:05:03,300
the in the input space

2025
02:05:04,770 --> 02:05:07,970
she n

2026
02:05:07,980 --> 02:05:12,480
he was

2027
02:05:12,490 --> 02:05:16,250
he was

2028
02:05:16,260 --> 02:05:20,710
so it's just in this case is just that it's just it is the sum

2029
02:05:20,720 --> 02:05:25,230
of these of these things that are all that all galson in the variables that

2030
02:05:25,230 --> 02:05:29,290
matter it doesn't actually really matter what they what the basis functions because that doesn't

2031
02:05:29,290 --> 02:05:32,230
include the variable

2032
02:05:32,240 --> 02:05:36,740
which has the gauss distribution which is gamma in this case

2033
02:05:36,760 --> 02:05:42,030
OK so this is the only u

2034
02:05:42,050 --> 02:05:45,410
what about the way

2035
02:05:45,430 --> 02:05:50,810
these people we have this is that

2036
02:05:50,870 --> 02:05:52,670
you will be

2037
02:05:52,680 --> 02:05:57,090
so the covariance matrix is fixed well

2038
02:05:57,100 --> 02:06:02,190
my prior to that he was born with

2039
02:06:02,210 --> 02:06:03,100
in vision

2040
02:06:03,120 --> 02:06:06,860
there's always generated by the

2041
02:06:06,870 --> 02:06:07,990
one thing

2042
02:06:08,150 --> 02:06:11,430
that's a good question question simply you know if if i come up with the

2043
02:06:11,430 --> 02:06:15,090
covariance function then you know is there a proper gaussianprocess

2044
02:06:15,140 --> 02:06:20,060
interpretation of that and it is the class of functions where and that's exactly correspondence

2045
02:06:20,080 --> 02:06:24,790
there there some criteria not all functions are covariance functions right so the critical command

2046
02:06:24,790 --> 02:06:28,400
functions of functions are positive definite functions right so it has to be

2047
02:06:28,520 --> 02:06:30,400
but if i evaluate the

2048
02:06:30,420 --> 02:06:35,180
the covariance function for any choices of x and x prime then the matrix i

2049
02:06:35,180 --> 02:06:39,800
get out of that has to be positive definite because otherwise there isn't that gas

2050
02:06:39,810 --> 02:06:48,600
in gauss interpretation of so any any covariance function would give rise to you know

2051
02:06:49,010 --> 02:06:53,060
properly define problem of course you want to go the other way right we wanted

2052
02:06:53,060 --> 02:06:56,090
we wanted to be the case that you give me a data set and i'll

2053
02:06:56,090 --> 02:06:59,660
tell you you know what is it good characterization of the dataset that's what's going

2054
02:06:59,660 --> 02:07:03,050
to be learning in these casting process so that's what we're going to get to

2055
02:07:03,080 --> 02:07:04,790
in a few minutes

2056
02:07:04,810 --> 02:07:10,490
OK so this this from this maybe remind you about a very similar model which

2057
02:07:10,490 --> 02:07:16,250
is a radial basis function model so there is a no model where you have

2058
02:07:16,250 --> 02:07:20,610
coefficients on the lookout in bonds the bumps centered on your training points like this

2059
02:07:20,610 --> 02:07:23,250
is a little bit different because you gets in everywhere

2060
02:07:23,270 --> 02:07:25,440
why not just on the training points

2061
02:07:25,460 --> 02:07:27,140
so but is a new real

2062
02:07:27,150 --> 02:07:30,280
the difference between those two models i mean in the end i'm just going to

2063
02:07:30,280 --> 02:07:33,080
use and dimensional in anyway

2064
02:07:33,120 --> 02:07:37,170
so let's let's look a little bit that you know how does this thing behave

2065
02:07:37,500 --> 02:07:40,190
when you do inference

2066
02:07:40,220 --> 02:07:41,640
over the model

2067
02:07:41,690 --> 02:07:44,900
and to illustrate that yes

2068
02:07:45,020 --> 02:07:49,080
just because were

2069
02:07:51,620 --> 02:07:53,000
with the kernel

2070
02:07:53,020 --> 02:07:58,020
your own

2071
02:08:00,420 --> 02:08:02,140
that's correct to trying to that

2072
02:08:03,890 --> 02:08:05,470
it's funny

2073
02:08:05,530 --> 02:08:08,020
it's fun you can do you can do all sorts of things like you define

2074
02:08:08,030 --> 02:08:11,640
things that are always symmetric functions and you look at the covariance function you say

2075
02:08:12,070 --> 02:08:15,410
you know how could know where this is going to be symmetric right

2076
02:08:15,440 --> 02:08:18,350
it's nice

2077
02:08:18,380 --> 02:08:20,820
i tried

2078
02:08:20,860 --> 02:08:27,000
OK so so here's here's the cartoon example which will illustrate why there's a big

2079
02:08:27,000 --> 02:08:32,780
difference between using a radial basis function network and gaussianprocess although computationally is going to

2080
02:08:32,780 --> 02:08:35,420
be the same i do still have to you still have to just solve the

2081
02:08:35,420 --> 02:08:36,890
system system of n

2082
02:08:36,900 --> 02:08:41,320
the night creations what just and by n matrix inversion involved so let's pretend that

2083
02:08:41,320 --> 02:08:44,980
we have three data points to three points in green here

2084
02:08:45,040 --> 02:08:49,640
and we are fitting it with the model that has three gas thousand bombs and

2085
02:08:49,640 --> 02:08:52,560
the problems are centered on the on the training points but so if you are

2086
02:08:52,570 --> 02:08:56,140
bayesian model of this thing and we have to put priors on the weights

2087
02:08:56,190 --> 02:09:00,380
and then once we see the data and then we'll have a posterior over the

2088
02:09:00,380 --> 02:09:04,680
weights in the you can think about the the since jiggling around they can move

2089
02:09:04,680 --> 02:09:08,900
around a little bit back and forth but they all they always have to some

2090
02:09:08,900 --> 02:09:12,330
of them always have to sort of more or less describing the data

2091
02:09:12,330 --> 02:09:13,910
up to the noise level

2092
02:09:13,970 --> 02:09:18,400
OK so you can think of the posterior as being these gaussians are wobbling around

2093
02:09:19,300 --> 02:09:22,430
describing always being put close to the data points

2094
02:09:22,490 --> 02:09:25,140
OK now i want to make a prediction

2095
02:09:25,140 --> 02:09:27,470
OK i want to make a prediction here

2096
02:09:27,480 --> 02:09:34,590
OK so what is the posterior distribution look like out there

2097
02:09:34,600 --> 02:09:35,930
zero yes

2098
02:09:35,950 --> 02:09:40,410
because the calcium pumps there they're moving around under the posterior distribution right but they

2099
02:09:40,410 --> 02:09:44,340
have they always have value zero over here it doesn't matter what the weights

2100
02:09:44,430 --> 02:09:48,730
so the predictive distribution for model like that is zero

2101
02:09:48,830 --> 02:09:51,030
with very high confidence

2102
02:09:52,730 --> 02:09:55,080
that was the wrong answer right

2103
02:09:55,080 --> 02:09:58,880
the answer is that OK the mean might be zero but we certainly not confident

2104
02:09:58,880 --> 02:10:01,490
about what function is doing here

2105
02:10:01,520 --> 02:10:05,620
like something went wrong and what is it that went wrong

2106
02:10:05,640 --> 02:10:09,690
for all we know

2107
02:10:09,740 --> 02:10:14,610
that's right there's something wrong with the model right to model is already saying apriori

2108
02:10:14,610 --> 02:10:19,040
it's impossible that the function does something up here is that your prior surprise as

2109
02:10:19,280 --> 02:10:22,380
a function can do stuff here but it can do some stuff over here then

2110
02:10:22,380 --> 02:10:25,480
once you've seen the data you still think it can do things that you

2111
02:10:26,780 --> 02:10:29,460
so there's something wrong with the model and actually the thing that's wrong with the

2112
02:10:29,460 --> 02:10:32,640
model is that it's finite dimensional model

2113
02:10:32,700 --> 02:10:35,930
you've already before you see the data you said there's only a finite number of

2114
02:10:35,930 --> 02:10:37,660
things that can happen right

2115
02:10:37,680 --> 02:10:41,610
and and things that that that are close to this can happen

2116
02:10:41,610 --> 02:10:45,040
OK so that's a very strong statement about usually you don't really want to make

2117
02:10:45,040 --> 02:10:46,160
a statement like that

2118
02:10:46,180 --> 02:10:51,440
so instead in the in the gaussianprocess formulation you have galson bumps everywhere right so

2119
02:10:51,440 --> 02:10:57,020
what would the posterior look like in that case

2120
02:10:57,020 --> 02:11:01,600
so we have bumps everywhere and then the data comes also so under the prior

2121
02:11:01,820 --> 02:11:06,250
to the bomb just bumping around right and then the data comes in

2122
02:11:06,270 --> 02:11:09,640
and the data says well the pumps that are jiggling around over here they actually

2123
02:11:09,640 --> 02:11:11,220
constrained by the data

2124
02:11:11,280 --> 02:11:15,520
but the bumps over here they're not constrained by the data

2125
02:11:15,530 --> 02:11:18,270
so the answer in that case is well maybe the mean is going to be

2126
02:11:18,270 --> 02:11:21,520
zero but more importantly it's going to say well i don't know what the function

2127
02:11:21,520 --> 02:11:24,820
is doing over here you could do lots of different things

2128
02:11:25,140 --> 02:11:29,580
and that's the right answer right along with the wrong answer by the radial basis

