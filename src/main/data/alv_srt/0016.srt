1
00:00:00,000 --> 00:00:01,110
given two

2
00:00:01,130 --> 00:00:03,560
treatment strategies which are the same

3
00:00:03,600 --> 00:00:06,510
but one of them include psychotherapy

4
00:00:06,510 --> 00:00:10,150
so an appointment with the shrink once a week and one of them doesn't include

5
00:00:11,730 --> 00:00:16,100
what's the difference of these two sets of people receive the same drug treatment

6
00:00:16,120 --> 00:00:20,280
but half of them actually also made a psychiatrist on a regular basis and half

7
00:00:20,280 --> 00:00:22,040
of them don't

8
00:00:22,050 --> 00:00:25,460
and the reason it's important to compare these two strategies is

9
00:00:25,470 --> 00:00:27,150
a lot of people don't really like

10
00:00:27,200 --> 00:00:31,280
the extra burden of going to the psychotherapist and i just prefer to take their

11
00:00:31,300 --> 00:00:34,050
drug in the morning and not bother with anything more

12
00:00:34,180 --> 00:00:38,310
so can we see what is the effect of the psychotherapy

13
00:00:38,580 --> 00:00:43,000
and what we see is that if you just look at expectation it seems that

14
00:00:43,000 --> 00:00:45,300
using psychotherapy

15
00:00:45,340 --> 00:00:47,080
has a positive effect

16
00:00:47,090 --> 00:00:50,640
but if you also include the variance estimation well

17
00:00:50,690 --> 00:00:56,670
it's not a meaningful effect and two standard deviation and this is for patient population

18
00:00:56,680 --> 00:01:00,420
in terms of the number of samples we had for each of these

19
00:01:00,440 --> 00:01:03,960
two different policies if you have many more

20
00:01:05,280 --> 00:01:06,720
registered in the study

21
00:01:06,730 --> 00:01:11,620
maybe it's a meaningful difference but based on the data we've collected in clinical trial

22
00:01:11,650 --> 00:01:14,460
this would be the recommendation in this case

23
00:01:14,460 --> 00:01:18,420
and this is the kind of results that are partners in health really interested in

24
00:01:18,440 --> 00:01:22,680
that you could add some confidence bounds to your estimate of the value function as

25
00:01:22,680 --> 00:01:24,170
opposed to just saying

26
00:01:24,190 --> 00:01:29,400
my reinforcement learning system recommends that everyone be registered for psychotherapy

27
00:01:29,430 --> 00:01:32,790
and by the way in this we didn't even include the cost of the psychotherapy

28
00:01:32,790 --> 00:01:36,940
always included in terms of reward function was

29
00:01:36,950 --> 00:01:41,290
the expectation of remission whether people will actually achieve

30
00:01:41,380 --> 00:01:45,830
real reduction of depression symptoms over some period fixed period of time

31
00:01:45,890 --> 00:01:48,470
and so at some point you could at the cost of

32
00:01:48,480 --> 00:01:51,330
cost of psychotherapy if you wanted to two

33
00:02:00,470 --> 00:02:06,480
the rate at which is

34
00:02:09,820 --> 00:02:11,330
right so you could

35
00:02:12,740 --> 00:02:15,110
we haven't tried it in terms of the empirical

36
00:02:15,130 --> 00:02:17,780
sense in terms of whether there are some real benefit

37
00:02:19,810 --> 00:02:22,240
in this case i think you

38
00:02:22,960 --> 00:02:27,220
you get in terms of doing the analysis this way

39
00:02:27,370 --> 00:02:30,170
if you get more i haven't thought about it very much

40
00:02:30,850 --> 00:02:35,570
the hope the hope is that somehow

41
00:02:35,580 --> 00:02:39,150
can leverage your data in a better way by estimating the error on these models

42
00:02:39,150 --> 00:02:42,940
precisely and if there's some independencies in terms of the errors and so on you

43
00:02:42,940 --> 00:02:46,240
can get away with better results with the smaller simple sample set

44
00:02:46,300 --> 00:02:47,940
now that i have no

45
00:02:47,980 --> 00:02:50,530
theoretical and empirical results support this

46
00:02:50,540 --> 00:02:53,100
well that's certainly our intuition

47
00:02:55,850 --> 00:02:57,210
that is

48
00:02:57,920 --> 00:02:59,550
and you can

49
00:02:59,730 --> 00:03:01,610
right right model

50
00:03:01,620 --> 00:03:05,240
there's some of that there is a whole literature on that two and we're looking

51
00:03:05,240 --> 00:03:09,540
at some of that that we haven't applied in this context

52
00:03:12,080 --> 00:03:18,070
and and

53
00:03:18,120 --> 00:03:22,800
so that's what i wanted to say on this particular question i should clarify that

54
00:03:22,840 --> 00:03:26,540
the work we is actually a generalization of earlier work that had been done in

55
00:03:26,540 --> 00:03:30,090
the MDP case and so some of the techniques in terms of the estimation of

56
00:03:30,090 --> 00:03:33,890
the variance with there before was just a question of generalizing it to the public

57
00:03:33,890 --> 00:03:35,240
in the

58
00:03:35,280 --> 00:03:41,500
and the rapid controller finite policy control representation but we think this is really useful

59
00:03:41,500 --> 00:03:46,920
to quantify performance when you have some critical task not necessary everywhere on the robotic

60
00:03:46,940 --> 00:03:49,150
systems are really don't need to worry about it

61
00:03:49,170 --> 00:03:54,320
but in some domains which have a critical component and this can give you a

62
00:03:54,320 --> 00:03:56,440
lot of power to analysis

63
00:03:58,350 --> 00:04:00,960
not just

64
00:04:00,960 --> 00:04:06,570
there's nothing here about how to adjust the control online if you're doing optimisation of

65
00:04:06,570 --> 00:04:09,860
your value function we don't quite know how to do this yet because then you

66
00:04:09,860 --> 00:04:13,820
have this max operator which really complicates things a little bit

67
00:04:13,880 --> 00:04:18,030
we have some ideas of how to do that but we haven't gotten there yet

68
00:04:18,850 --> 00:04:22,300
it turns out when your data is collected in a batch

69
00:04:22,330 --> 00:04:26,590
this works pretty well because there's a fixed ready to collect the data whether it's

70
00:04:26,590 --> 00:04:30,960
a randomized strategy or whether it's a strategy prescribed by clinical practice

71
00:04:30,990 --> 00:04:35,780
a strategy usually is they the same with dialogue system but we say nothing about

72
00:04:35,780 --> 00:04:39,060
how to estimate the value function directly

73
00:04:39,060 --> 00:04:40,070
so there

74
00:04:40,080 --> 00:04:44,060
o suited thousand based on this

75
00:04:44,910 --> 00:04:46,990
it was most recently

76
00:04:47,000 --> 00:04:50,340
and this is true

77
00:04:52,960 --> 00:04:54,260
of course if

78
00:04:58,150 --> 00:05:00,080
six years

79
00:05:03,260 --> 00:05:05,750
he's your

80
00:05:11,070 --> 00:05:18,410
which is

81
00:05:18,430 --> 00:05:19,480
and become

82
00:05:19,540 --> 00:05:27,940
and this is shown in the country

83
00:05:36,210 --> 00:05:40,270
it is the same

84
00:05:40,290 --> 00:05:42,730
in this show

85
00:05:43,710 --> 00:05:45,050
so this is

86
00:05:49,320 --> 00:05:50,980
two so

87
00:05:51,000 --> 00:05:53,570
and then you should

88
00:05:53,680 --> 00:05:56,630
this is one the

89
00:05:56,660 --> 00:05:57,790
between the

90
00:05:57,790 --> 00:06:01,530
the next step in

91
00:06:01,720 --> 00:06:10,210
so you to do is to explain the meaning of christmas

92
00:06:12,630 --> 00:06:18,320
what i to be was the that schools

93
00:06:18,430 --> 00:06:20,730
so the

94
00:06:21,800 --> 00:06:22,740
to say

95
00:06:24,880 --> 00:06:29,580
the meaning of this

96
00:06:29,610 --> 00:06:31,570
that's of course is

97
00:06:34,430 --> 00:06:36,310
there is

98
00:06:36,320 --> 00:06:38,490
it is of course

99
00:06:38,870 --> 00:06:41,320
no fuel

100
00:06:41,330 --> 00:06:46,070
so this is no more than one

101
00:06:48,160 --> 00:06:51,360
this is

102
00:06:51,460 --> 00:06:52,650
it's difficult

103
00:06:54,460 --> 00:07:02,190
this should not be able to propose to each person

104
00:07:05,280 --> 00:07:11,580
this is the distribution grid pretty

105
00:07:11,610 --> 00:07:15,730
so this is actually

106
00:07:15,730 --> 00:07:19,680
is the most important thing is

107
00:07:19,710 --> 00:07:21,580
this is the same thing

108
00:07:21,600 --> 00:07:22,850
is it

109
00:07:25,590 --> 00:07:28,170
well i think

110
00:07:29,220 --> 00:07:34,710
just a few days for the

111
00:07:34,820 --> 00:07:36,260
this is

112
00:07:36,270 --> 00:07:41,990
this is the version of the model is exactly the same

113
00:07:42,010 --> 00:07:43,250
it was

114
00:07:47,230 --> 00:07:48,730
it is

115
00:07:48,740 --> 00:07:50,540
it's so

116
00:07:54,100 --> 00:07:58,050
this is a small but

117
00:07:58,080 --> 00:08:01,340
of all sixty

118
00:08:01,360 --> 00:08:03,820
the solution was

119
00:08:03,820 --> 00:08:06,640
the information about its

120
00:08:06,650 --> 00:08:10,290
we see the small so chapters

121
00:08:11,420 --> 00:08:14,580
the special that can be

122
00:08:16,900 --> 00:08:20,160
and in fact is true

123
00:08:20,900 --> 00:08:22,300
two sources

124
00:08:22,530 --> 00:08:26,310
but what she

125
00:08:26,320 --> 00:08:29,820
in fact this is the same

126
00:08:31,680 --> 00:08:34,140
should also be for

127
00:08:34,220 --> 00:08:36,380
it is just

128
00:08:42,570 --> 00:08:44,940
so that's what

129
00:08:46,290 --> 00:08:48,080
this is

130
00:08:48,100 --> 00:08:50,070
all this

131
00:08:50,080 --> 00:08:54,430
so then

132
00:08:54,440 --> 00:08:56,130
the action is

133
00:08:56,150 --> 00:08:58,040
say no

134
00:08:59,690 --> 00:09:04,370
it's just doing this

135
00:09:06,280 --> 00:09:11,370
that's what it was

136
00:09:11,390 --> 00:09:16,560
circumstances sets so

137
00:09:17,540 --> 00:09:20,270
how should affect

138
00:09:23,650 --> 00:09:31,100
this is due to the fact that

139
00:09:31,120 --> 00:09:32,510
it's a

140
00:09:32,690 --> 00:09:34,580
the first one

141
00:09:34,580 --> 00:09:35,890
but i think it's

142
00:09:36,870 --> 00:09:38,780
she was

143
00:09:39,630 --> 00:09:44,270
and this is one of the social factors cues for instances

144
00:09:46,090 --> 00:09:47,510
so the

145
00:09:47,720 --> 00:09:53,510
all compositions which is the ratio of the for

146
00:09:55,670 --> 00:09:59,070
so this is not

147
00:09:59,330 --> 00:10:03,060
expression of same

148
00:10:03,080 --> 00:10:04,400
this is

149
00:10:04,490 --> 00:10:08,270
four should not be

150
00:10:08,410 --> 00:10:10,430
it's the

151
00:10:10,480 --> 00:10:15,320
the truth is you want

152
00:10:15,350 --> 00:10:17,070
according to the

153
00:10:17,080 --> 00:10:21,980
of the solution to do the same things so

154
00:10:24,320 --> 00:10:29,980
strong on the basis that produced

155
00:10:33,320 --> 00:10:36,910
four is close to

156
00:10:36,930 --> 00:10:41,650
so he decided use

157
00:10:42,790 --> 00:10:47,090
the public that his

158
00:10:47,110 --> 00:10:51,540
good news crews

159
00:10:52,610 --> 00:10:58,030
this is a list of course is that this is not

160
00:10:59,620 --> 00:11:01,290
the average

161
00:11:02,610 --> 00:11:03,810
because it

162
00:11:03,830 --> 00:11:06,340
this is

163
00:11:06,350 --> 00:11:07,810
this is a

164
00:11:08,910 --> 00:11:11,720
much so

165
00:11:14,590 --> 00:11:17,710
now it's obvious

166
00:11:17,720 --> 00:11:20,880
to to use the

167
00:11:22,110 --> 00:11:25,340
what is of this

168
00:11:25,370 --> 00:11:28,960
the this was

169
00:11:28,960 --> 00:11:31,240
so expect users

170
00:11:31,490 --> 00:11:34,240
it is possible to control the

171
00:11:34,620 --> 00:11:37,730
of to

172
00:11:37,740 --> 00:11:39,400
the is

173
00:11:39,590 --> 00:11:41,880
this is this

174
00:11:42,590 --> 00:11:44,760
this is

175
00:11:44,770 --> 00:11:48,330
the that exist

176
00:11:51,600 --> 00:11:54,420
with the physical world

177
00:11:54,440 --> 00:11:56,550
of course

178
00:12:03,160 --> 00:12:06,380
he years old

179
00:12:09,620 --> 00:12:12,370
this is much higher than average

180
00:12:12,370 --> 00:12:15,020
so it's also the EMF

181
00:12:15,080 --> 00:12:17,560
one is i times are five

182
00:12:17,580 --> 00:12:20,020
you see is the lower than

183
00:12:20,080 --> 00:12:21,480
the MS

184
00:12:21,590 --> 00:12:23,110
and the reason is

185
00:12:23,130 --> 00:12:27,010
this internal resistance here

186
00:12:27,030 --> 00:12:29,560
if i shorted out this battery

187
00:12:29,570 --> 00:12:33,490
a stupid thing to do but if i make are equal zero

188
00:12:33,570 --> 00:12:37,510
so i think the better and i just shorted out

189
00:12:38,690 --> 00:12:41,020
the maximum current

190
00:12:41,050 --> 00:12:42,700
but i can

191
00:12:44,780 --> 00:12:47,170
are is now zero

192
00:12:47,230 --> 00:12:50,000
so you can see that the maximum i did you can

193
00:12:50,060 --> 00:12:51,640
get is a divided by

194
00:12:51,650 --> 00:12:54,050
of i

195
00:12:55,000 --> 00:12:56,330
and the of b

196
00:12:56,450 --> 00:13:01,810
the voltage that you would measure now between point being in a goes to zero

197
00:13:01,830 --> 00:13:05,310
it doesn't mean that there is no current running

198
00:13:05,320 --> 00:13:08,620
but it means that between these points

199
00:13:08,730 --> 00:13:12,180
potential difference goes down to zero

200
00:13:12,230 --> 00:13:18,600
shorting out of that area of course is not a very smart thing to do

201
00:13:18,680 --> 00:13:21,470
you can put batteries in series

202
00:13:21,510 --> 00:13:23,320
and thereby getting

203
00:13:23,390 --> 00:13:26,490
a higher potential difference

204
00:13:26,530 --> 00:13:30,220
this is the negative side is positive

205
00:13:30,230 --> 00:13:33,340
i have an independent one negative positive

206
00:13:33,380 --> 00:13:36,980
and independent one negative positive

207
00:13:36,990 --> 00:13:38,480
each one

208
00:13:38,520 --> 00:13:41,390
was an EMF e

209
00:13:41,440 --> 00:13:44,280
i can connect the positive side of one

210
00:13:44,340 --> 00:13:47,820
with the negative side of the of adjust to conducting y

211
00:13:47,830 --> 00:13:51,120
the positive side of this was the negative side of the other

212
00:13:51,860 --> 00:13:53,560
potential difference

213
00:13:53,600 --> 00:13:55,490
between these two points

214
00:13:55,610 --> 00:13:57,390
now three

215
00:13:57,400 --> 00:13:58,480
open circuit

216
00:13:58,490 --> 00:14:00,680
i don't draw any current

217
00:14:00,690 --> 00:14:03,630
i draw current and of course i have to do again was the

218
00:14:09,530 --> 00:14:11,500
i'm going to

219
00:14:11,510 --> 00:14:15,300
built with you a

220
00:14:15,310 --> 00:14:17,470
copper zinc

221
00:14:17,470 --> 00:14:21,740
battery of the kind that we just discussed

222
00:14:21,750 --> 00:14:27,270
you see here

223
00:14:27,300 --> 00:14:31,770
he is the copper copper sulfate solution which h two s four

224
00:14:34,470 --> 00:14:36,270
he i might plates

225
00:14:36,280 --> 00:14:38,230
this is my

226
00:14:38,310 --> 00:14:39,300
zinc plate

227
00:14:39,310 --> 00:14:42,280
and this is my copper plates

228
00:14:42,310 --> 00:14:44,520
and you're going to see

229
00:14:44,550 --> 00:14:45,650
the voltage

230
00:14:47,360 --> 00:14:50,200
i think over there that is correct

231
00:14:50,230 --> 00:14:58,510
there is no potential difference now because they are not in place yet

232
00:14:58,530 --> 00:15:01,270
so here comes my

233
00:15:03,070 --> 00:15:05,850
here comes my copper

234
00:15:05,890 --> 00:15:07,850
they go into the solution

235
00:15:08,120 --> 00:15:11,030
you see about one four

236
00:15:11,090 --> 00:15:15,060
in general these potential differences of that or wonderful

237
00:15:15,070 --> 00:15:18,470
o point nine five

238
00:15:18,480 --> 00:15:20,150
now what i will do

239
00:15:20,170 --> 00:15:22,720
i'm going to create the double one

240
00:15:22,730 --> 00:15:29,230
five two independent

241
00:15:29,240 --> 00:15:33,470
batteries i have here

242
00:15:33,480 --> 00:15:36,100
one have

243
00:15:38,050 --> 00:15:40,230
and zinc

244
00:15:40,280 --> 00:15:42,220
i have another one

245
00:15:42,230 --> 00:15:45,130
i copper

246
00:15:45,200 --> 00:15:46,030
and saying

247
00:15:46,050 --> 00:15:48,950
i'm going to connect this one

248
00:15:48,990 --> 00:15:50,450
you will see now

249
00:15:50,470 --> 00:15:54,130
that the EMF of w

250
00:15:54,230 --> 00:15:57,820
ready for that

251
00:15:57,840 --> 00:16:01,600
my second one is going to be completely independent

252
00:16:01,640 --> 00:16:03,340
here comes

253
00:16:03,620 --> 00:16:06,310
two plates make sure they have the copper

254
00:16:06,320 --> 00:16:08,650
zinc not confused

255
00:16:08,670 --> 00:16:10,210
there we go

256
00:16:10,230 --> 00:16:13,800
now you should see twice the potentially you do see that open circuit there is

257
00:16:13,800 --> 00:16:15,850
no current running well this

258
00:16:15,850 --> 00:16:17,160
if a

259
00:16:17,160 --> 00:16:22,220
remove all spam and the problems of the real world

260
00:16:22,260 --> 00:16:23,180
OK so

261
00:16:23,200 --> 00:16:25,780
now we have presentation of our

262
00:16:25,800 --> 00:16:28,390
many of our link graph

263
00:16:28,390 --> 00:16:30,140
we have this formula

264
00:16:30,160 --> 00:16:32,100
and what we're going to do

265
00:16:32,180 --> 00:16:37,600
we're going to design the reason that can calculate this page around

266
00:16:37,600 --> 00:16:40,510
for the internet which is

267
00:16:40,510 --> 00:16:42,530
not not very small

268
00:16:42,550 --> 00:16:46,280
all i don't know on my laptop

269
00:16:46,330 --> 00:16:47,820
i can

270
00:16:48,390 --> 00:16:50,180
let's start from

271
00:16:50,200 --> 00:16:52,320
the most naive presentation

272
00:16:52,370 --> 00:16:53,870
first of all it it

273
00:16:53,890 --> 00:16:55,430
completely obvious

274
00:16:55,450 --> 00:16:56,970
that we don't need

275
00:16:56,990 --> 00:16:58,510
to keep our

276
00:16:58,530 --> 00:17:01,660
structure of graph a memory

277
00:17:01,660 --> 00:17:04,260
because if you look at this formula

278
00:17:04,320 --> 00:17:05,640
we see that

279
00:17:05,680 --> 00:17:09,300
i have everywhere

280
00:17:10,010 --> 00:17:12,600
we are working his one

281
00:17:12,620 --> 00:17:18,050
for me it's amazing are pagerank for one page

282
00:17:18,100 --> 00:17:21,680
and on the right we have

283
00:17:21,720 --> 00:17:26,430
this portal is to have all pages that are linked with this one

284
00:17:26,450 --> 00:17:28,970
so what we can do

285
00:17:28,970 --> 00:17:29,720
they can

286
00:17:30,620 --> 00:17:34,780
o five live line-by-line we can get

287
00:17:34,820 --> 00:17:36,640
for this particular page

288
00:17:36,660 --> 00:17:39,120
you can get idea of this page

289
00:17:39,140 --> 00:17:44,100
they can get a list of all ideas of all documents that are linked from

290
00:17:44,100 --> 00:17:45,700
this page

291
00:17:45,700 --> 00:17:49,050
and we can calculate how formula

292
00:17:49,510 --> 00:17:54,050
the big disadvantage of this formula that we need a number of links from this

293
00:17:55,740 --> 00:17:57,550
so how it works

294
00:17:57,600 --> 00:17:59,220
we are eating

295
00:18:00,530 --> 00:18:04,260
this number is actually the idea of the of the

296
00:18:05,390 --> 00:18:08,050
this idea of the page actually it

297
00:18:08,070 --> 00:18:11,700
some indication some array of result

298
00:18:13,180 --> 00:18:15,530
we are going to

299
00:18:15,600 --> 00:18:18,140
and to this i

300
00:18:18,140 --> 00:18:22,950
weight is from all pages that are linked to this page

301
00:18:24,100 --> 00:18:28,010
we then we are going through of course list

302
00:18:28,030 --> 00:18:32,740
and they have a and because of this is iteration formula we need to erase

303
00:18:32,740 --> 00:18:36,510
we need to raise of new ideas and all the way

304
00:18:37,410 --> 00:18:38,680
it fails to be

305
00:18:38,680 --> 00:18:42,220
i feel all our initial weight is real

306
00:18:42,240 --> 00:18:43,910
this is constant

307
00:18:43,950 --> 00:18:48,530
this initilisation first part of the sun

308
00:18:48,550 --> 00:18:55,390
and then what we are doing for every where you're from all postings lists glk

309
00:18:55,410 --> 00:18:59,050
getting previous value of pagerank

310
00:18:59,070 --> 00:19:03,820
divide each by number of links from this page

311
00:19:03,850 --> 00:19:05,470
and so it

312
00:19:05,530 --> 00:19:11,140
of course after some them was appliance our dumping factor and safe to all output

313
00:19:16,200 --> 00:19:18,720
when we are done with the whole

314
00:19:18,720 --> 00:19:21,600
ar is is the whole matrix

315
00:19:21,600 --> 00:19:23,780
have new result

316
00:19:23,800 --> 00:19:26,800
then we can simply wish this arrays

317
00:19:26,800 --> 00:19:30,640
so we are coping or resting place that pointers

318
00:19:30,660 --> 00:19:34,700
and now this writer a is previously areas

319
00:19:34,720 --> 00:19:37,030
and this rate will be

320
00:19:37,050 --> 00:19:39,740
and for new way

321
00:19:41,590 --> 00:19:46,260
so it's definitely it definitely works

322
00:19:46,350 --> 00:19:49,370
let's assume how many memory we need

323
00:19:49,370 --> 00:19:51,570
so we need festival

324
00:19:51,590 --> 00:19:53,160
two arrays

325
00:19:53,220 --> 00:19:55,430
i mean how pagerank

326
00:19:55,450 --> 00:19:58,260
and size of the array is equal to

327
00:19:58,280 --> 00:20:01,390
number of documents in our collection

328
00:20:01,410 --> 00:20:03,390
so imagine that the

329
00:20:03,910 --> 00:20:05,430
keep four bytes

330
00:20:05,450 --> 00:20:08,260
so we need

331
00:20:08,280 --> 00:20:09,220
and we have

332
00:20:09,300 --> 00:20:12,760
a collection of four billion documents

333
00:20:12,780 --> 00:20:17,160
so we need comment

334
00:20:18,950 --> 00:20:24,240
OK even if the same you got so now sixteen gigabytes of memory it

335
00:20:24,260 --> 00:20:29,240
pretty bleak and also we need some error is number of links that usually is

336
00:20:30,030 --> 00:20:33,470
of course we can keep to whites i don't believe that there are many documents

337
00:20:33,470 --> 00:20:34,820
with more than

338
00:20:34,850 --> 00:20:37,200
sixty five thousand

339
00:20:37,330 --> 00:20:39,330
link outgoing please

340
00:20:40,220 --> 00:20:44,620
it looks like that we can feed into into memory

341
00:20:44,660 --> 00:20:49,160
this is very simple doesn't work so definitely we can

342
00:20:49,180 --> 00:20:55,050
calculate pagerank on our laptop for some small subset of the internet

343
00:20:55,070 --> 00:21:01,350
but we cannot do it for some bigger graph

344
00:21:01,390 --> 00:21:07,220
next optimisation start next optimisation first step of optimisation

345
00:21:07,220 --> 00:21:08,700
looking again looking

346
00:21:08,720 --> 00:21:11,330
here what we see we see that

347
00:21:11,370 --> 00:21:12,910
every time

348
00:21:12,950 --> 00:21:14,890
when we get you

349
00:21:17,090 --> 00:21:22,390
actually trying to provide only one number is called

350
00:21:22,410 --> 00:21:25,390
getting getting only one reason

351
00:21:25,410 --> 00:21:29,120
and you see that this formula it works

352
00:21:29,140 --> 00:21:31,510
based on

353
00:21:31,530 --> 00:21:34,510
when we doing this

354
00:21:34,510 --> 00:21:35,660
we are using

355
00:21:35,680 --> 00:21:39,990
all leans the so we were trying to invert actually all matters because we are

356
00:21:39,990 --> 00:21:44,590
trying to get all links that are coming to maybe we can reduce

357
00:21:44,600 --> 00:21:47,010
amount of memories significantly

358
00:21:48,200 --> 00:21:51,760
can invert of representation

359
00:21:51,780 --> 00:21:54,050
what we get what we need to do

360
00:21:54,100 --> 00:21:55,090
we need to

361
00:21:55,100 --> 00:21:58,450
presented not going links

362
00:21:58,470 --> 00:22:01,620
but incoming links

363
00:22:01,640 --> 00:22:04,160
how can invert just metric

364
00:22:04,200 --> 00:22:05,700
how we can

365
00:22:05,760 --> 00:22:08,320
modify our

366
00:22:08,370 --> 00:22:10,450
it's actually not so hard

367
00:22:10,450 --> 00:22:12,780
you can you can do it using owl

368
00:22:12,830 --> 00:22:14,620
well known approach

369
00:22:14,660 --> 00:22:17,890
you can run through of wild ones and you can

370
00:22:17,950 --> 00:22:23,300
created this temporary parts of this file and then merge them again to the to

371
00:22:23,300 --> 00:22:25,280
the biggest inverted

372
00:22:25,330 --> 00:22:27,660
i just systematic

373
00:22:27,660 --> 00:22:31,350
and with this inverted one

374
00:22:31,370 --> 00:22:34,890
all calculations is extremely simple

375
00:22:34,930 --> 00:22:38,660
you need to keep only one error

376
00:22:38,660 --> 00:22:40,590
and size of the story

377
00:22:40,640 --> 00:22:45,300
is the number of places

378
00:22:46,910 --> 00:22:52,430
what we're going to do are reading one and one element of one string from

379
00:22:54,530 --> 00:22:57,490
from our input from our

380
00:22:57,510 --> 00:23:00,850
graph presentation and also we don't need to

381
00:23:00,890 --> 00:23:02,700
keep in memory of

382
00:23:02,700 --> 00:23:04,300
previous results

383
00:23:04,320 --> 00:23:05,910
because the process is

384
00:23:05,930 --> 00:23:07,680
step by step

385
00:23:07,680 --> 00:23:12,420
okay that the key on these things is how many variables you getting rid of by integrating

386
00:23:16,090 --> 00:23:17,030
think about paean end

387
00:23:19,580 --> 00:23:21,570
if any is larger than p e

388
00:23:22,790 --> 00:23:26,280
you're getting rid of more variables by integrating out x

389
00:23:27,390 --> 00:23:28,580
if p is large and and

390
00:23:29,990 --> 00:23:32,880
you getting more variables by integrating out w

391
00:23:34,500 --> 00:23:36,480
actually we know from all the

392
00:23:39,550 --> 00:23:40,650
all these uh

393
00:23:41,070 --> 00:23:43,150
consistency proofs and so on and so forth

394
00:23:43,730 --> 00:23:46,990
it is large and then x is gonna become very well determined

395
00:23:48,100 --> 00:23:53,050
if he is very very large large appears x becomes better determined so the last

396
00:23:53,050 --> 00:23:56,360
piece small these parameters very very well determined

397
00:23:57,290 --> 00:24:02,790
in the large and small pe these parameters very well determined but if any small

398
00:24:03,340 --> 00:24:07,080
and p large these parameters are badly determined but that doesn't matter bayesian if you

399
00:24:07,080 --> 00:24:10,460
integrate them out as matter having said all at it doesn't have a actually the

400
00:24:10,460 --> 00:24:11,060
same solution

401
00:24:12,870 --> 00:24:16,230
a little bit about stuff going on there because the special cases linear linear

402
00:24:16,680 --> 00:24:18,280
the solution is the same but

403
00:24:18,660 --> 00:24:23,080
your instinct should be to integrate out the largest set of variables and then this

404
00:24:23,080 --> 00:24:28,740
is a nice trick integrating out the mappings i've used it in several different areas

405
00:24:28,740 --> 00:24:29,170
since then

406
00:24:30,150 --> 00:24:33,040
it's good because other people don't seem to spotted how useful it can be

407
00:24:33,810 --> 00:24:34,470
on occasion

408
00:24:37,260 --> 00:24:38,100
scale i just wanted

409
00:24:41,860 --> 00:24:45,780
o point out what this implies and then we'll talk a little bit about gaussians

410
00:24:46,160 --> 00:24:48,110
so this is an independent

411
00:24:49,030 --> 00:24:49,960
so this is a

412
00:24:50,410 --> 00:24:51,670
calcium density

413
00:24:52,120 --> 00:24:55,410
with eh covariance is dependent on x

414
00:24:56,910 --> 00:24:59,070
and what we call that's is a process

415
00:25:01,490 --> 00:25:06,450
just as before we talked about the difficulty we talk about in models that independent across

416
00:25:08,010 --> 00:25:11,300
it would be easy to add a feature difficult to add a new variable end

417
00:25:11,490 --> 00:25:15,360
well that's what i casting process allows you to do to add new variables tend

418
00:25:15,360 --> 00:25:20,070
to be consistent across that and the reasons calcium processes work is because they have

419
00:25:20,070 --> 00:25:23,990
covariance functions that fulfill the properties of a massacre and all you know

420
00:25:24,310 --> 00:25:27,930
i have to know the representer theorem to gaussianprocess processes because you can show everything

421
00:25:27,930 --> 00:25:29,990
about it is called common graph consistency

422
00:25:30,950 --> 00:25:35,700
but it's basically telling you the same thing about casting process and support vector machine

423
00:25:35,930 --> 00:25:38,640
so i want just talk a little bit about galcians

424
00:25:39,680 --> 00:25:41,240
which may be actually done right at the beginning

425
00:25:42,140 --> 00:25:43,930
after and introduce calcium processes

426
00:25:47,200 --> 00:25:49,220
i like using height and weight examples

427
00:25:49,620 --> 00:25:53,210
for gaussians even though the positive variables so that they shouldn't be modeled with a

428
00:25:53,210 --> 00:25:57,450
guassian but they can be well approximated by calcium so i'm gonna sample heights from

429
00:25:57,450 --> 00:26:02,030
a distribution and weights from a distribution so his heights mean one point seven metres

430
00:26:02,030 --> 00:26:05,490
weights means seventy five kilos cocaine and is the variance

431
00:26:06,330 --> 00:26:08,870
i and here's the two distributions so

432
00:26:10,080 --> 00:26:10,680
i'm around there

433
00:26:12,780 --> 00:26:13,720
and i'm around there

434
00:26:15,370 --> 00:26:15,990
and so

435
00:26:16,370 --> 00:26:17,080
we can sample

436
00:26:18,010 --> 00:26:21,600
jointly from these two distributions and the way we do that's want to look at

437
00:26:21,640 --> 00:26:24,030
the distribution of heights and weights that sample height

438
00:26:24,580 --> 00:26:25,870
sample way let's problem them

439
00:26:26,580 --> 00:26:28,850
sample high sample way this problem

440
00:26:29,260 --> 00:26:31,370
sample sample weights problem yeah

441
00:26:32,140 --> 00:26:34,200
so this is independent gaussians sampling

442
00:26:34,680 --> 00:26:35,800
what's wrong with this picture

443
00:26:39,890 --> 00:26:41,140
and no independent yeah

444
00:26:43,050 --> 00:26:43,780
it's not that

445
00:26:44,330 --> 00:26:48,330
if you taller you typically heavier and body mass index is about that's so this

446
00:26:48,330 --> 00:26:51,140
is an independence assumption but in reality they are dependent

447
00:26:52,990 --> 00:26:53,640
the new model

448
00:26:54,410 --> 00:26:55,660
this is correlated guassian

449
00:26:56,580 --> 00:26:57,280
so here

450
00:26:57,830 --> 00:27:02,050
i sample heights hand if i sample high height the weight is also high

451
00:27:04,120 --> 00:27:05,530
condition on the height

452
00:27:06,330 --> 00:27:10,100
the weight is not this is the marginal distributions the marginals look the same

453
00:27:17,220 --> 00:27:21,200
well actually tend be a large height will tend to be associated with a larger way are low weight

454
00:27:21,700 --> 00:27:22,720
and a low height yeah

455
00:27:23,580 --> 00:27:26,120
so that's correlation is a simple thing isn't it

456
00:27:28,220 --> 00:27:32,160
what we looked at before with really appreciate and peace yeah what it's doing is

457
00:27:32,390 --> 00:27:36,870
taking a matrix like this it's rotating it back to the matrix we saw axisaligned

458
00:27:37,530 --> 00:27:42,100
that's in fact what does the rotation does back and these guys the igon values

459
00:27:42,180 --> 00:27:46,510
i tell you what the variances are any axisaligned state so to me and i

460
00:27:46,510 --> 00:27:49,470
can value problem is that i mean i know it has other purposes but that's

461
00:27:49,470 --> 00:27:53,760
how i physically think about and i can value problem that it's rotating calcium

462
00:27:54,160 --> 00:27:59,470
not really calcium a quadratic form but that's how i think about is rotating about soon-to-be axis aligned

463
00:28:00,660 --> 00:28:03,510
so that's given to the curvature are all the properties of the gas you

464
00:28:04,490 --> 00:28:10,130
so gas in distribution the multivariate one is dependent on the covariance anomie now we're

465
00:28:10,130 --> 00:28:11,740
going ignore the mean and it's

466
00:28:12,660 --> 00:28:15,780
doesn't matter taller ignoring the mean some people get fuss about it

467
00:28:16,550 --> 00:28:20,200
and if you want us about it with me and prepared argue till

468
00:28:22,140 --> 00:28:25,120
the dogs come home or whatever the expression is an but i will tell you

469
00:28:25,120 --> 00:28:29,120
is arguing me is not a good idea but we can assume means we don't

470
00:28:29,120 --> 00:28:32,550
lose anything by doing it seems odd because people they all means everything in a

471
00:28:32,550 --> 00:28:33,300
guassian well okay

472
00:28:33,830 --> 00:28:35,280
you can do this without mean zero

473
00:28:35,680 --> 00:28:40,100
so what i wanna do is i wanted to calcium with a particular covariance structure

474
00:28:42,390 --> 00:28:46,950
we've already seen we've got gas with a particular covariance structure x x transpose is not gonna be about one

475
00:28:47,330 --> 00:28:48,680
can be different covariance structure

476
00:28:49,370 --> 00:28:52,760
and i'm gonna take this calcium is a twenty five dimensional calcium

477
00:28:54,350 --> 00:28:56,260
i'm gonna take one sample from it

478
00:28:57,470 --> 00:28:59,140
so it's gonna twenty five elements

479
00:28:59,510 --> 00:29:00,910
and i'm gonna plot those those

480
00:29:01,580 --> 00:29:05,430
those samples from this calcium against their index o and x one t twenty five

481
00:29:05,640 --> 00:29:07,330
brought the samples against those index

482
00:29:08,470 --> 00:29:09,140
and there you have it

483
00:29:10,080 --> 00:29:13,330
this is the sample one twenty five plotted against their index

484
00:29:14,450 --> 00:29:16,970
this is the covariance are used

485
00:29:17,600 --> 00:29:22,350
i've drawn as a color map and because it's got unit value along the mean

486
00:29:22,680 --> 00:29:25,580
you can sort of see by the fact that these guys fall within two standard

487
00:29:25,580 --> 00:29:27,760
deviations above the mean the marginal

488
00:29:28,950 --> 00:29:30,580
will be within two standard deviations

489
00:29:32,350 --> 00:29:36,080
i can say this is also a correlation so what you can actually see is

490
00:29:36,220 --> 00:29:40,240
these are very dark red hair up in this region these two points very strongly

491
00:29:40,240 --> 00:29:45,240
correlated that's example model showing with rho equals point nine eight the joint density these

492
00:29:45,240 --> 00:29:47,260
two points is strongly correlated

493
00:29:47,260 --> 00:29:53,920
basically frequence of graphs ordered by frequency so this would be the rank

494
00:29:54,040 --> 00:29:58,480
and you can look at them but what what you see is that most of

495
00:29:58,480 --> 00:30:03,300
them are threes right we get a lot of these stars we get some chains more

496
00:30:03,300 --> 00:30:11,360
complicated shapes are less less common and it's also not really clear how

497
00:30:11,360 --> 00:30:15,360
what would be what is the relation between the structure and the

498
00:30:15,360 --> 00:30:19,500
frequency or the rank right you can have some that have that are that are

499
00:30:19,510 --> 00:30:24,920
big for example this one is on seven nodes and it's more frequent than this guy on three right so even

500
00:30:24,920 --> 00:30:31,940
though more nodes more edges this is less frequent and so on right similar similarly here we

501
00:30:31,940 --> 00:30:37,140
have this chains on four and so on right so the question is what is the

502
00:30:37,140 --> 00:30:42,600
relation and just to say it again right what we would like to know and I don't

503
00:30:42,600 --> 00:30:48,480
think we have the the very good answer is how to use like social context cascade shape and frequency relate

504
00:30:48,480 --> 00:30:52,040
right how does the how can I determine the frequency of

505
00:30:52,040 --> 00:30:57,220
the cascade from its from its from its shape and the questions we would like to

506
00:30:57,220 --> 00:31:01,890
find answers to is you know why is why is this pattern more frequent than

507
00:31:01,890 --> 00:31:12,580
that pattern and what is the interpretation of them one thing just to show more power loss

508
00:31:12,580 --> 00:31:16,780
I guess so you can also go and measure the cascade size distribution right

509
00:31:16,780 --> 00:31:19,740
so here we have the size which is means the number of nodes in the

510
00:31:19,760 --> 00:31:26,840
cascade  and the count and this is now for books so again you can see an so I'm plotting this on

511
00:31:26,840 --> 00:31:32,880
log so this is the relation between cascade size and the count and you can see it's

512
00:31:32,880 --> 00:31:38,140
it's a straight line it's a heavy tailed distribution and there are few cascades that are that are large and they are

513
00:31:38,320 --> 00:31:43,100
not even that large it's a hundred nodes and we had three million people that were making book

514
00:31:43,100 --> 00:31:47,580
recommendations and the largest propagation was less than a hundred nodes

515
00:31:48,060 --> 00:31:54,120
oh and maybe one more thing that also notice the exponent is very very

516
00:31:54,120 --> 00:31:59,500
high right so the fail the the the decay is very quick very strong so this is

517
00:31:59,510 --> 00:32:03,720
now for DVD's right here the situation is is a bit different again we are plotting the

518
00:32:03,720 --> 00:32:09,780
same cascade size so number of nodes in the cascade versus count and you can see here

519
00:32:09,780 --> 00:32:14,060
cascades go up to a thousand nodes so here we have like eight hundred thousand people making these

520
00:32:14,190 --> 00:32:19,760
recommendations and the cascade size goes up to a thousand nodes and also it decays much slower

521
00:32:19,760 --> 00:32:25,000
also there is this knee right and what we found is that since there are incentives right

522
00:32:25,000 --> 00:32:28,780
there were websites or forums where a person could go and say I am interested

523
00:32:28,790 --> 00:32:35,000
in buying these DVD can please someone recommend it to me right and both parties would benefit

524
00:32:35,040 --> 00:32:40,720
right person who makes a recommendation gets gets credit and the person who buys

525
00:32:40,730 --> 00:32:44,480
gets ten percent discount so there there was incentive so we think that these

526
00:32:44,480 --> 00:32:52,050
big ones may be the result of these websites and same for blogs so count

527
00:32:52,120 --> 00:32:55,140
cascade size and again a power low with a different exponent

528
00:32:56,580 --> 00:33:04,420
so just to wrap up on the on the empirical analysis of the cascades right we saw the

529
00:33:04,420 --> 00:33:11,320
they've the cascade sizes follow a heavy tail distribution power low  or something else right with different

530
00:33:11,320 --> 00:33:17,980
exponents so this suggests that something more complicated is going on that that some kind

531
00:33:17,980 --> 00:33:22,820
simple benching process on a tree because if it would be a branching process

532
00:33:22,840 --> 00:33:29,990
we would have a the distribution should be exponentially in the size right so the questions are what does

533
00:33:29,990 --> 00:33:34,800
the underlying social net what role does the underlying social network play and can you make steps towards

534
00:33:34,810 --> 00:33:41,160
more realistic cascade generation or propagation models and here is a here is a

535
00:33:41,160 --> 00:33:46,720
simple simple step that sort of works well its for blogs the idea is

536
00:33:46,720 --> 00:33:51,620
so basically this is a SAS virus propagation model right the idea is that we have

537
00:33:51,620 --> 00:33:55,500
these blogs and we have a and also we have these blogs and also we have a

538
00:33:55,500 --> 00:34:00,280
social network between them so what we'll do now is we will we will pick a new

539
00:34:00,280 --> 00:34:06,220
post and insert it to to the to one of the blogs and say now the information starts

540
00:34:06,230 --> 00:34:10,860
propagating here and now based on the links on the social network this will

541
00:34:10,860 --> 00:34:15,850
will just randomly propagate this information and once the node blog gets infected it can infect

542
00:34:15,860 --> 00:34:20,480
it's it can infect its neighbor in the network and so on so

543
00:34:20,480 --> 00:34:26,500
it's basically a simple virus propagation network where you every node heals with probability

544
00:34:26,500 --> 00:34:32,240
one so that the information can go back and forth between two

545
00:34:32,240 --> 00:34:34,580
blogs if they are strongly linked

546
00:34:35,080 --> 00:34:42,780
and if we go and simulate this process we can see it matches reality quite well so here I have

547
00:34:42,780 --> 00:34:48,760
log log eigenvalue versus log rank us usually you find a power-law

548
00:34:52,780 --> 00:34:59,650
so now I showed you the properties of sort of static graphs, right, you have a graph and you look at it and you ask what can I see in there okay

549
00:34:59,670 --> 00:35:04,130
now the question is what what if I have a graph that grows

550
00:35:04,170 --> 00:35:09,870
right and there are two two things that you can thick think of graphs as they grow, first one is that

551
00:35:09,890 --> 00:35:15,760
you would think that whenever a node would come they create about the same number of edges, right, so the the

552
00:35:15,760 --> 00:35:25,040
average degree or the average number of links per node should be pretty much should remain constant as the graph grows, right, so this is one and the other one is you would think that

553
00:35:25,060 --> 00:35:34,670
and the the diameter so the distances would slowly grow, as I grow my network, the nodes would sort of get farther apart on the average, as the network gets larger, right, so this is

554
00:35:34,720 --> 00:35:38,960
something that you'd expect and what prior work would agree

555
00:35:38,980 --> 00:35:44,180
so for for the relation between the number of edges and number of nodes over time

556
00:35:44,190 --> 00:35:54,260
so we have this basically simple question, right, what is the relation between number of nodes and number of edges in the network over time, so we have these two quantities we are interested in and basically

557
00:35:54,260 --> 00:35:56,660
the question is if the number of nodes doubles

558
00:35:57,480 --> 00:36:02,820
T to T to T plus one, will the number of edges also double

559
00:36:03,820 --> 00:36:09,500
pretend this is an animation and now it comes up, okay, it turns out it that's not true

560
00:36:09,520 --> 00:36:11,350
this is what is going on

561
00:36:12,560 --> 00:36:15,870
for example, if I take the graph of internet for two days

562
00:36:16,500 --> 00:36:23,340
we we create a new one every day, so we have like more than seven hundred graphs and we plot the number of nodes

563
00:36:23,360 --> 00:36:27,460
at some particular time and we plot the number of edges at some particular time

564
00:36:27,480 --> 00:36:29,040
again on log log scales

565
00:36:29,080 --> 00:36:30,300
you can see this

566
00:36:30,320 --> 00:36:31,520
falls on a line

567
00:36:31,540 --> 00:36:35,000
right and what is so

568
00:36:35,020 --> 00:36:45,290
we see that there is this relationship this power-law relationship between the number of edges and the number of nodes in the network over time, so this is the graph of internet, for example, this would be citations

569
00:36:45,320 --> 00:36:51,190
this would be all the high energy physics citations from nineteen ninety-three to two thousand and three and again I have

570
00:36:51,240 --> 00:36:56,320
log log log log N versus log T and now

571
00:36:56,360 --> 00:36:59,420
okay, this this falls on a line and we have this exponent A

572
00:36:59,430 --> 00:37:11,720
now you can ask what would be this what are the usual va what would be the values of this exponent, right, if the value of the exponent is one, then I have no densification, then I have, average degree remains constant over time okay

573
00:37:11,720 --> 00:37:12,680
if I have

574
00:37:12,690 --> 00:37:14,220
exponent of two

575
00:37:14,220 --> 00:37:16,180
that would mean I'm building

576
00:37:16,220 --> 00:37:40,020
a very dense graph, so I'm building a clique, everyone is connected to everyone else, right, because in a graph I can have quadratically many edges as I have nodes, right, a graph can be represented as adjacent symetrics, so as the as the adjacent as the matrix grows, the number of cells in the matrix is quadratic to the num to the size of the matrix, right, that's the reason why A can be between one and two and as I said for one we have linear growth

577
00:37:40,040 --> 00:37:44,800
and for two we would have a quadratic growth, which is a clique, and know you can ask what do we see

578
00:37:44,840 --> 00:37:46,640
and interestingly

579
00:37:46,650 --> 00:38:03,380
we are seeing something that is much higher than one, right, we'd expect one we see here one point two and here one point seven, so what so what this basically means is that our reference lists are getting longer over time, right, this basically tells, how does the length of the reference list increase over time in some sense

580
00:38:05,040 --> 00:38:06,720
so this is this is one

581
00:38:09,080 --> 00:38:10,920
let me skip this one, it's too dense

582
00:38:10,960 --> 00:38:13,100
another thing that you can ask

583
00:38:13,140 --> 00:38:18,920
is how does the diameter, how do the distances change and if you look for example at the erdos-renyi random graph

584
00:38:18,930 --> 00:38:30,740
there and go and analyze it you can you can find you can get results that the the diameter of the network scales as log N or log log N so basically it slowly grows as the network would grow

585
00:38:30,740 --> 00:38:33,080
so what turns in real life is that

586
00:38:33,100 --> 00:38:37,600
which is sort of counterintuitive is that as the network grows, the distances shrink

587
00:38:37,620 --> 00:38:42,120
okay and here here's what I mean by that so

588
00:38:42,140 --> 00:39:00,880
for this again I have the internet and the citation network, so here i'm plotting the size of the graph for time and here I plot the diameter or this is the effective diameter so this would be some kind notion of what is the what is the distance between the node how how how much is the network spread apart and you can see that

589
00:39:00,890 --> 00:39:04,370
it decreases as the graph grows and for citations

590
00:39:04,390 --> 00:39:08,950
the whole thing is even is even more obvious, right, it goes from

591
00:39:10,170 --> 00:39:11,220
more than nine

592
00:39:11,220 --> 00:39:12,680
to something like five

593
00:39:13,670 --> 00:39:16,820
in ten years of day that we that we had the data

594
00:39:16,820 --> 00:39:31,740
so this is sort of counterintuitive, right, because he my network is growing exponentionally, right, so here I started, I don't know, with several thousands of nodes, here I have a few hundred of them and you'd expect, the more they are the the more far apart they are

595
00:39:31,760 --> 00:39:34,420
isn't it a consequence

596
00:39:38,360 --> 00:39:39,920
you could assume that

597
00:39:43,840 --> 00:39:44,320
and especially

598
00:39:45,410 --> 00:39:46,450
the degree

599
00:39:52,340 --> 00:39:57,280
not necessarily, no, not necessarily, for example, if you take

600
00:39:57,320 --> 00:40:09,680
now I'm jumping ahead, but if you take preferential attachment, right, and sort of force it to have densification that still has increasing diameter, so even though they seem similar, they are they are disjoint and also when I'll be talking about model

601
00:40:09,700 --> 00:40:15,620
the first model I'll show is the one where you get densification, but you don't get shrinking diameter, so they are different

602
00:40:17,660 --> 00:40:25,440
how how they are different is still an open question, so okay so I showed you all this about graphs

603
00:40:27,180 --> 00:40:31,040
these these properties can be find can be found

604
00:40:32,900 --> 00:40:48,560
in many different graphs coming from various domains, for example, worldwide web, online communities, telephone networks, autonomous systems and so on, so this heavy tailed small world communities' resilience to attacks all this

605
00:40:48,580 --> 00:40:50,180
was was found in

606
00:40:50,220 --> 00:40:52,070
pretty much any network that was looked up

607
00:41:00,260 --> 00:41:01,760
yeah sure

608
00:41:08,950 --> 00:41:11,930
if you don't then usually you have a very good explanation

609
00:41:11,950 --> 00:41:15,820
for for example

610
00:41:16,760 --> 00:41:30,010
how to say, for example, some some graphs that if you would look, I don't know, the road networks at some particular particular scale or particular granularity, then it would look a bit different because it has to leave

611
00:41:30,030 --> 00:41:31,370
on on the surface

612
00:41:31,380 --> 00:41:43,180
right, but normally, normally what I showed is what you expect, if you don't see that, then you start to ask, why I don't see that, for example, in instant messaging right there is a limit on how many contacts you have and

613
00:41:43,880 --> 00:41:50,580
depending on where is that limit you will see the spike in the distribution, it will be cut off, it will end there, so I'll also show this later

614
00:41:53,720 --> 00:42:04,890
so this is the motivation now for the second part, how do we think about these things, so now I was basically like throwing at you throwing them at you one by one and now the question is how do we think about them why are they there what would be a simple

615
00:42:04,920 --> 00:42:08,740
generative model or a mechanism that would that could sort of generate them

616
00:42:08,740 --> 00:42:12,190
holiday photos anything of that form

617
00:42:12,200 --> 00:42:14,630
you might have graphs

618
00:42:14,680 --> 00:42:15,700
well done

619
00:42:15,710 --> 00:42:19,760
is for is something that occurs in social networks quite obviously so if you use

620
00:42:19,760 --> 00:42:21,160
facebook myspace

621
00:42:21,210 --> 00:42:25,260
a lot of that information is represented as the interaction graph

622
00:42:25,330 --> 00:42:28,320
that's in a way where a lot of the money states or if you have

623
00:42:28,330 --> 00:42:33,720
collaborative filtering they essentially have one big bipartite graph on one side and got all

624
00:42:33,720 --> 00:42:36,510
the movies does of got all the users

625
00:42:36,520 --> 00:42:38,380
and i have

626
00:42:38,390 --> 00:42:42,710
and h whatever going around rented some movie and then decided that he liked and

627
00:42:42,710 --> 00:42:44,440
didn't like it

628
00:42:44,570 --> 00:42:48,690
you actually want to make use of that in some cases to infer

629
00:42:48,740 --> 00:42:50,370
additionally edges in the graph

630
00:42:50,410 --> 00:42:55,910
like well you might actually like my heart number thirteen was you've seen the die-hard

631
00:42:55,910 --> 00:42:57,580
one up to ten right

632
00:42:58,710 --> 00:43:02,640
things like that would be inference or you could say well given that the guy

633
00:43:02,640 --> 00:43:06,950
has seen all those maybe you know i should send him an invitation to the

634
00:43:12,400 --> 00:43:14,750
optical character recognition this is

635
00:43:14,760 --> 00:43:17,140
it's actually very popular example

636
00:43:17,140 --> 00:43:19,090
and this is what people

637
00:43:19,140 --> 00:43:23,000
i used to work on the lot basically in the mid nineties

638
00:43:23,010 --> 00:43:24,760
fact from the united

639
00:43:24,830 --> 00:43:27,890
united states postal service one of the data

640
00:43:27,970 --> 00:43:29,890
the dot actually play a very

641
00:43:29,910 --> 00:43:31,680
key role in machine learning

642
00:43:31,710 --> 00:43:36,000
the key role in the sense that somebody goes and releases about it

643
00:43:36,010 --> 00:43:40,640
maybe for the specific purpose in this case of recognising handwritten digits

644
00:43:40,650 --> 00:43:43,450
and people actually go and deploy algorithms for

645
00:43:43,560 --> 00:43:47,520
and this thing takes on a life of its own because it becomes kind of

646
00:43:48,580 --> 00:43:53,270
against which everybody benchmarks against as if you come up with the new algorithm then

647
00:43:53,270 --> 00:43:54,440
it's almost

648
00:43:54,490 --> 00:43:58,390
a matter of necessity that you try give me out on that participate

649
00:43:59,700 --> 00:44:03,840
it's good if you can show that this stuff works better than anybody else

650
00:44:04,970 --> 00:44:08,760
so for instance the netflix dataset is another one of those cases where a new

651
00:44:08,760 --> 00:44:13,570
dataset actually has changed the way research is being done considerably

652
00:44:16,920 --> 00:44:20,730
just actually one thing to bear in mind that you read the machine learning paper

653
00:44:20,760 --> 00:44:21,730
and they only two

654
00:44:21,770 --> 00:44:25,260
an experiment on a on a certain small subset of other

655
00:44:25,300 --> 00:44:30,260
it's pretty easy to tell that OK well that's getting issues

656
00:44:30,350 --> 00:44:34,090
so for instance if somebody just runs it algorithm was on so now i know

657
00:44:34,090 --> 00:44:38,840
why located at probably didn't go beyond two hundred three observations

658
00:44:39,000 --> 00:44:42,510
that would be the right to start the base so

659
00:44:42,540 --> 00:44:46,180
the idea is to find out what they category it seems this will be

660
00:44:46,190 --> 00:44:48,270
are and it's about the USA

661
00:44:48,520 --> 00:44:51,300
OK if you look at this stuff from and you can see that

662
00:44:51,320 --> 00:44:53,900
well not quite sure what they're selling here

663
00:44:56,820 --> 00:44:59,710
some company reported some profit

664
00:45:00,650 --> 00:45:03,190
fourth quarter returned with the

665
00:45:04,350 --> 00:45:06,300
faces well

666
00:45:06,390 --> 00:45:07,300
look at this

667
00:45:07,310 --> 00:45:08,820
look at the next one

668
00:45:08,840 --> 00:45:10,760
now what can i do with those who

669
00:45:10,790 --> 00:45:12,590
with the start of

670
00:45:12,710 --> 00:45:16,110
i could try to figure out who that person is

671
00:45:18,340 --> 00:45:22,570
or i could try to find out whether the guy on the picture is happy

672
00:45:22,670 --> 00:45:27,350
so it's not immediately clear that just because you're given a certain dance it

673
00:45:27,440 --> 00:45:30,250
a certain question associated with it

674
00:45:30,280 --> 00:45:32,280
as you can see here there are actually

675
00:45:32,290 --> 00:45:34,540
well live different expressions

676
00:45:34,590 --> 00:45:39,240
and this particular about this it has maybe a hundred people for what they probably

677
00:45:39,240 --> 00:45:42,020
did is they went around the camera in the lab and

678
00:45:42,070 --> 00:45:47,040
they had a hundred people in the lab and everybody had his picture taken

679
00:45:47,370 --> 00:45:52,480
which also tells you that getting very large face recognition datasets unless you pay for

680
00:45:52,480 --> 00:45:54,540
that is actually not that easy

681
00:45:54,640 --> 00:45:56,520
there are not many free one

682
00:45:56,530 --> 00:45:59,140
microarray data is another case

683
00:46:00,680 --> 00:46:03,160
these will be different genes basically

684
00:46:03,180 --> 00:46:05,220
i think of those reading queen

685
00:46:06,510 --> 00:46:10,660
as information with the gene is particularly expressed or not expressed in other words whether

686
00:46:10,660 --> 00:46:13,360
it's really working a lot with is

687
00:46:14,550 --> 00:46:18,120
so somebody actually win class of this in some direct away

688
00:46:18,160 --> 00:46:20,020
probably not terribly pretty

689
00:46:21,040 --> 00:46:24,240
the point is somehow that within that cluster

690
00:46:24,990 --> 00:46:28,460
a lot of those genes are turned on in here that are not well

691
00:46:28,540 --> 00:46:31,560
doesn't really mean terribly much but if you now

692
00:46:31,570 --> 00:46:34,670
i have something like the cancer diagnosis system

693
00:46:34,740 --> 00:46:39,880
we might want to find out OK maybe ten fifteen maybe twenty different genes

694
00:46:39,930 --> 00:46:43,400
which you can actually then used to find out whether somebody

695
00:46:43,420 --> 00:46:44,610
a second order

696
00:46:44,620 --> 00:46:48,150
and it makes a lot of difference if you actually manage to find those genes

697
00:46:48,280 --> 00:46:52,740
so the reason i mean and usually with mission maybe one hundred thousand two hundred

698
00:46:52,740 --> 00:46:54,710
thousand different genes so

699
00:46:54,750 --> 00:46:56,990
after matrix i think at the moment

700
00:46:57,030 --> 00:46:59,670
two hundred thousand three much the next what you can get

701
00:46:59,750 --> 00:47:02,510
so why would you care about finding a small number

702
00:47:02,550 --> 00:47:06,850
because if you now afterwards deployed as the hospital you don't actually want to happen

703
00:47:06,850 --> 00:47:07,930
in talk

704
00:47:07,980 --> 00:47:10,760
microarray was two hundred thousand genes measures

705
00:47:10,910 --> 00:47:14,130
that's really expensive can cost a few hundred dollars

706
00:47:14,160 --> 00:47:16,150
what you want to stick to is

707
00:47:16,210 --> 00:47:20,390
move to cheaper technology which uses only ten or twenty of those

708
00:47:20,440 --> 00:47:25,420
and maybe the experiment only cos i don't know maybe ten maybe fifty dollars

709
00:47:25,430 --> 00:47:29,290
so there is immediate cost savings if you manage to reduce it to a certain

710
00:47:29,290 --> 00:47:30,760
number of genes

711
00:47:30,840 --> 00:47:36,190
as of course also the biological motivation of assuming that well maybe there's only a

712
00:47:36,190 --> 00:47:39,470
couple of key genes and their relevant for that problem

713
00:47:39,520 --> 00:47:42,340
but that's a lot more risky

714
00:47:42,350 --> 00:47:47,300
so a lot of people have written wonderful papers on widely you fancy algorithm is

715
00:47:47,300 --> 00:47:49,890
actually good at selecting g

716
00:47:49,890 --> 00:47:51,210
i've done that too

717
00:47:51,260 --> 00:47:52,250
that's it

718
00:47:52,260 --> 00:47:57,690
there's no real good guarantee why this algorithm will actually be very meaningful biological

719
00:47:57,810 --> 00:48:02,610
the only thing that we can usually argues that hey it actually works

720
00:48:04,150 --> 00:48:07,520
here's another example of biological sequences

721
00:48:07,590 --> 00:48:10,010
so this is basically

722
00:48:10,040 --> 00:48:13,140
well triplets which tells you which proteins are

723
00:48:13,140 --> 00:48:17,470
synthesized here and you want to find out basically what they're doing

724
00:48:17,550 --> 00:48:21,130
this is something you can get very easily what sequencing techniques

725
00:48:21,210 --> 00:48:26,030
pointing out that this is an interminable domain of vascular cell adhesion blah blah blah

726
00:48:26,050 --> 00:48:29,190
it's actually hard is it means you actually need some

727
00:48:30,000 --> 00:48:35,840
bioinformatics post-doc or about biochemist opposed stand in the lab and maybe spain's a couple

728
00:48:35,840 --> 00:48:38,500
of months to establish that this belongs there

729
00:48:38,510 --> 00:48:40,640
if you want automated

730
00:48:40,680 --> 00:48:43,750
fortunately people have already done a lot of these

731
00:48:43,760 --> 00:48:45,760
it may actually be that so

732
00:48:45,760 --> 00:48:47,750
this annotation here is

733
00:48:47,790 --> 00:48:51,870
well two point one point one point four point three means this is where i

734
00:48:51,870 --> 00:48:53,860
need to branch down in my

735
00:48:53,880 --> 00:48:57,040
ontology to tell me what the function is

736
00:48:57,140 --> 00:48:59,740
sometimes you can actually have situations where

737
00:48:59,750 --> 00:49:02,140
individual sequences have several function

738
00:49:02,140 --> 00:49:03,460
we might have protein

739
00:49:03,480 --> 00:49:05,180
simply playing several roles

740
00:49:05,210 --> 00:49:07,320
he might have several of those terms here

741
00:49:07,350 --> 00:49:09,540
this is the so-called gene ontology

742
00:49:09,590 --> 00:49:12,820
directed acyclic graphs or in short kodak

743
00:49:12,840 --> 00:49:16,760
so what you want to do you want to map this to that

744
00:49:17,840 --> 00:49:21,500
not very it's a hierarchical probably labels

745
00:49:21,520 --> 00:49:23,140
are actually in some

746
00:49:23,160 --> 00:49:24,860
fairly elaborate structure

747
00:49:24,870 --> 00:49:27,300
you might actually have a lot of different labels

748
00:49:27,350 --> 00:49:31,160
and now it actually matters how wrong you get things

749
00:49:31,230 --> 00:49:32,870
so let's assume

750
00:49:32,880 --> 00:49:34,190
well you get the first

751
00:49:34,210 --> 00:49:38,160
four digits here right and you get the first one wrong well maybe actually some

752
00:49:38,160 --> 00:49:40,020
biologists might be able to

753
00:49:40,030 --> 00:49:42,520
you know i just tried five different

754
00:49:42,550 --> 00:49:46,390
ideas of what this protein might do you might actually be able to find it

755
00:49:46,390 --> 00:49:47,390
is that

756
00:49:47,970 --> 00:49:49,580
you compete with

757
00:49:49,590 --> 00:49:51,330
a supervised learning algorithm

758
00:49:51,340 --> 00:49:54,230
using the same number of labelled samples

759
00:49:54,900 --> 00:49:58,480
you me you ask for some labels

760
00:49:58,500 --> 00:50:04,520
and you're error area the classes that you choose at the end will be similar

761
00:50:04,540 --> 00:50:07,580
to very low supervised learning algorithm looking at

762
00:50:07,590 --> 00:50:10,920
the same number of labelled examples

763
00:50:11,440 --> 00:50:12,650
in general

764
00:50:12,660 --> 00:50:15,110
when you know supervised learning

765
00:50:15,120 --> 00:50:18,850
the area rate is going to be something like the VC dimension

766
00:50:18,900 --> 00:50:20,580
time is

767
00:50:20,590 --> 00:50:24,150
the minimum error rate of the classifier

768
00:50:24,200 --> 00:50:25,140
divided by

769
00:50:25,140 --> 00:50:27,840
the number of labelled samples

770
00:50:27,860 --> 00:50:28,980
because the

771
00:50:29,000 --> 00:50:31,790
there's this controls the variance

772
00:50:31,830 --> 00:50:35,940
of the minimum error classifier which is going to control the deviations which is going

773
00:50:35,940 --> 00:50:39,810
to control the you can select this individual parts out

774
00:50:39,820 --> 00:50:45,310
and if you just put this quantity and what you see is that

775
00:50:48,080 --> 00:50:50,410
new square repsonse square

776
00:50:51,580 --> 00:50:54,050
this is a square root but square

777
00:50:54,070 --> 00:50:57,110
the new party cities council news cancel

778
00:50:57,120 --> 00:50:59,990
the eighteen

779
00:51:00,020 --> 00:51:01,150
so that says that

780
00:51:01,160 --> 00:51:03,490
when you have some fundamental noise

781
00:51:03,510 --> 00:51:04,490
it's going to

782
00:51:04,500 --> 00:51:06,070
to put of four

783
00:51:06,860 --> 00:51:09,630
the number of times that

784
00:51:09,650 --> 00:51:11,830
that you ask for

785
00:51:11,850 --> 00:51:16,320
for for this style

786
00:51:16,320 --> 00:51:19,380
practical learning things like this

787
00:51:19,400 --> 00:51:27,160
so the proof that this is a very simple

788
00:51:27,360 --> 00:51:30,850
so what happens is if you have a VC dimension of it means there are

789
00:51:30,850 --> 00:51:32,650
d inputs that you can label

790
00:51:32,660 --> 00:51:34,900
either way

791
00:51:35,450 --> 00:51:38,740
you can label all configurations

792
00:51:39,920 --> 00:51:44,380
and then what you're going to do is even make there be quite a bit

793
00:51:45,490 --> 00:51:49,730
no it on these input so probably the y equals one given one of these

794
00:51:50,960 --> 00:51:53,700
is going to be very close to the problem y equals zero given one of

795
00:51:53,700 --> 00:51:56,960
these inputs

796
00:51:57,010 --> 00:51:59,490
OK so you want this level

797
00:51:59,510 --> 00:52:02,060
every time you ask for the label

798
00:52:02,060 --> 00:52:03,080
it's going to be

799
00:52:03,100 --> 00:52:06,400
stem probability that's a one some properties here

800
00:52:06,480 --> 00:52:09,320
this problem is to be very close to get it out so you have

801
00:52:09,560 --> 00:52:10,900
the coin

802
00:52:10,970 --> 00:52:14,760
the queen has some particular bias the bias is very near to point five

803
00:52:14,900 --> 00:52:17,870
the question is how many times due to flip

804
00:52:17,880 --> 00:52:21,450
this coin before you pretty sure that the most likely

805
00:52:22,630 --> 00:52:23,800
is one

806
00:52:23,900 --> 00:52:27,520
or the most likely choice is here

807
00:52:27,540 --> 00:52:31,620
and there's the people have worked out lower bounds on the number of times you

808
00:52:31,620 --> 00:52:34,940
have to flip a coin in to figure this out

809
00:52:35,810 --> 00:52:39,960
chris restored

810
00:52:40,020 --> 00:52:41,830
OK so

811
00:52:43,930 --> 00:52:54,320
in terms of data

812
00:52:57,320 --> 00:52:58,550
so there's something in

813
00:52:58,580 --> 00:53:03,120
steve hanneke is paper where he introduced the disagreement coefficient

814
00:53:03,130 --> 00:53:06,210
but it's not with independent or bound

815
00:53:06,240 --> 00:53:07,900
so i said that

816
00:53:08,410 --> 00:53:12,220
this where and i showed you

817
00:53:12,250 --> 00:53:14,230
has all bound the of data

818
00:53:14,250 --> 00:53:18,330
so it's kind of intuitive it should be lower bound the data

819
00:53:19,920 --> 00:53:24,890
if you have some particular data it says there's some merit

820
00:53:24,900 --> 00:53:28,200
for which you can have to require a substantial number of

821
00:53:28,250 --> 00:53:29,980
of labels in order to

822
00:53:30,040 --> 00:53:32,270
in order to recorders

823
00:53:32,290 --> 00:53:36,250
and since we a logarithmic dependence if we are rehearsing

824
00:53:36,300 --> 00:53:37,390
it's the the

825
00:53:37,390 --> 00:53:45,830
one worst step of recursion which really dominates our label complexity

826
00:53:45,850 --> 00:53:50,250
this is just another independent around which is

827
00:53:50,270 --> 00:53:53,500
so far

828
00:53:53,560 --> 00:54:01,140
yes of they data applies to

829
00:54:01,140 --> 00:54:03,600
all of these mellow

830
00:54:03,600 --> 00:54:05,370
active learning our lands

831
00:54:05,390 --> 00:54:10,370
you can imagine there might be non mel active learning algorithms

832
00:54:10,390 --> 00:54:13,540
first for me maybe even for the nonseparable case

833
00:54:13,580 --> 00:54:18,310
but we don't know what those are yet

834
00:54:36,500 --> 00:54:38,390
could be

835
00:54:45,600 --> 00:54:50,020
so all of the same importance weighted active learning

836
00:54:50,060 --> 00:54:54,420
the basic idea in importance weighted active learning is going to create algorithms which had

837
00:54:54,420 --> 00:54:56,080
this to rough outline

838
00:54:56,980 --> 00:54:59,420
you're running through unlabelled apples

839
00:54:59,440 --> 00:55:03,790
and now for something about the streaming model

840
00:55:03,790 --> 00:55:05,750
we're going to choose

841
00:55:05,770 --> 00:55:07,420
some probability

842
00:55:07,500 --> 00:55:09,370
we ask for the label

843
00:55:09,420 --> 00:55:11,730
so we have some subroutine rejection threshold

844
00:55:11,730 --> 00:55:15,120
which decides to ask the label or not

845
00:55:15,390 --> 00:55:17,850
and then with

846
00:55:17,920 --> 00:55:20,420
probability p we ask for the label

847
00:55:20,500 --> 00:55:22,580
and then we're going to add

848
00:55:22,600 --> 00:55:26,140
the features and the ability get and one over p

849
00:55:26,160 --> 00:55:27,560
and importance weight

850
00:55:28,770 --> 00:55:32,810
two and importance weighted set of examples

851
00:55:32,830 --> 00:55:34,080
and they going to

852
00:55:34,120 --> 00:55:39,000
learn using this importance weighted set of examples so the meaning of importance weight is

853
00:55:39,020 --> 00:55:42,710
what if if one of p is ten

854
00:55:42,730 --> 00:55:47,620
because i think this is the example ten times more important to get right then

855
00:55:47,670 --> 00:55:51,540
one was an importance weighted one

856
00:55:51,770 --> 00:55:56,480
often i think people use importance weights in various ways so for example comes up

857
00:55:56,480 --> 00:55:58,810
and boosting

858
00:55:58,810 --> 00:56:02,500
this is a very simple

859
00:56:02,540 --> 00:56:03,690
however the skeleton

860
00:56:03,710 --> 00:56:08,400
and the claim is that just from the skeleton you can easily construct

861
00:56:10,080 --> 00:56:12,710
active learning algorithms for the

862
00:56:12,790 --> 00:56:16,350
nonseparable case

863
00:56:16,370 --> 00:56:18,000
so in particular

864
00:56:19,620 --> 00:56:20,480
for any

865
00:56:20,920 --> 00:56:25,710
rejection threshold if probabilities about some minimum

866
00:56:25,730 --> 00:56:29,060
and then i was just to constant factor worse than supervised learning in terms of

867
00:56:29,060 --> 00:56:33,480
the number of labelled examples that required

868
00:56:33,500 --> 00:56:38,850
it should be intuitive right because we have some probability of of living anywhere

869
00:56:38,870 --> 00:56:41,540
and these important base allow us to

870
00:56:43,730 --> 00:56:49,850
it essentially reconstructs the draws from the original distribution

871
00:56:49,870 --> 00:56:51,750
so if we optimize well

872
00:56:51,770 --> 00:56:56,230
with respect to these importance weighted samples we'll end of optimizing wealth back to the

873
00:56:56,230 --> 00:57:00,750
original distribution

874
00:57:00,810 --> 00:57:04,230
OK so this is that we have some safety if guarantee but we still would

875
00:57:04,230 --> 00:57:11,060
like to know if we can get label complexity improvements

876
00:57:12,170 --> 00:57:13,250
in order to

877
00:57:13,250 --> 00:57:17,890
to get label most improvements we need to fill in the rejection threshold subroutine

878
00:57:17,940 --> 00:57:23,290
so this is one way to fill in for which we can prove some improvements

879
00:57:23,330 --> 00:57:24,520
so we can have

880
00:57:24,580 --> 00:57:27,940
the unlabelled examples are going to the history of

881
00:57:27,940 --> 00:57:28,880
the model

882
00:57:28,940 --> 00:57:32,690
and and at this point the information goes back into the cycle and the two

883
00:57:32,870 --> 00:57:36,420
can move on and select for the

884
00:57:36,440 --> 00:57:40,980
actions such as for instance provide hints feedback remediation and so on and so forth

885
00:57:41,000 --> 00:57:42,240
now i should

886
00:57:42,340 --> 00:57:47,030
bifida currency i said ideally in this slide because there is no tutoring system that

887
00:57:47,030 --> 00:57:48,520
has all this

888
00:57:48,740 --> 00:57:53,820
knowledge and can deal with old knowledge what's different different systems that focus on different

889
00:57:53,820 --> 00:58:00,140
parts of his architecture or to to insist that have more sophisticated the presentation of

890
00:58:00,140 --> 00:58:04,540
the target domain there are two theories systems that have more sophisticated models or the

891
00:58:04,570 --> 00:58:10,360
kind of communication models for instance allow them to dialogue in natural language but you

892
00:58:10,380 --> 00:58:13,520
insist on that does it all still doesn't exist

893
00:58:16,120 --> 00:58:21,520
was there have we've seen quite good achievements in the last twenty years there is

894
00:58:21,530 --> 00:58:26,220
there have been many successful i ts is that have been deployed in the field

895
00:58:26,220 --> 00:58:27,750
and you successfully

896
00:58:27,760 --> 00:58:30,480
to improve student learning in the classroom

897
00:58:30,490 --> 00:58:34,440
and i won't be able to i don't like and we put all references for

898
00:58:34,470 --> 00:58:38,760
individual achievements but what i listed there it's book they just recently

899
00:58:38,770 --> 00:58:43,390
there has been recently published that provides a very good overview of the state-of-the-art if

900
00:58:43,390 --> 00:58:49,630
you're interested and there's also been a company that a spin-off project funded by researchers

901
00:58:49,630 --> 00:58:55,960
from carnegie mellon university that has been commercialized commercializing intelligent tutoring systems in for math

902
00:58:55,960 --> 00:58:59,860
and science in hundreds of high schools in the united states and some of the

903
00:58:59,860 --> 00:59:05,380
curriculum in general teaching of mine into schools have changed because of the introduction of

904
00:59:05,380 --> 00:59:10,870
this to system so tutoring system are having an impact on education or right

905
00:59:11,500 --> 00:59:15,960
just to quantify this if you remember the two sigma should where human tutors can

906
00:59:15,960 --> 00:59:20,380
generate the standard deviation better learning the classroom instruction

907
00:59:20,390 --> 00:59:22,010
up to now

908
00:59:22,090 --> 00:59:26,160
the tutoring system that have been formally evaluated along these dimensions have reached up to

909
00:59:27,250 --> 00:59:29,650
standard deviation improvement OK

910
00:59:29,660 --> 00:59:32,830
not as good as human tutors but you know that's

911
00:59:32,900 --> 00:59:37,090
to be expected certainly better than the most sophisticated case systems

912
00:59:37,100 --> 00:59:41,970
kai systems that so far have managed to see trip to achieve in improvements just

913
00:59:41,970 --> 00:59:44,230
half standard deviation

914
00:59:46,530 --> 00:59:48,240
so we are done

915
00:59:48,240 --> 00:59:53,820
well not really because mostly the eighty s is that i described that have been

916
00:59:53,820 --> 00:59:55,870
successfully deployed

917
00:59:55,880 --> 01:00:00,080
they usually provide individualized support for

918
01:00:00,100 --> 01:00:05,950
problem solving activities via twitter let interactions and this is also a technique called coached

919
01:00:05,950 --> 01:00:07,020
problem solving

920
01:00:07,080 --> 01:00:13,270
and basically involves having students solve problems in science in some usually science or math

921
01:00:13,280 --> 01:00:15,640
domain in this case for instance physics

922
01:00:15,700 --> 01:00:18,590
the two to follows the student the student has to do

923
01:00:18,600 --> 01:00:24,540
solve the exercises presented by the tudor could can provide feedback if this student makes

924
01:00:24,540 --> 01:00:27,780
mistakes him for instance turns certain and read

925
01:00:27,870 --> 01:00:29,570
can provide help

926
01:00:29,590 --> 01:00:34,090
but this is basically the kind of instruction on which

927
01:00:34,140 --> 01:00:39,220
intelligent tutoring systems have been successful however there are other forms of

928
01:00:39,240 --> 01:00:43,740
educational activities that can actually help learners acquire

929
01:00:43,830 --> 01:00:49,260
the target cognitive skills and knowledge and they can usually be useful perhaps at different

930
01:00:49,260 --> 01:00:54,350
stages of the learning process and can be better suitable for different kinds of learners

931
01:00:54,350 --> 01:00:55,720
with different kind of

932
01:00:55,730 --> 01:00:58,040
cognitive skills and learning abilities

933
01:00:58,070 --> 01:01:00,780
so this is where

934
01:01:00,800 --> 01:01:05,260
the current trends of AI really of i'ts research are trying to push

935
01:01:05,650 --> 01:01:09,380
actually advances in intelligent tutoring systems research

936
01:01:09,390 --> 01:01:13,400
and i'm going to mention what i believe are for all the mean the four

937
01:01:13,400 --> 01:01:19,040
of the most important areas of research current so one is to try to develop

938
01:01:19,050 --> 01:01:23,510
adaptive learning environments that systems that instead of giving

939
01:01:23,530 --> 01:01:30,960
helping students do structured problem solving try to provide activities that enhance that trigger free

940
01:01:30,960 --> 01:01:36,100
exploration learning from exploration on the target domain and that usually done by using for

941
01:01:36,100 --> 01:01:41,230
instance virtual worlds interactive simulations or educational games

942
01:01:41,240 --> 01:01:46,740
meta cognitive tutors that his tutors that instead of focusing only on teaching helping students

943
01:01:46,740 --> 01:01:51,770
learn knowledge about a specific domain they also tried to teach learning and reasoning skills

944
01:01:52,070 --> 01:01:56,110
that in cognitive science are known as meta cognitive skills because they trust and the

945
01:01:56,110 --> 01:02:00,000
specific domain tend to be in general for learning and research

946
01:02:00,030 --> 01:02:05,600
affective tutors that tutors that can react not only to harvest and learns in reasons

947
01:02:05,600 --> 01:02:10,170
but also what students feeling during the interaction this is very important because they have

948
01:02:10,210 --> 01:02:14,800
been shown to be strong ties between affect and learning

949
01:02:14,810 --> 01:02:21,470
and last but not least collaborative learning environments that is environment to try to provide

950
01:02:21,470 --> 01:02:28,130
individualized support to help students learning

951
01:02:28,130 --> 01:02:29,520
all this

952
01:02:30,580 --> 01:02:37,800
areas of research have what i feel i think have two main common challenges that

953
01:02:37,800 --> 01:02:43,000
need to be addressed their activities they're targeted tend to be more open ended and

954
01:02:43,000 --> 01:02:48,270
less well defined than the structure problem solving that we saw earlier in structure problem

955
01:02:48,270 --> 01:02:51,730
solving usually it's clear what the learner needs to do in order to generate a

956
01:02:51,730 --> 01:02:56,890
good solution and was a clear definition of correct behaviors in this kinds of activities

957
01:02:56,890 --> 01:02:59,250
such as for example and exploration of

958
01:02:59,340 --> 01:03:04,020
an interactive simulation or interaction with the occasional game what is it good behaviour is

959
01:03:04,020 --> 01:03:06,790
much more open and less well defined

960
01:03:06,900 --> 01:03:12,490
also what we are trying to model with this new generation of intelligent tutors are

961
01:03:12,500 --> 01:03:15,180
small are student raised their much

962
01:03:15,190 --> 01:03:21,540
there are higher level than knowledge goals that were tracked by previous generation after systems

963
01:03:21,800 --> 01:03:24,990
for instance meta cognitive skills and affective states

964
01:03:25,000 --> 01:03:28,930
so what is this basically can be summarized by saying that now

965
01:03:29,010 --> 01:03:33,720
this students have to deal with much higher level of uncertainty during the interaction because

966
01:03:33,720 --> 01:03:37,910
of the nature of the interaction that they want to that want to support

967
01:03:37,910 --> 01:03:41,390
i'm going to start going to be giving the second half of the introduction to

968
01:03:41,390 --> 01:03:43,660
cognitive science

969
01:03:43,680 --> 01:03:47,010
this is particularly funds but i feel like i have maybe the most fun spot

970
01:03:47,010 --> 01:03:49,890
on the whole program because it's it's sort of the end of the first day

971
01:03:49,890 --> 01:03:54,850
i could talk about picture general introductory issues but also playing off the team's very

972
01:03:54,850 --> 01:04:01,250
interesting ideas and questions for debated in raising in all talks today so hopefully this

973
01:04:01,250 --> 01:04:05,140
will give you further background in what we're interested in cognitive science what these two

974
01:04:05,140 --> 01:04:08,390
fields of cognitive science machine learning how to say to each other and also laid

975
01:04:08,390 --> 01:04:11,430
the groundwork for say some of the debates are going to be having today and

976
01:04:11,430 --> 01:04:14,390
for the rest of the summer school

977
01:04:14,460 --> 01:04:20,250
now as many of the speakers have emphasised there's there's a long history of interplay

978
01:04:20,250 --> 01:04:24,640
between these fields of psychology and cognitive science on the one hand and machine learning

979
01:04:24,850 --> 01:04:26,060
here i just put up

980
01:04:26,070 --> 01:04:33,680
a list of not not complete but representative list of various approaches to machine learning

981
01:04:33,960 --> 01:04:37,750
but in some sense have their origins in psychology and cognitive science there either invented

982
01:04:37,750 --> 01:04:42,990
by psychologists were first published in psychology journals and that includes ones have been mentioned

983
01:04:42,990 --> 01:04:47,130
already in that context like the perceptron was published in the journal psychological review which

984
01:04:47,130 --> 01:04:52,360
is the main reading journal for theoretical psychology other things which have been mentioned here

985
01:04:53,490 --> 01:04:58,190
factor analysis multidimensional scaling right which are somewhat old techniques and that we call those

986
01:04:58,190 --> 01:05:02,690
unsupervised learning techniques but also things which you know you you you might all think

987
01:05:02,690 --> 01:05:10,350
of as a psychological either psychologically inspired psychologically motivated learning algorithms things like spectral clustering

988
01:05:10,460 --> 01:05:14,130
collaborative filtering with low rank factorizations

989
01:05:14,140 --> 01:05:20,030
kernel based classification bayesian concept learning these ideas although i don't wanna not trying to

990
01:05:20,030 --> 01:05:23,660
claim that psychologists that everything machine learning is just picking up the pieces but you

991
01:05:23,660 --> 01:05:24,910
can find

992
01:05:24,910 --> 01:05:25,830
either the

993
01:05:25,830 --> 01:05:27,470
actual idea or key

994
01:05:27,570 --> 01:05:34,660
intuitions antimatter that's not the case that psychology is just about cool phenomena and need

995
01:05:34,660 --> 01:05:39,160
experiments or even it's not just that the rigorous psychology is contained to the to

996
01:05:39,160 --> 01:05:43,530
the rigour of experimental design and data analysis but the that field just like machine

997
01:05:43,530 --> 01:05:47,550
learning has long theoretical tradition and what's exciting here is up to you bring these

998
01:05:47,840 --> 01:05:51,510
two theoretical traditions into contact and learn from each other

999
01:05:51,520 --> 01:05:53,550
now as again

1000
01:05:53,730 --> 01:05:58,170
the earliest because it emphasised this the history of interactions between these fields it's it's

1001
01:05:58,170 --> 01:06:02,020
long but it kind of goes in waves and they come together and move apart

1002
01:06:02,040 --> 01:06:06,670
and so on and what what what motivates this summer school is the sense of

1003
01:06:06,670 --> 01:06:11,330
these two fields are coming back together again after a somewhat long history of having

1004
01:06:11,330 --> 01:06:17,550
been a part and looking back with you for inspiration and lessons learned in the

1005
01:06:17,560 --> 01:06:22,560
last moment in history when these two fields are really very tightly interacting this was

1006
01:06:22,560 --> 01:06:29,170
the connectionist neural network whom the the time when actually i think of all the

1007
01:06:29,180 --> 01:06:30,550
all the speakers today

1008
01:06:30,550 --> 01:06:35,210
maybe all the faculty here were about the right generation we kind of got into

1009
01:06:35,210 --> 01:06:37,830
the field around this time and this is probably a lot of what excited this

1010
01:06:37,860 --> 01:06:40,730
is that is that a fair statement certainly neale said that i think they could

1011
01:06:40,730 --> 01:06:45,920
say that i would say that so so what was this paradigm about what was

1012
01:06:45,920 --> 01:06:50,450
incomplete and what lessons can be learned take forward just to recap my own version

1013
01:06:50,450 --> 01:06:52,300
of the same kind of history

1014
01:06:52,310 --> 01:06:57,360
what was exciting about this view was that it did give in the sense that

1015
01:06:57,360 --> 01:07:02,890
nick was talking about the very and this multi-level analysis perspective on how the mind

1016
01:07:02,900 --> 01:07:07,900
the brain work formulating problems of cognition and really with a focus on learning at

1017
01:07:07,900 --> 01:07:12,140
what mark called the computational theory level the abstract logic of computation

1018
01:07:12,180 --> 01:07:16,730
the algorithmic level that made contact with traditional ideas from psychology prof what with the

1019
01:07:16,730 --> 01:07:20,980
steps of mental processing and even down to the neural implementation level and all these

1020
01:07:20,980 --> 01:07:24,140
things seem to fit together in a very neat way so learning is some kind

1021
01:07:24,140 --> 01:07:28,330
of statistics with the the general motivation that what our brains do is the fine

1022
01:07:28,360 --> 01:07:32,950
structure in the world this can be unsupervised could be task oriented could be driven

1023
01:07:32,950 --> 01:07:37,840
by rewards spread out over time as the paradigms of supervised unsupervised reinforcement learning

1024
01:07:37,920 --> 01:07:42,680
but it's basically some kind of statistical inference finding patterns in massive high dimensional data

1025
01:07:42,680 --> 01:07:45,450
finding clusters low dimensional manifolds and so on

1026
01:07:45,520 --> 01:07:50,210
so that's the abstract math problem we then knew how to formulate this

1027
01:07:50,230 --> 01:07:55,280
the statistical inference problem as some kind of an opposition or like some kind of

1028
01:07:55,280 --> 01:07:59,680
maximum likelihood you as as neil so those are kind of two different takes on

1029
01:07:59,680 --> 01:08:03,260
the same problem but there's basically some kind of functions some cost function to be

1030
01:08:03,260 --> 01:08:09,180
minimized we knew how to write learning rules are algorithms for for effectively doing that

1031
01:08:09,180 --> 01:08:13,110
these gradient descent chain rule back propagation sorts of things

1032
01:08:13,130 --> 01:08:17,600
we also this is where the connection to neuroscience comes in of course knew how

1033
01:08:17,600 --> 01:08:21,520
to make a link to these neural networks these networks of neurons like processing units

1034
01:08:21,720 --> 01:08:25,450
where the idea is that this is high dimensional data analysis problem that comes down

1035
01:08:25,450 --> 01:08:30,440
to learning lots and lots of numerical parameters say the what the

1036
01:08:30,470 --> 01:08:33,970
this picture here which is the dimension reduction from two dimensions down to one of

1037
01:08:34,720 --> 01:08:38,240
cartoon sketch a much higher dimensional problem if we think about the data coming from

1038
01:08:38,240 --> 01:08:42,550
sensors and there you have parameter specifying say the

1039
01:08:42,600 --> 01:08:44,400
the slope and the intercept of the

1040
01:08:44,420 --> 01:08:49,430
the the weights that linear surface in a very high dimensional space or the parameters

1041
01:08:49,430 --> 01:08:54,230
specifying say means and covariances of clusters all those numerical parameters would correspond to the

1042
01:08:54,230 --> 01:08:56,800
weights connected these units

1043
01:08:56,990 --> 01:09:02,180
and the learning rules were ways of adjusting the weights in response to the data

1044
01:09:02,230 --> 01:09:07,300
and then actually making contact with the real their science these these these pictures the

1045
01:09:07,300 --> 01:09:10,740
reason why we could say these are like the brain is because these circles can

1046
01:09:10,740 --> 01:09:15,810
be interpreted neurons these lines as soon as connections were one neuron sends a signal

1047
01:09:15,820 --> 01:09:20,170
to another neuron and the weights correspond to the strength of the synapses how much

1048
01:09:20,170 --> 01:09:25,350
when an electrical impulse comes in from from the presynaptic neuron how much chemical is

1049
01:09:25,350 --> 01:09:28,990
dumped into the space between the neurons that then picked up by the postsynaptic neuron

1050
01:09:28,990 --> 01:09:34,000
determines basically how strongly the postsynaptic neuron responds to the input coming in from the

1051
01:09:34,000 --> 01:09:38,740
presynaptic the more that neurotransmitter of the more sensitive the postsynaptic cell is the same

1052
01:09:38,740 --> 01:09:41,370
amount of neurotransmitter the stronger the connection

1053
01:09:41,450 --> 01:09:45,870
and were rules meant that pure clear how the strength of these synapses in the

1054
01:09:45,870 --> 01:09:50,350
brain adjusted in response to experience these are things like long term potentiation that's an

1055
01:09:50,350 --> 01:09:55,070
empirical phenomenon which was found to follow some of the basic characteristics of say heavy

1056
01:09:55,240 --> 01:09:58,430
or the are delta rule type learning

1057
01:09:58,440 --> 01:10:03,860
just basically just some function of pre and post synaptic input and output activations and

1058
01:10:03,860 --> 01:10:07,180
then you can find circuits in the brain this is a snaps in the hippocampus

1059
01:10:07,180 --> 01:10:10,860
the hippocampus is a structure in your brain kind of around there and it's also

1060
01:10:10,860 --> 01:10:15,690
in other mammals like there approximately there in the mouse brain and a lot of

1061
01:10:15,690 --> 01:10:20,740
neuropsychology and other kinds of neuroscience shows that you know basically without this structure working

1062
01:10:20,740 --> 01:10:26,180
well without the synapse is even doing long-term potentiation you can learn certain basic things

1063
01:10:26,240 --> 01:10:32,490
so that's very elegant picture in understanding the mind in the brain in computational terms

1064
01:10:32,670 --> 01:10:35,990
but it's also been useful not just for this kind of reverse engineering enterprise but

1065
01:10:35,990 --> 01:10:38,400
for forward engineering so on the technology side

1066
01:10:38,610 --> 01:10:42,800
these basic learning algorithms what we would now call things like principal component analysis or

1067
01:10:42,800 --> 01:10:47,160
k means clustering or mixture models and so on until i many of the basic

1068
01:10:47,160 --> 01:10:51,400
learning technologies that we now take for granted so face detection this is a classic

1069
01:10:51,510 --> 01:10:56,130
example of i can faces and more modern ways of detecting faces in images which

1070
01:10:56,220 --> 01:10:58,240
i think we'll was referring to

1071
01:10:58,250 --> 01:11:00,060
how does facebook

1072
01:11:00,240 --> 01:11:04,100
i suggest to your friends should be or how does amazon recommended products well they

1073
01:11:04,110 --> 01:11:09,160
do they use the various essentially very simple statistical pattern finding like this which is

1074
01:11:09,180 --> 01:11:10,570
scaled up

1075
01:11:10,590 --> 01:11:14,740
that's you that's the google recipe google does this past how does google organize information

1076
01:11:14,740 --> 01:11:16,850
on the web and return useful

1077
01:11:16,910 --> 01:11:22,170
search hits for your query well with the first technical technical innovation google there are

1078
01:11:22,170 --> 01:11:25,740
now for many but the one that google famous for what distinguished it from earlier

1079
01:11:25,740 --> 01:11:29,910
what that's going to use the twenty pull feedbacks it's going to be the intro

1080
01:11:38,390 --> 01:11:42,200
so i skip the step there

1081
01:11:42,240 --> 01:11:44,930
hopefully you can fill in this step is skipped

1082
01:11:44,970 --> 01:11:46,100
so i'm going to have

1083
01:11:46,100 --> 01:11:47,370
this integral

1084
01:11:47,390 --> 01:11:49,950
with the in front divided by

1085
01:11:49,970 --> 01:11:51,580
that same integral which is just

1086
01:11:51,600 --> 01:11:54,370
integral against qx

1087
01:11:54,470 --> 01:11:57,660
so in other difficulties zero

1088
01:11:57,740 --> 01:11:59,430
implies that

1089
01:11:59,450 --> 01:12:01,200
i nash moments

1090
01:12:01,200 --> 01:12:05,030
so i so the optimal q must have the same moment as p

1091
01:12:05,080 --> 01:12:09,720
and by and what moments means not very precise the moments are exactly the expectations

1092
01:12:09,800 --> 01:12:14,030
of these features that appear in the exponential family so gas in this would be

1093
01:12:14,030 --> 01:12:19,100
x and x squared and some preserving exactly those moments

1094
01:12:19,200 --> 01:12:26,240
questions on that

1095
01:12:28,050 --> 01:12:30,120
this is only true for exponential families

1096
01:12:30,160 --> 01:12:33,620
OK now

1097
01:12:33,660 --> 01:12:38,050
this projection step however we didn't do that globally we did only locally right so

1098
01:12:38,050 --> 01:12:40,890
we did a projection only for one factor at at the time it into production

1099
01:12:40,890 --> 01:12:44,550
of all the factors and i was sort of one of the approximations of made

1100
01:12:46,450 --> 01:12:50,970
so what's happening is we're going essentially from a global divergence to local divergence is

1101
01:12:51,030 --> 01:12:55,340
sort of the second approximation made in the first is your approximate gas

1102
01:12:55,490 --> 01:12:58,280
and the second one is not necessarily getting exactly the right moment to that out

1103
01:12:58,320 --> 01:13:00,780
in because of this approximation

1104
01:13:00,800 --> 01:13:04,340
and then we can think of that more generally is that we would like to

1105
01:13:04,340 --> 01:13:06,620
minimize the divergence between p and q

1106
01:13:08,140 --> 01:13:11,890
and what we do is going to write that out in terms of individual factors

1107
01:13:12,950 --> 01:13:15,890
recall that p of x

1108
01:13:15,930 --> 01:13:18,640
it is the product

1109
01:13:18,800 --> 01:13:20,720
one factors

1110
01:13:20,800 --> 01:13:23,300
q of x which is my approximation

1111
01:13:23,350 --> 01:13:27,700
is the product of all of the approximate factors

1112
01:13:27,720 --> 01:13:31,030
and throwing in the prior is one of the as one of the factors as

1113
01:13:35,160 --> 01:13:38,030
now i can rewrite that global divergence in the following way

1114
01:13:38,050 --> 01:13:40,620
this is this is completely going to the top line all have done is taken

1115
01:13:40,620 --> 01:13:44,340
one factor for the distinguished one of the factors

1116
01:13:44,350 --> 01:13:45,570
in p

1117
01:13:45,660 --> 01:13:48,120
to distinguish the corresponding factor and q

1118
01:13:48,180 --> 01:13:51,390
five and

1119
01:13:51,390 --> 01:13:54,680
you know what we did when we when we actually minimize variance in a b

1120
01:13:54,720 --> 01:13:58,430
it was we didn't really use that is our context we instead use this is

1121
01:13:58,430 --> 01:14:02,280
our context when we minimize the reprojection

1122
01:14:02,320 --> 01:14:04,180
so that that's the

1123
01:14:04,200 --> 01:14:06,840
that's that's second approximation the be so

1124
01:14:06,850 --> 01:14:12,910
we can use exactly the right context we should have to use approximate context

1125
01:14:13,070 --> 01:14:21,080
now having made his replacement you essentially now can just solve for

1126
01:14:21,140 --> 01:14:23,970
the FAA till that minimizes the variance and you get the

1127
01:14:33,240 --> 01:14:34,680
OK so

1128
01:14:34,910 --> 01:14:37,200
a bit of theory that's available three d

1129
01:14:37,220 --> 01:14:42,050
is if we're not exactly minimizing the KL divergence which were not what function or

1130
01:14:42,050 --> 01:14:47,180
we actually minimizing so there there is the function just justice for belief propagation people

1131
01:14:47,180 --> 01:14:53,140
found a energy function which propagation fine stationary points of three b you have said

1132
01:14:53,420 --> 01:14:56,620
that this is a generalized version of the same energy function

1133
01:14:56,640 --> 01:14:59,450
which is the function that taking fixed points

1134
01:14:59,470 --> 01:15:02,890
and is a particularly nice way of writing the energy function

1135
01:15:03,010 --> 01:15:04,350
which have given here

1136
01:15:04,370 --> 01:15:06,870
and what's nice about it is that this

1137
01:15:06,890 --> 01:15:09,350
this energy function is exactly

1138
01:15:09,370 --> 01:15:12,180
the model evidence approximation you get out of the

1139
01:15:12,180 --> 01:15:15,100
so what i was doing the bayes point machine i was comparing the evidence that

1140
01:15:15,100 --> 01:15:17,870
models the formula that i was using

1141
01:15:18,640 --> 01:15:20,580
here's our work so

1142
01:15:20,600 --> 01:15:22,850
it's a function only of the

1143
01:15:22,910 --> 01:15:26,640
approximate factor five approximated the factors in some way

1144
01:15:26,680 --> 01:15:29,320
and they don't have to be normalised so they can they can have negative variance

1145
01:15:29,320 --> 01:15:30,890
and so on the fly

1146
01:15:30,890 --> 01:15:34,100
what i do is i take all of those approximate factors

1147
01:15:34,220 --> 01:15:38,070
and i multiply them all together to get q of x

1148
01:15:38,950 --> 01:15:40,870
which is an unnormalized

1149
01:15:40,890 --> 01:15:43,240
you know got in density

1150
01:15:43,820 --> 01:15:45,120
and why do they take

1151
01:15:45,140 --> 01:15:49,100
i find the normalizing constant of the distribution that get for multiplying all those factors

1152
01:15:50,550 --> 01:15:55,890
and there is the power one minus and where n is the number of factors

1153
01:15:56,120 --> 01:15:59,180
number factors in the model

1154
01:16:01,070 --> 01:16:03,720
i take this product over all the factors

1155
01:16:03,740 --> 01:16:07,530
and this product is basically the normalizing constant

1156
01:16:07,550 --> 01:16:13,120
but you computed while you're running p of the thing you project because

1157
01:16:13,140 --> 01:16:16,280
basically this formula f of i x

1158
01:16:16,320 --> 01:16:20,120
times over f i tilt thank you have access is the same as of IQ

1159
01:16:20,120 --> 01:16:22,820
not x

1160
01:16:22,850 --> 01:16:23,350
right i

1161
01:16:23,350 --> 01:16:27,350
that's the same this

1162
01:16:27,370 --> 01:16:30,930
the only difference is that the normalizing constant is is in the in the correct

1163
01:16:32,600 --> 01:16:38,300
the normalizing constants work out correctly when you're it this way

1164
01:16:38,410 --> 01:16:39,370
but essentially

1165
01:16:39,370 --> 01:16:40,430
this is the

1166
01:16:40,430 --> 01:16:43,220
this is the form function it the project when you're running a p

1167
01:16:44,550 --> 01:16:45,910
the interesting thing is that

1168
01:16:45,910 --> 01:16:47,740
what important

1169
01:16:47,760 --> 01:16:49,240
of these

1170
01:16:49,510 --> 01:16:56,210
one the state of the aspects which are important in modelling dynamical systems

1171
01:16:56,230 --> 01:17:01,160
is that we can show that the rise of dynamical system

1172
01:17:01,160 --> 01:17:09,520
in the world in the first so-called phase space or state space so we can

1173
01:17:12,280 --> 01:17:18,120
the system page taking into loop was going on the state variable

1174
01:17:18,120 --> 01:17:23,210
because we have understood from the example of the lamps that are the state variables

1175
01:17:23,210 --> 01:17:26,580
which methods for studying the dynamics

1176
01:17:26,630 --> 01:17:32,120
one of the aspects which are important thing is to understand the for

1177
01:17:33,150 --> 01:17:36,300
the system doesn't change over time

1178
01:17:37,250 --> 01:17:41,690
if for example i put the constant inputs

1179
01:17:41,710 --> 01:17:45,960
what is important to understand if i reach

1180
01:17:45,970 --> 01:17:50,600
in certain point in which the system doesn't change any more

1181
01:17:52,410 --> 01:17:56,220
this is going to school a community

1182
01:17:56,280 --> 01:18:00,100
or is one of the six so-called fixed point of the

1183
01:18:01,530 --> 01:18:03,730
and of course

1184
01:18:03,970 --> 01:18:08,470
certainly equilibrium state i cannot serve in output

1185
01:18:08,490 --> 01:18:13,160
which is my mind things itself over time

1186
01:18:13,200 --> 01:18:18,280
this notion of the community is very important to study the dynamical system

1187
01:18:21,430 --> 01:18:24,070
a lot of theory available

1188
01:18:24,080 --> 01:18:25,450
so is new

1189
01:18:25,470 --> 01:18:28,470
what happens to dynamical systems

1190
01:18:28,530 --> 01:18:34,650
in presence of small perturbations around the circuit in equilibrium point

1191
01:18:36,200 --> 01:18:43,310
nature and we observe the equilibrium points which are fixed to the boys or for

1192
01:18:44,260 --> 01:18:46,560
dynamic behavior which are

1193
01:18:46,870 --> 01:18:53,110
also relations of course the summations over time so sometimes so that our system arrives

1194
01:18:53,120 --> 01:18:54,530
at a certain stage

1195
01:18:54,550 --> 01:19:00,660
well it doesn't change unless i change the name of the variables

1196
01:19:00,670 --> 01:19:05,530
in order to give you an example not to be too too much to ask

1197
01:19:05,660 --> 01:19:07,770
in this in this population

1198
01:19:09,230 --> 01:19:11,490
study these simple models

1199
01:19:11,510 --> 01:19:12,650
in this model

1200
01:19:12,650 --> 01:19:16,960
we observe what happens to the concentration

1201
01:19:17,430 --> 01:19:19,450
of the drug

1202
01:19:19,490 --> 01:19:22,440
which is given to the patient

1203
01:19:23,730 --> 01:19:24,710
this is

1204
01:19:24,760 --> 01:19:28,360
drug can be given in two different ways

1205
01:19:28,400 --> 01:19:32,240
by oral delivery so take appeal

1206
01:19:32,330 --> 01:19:33,270
in the city

1207
01:19:34,220 --> 01:19:37,680
i can be the drug through injection so

1208
01:19:37,730 --> 01:19:43,930
you have direct injection into the central compartment with the plasma

1209
01:19:47,450 --> 01:19:48,560
a lot of

1210
01:19:48,560 --> 01:19:54,450
c of modelling efforts have been devoted to it you can understand also for commercial

1211
01:19:54,450 --> 01:20:00,920
pop quizzes for industrial partners is to model those kind of dynamical systems so try

1212
01:20:00,970 --> 01:20:08,820
to understand what are the laws which characterizes the behavior of the system over time

1213
01:20:08,870 --> 01:20:16,950
and if for example if i have a fixed ingestion or injection

1214
01:20:16,980 --> 01:20:23,180
if i arrive surfing wikipedia which is what is the level of

1215
01:20:24,200 --> 01:20:29,400
drug which can be measured in the blocked

1216
01:20:29,410 --> 01:20:35,130
so this is an example of a dynamical system in which we just more than

1217
01:20:35,810 --> 01:20:37,820
states transformations

1218
01:20:37,820 --> 01:20:44,190
these equations are very simple because it just tells you that here

1219
01:20:44,220 --> 01:20:47,060
we are what we call the compartment

1220
01:20:47,060 --> 01:20:52,800
which is the amount of drug is is reported to elicit drug concentration in the

1221
01:20:52,800 --> 01:20:55,410
gastrointestinal compartment

1222
01:20:55,420 --> 01:20:56,530
this one

1223
01:20:56,610 --> 01:20:58,720
which is of course affected by

1224
01:20:58,740 --> 01:21:00,840
the whole

1225
01:21:00,840 --> 01:21:03,910
how much drug i do i take when i think

1226
01:21:03,980 --> 01:21:10,140
and the second equation and the second compartment also

1227
01:21:10,140 --> 01:21:15,960
we call it which is the right concentration the market compartment that for example can

1228
01:21:15,960 --> 01:21:17,230
be our goal

1229
01:21:17,260 --> 01:21:18,670
of the study

1230
01:21:19,340 --> 01:21:22,480
if we write must balance equations here

1231
01:21:22,500 --> 01:21:24,490
we are doing the states

1232
01:21:24,570 --> 01:21:28,580
equation so the equations which are rise

1233
01:21:28,620 --> 01:21:30,250
state transition

1234
01:21:30,270 --> 01:21:35,520
here we say that the variation of the drug the concentration of the drug in

1235
01:21:35,520 --> 01:21:41,880
the gastrointestinal compartment is just the balance of the input looks which is the

1236
01:21:41,890 --> 01:21:48,160
quantity of the drug we change ingesting in the output looks which is the quantity

1237
01:21:48,160 --> 01:21:50,700
of the stator flux of the

1238
01:21:50,760 --> 01:21:55,100
drag moves from the gastrointestinal compartment to the plasma

1239
01:21:55,140 --> 01:21:59,620
OK like it's very simple because i have to take the field

1240
01:21:59,630 --> 01:22:02,500
then the solution is the just the

1241
01:22:02,520 --> 01:22:05,480
it goes to the blast

1242
01:22:05,520 --> 01:22:10,380
the second equation is a similar equation here

1243
01:22:10,400 --> 01:22:11,920
and just as to how

1244
01:22:11,950 --> 01:22:13,370
much harder

1245
01:22:13,370 --> 01:22:19,900
the concentration in the multi compartment changes as a function of the inputs

1246
01:22:19,910 --> 01:22:22,410
one of the input is the quantity

1247
01:22:22,420 --> 01:22:25,240
which moves from the

1248
01:22:25,280 --> 01:22:31,270
oral ingestion into the compartment the other one is that if it's present that if

1249
01:22:31,280 --> 01:22:37,470
there is an injection also injection directly into the blood we have the quantity which

1250
01:22:37,470 --> 01:22:41,410
is directly injected into the

1251
01:22:41,410 --> 01:22:43,560
the combustion

1252
01:22:43,570 --> 01:22:45,230
here we have a lost

1253
01:22:45,310 --> 01:22:46,760
which tells you

1254
01:22:46,840 --> 01:22:51,440
but luckily the drug is used by the system so

1255
01:22:51,460 --> 01:22:53,100
there is a

1256
01:22:53,160 --> 01:22:57,730
what we call the in illumination and of the

1257
01:22:57,780 --> 01:23:02,210
of the drug from the body

1258
01:23:02,500 --> 01:23:06,320
we have the state states x one x two

1259
01:23:06,350 --> 01:23:09,050
reported here we have the two input

1260
01:23:09,080 --> 01:23:14,680
we have the equations are we able to derive the equilibrium points

1261
01:23:14,740 --> 01:23:19,670
yes it is to do it because if we if we think that u one

1262
01:23:19,940 --> 01:23:25,360
u two are constant but we put these two equations equal to zero

1263
01:23:25,440 --> 01:23:30,980
and we derive what is called what are called equilibrium point of the exercise is

1264
01:23:32,650 --> 01:23:35,980
we have the two equations

1265
01:23:36,010 --> 01:23:38,830
we can put the derivative is zero

1266
01:23:38,830 --> 01:23:39,880
and we see

1267
01:23:39,890 --> 01:23:44,650
and if you present all to constantinople was one of the two

1268
01:23:44,670 --> 01:23:46,960
can be zero as you can imagine

1269
01:23:47,010 --> 01:23:48,810
we have

1270
01:23:48,930 --> 01:23:51,280
in equilibrium point

1271
01:23:51,430 --> 01:23:55,430
for the two state variables x one and x two

1272
01:23:55,460 --> 01:23:58,140
we have yes of course because

1273
01:23:58,640 --> 01:24:01,820
u one constant from this equation

1274
01:24:01,820 --> 01:24:03,780
system is

1275
01:24:03,820 --> 01:24:06,400
because that's an important question

1276
01:24:07,320 --> 01:24:11,530
in practice a how things work that usually you have a lot of knowledge about

1277
01:24:11,530 --> 01:24:13,490
your domain

1278
01:24:15,990 --> 01:24:18,470
if you know both graphical models you

1279
01:24:18,490 --> 01:24:21,710
for example you sure your computer vision scientists and

1280
01:24:21,710 --> 01:24:26,280
you know about computer vision we also know about graphical models

1281
01:24:26,300 --> 01:24:31,340
you start immediately tried to a graphical model because you are the same person that

1282
01:24:31,340 --> 01:24:34,030
has both knowledge right so

1283
01:24:34,110 --> 01:24:37,260
you don't really get into the trouble of

1284
01:24:37,360 --> 01:24:41,030
figuring out which is the right spot but

1285
01:24:41,030 --> 01:24:46,690
formerly the problem right but i mean it turns out that in many cases that's

1286
01:24:46,690 --> 01:24:52,130
not the true mean these tools you can apply to possibly anything you can imagine

1287
01:24:52,130 --> 01:24:56,170
right because are basically stochastic processes where you have

1288
01:24:56,280 --> 01:24:58,920
interrelated viable so

1289
01:24:58,920 --> 01:25:00,190
i mean

1290
01:25:00,230 --> 01:25:03,880
you can apply for the factor example for any other examples

1291
01:25:12,820 --> 01:25:16,760
true that some works on graphical doing that for example i mean i guess

1292
01:25:17,170 --> 01:25:20,610
what i think one of the first works of

1293
01:25:25,570 --> 01:25:26,710
i think

1294
01:25:26,780 --> 01:25:30,880
probably single color and collaborators and stuff i mean

1295
01:25:30,880 --> 01:25:34,800
the beginning of this century yesterday was working on they were working on this to

1296
01:25:34,800 --> 01:25:36,920
trying to use graphical models for

1297
01:25:38,730 --> 01:25:41,070
figuring out probably

1298
01:25:43,550 --> 01:25:45,300
pathways in

1299
01:25:45,340 --> 01:25:47,590
gene regulatory networks

1300
01:25:47,670 --> 01:25:50,740
and yes there is some exploratory

1301
01:25:50,970 --> 01:25:54,760
work that you need to do is usually the models who is the first model

1302
01:25:54,760 --> 01:25:57,440
that came they were very simple

1303
01:25:57,460 --> 01:26:00,130
assuming that each gene for example would be

1304
01:26:00,190 --> 01:26:01,840
associate was single

1305
01:26:01,840 --> 01:26:04,420
pathway and things like that but

1306
01:26:04,440 --> 01:26:05,300
what you can

1307
01:26:05,300 --> 01:26:10,190
already see that even extremely simple graphical models

1308
01:26:10,240 --> 01:26:11,840
they sometimes you

1309
01:26:11,860 --> 01:26:18,260
impressively good results when compared with models that actually don't want assume an entire

1310
01:26:18,320 --> 01:26:23,280
network of relations between the part because the nice thing about graphical models that is

1311
01:26:23,280 --> 01:26:25,150
you have a complex model

1312
01:26:25,240 --> 01:26:30,360
what you have a reasonably disempower comes to perform approximate inference

1313
01:26:30,400 --> 01:26:31,260
in this

1314
01:26:31,260 --> 01:26:34,820
in this graphical model like belief propagation for example

1315
01:26:36,380 --> 01:26:41,280
other major theoretical problems there i mean in practice this thing works

1316
01:26:41,300 --> 01:26:43,530
very well and

1317
01:26:43,570 --> 01:26:45,320
people have been using full

1318
01:26:46,990 --> 01:26:49,210
many many rivals

1319
01:26:49,260 --> 01:26:51,460
thousands of images of

1320
01:26:51,510 --> 01:26:55,300
also three minutes of

1321
01:26:55,360 --> 01:26:57,030
OK now finally i mean

1322
01:26:57,090 --> 01:27:00,360
i think it was always some slides and markov blankets as well

1323
01:27:00,380 --> 01:27:02,380
so here they are

1324
01:27:02,380 --> 01:27:04,880
you need to look at what the markov blanket

1325
01:27:06,440 --> 01:27:07,170
a believe

1326
01:27:07,190 --> 01:27:09,070
so then what to blank

1327
01:27:11,990 --> 01:27:16,010
you may not much of it so let's see if you can understand what's the

1328
01:27:16,010 --> 01:27:17,030
markov blanket

1329
01:27:19,420 --> 01:27:23,130
so basically let's let's see what's written here

1330
01:27:23,170 --> 01:27:25,570
so the markov blanket of a given all

1331
01:27:25,590 --> 01:27:29,590
in in the vision that first order markov random fields

1332
01:27:29,590 --> 01:27:33,130
is the smallest set of nodes

1333
01:27:33,190 --> 01:27:38,210
such that when you condition on those nodes

1334
01:27:38,240 --> 01:27:47,340
you get the same distribution as you condition on all the rest of the norm

1335
01:27:47,360 --> 01:27:50,090
that's important why it's important

1336
01:27:50,110 --> 01:27:53,690
it's important because

1337
01:27:53,690 --> 01:27:57,130
obviously if this is true for

1338
01:27:57,190 --> 01:27:58,860
for some a

1339
01:27:58,860 --> 01:28:03,400
you want to find what's the smallest a four which is is still true

1340
01:28:03,420 --> 01:28:06,630
because you don't want to work with these

1341
01:28:06,670 --> 01:28:07,740
beast here

1342
01:28:07,740 --> 01:28:11,650
because these beast is too large

1343
01:28:11,710 --> 01:28:13,690
you want to work with these ones

1344
01:28:13,840 --> 01:28:17,820
so you look forward to a which is smallest among

1345
01:28:19,320 --> 01:28:22,320
but set such that you still have these equality

1346
01:28:24,190 --> 01:28:25,610
well the question is

1347
01:28:25,610 --> 01:28:26,610
what's that

1348
01:28:26,670 --> 01:28:28,460
i mean

1349
01:28:28,460 --> 01:28:31,030
what's the markov blanket of node

1350
01:28:31,970 --> 01:28:33,150
vision therefore

1351
01:28:33,320 --> 01:28:37,320
what's the markov blanket of a node in the market random field

1352
01:28:38,490 --> 01:28:43,820
so in the business of the markov blanket are the parents of all

1353
01:28:43,880 --> 01:28:48,210
the children of the norm and what to call the court parents often not

1354
01:28:52,030 --> 01:28:53,400
so called parents

1355
01:28:53,400 --> 01:28:56,760
it would be you know

1356
01:29:04,420 --> 01:29:05,360
we have an

1357
01:29:06,470 --> 01:29:09,010
so that capacity

1358
01:29:11,800 --> 01:29:14,920
in my markov random feel disempowered neighbors of the north

1359
01:29:23,960 --> 01:29:25,650
go back

1360
01:29:45,470 --> 01:29:46,710
it's not

1361
01:29:46,710 --> 01:29:48,940
exactly that because

1362
01:29:48,990 --> 01:29:49,960
x i

1363
01:29:49,970 --> 01:29:54,760
if you have a bayesian network or as you may have a markov random field

1364
01:29:54,760 --> 01:29:56,840
i have no observation

1365
01:29:56,990 --> 01:29:59,630
and you have no isolated

1366
01:29:59,690 --> 01:30:03,300
we saw that every node will depend on every no

1367
01:30:03,360 --> 01:30:06,050
there's no in the in the in the sense that

1368
01:30:06,050 --> 01:30:09,890
two it's own inputs and it's going to

1369
01:30:09,900 --> 01:30:14,630
you know past this relative to the next murder which i mean the previous motor

1370
01:30:14,630 --> 01:30:18,600
which is going to use that then to compute its

1371
01:30:18,650 --> 01:30:23,100
they are achieved with respect to its own parliament

1372
01:30:23,120 --> 01:30:26,660
so the problem here is

1373
01:30:26,680 --> 01:30:28,310
how can we interpret

1374
01:30:28,320 --> 01:30:31,820
nine network outputs as priorities

1375
01:30:31,840 --> 01:30:34,460
was is a very simple way

1376
01:30:34,490 --> 01:30:37,330
so many people use actually which is called

1377
01:30:37,510 --> 01:30:41,520
usually cross entropy criterion

1378
01:30:43,230 --> 01:30:49,790
you consider only outputs goal of knowledge through you have one outputs scope class of

1379
01:30:49,790 --> 01:30:51,440
tag of interest in you

1380
01:30:51,460 --> 01:30:53,420
in your task

1381
01:30:55,530 --> 01:31:00,480
well you know you pursue but it was exposure just so it's positive and you

1382
01:31:00,480 --> 01:31:03,950
know my eyes with respect to all the classes

1383
01:31:03,990 --> 01:31:05,720
so you get the conditional

1384
01:31:07,660 --> 01:31:10,250
so here for some

1385
01:31:10,260 --> 01:31:14,810
no reasons i defines this quite handy operation which is

1386
01:31:14,850 --> 01:31:16,650
and when i e

1387
01:31:16,660 --> 01:31:19,330
that's not true what i basically

1388
01:31:19,340 --> 01:31:20,670
get this

1389
01:31:20,720 --> 01:31:23,460
log likelihood that i want to maximize

1390
01:31:23,480 --> 01:31:26,680
so you can see the log as as a kind of you know

1391
01:31:26,700 --> 01:31:30,300
softmax in july so basically what you are doing here is that

1392
01:31:30,310 --> 01:31:34,360
you push this school

1393
01:31:34,370 --> 01:31:39,580
of the right last why you imposed on this course of years are classes

1394
01:31:39,590 --> 01:31:43,680
and actually the extreme if like those of max is really max you would personally

1395
01:31:43,680 --> 01:31:46,770
like the the school of

1396
01:31:46,800 --> 01:31:50,740
the class and s class according to the network

1397
01:31:50,760 --> 01:31:55,880
the problem this criterion is that it doesn't take into account that we are dealing

1398
01:31:55,880 --> 01:31:57,870
with light sentences

1399
01:31:57,880 --> 01:32:00,810
it's like just saying one world at the time

1400
01:32:00,820 --> 01:32:03,500
so it kind of sucks

1401
01:32:03,560 --> 01:32:04,850
so instead

1402
01:32:04,850 --> 01:32:08,140
we prefer to cluster don't like the complete sentence

1403
01:32:09,890 --> 01:32:11,780
it's one in the sentence

1404
01:32:11,800 --> 01:32:14,920
you apply on the network is going to produce you

1405
01:32:14,940 --> 01:32:16,190
first score

1406
01:32:18,030 --> 01:32:20,960
in your r and b task

1407
01:32:20,980 --> 01:32:26,630
and here we consider an additional school which is going to be a school that

1408
01:32:26,630 --> 01:32:30,420
you gave when you jump from one labelled with the other one

1409
01:32:30,440 --> 01:32:32,010
so we define

1410
01:32:32,030 --> 01:32:34,970
then this goal sentence with like

1411
01:32:34,980 --> 01:32:36,320
he wants

1412
01:32:36,330 --> 01:32:41,940
for bus of tags i to be led the some of the schools belongs is

1413
01:32:43,060 --> 01:32:47,140
plus as far as well as the transition called right

1414
01:32:47,190 --> 01:32:51,100
and once again we can apply the same before

1415
01:32:51,100 --> 01:32:53,480
and interpret the scores appointed

1416
01:32:53,500 --> 01:32:59,390
but by just normalizing over all the buses so before we went normalizing raise respect

1417
01:32:59,390 --> 01:33:04,140
to all the classes this time we normalize respect to all of the past

1418
01:33:04,150 --> 01:33:10,580
and basically these uses look like you that you want to maximize so

1419
01:33:10,620 --> 01:33:13,910
once again you basically special disco

1420
01:33:13,960 --> 01:33:18,820
after the right bus while pushing down the scores of all the other blogs as

1421
01:33:18,820 --> 01:33:20,590
the extreme if the

1422
01:33:20,630 --> 01:33:22,910
if so log and is like just max

1423
01:33:22,930 --> 01:33:28,120
it's basically pushing down to of the best past from maisonette

1424
01:33:28,130 --> 01:33:31,840
the only party that this

1425
01:33:31,860 --> 01:33:32,770
look at

1426
01:33:32,800 --> 01:33:36,200
it's not like all past and its

1427
01:33:36,220 --> 01:33:40,110
actually very expensive to compute if you do it you know very naive

1428
01:33:41,900 --> 01:33:43,770
but fortunately i mean it has been

1429
01:33:45,430 --> 01:33:48,020
a long time ago there is something called the

1430
01:33:48,030 --> 01:33:52,450
the formalism which are used to compute is located in a recursive manner

1431
01:33:52,470 --> 01:33:57,570
you just have to consider this logadd but computed

1432
01:33:57,570 --> 01:34:00,820
on the bus to the world to minus one

1433
01:34:00,870 --> 01:34:03,410
so you suppose you already uses computation

1434
01:34:03,430 --> 01:34:09,820
and an amazing simple or recursion you can just computes the log and over all

1435
01:34:09,820 --> 01:34:13,720
over the past up to the world

1436
01:34:13,740 --> 01:34:18,590
then you and then you can just you know trained in our network

1437
01:34:18,610 --> 01:34:25,150
back propagating this was this regression using like the chain or as you did before

1438
01:34:25,150 --> 01:34:30,130
falling you know there is no more layers using this in different to be tricky

1439
01:34:30,130 --> 01:34:31,820
mathematically that

1440
01:34:31,830 --> 01:34:35,280
it's it's OK you can do it you know it's important

1441
01:34:36,980 --> 01:34:38,030
so this

1442
01:34:38,080 --> 01:34:39,460
it's actually again

1443
01:34:39,950 --> 01:34:46,180
quite all stuff it's about your colour case of what is called like graph transformer

1444
01:34:47,140 --> 01:34:52,190
o posed like by early on with two young in ninety seven ninety eight

1445
01:34:52,210 --> 01:34:57,890
and well if you if you like you can compare it to the conditional on

1446
01:34:57,890 --> 01:34:59,450
the film

1447
01:35:01,450 --> 01:35:05,590
would be basically the female version of this

1448
01:35:05,730 --> 01:35:07,440
for sequences

1449
01:35:07,500 --> 01:35:09,970
so here we are really no no

1450
01:35:09,980 --> 01:35:14,650
and we do the same the conditional all which training by stochastic gradient descent

1451
01:35:14,660 --> 01:35:15,990
but it

1452
01:35:16,010 --> 01:35:22,200
yes the same notes you know what inference you need to find the best pass

1453
01:35:22,550 --> 01:35:25,220
and you do that by applying the viterbi algorithm

1454
01:35:25,280 --> 01:35:29,780
which is basically the same as in the goes on but when you replace logadd

1455
01:35:33,160 --> 01:35:44,970
so what we can apply the you know all networks straight out of the box

1456
01:35:44,990 --> 01:35:47,490
two of task of interest

1457
01:35:49,970 --> 01:35:52,820
we actually took the window approach

1458
01:35:52,840 --> 01:35:57,070
four part of speech part of speech tracking on any of

1459
01:35:57,090 --> 01:36:03,450
the window was of size five on the network like three hundred hidden units

1460
01:36:03,490 --> 01:36:10,570
we use the coalition approach for semantic labeling the come of the coalition with three

1461
01:36:10,570 --> 01:36:14,950
and it had a history of hidden units for the college london and surface of

1462
01:36:16,760 --> 01:36:22,070
we use both the world the tag like you and sentence tag likelihood

1463
01:36:22,090 --> 01:36:24,180
and we felt as features

1464
01:36:24,590 --> 01:36:28,780
was actually we took lower case was to limit the number of words in the

1465
01:36:29,820 --> 01:36:34,720
and to keep the information of the case we basically i'd like a capital letter

1466
01:36:36,280 --> 01:36:39,070
so we just played

1467
01:36:39,090 --> 01:36:40,400
that's what i get

1468
01:36:40,420 --> 01:36:44,420
on all the task and you can see the results are compared with the benchmark

1469
01:36:44,420 --> 01:36:47,040
is we run through each of these arguments say

1470
01:36:47,060 --> 01:36:49,960
i'm not convinced by and here's why

1471
01:36:49,990 --> 01:36:53,850
now since since i think that the argument about to get and i just started

1472
01:36:53,900 --> 01:36:56,690
getting the first of its fails

1473
01:36:56,690 --> 01:37:00,500
you know i hope you'll you'll think it over and you'll eventually come to agree

1474
01:37:00,500 --> 01:37:05,350
with me yet these arguments don't really work after all but what is more important

1475
01:37:05,350 --> 01:37:10,420
to me is that you please think about it of these arguments in this a

1476
01:37:10,420 --> 01:37:13,800
convincing argument for the existence of us soul

1477
01:37:13,820 --> 01:37:16,390
if if if if you if you think so

1478
01:37:16,390 --> 01:37:20,900
what response to you want to offer to the objections that i'm giving

1479
01:37:20,950 --> 01:37:24,930
if this argument doesn't work is there another argument for the existence of a soul

1480
01:37:24,950 --> 01:37:27,470
you thanks a better one

1481
01:37:27,520 --> 01:37:28,320
all right

1482
01:37:28,340 --> 01:37:29,820
so first arguement

1483
01:37:29,950 --> 01:37:33,660
you need the solar to explain the animation of the body

1484
01:37:33,710 --> 01:37:38,000
well from the physical point of view of course the answer is going to be

1485
01:37:38,030 --> 01:37:40,670
two quick

1486
01:37:40,670 --> 01:37:42,550
to have an animated body

1487
01:37:42,580 --> 01:37:46,150
you need to have a functioning body

1488
01:37:46,160 --> 01:37:47,940
and it's true

1489
01:37:49,220 --> 01:37:52,260
when you got caught she got all the parts there

1490
01:37:52,280 --> 01:37:54,320
but clearly they are not

1491
01:37:54,330 --> 01:37:57,230
functioning properly

1492
01:37:57,230 --> 01:38:02,190
but all that shows us is the parts have broken

1493
01:38:02,200 --> 01:38:04,280
i remember my stereo

1494
01:38:04,340 --> 01:38:06,560
i dropped my stereo

1495
01:38:06,630 --> 01:38:11,750
it falls on the stage it doesn't work anymore it stops giving off music by

1496
01:38:11,770 --> 01:38:15,330
boombox stops giving off music

1497
01:38:15,350 --> 01:38:17,970
that's not because previously

1498
01:38:17,970 --> 01:38:22,380
we had we had cd inside of it we have some batteries we drop the

1499
01:38:22,380 --> 01:38:27,730
whole thing it's not as though previously there was something non material

1500
01:38:29,580 --> 01:38:33,490
you've got all the same parts there but the parts are now

1501
01:38:34,680 --> 01:38:37,640
they're not connected to each other in the right way

1502
01:38:37,650 --> 01:38:46,470
the energy is not flowing from the batteries through the wires to the CD components

1503
01:38:46,490 --> 01:38:52,910
there is nothing mysterious from the physical is perspective about the idea that a physical

1504
01:38:52,910 --> 01:38:55,400
object can break

1505
01:38:55,500 --> 01:38:58,640
so although we need to offer a story

1506
01:39:00,600 --> 01:39:04,690
what makes the parts work when they are connected with each other and interact in

1507
01:39:04,700 --> 01:39:05,530
the right way

1508
01:39:05,540 --> 01:39:09,580
there's no need to appeal to anything beyond the physical

1509
01:39:09,630 --> 01:39:13,730
let's try to refine the argument suppose we say

1510
01:39:13,760 --> 01:39:16,690
you need to appeal to the soul in order to explain that is that the

1511
01:39:16,690 --> 01:39:19,120
body moves around flails

1512
01:39:20,140 --> 01:39:21,730
the body sort of

1513
01:39:21,740 --> 01:39:24,980
act purposefully

1514
01:39:25,020 --> 01:39:30,370
we need some things that have to be pulling the strings to be directing the

1515
01:39:32,010 --> 01:39:34,180
that's what the soul does

1516
01:39:34,230 --> 01:39:37,290
so says that we do a-list

1517
01:39:37,410 --> 01:39:40,220
and response to the physical it is going to say

1518
01:39:40,220 --> 01:39:45,830
yes it's true bodies don't just move around in random patterns human bodies don't do

1519
01:39:46,680 --> 01:39:51,330
so we need something to direct it but why could not just be

1520
01:39:52,300 --> 01:39:53,540
in particular

1521
01:39:53,550 --> 01:39:55,790
part of the body

1522
01:39:55,850 --> 01:40:01,140
plays the that part of the command module

1523
01:40:01,140 --> 01:40:05,450
suppose i got a heat seeking missile

1524
01:40:05,500 --> 01:40:07,750
you know which

1525
01:40:07,770 --> 01:40:09,980
tracks down the

1526
01:40:09,980 --> 01:40:12,280
the plane

1527
01:40:12,340 --> 01:40:14,490
as the plane tries to dodge it

1528
01:40:14,500 --> 01:40:15,630
the missile

1529
01:40:15,630 --> 01:40:17,840
correct its course

1530
01:40:17,900 --> 01:40:21,940
it's not just moving randomly moving purposefully

1531
01:40:21,960 --> 01:40:28,300
the better be something that explains its controlling the motions of the missile

1532
01:40:28,320 --> 01:40:33,010
for all that it could just be a particular piece of the missile but does

1533
01:40:35,960 --> 01:40:41,410
more gloriously we could imagine building some kind of a robot

1534
01:40:42,460 --> 01:40:45,430
does a variety of tasks

1535
01:40:45,470 --> 01:40:50,980
and it's not moving randomly but the test for all that are controlled by

1536
01:40:50,990 --> 01:40:52,940
the CPU

1537
01:40:52,980 --> 01:40:55,250
within the robot

1538
01:40:55,270 --> 01:41:00,160
so the the physical it says we don't need to appeal to anything as extravagant

1539
01:41:00,160 --> 01:41:04,050
as the sole in order to explain the fact that the bodies don't just move

1540
01:41:04,050 --> 01:41:07,620
randomly but they move in purposeful ways in our control

1541
01:41:10,030 --> 01:41:15,720
for each objection is the response you can imagine the jules coming back and saying

1542
01:41:17,210 --> 01:41:19,660
in that case the heat-seeking missile

1543
01:41:19,980 --> 01:41:24,830
the or the robot for that matter although it's doing things it's just obeying orders

1544
01:41:24,850 --> 01:41:30,850
and the orders were given to it from something outside itself

1545
01:41:30,910 --> 01:41:34,170
something program the robot or the muscle

1546
01:41:34,180 --> 01:41:40,450
so don't we need there to be something outside the body that programs the body

1547
01:41:40,460 --> 01:41:44,640
and that could be the all

1548
01:41:44,690 --> 01:41:50,870
that's a harder question must be something outside the body that controls the body well

1549
01:41:50,910 --> 01:41:52,970
one possibility of courses

1550
01:41:54,120 --> 01:41:58,380
why not say that people are just robots as well and we get our commands

1551
01:41:58,380 --> 01:41:59,890
from outside and

1552
01:41:59,920 --> 01:42:02,130
familiar religious views

1553
01:42:02,150 --> 01:42:05,540
god fields at order

1554
01:42:05,600 --> 01:42:09,500
dust adam is just a certain kind of robot then

1555
01:42:09,550 --> 01:42:13,770
god sort of breeze in the atom that are turning it on gets so we

1556
01:42:14,080 --> 01:42:19,700
maybe people are just commands robots commander from outside by god but that doesn't mean

1557
01:42:19,700 --> 01:42:25,130
that there's anything more of us than there is to the robot

1558
01:42:25,250 --> 01:42:27,460
that's one possible

1559
01:42:28,670 --> 01:42:35,690
the response of course is why couldn't we have robots that just building more robots

1560
01:42:35,690 --> 01:42:40,880
and then if you ask where did the commands come from the answer is well

1561
01:42:40,880 --> 01:42:43,300
when they were built

1562
01:42:43,320 --> 01:42:46,410
it were built in such a way as to sort of have certain instructions and

1563
01:42:46,410 --> 01:42:51,120
they begin to fall followed just like people have the genetic code perhaps

1564
01:42:51,250 --> 01:42:55,630
gives us various instructions that we begin to fall under certain innate psychology or what

1565
01:42:55,630 --> 01:42:57,700
have you

1566
01:42:57,700 --> 01:43:02,130
well that's the i quickly becomes very very messy

1567
01:43:02,150 --> 01:43:05,310
and the of the fan of the soul

1568
01:43:05,330 --> 01:43:07,380
begins to one approaches but look

1569
01:43:07,390 --> 01:43:10,220
we're not just robots

1570
01:43:10,380 --> 01:43:14,890
we're not just robot with some sort of program in our brain that we're following

1571
01:43:14,890 --> 01:43:17,480
we've got free will

1572
01:43:17,490 --> 01:43:21,510
and and robots cannot have free will and so there's got to be something more

1573
01:43:21,620 --> 01:43:24,750
less than robots can be physical things

1574
01:43:24,910 --> 01:43:30,700
this is an interesting argument but i think the new argument

1575
01:43:30,710 --> 01:43:35,140
so so we started with the the idea you needed to appeal to solve in

1576
01:43:35,140 --> 01:43:37,310
order to roughly explain why

1577
01:43:37,330 --> 01:43:42,510
human bodies move why were animated why we move non random ways

1578
01:43:42,520 --> 01:43:43,530
and i think it's

1579
01:43:43,550 --> 01:43:45,550
it's fairly clear

1580
01:43:46,280 --> 01:43:50,190
you don't need to appeal the souls in order to do that

1581
01:43:50,220 --> 01:43:53,070
appeal to the physical body suffices

1582
01:43:53,150 --> 01:43:55,980
i think to have an explanation as to

1583
01:43:55,990 --> 01:43:59,970
the difference between an animated and inanimate body

1584
01:44:00,000 --> 01:44:05,310
how bodies will move in non random ways of the brain is our CPU

1585
01:44:07,020 --> 01:44:12,020
will behave in a deliberate purposeful ways just like the robot will behave in deliberate

1586
01:44:12,020 --> 01:44:17,140
purposeful ways so this initial argument things not compelling

1587
01:44:17,200 --> 01:44:20,730
still we might wonder what about this new argument

1588
01:44:20,990 --> 01:44:23,750
what about the fact that so we said look there is that there is a

1589
01:44:23,750 --> 01:44:26,100
family of

1590
01:44:26,140 --> 01:44:30,370
arguments all of which have the general structure inference to the best explanation you need

1591
01:44:30,370 --> 01:44:35,370
sold in order to explain feature as plug different feature f and you get a

1592
01:44:36,020 --> 01:44:38,620
argument the one we started with

1593
01:44:38,620 --> 01:44:42,180
you need the body to explain the at the animation in either sold to explain

1594
01:44:42,180 --> 01:44:47,390
so this will be i would be tried to go very slow in the first

1595
01:44:47,390 --> 01:44:50,930
five minutes so that we can weigh the way for people come in because you

1596
01:44:50,930 --> 01:44:54,960
know people are still lying up for stop coffee

1597
01:44:56,980 --> 01:45:01,950
so this is the tutorial joint work we use my

1598
01:45:02,070 --> 01:45:07,750
phd student meanwhile and she's graduating so if you're looking for someone working on uncertain

1599
01:45:07,750 --> 01:45:09,770
data trees can occur

1600
01:45:10,040 --> 01:45:15,950
and professor UV in chinese university of hong kong and germany in the university of

1601
01:45:15,950 --> 01:45:18,670
new south wales in australia

1602
01:45:18,690 --> 01:45:23,100
and the statue of the talk will be as follows

1603
01:45:24,310 --> 01:45:26,030
you know

1604
01:45:26,050 --> 01:45:31,850
four KDE we are really concerned about the application how to motivate the problem so

1605
01:45:31,850 --> 01:45:39,560
we spend the first half hour to give you a series of examples showing how

1606
01:45:39,580 --> 01:45:45,340
uncertainty may come into data in what applications you see uncertainty and how why we

1607
01:45:45,340 --> 01:45:48,520
need to model uncertainty carefully

1608
01:45:48,570 --> 01:45:53,660
and then mean we'll talk about it from all those of uncertain probabilistic data

1609
01:45:53,790 --> 01:45:58,800
then she would how you was the simplest case and how complexity can come in

1610
01:45:58,950 --> 01:46:04,120
when you have all kinds of constraints and was the state of the state of

1611
01:46:04,120 --> 01:46:08,240
the art and also tell you what cannot be done so far

1612
01:46:08,340 --> 01:46:11,700
and then we have a coffee break

1613
01:46:13,600 --> 01:46:15,570
after that we in that switch

1614
01:46:15,600 --> 01:46:19,540
the order from the US slice you have

1615
01:46:19,630 --> 01:46:23,820
we first talk about olaf and uncertain

1616
01:46:23,850 --> 01:46:26,040
on uncertain public data

1617
01:46:26,060 --> 01:46:31,070
and we would talk about mining uncertain copyright status

1618
01:46:31,070 --> 01:46:32,760
after that you

1619
01:46:32,760 --> 01:46:37,260
so i say OK since we need to do data mining on uncertain

1620
01:46:37,710 --> 01:46:40,370
published data so

1621
01:46:40,480 --> 01:46:44,930
there are already some reasons on query answering

1622
01:46:45,040 --> 01:46:48,210
those reasons could be useful

1623
01:46:48,230 --> 01:46:52,240
for data mining for example there are some index methods

1624
01:46:52,300 --> 01:46:58,920
and there are some of the fiction query answering methods from properties data base committee

1625
01:46:58,930 --> 01:47:05,770
so we talk about a particular type of indexing and ranking and krisper preference korean

1626
01:47:05,770 --> 01:47:06,960
spatial queries

1627
01:47:07,000 --> 01:47:11,780
then you summarize the clock and then we can have some discussion

1628
01:47:16,020 --> 01:47:17,020
in fact

1629
01:47:17,150 --> 01:47:21,670
the first came we want to make why this tutorial is important why you in

1630
01:47:21,670 --> 01:47:22,780
the right role

1631
01:47:22,840 --> 01:47:26,560
is that uncertainty is inherent

1632
01:47:26,640 --> 01:47:30,300
in many many applications almost everywhere

1633
01:47:30,300 --> 01:47:31,990
basically this is due to

1634
01:47:32,000 --> 01:47:36,090
two kinds of reasons the first reason is that when we collect data we need

1635
01:47:36,090 --> 01:47:37,310
to use

1636
01:47:37,360 --> 01:47:39,050
all kinds of equipment

1637
01:47:39,060 --> 01:47:42,740
and we use equipment equipment has its limitations

1638
01:47:44,190 --> 01:47:46,250
that's the way we

1639
01:47:46,270 --> 01:47:47,490
observe the

1640
01:47:47,770 --> 01:47:50,620
the real world and the way we collect data

1641
01:47:51,370 --> 01:47:55,520
may introduce some uncertainty in the data we collect

1642
01:47:55,530 --> 01:48:00,810
and another way is that well in some application part degree those of social science

1643
01:48:01,960 --> 01:48:04,550
uncertainty is so in higher even

1644
01:48:04,650 --> 01:48:09,750
no matter how you improve your data collection you cannot completely

1645
01:48:09,770 --> 01:48:10,750
remove it

1646
01:48:10,800 --> 01:48:15,530
for example one question is that if there's is some way on say whether how

1647
01:48:15,530 --> 01:48:17,990
much you like or you dislike

1648
01:48:18,000 --> 01:48:20,560
mccain and obama

1649
01:48:20,610 --> 01:48:23,900
well you one object to give you a school

1650
01:48:23,960 --> 01:48:25,300
you know

1651
01:48:25,370 --> 01:48:28,240
the school is sort of like estimation

1652
01:48:29,110 --> 01:48:35,680
so this is where the important aspects

1653
01:48:35,720 --> 01:48:40,990
so let's look at a little bit more detail on data collection using sensors

1654
01:48:41,000 --> 01:48:46,340
we know that says the network is already a hot topic in these days

1655
01:48:46,360 --> 01:48:50,930
and we use all kinds different sensors to collect data for example we use the

1656
01:48:50,930 --> 01:48:59,510
most sensors electronic magnetic sensors mechanical sensors on the thickness and applications that involve like

1657
01:48:59,510 --> 01:49:02,910
in environment surveillance security

1658
01:49:02,920 --> 01:49:07,010
manufacturers extends all the all these different aspects

1659
01:49:07,030 --> 01:49:09,700
but was an ideal sense

1660
01:49:09,740 --> 01:49:13,500
and i do census you have such a property is that well

1661
01:49:13,510 --> 01:49:14,230
it is

1662
01:49:16,290 --> 01:49:20,070
so that the output signal of censor is

1663
01:49:20,120 --> 01:49:25,430
linearly proportional to the to the value you want to monitor

1664
01:49:25,430 --> 01:49:26,560
and also

1665
01:49:26,560 --> 01:49:30,910
if we have such an ideal sense then we only need to meet the need

1666
01:49:30,910 --> 01:49:37,180
to know old one thing is called sensitivity which is the ratio between the output

1667
01:49:37,180 --> 01:49:41,240
signal and measured property

1668
01:49:41,300 --> 01:49:45,300
then you you know there's a o point then you can calculate once you have

1669
01:49:45,300 --> 01:49:52,810
the output value then you can derive measure to measure and and monitor

1670
01:49:52,830 --> 01:49:56,760
OK so this is the general that's the ideal case

1671
01:49:58,880 --> 01:50:02,110
sensors have all kinds of different areas

1672
01:50:02,300 --> 01:50:07,030
in that not every error may be to uncertain

1673
01:50:07,060 --> 01:50:09,120
let's first look at some

1674
01:50:09,130 --> 01:50:10,740
not too bad errors which

1675
01:50:10,740 --> 01:50:13,250
do not introduce uncertainty

1676
01:50:13,260 --> 01:50:14,990
the first kind of

1677
01:50:15,100 --> 01:50:18,240
measure is that it is cos st

1678
01:50:18,300 --> 01:50:20,320
sensitivity error

1679
01:50:20,370 --> 01:50:25,390
that is when the sensor is a sign he has a specified

1680
01:50:26,660 --> 01:50:29,930
however when it says he's produced

1681
01:50:29,940 --> 01:50:31,980
and also deployed after a while

1682
01:50:31,990 --> 01:50:34,440
the sensitivity may change

1683
01:50:34,500 --> 01:50:37,670
so that there is a difference between the specify

1684
01:50:37,680 --> 01:50:42,930
since this value and the real sensitivity value so there is difference

1685
01:50:43,030 --> 01:50:48,140
but why this error is not the important because you can measure that

1686
01:50:48,160 --> 01:50:52,040
you can use these are the test that you can really calculate the difference band

1687
01:50:52,040 --> 01:50:53,380
you can correct

1688
01:50:53,390 --> 01:50:59,990
and another important is also is the bias in this case that the zero point

1689
01:50:59,990 --> 01:51:01,070
was the

1690
01:51:02,760 --> 01:51:04,380
and then this may

1691
01:51:04,410 --> 01:51:06,000
change over time

1692
01:51:06,010 --> 01:51:10,220
due to the a k of the all the

1693
01:51:10,260 --> 01:51:15,740
secrecy all those things but again this what these error can be corrected by some

1694
01:51:17,980 --> 01:51:19,730
and also

1695
01:51:19,780 --> 01:51:22,490
while we want the censorship in linear

1696
01:51:22,500 --> 01:51:24,970
it may be nonlinear

1697
01:51:25,690 --> 01:51:27,100
well no matter how

1698
01:51:27,120 --> 01:51:30,660
i mean it is as long as you can have enough test data you can

1699
01:51:30,660 --> 01:51:36,800
always you know corrected off the rikers so this is not the problem

1700
01:51:37,510 --> 01:51:39,630
there some errors which

1701
01:51:39,980 --> 01:51:43,740
that in terms of introducing a lot of uncertainty

1702
01:51:44,950 --> 01:51:46,110
we call these

1703
01:51:46,120 --> 01:51:49,530
uncertain errors of dynamic errors

1704
01:51:49,550 --> 01:51:50,790
this history

1705
01:51:50,800 --> 01:51:53,060
i wonder if there is that

1706
01:51:53,170 --> 01:51:55,420
it's estimated that deviate

1707
01:51:55,450 --> 01:51:56,860
you know by

1708
01:51:56,870 --> 01:52:01,430
rapid change of the measure for example if you want to measure the temperature if

1709
01:52:01,430 --> 01:52:05,230
the temperature change very fast your sense may not be able to catch up with

1710
01:52:05,230 --> 01:52:06,380
the change

1711
01:52:06,430 --> 01:52:08,320
and you never

1712
01:52:08,330 --> 01:52:16,060
can be exactly accurate exactly calculate was the you know the difference

1713
01:52:16,060 --> 01:52:19,310
heuristic which is that in that i showed you and what is it saying what

1714
01:52:19,310 --> 01:52:20,710
the number it's giving

1715
01:52:20,730 --> 01:52:25,480
measure the actual distance of this step from the walls

1716
01:52:25,500 --> 01:52:28,590
and since you have an actual plan in front of you you can say that

1717
01:52:28,590 --> 01:52:32,810
if you of the four state and the plan is fifteen states land fifteen minutes

1718
01:52:32,810 --> 01:52:34,130
before your post

1719
01:52:34,170 --> 01:52:38,460
OK and some other that that you're trying to predict

1720
01:52:38,530 --> 01:52:42,380
and so given state you want to predict what do and what they want to

1721
01:52:42,380 --> 01:52:47,540
bring in the training example for inductive learning is features of the relaxed plan sort

1722
01:52:47,540 --> 01:52:50,750
other than the data which they tried to but the thing that seems to work

1723
01:52:52,090 --> 01:52:53,560
last plan itself

1724
01:52:53,560 --> 01:52:58,880
and describe the relaxed plan in terms of some feature of the features that uses

1725
01:52:58,940 --> 01:53:01,190
the taxonomic representation

1726
01:53:01,210 --> 01:53:07,020
and they then of the class but each of these flat lands the class label

1727
01:53:07,020 --> 01:53:12,210
the that distance between what the like plan idealistic because they and what the true

1728
01:53:12,210 --> 01:53:13,770
heuristic is

1729
01:53:13,790 --> 01:53:20,150
OK so you actually computing that and they essentially do a straightforward in active learning

1730
01:53:20,150 --> 01:53:23,500
and that and they do quite well in terms of

1731
01:53:23,560 --> 01:53:26,110
guessing the correct difference

1732
01:53:26,190 --> 01:53:30,110
OK and things that we the heuristic level one of the nice things

1733
01:53:31,040 --> 01:53:34,400
that was the planner were going the wrong direction

1734
01:53:34,420 --> 01:53:38,170
OK but you know what give you an unsound black

1735
01:53:38,330 --> 01:53:44,080
OK and actually they present results showing that they do quite well at least compared

1736
01:53:44,110 --> 01:53:48,880
to f which is one of the better ones which doesn't

1737
01:53:49,150 --> 01:53:56,330
OK so one thing that the study of complete this part of the lecture

1738
01:53:56,360 --> 01:53:57,860
in terms of search for

1739
01:53:57,880 --> 01:53:59,520
that's control knowledge and learning

1740
01:53:59,540 --> 01:54:02,020
i want you to learn now

1741
01:54:02,040 --> 01:54:07,670
are things like invariances things saying you know if this you cannot have these facts

1742
01:54:07,670 --> 01:54:09,110
for any given

1743
01:54:09,780 --> 01:54:15,020
are you can learn simple search control rules of you that talked about you can

1744
01:54:15,020 --> 01:54:19,750
talk about you can learn adjustment rules in order to learn dual rules if you're

1745
01:54:19,750 --> 01:54:25,250
going to use on plants and so on but still open is the kinds of

1746
01:54:25,250 --> 01:54:30,020
complex state sequence rules that land uses you can do with a simple which is

1747
01:54:30,040 --> 01:54:35,440
that i work in general are better now i think the evidence can learn about

1748
01:54:35,460 --> 01:54:36,730
local knowledge

1749
01:54:36,750 --> 01:54:42,690
and that kind of the came that shop that's on the knowledge base planner uses

1750
01:54:43,000 --> 01:54:43,940
that do

1751
01:54:44,000 --> 01:54:47,880
also is not actually to learn

1752
01:54:47,900 --> 01:54:52,080
because of those that's about the state of the art right now in

1753
01:54:52,130 --> 01:54:55,960
learning such control problems

1754
01:54:58,090 --> 01:54:59,420
if you remember

1755
01:54:59,440 --> 01:55:04,500
i was talking about decided to improve the the existing planning to speed up planning

1756
01:55:04,500 --> 01:55:07,560
performance another way of speeding up

1757
01:55:07,610 --> 01:55:13,130
the performance is to forget about the planet and that the known domain specific plan

1758
01:55:13,150 --> 01:55:14,790
from examples

1759
01:55:14,790 --> 01:55:20,730
OK so i see enough successful plans and i learned how to do planning domain

1760
01:55:20,770 --> 01:55:25,310
OK there is in fact a large amount of literature this in fact this comes

1761
01:55:25,310 --> 01:55:32,610
close to some of the reinforcement learning work that has probably been discussed by setting

1762
01:55:32,610 --> 01:55:36,730
the saying might be discovered by the money and i'll talk about a couple of

1763
01:55:36,750 --> 01:55:43,360
points of contact with the fact this picture of me

1764
01:55:43,380 --> 01:55:48,460
so learning from scratch basically you don't have a plan that you're trying to improve

1765
01:55:48,480 --> 01:55:51,540
any more for all you care about is how do you do planning you know

1766
01:55:51,560 --> 01:55:52,580
the past

1767
01:55:52,630 --> 01:55:58,360
and one idea is that well i need to to know me global policy

1768
01:55:58,360 --> 01:55:59,230
OK so

1769
01:55:59,250 --> 01:56:00,690
what i need is

1770
01:56:00,710 --> 01:56:03,480
for a given state and the goals

1771
01:56:03,500 --> 01:56:06,020
i should know what is the action to be them

1772
01:56:06,040 --> 01:56:10,920
if i were in a state s and my set of rules that you want

1773
01:56:10,920 --> 01:56:13,860
to g seven this is the action but

1774
01:56:13,880 --> 01:56:18,810
OK so if you don't have these goals here then that just simple is an

1775
01:56:18,900 --> 01:56:20,060
MDP policy

1776
01:56:20,130 --> 01:56:24,770
now you're doing is that MTP policy assumes that that's for the single MDP here

1777
01:56:24,770 --> 01:56:27,610
you're talking about changing also learning

1778
01:56:27,630 --> 01:56:31,080
o estado de goals and then we're back into

1779
01:56:31,110 --> 01:56:35,460
OK and a variety of approaches have been done and if if you can learn

1780
01:56:35,630 --> 01:56:41,230
either this and the quality of the approximation is in essence you learning domain specific

1781
01:56:41,230 --> 01:56:42,830
that's wonderful

1782
01:56:42,830 --> 01:56:47,670
o the idea of approaches have been done for example in edinburgh in the last

1783
01:56:47,670 --> 01:56:48,770
sold do

1784
01:56:48,770 --> 01:56:53,900
explanation based learning and induction to learn domain specific planners to talk about a little

1785
01:56:53,900 --> 01:57:01,250
bit card rounded explanation is that the inductive learning but we expect that successful

1786
01:57:01,270 --> 01:57:04,630
i mean that article is about successful

1787
01:57:04,630 --> 01:57:08,890
and drawing directions on the edges not because we're looking at directed graphical models but

1788
01:57:08,890 --> 01:57:14,120
this because we're thinking about how the message passing updates work

1789
01:57:18,140 --> 01:57:20,430
thank you

1790
01:57:20,440 --> 01:57:30,840
right so got a bunch of other messages on

1791
01:57:39,150 --> 01:57:40,360
sorry about this

1792
01:57:40,380 --> 01:57:42,710
if hopefully we'll get it right over time right

1793
01:57:44,030 --> 01:57:44,940
so got

1794
01:57:44,950 --> 01:57:47,680
other nodes that are passing messages

1795
01:57:49,380 --> 01:57:51,370
there's no t

1796
01:57:51,380 --> 01:57:55,570
right so the arrows as i was saying indicate the direction in which the messages

1797
01:57:55,570 --> 01:57:58,250
are traveling

1798
01:57:58,260 --> 01:58:02,270
so we have for instance you want to t that the function of xt that

1799
01:58:02,270 --> 01:58:03,760
flows along that edge

1800
01:58:05,450 --> 01:58:09,700
then t is taking these messages it's taking the product of them it's doing local

1801
01:58:09,700 --> 01:58:12,190
computation it's either max or some

1802
01:58:12,300 --> 01:58:15,420
and it's passing that result

1803
01:58:15,430 --> 01:58:18,400
along this edge to its neighbour

1804
01:58:18,900 --> 01:58:20,770
so what we'd like to understand this

1805
01:58:20,780 --> 01:58:25,820
how is this message updated and the precise way in which it's updated is what

1806
01:58:25,820 --> 01:58:27,560
i've written there

1807
01:58:27,630 --> 01:58:30,380
right so you taken max

1808
01:58:30,430 --> 01:58:32,510
that's the max product

1809
01:58:32,520 --> 01:58:34,720
you maximize over XT

1810
01:58:35,450 --> 01:58:39,670
if you're doing some product you some over XT that's the only difference between the

1811
01:58:39,670 --> 01:58:41,570
two forms of the algorithm

1812
01:58:41,580 --> 01:58:43,010
and then

1813
01:58:43,610 --> 01:58:47,880
i'll use my compatibility notation it's the

1814
01:58:47,930 --> 01:58:52,620
whether we use the site functions or the exponentials of them it's the same thing

1815
01:58:53,440 --> 01:58:57,320
right so this

1816
01:58:57,330 --> 01:58:59,800
site he sits there and

1817
01:58:59,840 --> 01:59:02,120
so i t is the

1818
01:59:02,200 --> 01:59:04,250
function that sits on the edge

1819
01:59:04,260 --> 01:59:07,970
and then we have a product the red terms this product over all your neighbours

1820
01:59:07,970 --> 01:59:12,760
except the one that you're passing two

1821
01:59:12,870 --> 01:59:17,030
so everything and if t means the set of neighbors of node t

1822
01:59:17,060 --> 01:59:21,010
so this means the product over all you all vertices that are neighbors of t

1823
01:59:21,020 --> 01:59:22,370
all these guys

1824
01:59:22,400 --> 01:59:24,880
except for the guy that you're passing two

1825
01:59:24,890 --> 01:59:28,890
and what you do is you should collect the messages from the neighbours and you

1826
01:59:28,890 --> 01:59:35,570
should multiply them up

1827
01:59:38,970 --> 01:59:43,280
rearrange hopefully

1828
01:59:43,310 --> 01:59:44,960
i can see everything at once

1829
01:59:45,260 --> 01:59:50,760
to focus

1830
01:59:52,510 --> 01:59:56,140
closer for a different way

1831
02:00:03,720 --> 02:00:05,220
it's a little bit better

1832
02:00:05,230 --> 02:00:07,300
is it

1833
02:00:12,400 --> 02:00:16,740
right so now you have is you have an operation you have an algorithm

1834
02:00:16,940 --> 02:00:20,520
which at some level the nodes don't need to know anything about the rest of

1835
02:00:20,520 --> 02:00:24,310
the graph apart from their neighbors as the only thing they need to know

1836
02:00:24,840 --> 02:00:26,140
right so no t

1837
02:00:26,210 --> 02:00:31,420
at every time in every round for every one of its outgoing everyone's neighbors it's

1838
02:00:31,420 --> 02:00:38,070
can collect messages other neighbours multiply them perform local operation max or some it's going

1839
02:00:38,070 --> 02:00:41,690
to pass or relay the message along the outgoing edge

1840
02:00:41,800 --> 02:00:45,970
i the same time it would also collect things from u one u two and

1841
02:00:45,970 --> 02:00:49,240
as an would relay that information to u three

1842
02:00:49,250 --> 02:00:53,950
so t would actually compute in this case for messages every round it was in

1843
02:00:53,950 --> 02:00:58,920
one scheduled compute for messages and send them out along its outgoing edges

1844
02:00:58,930 --> 02:01:03,290
at the same time as would be computing things and sending it to t

1845
02:01:03,300 --> 02:01:07,890
as might also be sending things to other parts of the graph

1846
02:01:07,900 --> 02:01:18,440
OK so that that is the max product algorithm what we've derived and

1847
02:01:18,460 --> 02:01:19,590
what happens

1848
02:01:19,620 --> 02:01:25,730
is are used different she but so those are the message updates

1849
02:01:25,790 --> 02:01:29,770
at the end of the day when the algorithms converge to talk about convergence in

1850
02:01:29,770 --> 02:01:31,500
a minute

1851
02:01:35,290 --> 02:01:37,960
you can use this algorithm to compute

1852
02:01:38,020 --> 02:01:43,470
either what are called max marginals

1853
02:01:43,530 --> 02:01:47,590
or some marginals

1854
02:01:47,610 --> 02:01:50,460
so some marginals the thing you use to

1855
02:01:50,520 --> 02:01:51,930
it's just the

1856
02:01:51,930 --> 02:01:57,160
we will talk during the following forty five minutes of ontological engineering

1857
02:01:57,180 --> 02:02:02,030
and ontological engineering refers to the set of activities that we

1858
02:02:02,040 --> 02:02:05,230
we do when you we build ontologies

1859
02:02:05,270 --> 02:02:09,950
it is also related with the ontology lifecycle that we

1860
02:02:09,960 --> 02:02:14,220
that we follow the myth of the methodology that we use for building the ontologies

1861
02:02:14,220 --> 02:02:15,520
and of the tools

1862
02:02:15,540 --> 02:02:20,650
and languages that support the the building the building process

1863
02:02:20,660 --> 02:02:27,570
so i probably know there are several methodologies that allows you to build ontologies from

1864
02:02:28,510 --> 02:02:32,530
some of these methodologies

1865
02:02:32,540 --> 02:02:34,010
there was

1866
02:02:34,020 --> 02:02:37,150
method they have force methodology

1867
02:02:37,160 --> 02:02:40,600
i think this approach this ontology sense of

1868
02:02:40,870 --> 02:02:48,550
knowledge and the agent but you know this method were built at the end of

1869
02:02:49,110 --> 02:02:55,400
nine days during the last year and new needs appear to automated with the construction

1870
02:02:57,990 --> 02:03:00,750
and with the network ontologies

1871
02:03:00,800 --> 02:03:03,500
by distributed teams in the sense that

1872
02:03:03,570 --> 02:03:13,070
we can build ontologies collaboratively by people who were located in different locations

1873
02:03:13,090 --> 02:03:16,320
and this ontology should be

1874
02:03:16,340 --> 02:03:17,680
should be

1875
02:03:17,690 --> 02:03:24,090
by means of use of existing resources not building this ontologies so

1876
02:03:24,100 --> 02:03:27,740
in this talk i will focus on the process of building

1877
02:03:27,790 --> 02:03:30,330
ontology ontology networks

1878
02:03:30,420 --> 02:03:31,670
by reusing

1879
02:03:31,680 --> 02:03:33,960
this is the resources

1880
02:03:35,530 --> 02:03:40,540
the question we have is that we want to build an ontology sound like to

1881
02:03:40,550 --> 02:03:45,400
know which one of the activities involved in the ontology development process

1882
02:03:46,000 --> 02:03:48,770
which one is the goal of each activity

1883
02:03:49,830 --> 02:03:54,820
so they carry out each its activity what is the relationship of the different activities

1884
02:03:54,820 --> 02:03:55,950
with the others

1885
02:03:55,960 --> 02:04:00,200
where can i find ontologies with the goal of reusing them

1886
02:04:00,220 --> 02:04:01,600
and how

1887
02:04:01,700 --> 02:04:03,840
can i be the ontology for

1888
02:04:03,860 --> 02:04:06,880
my application the most important thing is if i need

1889
02:04:06,920 --> 02:04:11,470
a single ontology or if i need an ontology network

1890
02:04:11,490 --> 02:04:13,450
so first we

1891
02:04:13,460 --> 02:04:16,570
so the first question

1892
02:04:16,590 --> 02:04:22,110
that we will analyse see what is the neon glossary of

1893
02:04:24,160 --> 02:04:26,260
see what is in a table

1894
02:04:26,310 --> 02:04:34,000
of recommended and if applicable activities what is the development process when we build ontologies

1895
02:04:34,010 --> 02:04:34,930
in the network

1896
02:04:34,950 --> 02:04:41,210
so the idea already know there are a lot of activities involved in the ontology

1897
02:04:41,230 --> 02:04:44,440
development process people talk about

1898
02:04:44,460 --> 02:04:54,550
ontology specification ontology conceptualisation ontology mappings ontology alignment ontology evolution merging and

1899
02:04:54,600 --> 02:04:58,830
if you analyse it turned out to you will find a lot of activities and

1900
02:04:58,860 --> 02:05:04,580
sometimes the activities are not defined in the same way in different papers so the

1901
02:05:05,180 --> 02:05:12,170
continental city of activities to indentify and also which won rdf TV these two we

1902
02:05:12,170 --> 02:05:14,630
carry out when building ontologies

1903
02:05:15,360 --> 02:05:20,150
global collaborative and we have identified

1904
02:05:20,200 --> 02:05:23,910
fifty five activities they are publishing the

1905
02:05:23,920 --> 02:05:25,310
now website

1906
02:05:25,330 --> 02:05:29,100
there you can find definitions of

1907
02:05:30,680 --> 02:05:33,520
i mean after all these activities on this slide

1908
02:05:35,260 --> 02:05:39,900
for instance if we focus on knowledge acquisition for

1909
02:05:39,950 --> 02:05:43,900
ontologies you will get to the finish line

1910
02:05:43,910 --> 02:05:46,380
knowledge acquisition for

1911
02:05:46,390 --> 02:05:49,830
ontology comprises activity for capturing knowledge

1912
02:05:49,850 --> 02:05:56,700
from a variety of sources so we distinguish between ontology elicitation ontology learning ontology population

1913
02:05:56,700 --> 02:05:59,610
and for each of these activities you will

1914
02:05:59,630 --> 02:06:07,220
ontology elicitation ontology learning ontology population you will find a natural language definition the most

1915
02:06:07,220 --> 02:06:13,170
important thing is that these activities definition has been already in

1916
02:06:13,190 --> 02:06:15,720
there has been consensus weighted by

1917
02:06:15,800 --> 02:06:22,750
the thirteen parties participating in the near close oceans for forty forty five fifty

1918
02:06:22,800 --> 02:06:25,260
people has already participated

1919
02:06:25,310 --> 02:06:27,470
in this in this process

1920
02:06:27,480 --> 02:06:32,520
so the second thing i mean once you know that there is a lot of

1921
02:06:32,530 --> 02:06:39,640
david is involved in the ontology development process the next question is that of the

1922
02:06:39,640 --> 02:06:44,240
national that we have in this district is to provide you with a set of

1923
02:06:44,240 --> 02:06:49,220
recommended i enough if applicable table of activities i mean some sort of these activities

1924
02:06:49,220 --> 02:06:50,440
are mandatory

1925
02:06:50,450 --> 02:06:55,170
and other activities are optional when you build your ontologies and you can see here

1926
02:06:55,190 --> 02:07:01,100
that we end up with an ontology ontology concert twenty station to carry out

1927
02:07:01,110 --> 02:07:02,760
ontology evaluation

1928
02:07:02,820 --> 02:07:08,030
you should integrate the ontology with other ontologies are you should perform kind of knowledge

1929
02:07:10,050 --> 02:07:15,480
probably you will also do some kind of ontology served as well as kind of

1930
02:07:15,480 --> 02:07:18,640
ontology a specification

1931
02:07:18,660 --> 02:07:20,700
so maybe these activities could be

1932
02:07:20,720 --> 02:07:27,010
it could be i mean are required for the ontology building process but other activities

1933
02:07:27,010 --> 02:07:31,520
are up to it are not mother told in the sense that it depends on

1934
02:07:31,520 --> 02:07:34,100
the case it depends on your ontology

1935
02:07:34,110 --> 02:07:39,170
so in that case ontology learning which is the process of building automatically the ontology

1936
02:07:39,170 --> 02:07:41,120
is not always needed for

1937
02:07:41,140 --> 02:07:47,190
building an ontology or ontology localisation which means that to transform an ontology which is

1938
02:07:47,190 --> 02:07:52,230
written in the language for instance in these two core and or or two french

1939
02:07:52,230 --> 02:07:58,640
or german so ontology localisation could be seen like a kind of option at

