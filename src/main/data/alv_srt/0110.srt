1
00:00:00,000 --> 00:00:03,640
the camera translation zoom and out of plane rotation

2
00:00:03,660 --> 00:00:06,320
and the values of other right so

3
00:00:06,340 --> 00:00:07,340
the see

4
00:00:07,350 --> 00:00:11,620
once you notice that as you go from within the red black and green block

5
00:00:11,620 --> 00:00:15,220
as you go from darker shades lighter shades you go from right to left that's

6
00:00:15,220 --> 00:00:19,040
as you go from the deep seated as go from the shallow networks to keep

7
00:00:19,120 --> 00:00:23,610
network and so the analysis of these results is that as the governor shall endeavour

8
00:00:23,610 --> 00:00:29,000
to deepen were these algorithms actually do exhibit more invariance to a huge range of

9
00:00:29,000 --> 00:00:33,820
invariances of honesty i have no idea why this is true is an empirical result

10
00:00:33,820 --> 00:00:37,720
i think is actually fascin there questions still to figure out why why you get

11
00:00:37,720 --> 00:00:42,940
this effect but you should on this morning mentioned one of the motivations for wanting

12
00:00:42,940 --> 00:00:47,910
to do deep learning as that of the it's the theoretical motivation for doing deep

13
00:00:47,910 --> 00:00:52,320
learning in terms of computing these circuits needed service the and i think of this

14
00:00:52,320 --> 00:00:57,450
as maybe a empirical answers to the question of why i want to do these

15
00:00:57,450 --> 00:01:03,760
in these results empirically invariant representations you and increases with the depth of the presentation

16
00:01:03,770 --> 00:01:06,000
and i don't know why this is true

17
00:01:10,590 --> 00:01:17,150
of it unless you get zero the zero of because this normalized with the scale

18
00:01:17,150 --> 00:01:19,890
the features is divided by the variance there

19
00:01:19,910 --> 00:01:25,870
i don't know i do not believe there is up with what about this i

20
00:01:25,870 --> 00:01:31,390
haven't family tree but this stuff going on here so various hypotheses of i think

21
00:01:31,390 --> 00:01:34,120
everyone all the ones i would do in common with the mean is that if

22
00:01:34,120 --> 00:01:35,910
you want to work

23
00:01:35,920 --> 00:01:44,330
the see so some conclusion one generally deeper features appear to be more variance

24
00:01:44,350 --> 00:01:49,520
additionally sparsity also seems help make the features more there and with the exception of

25
00:01:49,520 --> 00:01:53,470
one for two and three their models the red lines are generally lower than the

26
00:01:53,480 --> 00:01:58,270
lines and so somehow sparse they to make the features one variant of game i

27
00:01:58,270 --> 00:02:02,400
don't really know why other kind has has post giving you think lyrics and this

28
00:02:08,020 --> 00:02:13,520
yes that was one of my other hypotheses i do not believe that's all that's

29
00:02:13,520 --> 00:02:18,580
going on on that may get some of these but if visualizations units actually although

30
00:02:18,580 --> 00:02:22,200
in a very different things so you can't so i don't believe this is going

31
00:02:22,200 --> 00:02:24,450
so you are the

32
00:02:29,610 --> 00:02:30,510
they are

33
00:02:30,550 --> 00:02:39,610
right so this not the thing not in this specific thought so all of these

34
00:02:39,610 --> 00:02:43,490
were forty by forty fourteen image patch so i have no idea how the album

35
00:02:43,490 --> 00:02:46,330
was the invariants from which is

36
00:02:46,350 --> 00:02:51,320
they just mention one more comparison which is the blue line here this template matching

37
00:02:51,430 --> 00:02:56,920
game details the complicated some of the the tenth nationalistic an image patch learning that

38
00:02:56,920 --> 00:02:57,640
p of p

39
00:02:57,650 --> 00:03:01,430
and so with deep architectures here these we do

40
00:03:01,440 --> 00:03:06,140
the more their features that have matching and this is another common i think when

41
00:03:06,140 --> 00:03:07,680
we visualize in it

42
00:03:07,880 --> 00:03:11,240
i work on the post as we saw a lot of our we tend to

43
00:03:11,240 --> 00:03:15,500
visualize second third features by squashing down into flat image

44
00:03:15,520 --> 00:03:17,270
and i think that's useful thing to do

45
00:03:17,390 --> 00:03:22,550
but i think there's also captures that squash representation is actually not telling the entire

46
00:03:22,550 --> 00:03:27,810
story in fact of these features are actually more invariant and yes simple template matching

47
00:03:27,810 --> 00:03:32,880
the what you get with these things and squashed into factor

48
00:03:34,480 --> 00:03:35,990
on internally

49
00:03:36,010 --> 00:03:40,460
my sons using these sources in various tests to guide their own the development of

50
00:03:40,500 --> 00:03:41,980
the learning algorithms

51
00:03:42,000 --> 00:03:46,420
there are actually many other possible in the test that that you can use this

52
00:03:46,430 --> 00:03:50,400
on this is one that i guess number of others that this made closer to

53
00:03:50,400 --> 00:03:56,380
prior art in which you can show you a sinusoidal grating have an enormous is

54
00:03:56,380 --> 00:04:00,940
my album in the face of creating the all the rotational gratings of various people

55
00:04:00,940 --> 00:04:04,690
have done these especially common context modelling complex cells

56
00:04:05,560 --> 00:04:09,250
this one in various has an extremely difficult i don't think any deep learning how

57
00:04:09,470 --> 00:04:13,360
does long that was we tested do not but which is casually out of a

58
00:04:13,360 --> 00:04:18,160
lot of images of objects and find the feature this the means optical i don't

59
00:04:18,160 --> 00:04:22,050
think any album does one is but these are the sorts of invariants as we

60
00:04:22,050 --> 00:04:27,380
called to give it another evaluation metric

61
00:04:27,390 --> 00:04:32,330
so the second evaluation method is the application of these ideas to classification as the

62
00:04:32,350 --> 00:04:33,770
sort of self learning

63
00:04:35,420 --> 00:04:39,460
and you we do pretty much exactly what you expect we take the unlabelled natural

64
00:04:39,460 --> 00:04:44,480
the use of learning a sparse coding of one or two or three layer deep

65
00:04:44,480 --> 00:04:47,100
belief that almost sparse coding representation

66
00:04:47,220 --> 00:04:53,450
use the representation and and off the shelf classifier and we find that for many

67
00:04:55,230 --> 00:05:01,470
using these learned feature representations of learning from unlabeled images we get reasonable performance of

68
00:05:01,470 --> 00:05:05,750
this was the table the whole at present the main conference like caltech one one

69
00:05:05,780 --> 00:05:10,220
we get your sixty five percent performance which has come to the fore in many

70
00:05:10,220 --> 00:05:15,130
cases better than the sort of had the features you get from computer vision

71
00:05:15,400 --> 00:05:21,170
of and so the cost to the same thing from all the old this is

72
00:05:21,190 --> 00:05:25,710
these also holds results and the cool thing about this is that hot a feature

73
00:05:25,710 --> 00:05:28,360
resentation from label audio

74
00:05:28,390 --> 00:05:33,950
and use exactly the same feature vector exactly the same you know what the representations

75
00:05:34,010 --> 00:05:36,790
for the six different talks and in

76
00:05:36,850 --> 00:05:44,000
almost all of them on youtube terrapins of the hand engineered features mfccs

77
00:05:44,020 --> 00:05:47,130
and so on and over the years because the tried is a large range of

78
00:05:47,130 --> 00:05:53,570
problems like text classification heritage everyone knows how to

79
00:05:53,590 --> 00:05:57,530
OK on

80
00:05:57,550 --> 00:05:58,330
real quick

81
00:05:58,350 --> 00:06:02,420
comparisons i know polish comparisons are not

82
00:06:03,680 --> 00:06:08,090
something i don't know but for inspiration that percy find it useful

83
00:06:08,110 --> 00:06:13,220
not not as the ultimate evaluation metric for inspiration often find it useful to look

84
00:06:13,220 --> 00:06:17,230
at possible brain what i was doing this even our own sun making this also

85
00:06:17,240 --> 00:06:20,740
computations we think possibly may be doing

86
00:06:20,740 --> 00:06:21,700
and so

87
00:06:22,450 --> 00:06:24,240
very quickly

88
00:06:24,250 --> 00:06:30,180
one thing we did was learned to their response model and compare that to visual

89
00:06:30,180 --> 00:06:33,500
cortical area v two which is in a ball in the second stage of visual

90
00:06:33,500 --> 00:06:34,980
processing pipeline

91
00:06:36,150 --> 00:06:40,600
i don't know so so once again this is all the source representations you over

92
00:06:40,630 --> 00:06:44,700
to their model where you the edges the first their combined age is the second

93
00:06:44,700 --> 00:06:49,920
would tell us or an explanation for why they should all cancel and the true

94
00:06:49,920 --> 00:06:55,180
value of the cosmological constant should be zero but perhaps there is one

95
00:06:55,440 --> 00:07:01,980
perhaps son some unknown dynamics or principle tells us that the global vacuum energy is equal

96
00:07:01,980 --> 00:07:07,700
to zero but we're still getting there we haven't reached this final state

97
00:07:07,700 --> 00:07:11,880
there is some scalar field that you can describe and this vacuum energy is a

98
00:07:11,880 --> 00:07:17,830
cosmological constant and the scalar field will eventually far in the future reach phi equal

99
00:07:17,830 --> 00:07:25,020
to zero so these are the approaches to modify the right hand side of the Einstein

100
00:07:25,020 --> 00:07:32,140
equations like adding something to T zero zero there are also approaches to modify the left

101
00:07:32,140 --> 00:07:38,740
hand side of Einstein's equation by changing somehow the framework of the gravitational model in

102
00:07:38,740 --> 00:07:47,720
which the expansion rate is calculated there was an observation in nineteen ninety-nine by Binetruy

103
00:07:47,720 --> 00:07:54,420
et al  that in brain world models the Einstein excuse me the Friedmann equation does

104
00:07:54,420 --> 00:08:00,100
not come from the zero zero component of the Einstein equation rather the Friedmann

105
00:08:00,100 --> 00:08:07,730
equation if you have branes and certain extra dimension frameworks comes from matching discontinuities

106
00:08:07,740 --> 00:08:13,620
in the derivative of the metric as you cross a brane and it's possible you know

107
00:08:13,620 --> 00:08:17,680
the idea that the Friedmann equation does not come from something simple like the

108
00:08:17,680 --> 00:08:25,700
zero zero component of the Einstein equation perhaps tells us that in some brane theory

109
00:08:25,700 --> 00:08:33,460
we might get  of a Friedmann equation that's not H squared is equal to rho

110
00:08:33,780 --> 00:08:39,800
then people looked in extra dimensional theories and there was some ideas that the gravitational

111
00:08:39,800 --> 00:08:46,440
force law becomes is modified at large distances there is the idea that the universe actually

112
00:08:46,440 --> 00:08:53,420
becomes five-dimensional at cosmic distances then there's this idea I don't think many of

113
00:08:53,420 --> 00:08:58,080
you are old enough to remember people used to explain the redshift in terms of

114
00:08:58,080 --> 00:09:04,680
something known as tired light that light as it propagates across the universe gets tired and

115
00:09:04,680 --> 00:09:12,260
its wavelength gets  redshifted well there's an approach that I describe as tired gravitons that

116
00:09:12,260 --> 00:09:18,120
when gravitons are  produced they're confined to our brane but if you go

117
00:09:18,120 --> 00:09:23,520
over cosmological distances their wave function  could spread a bit and they could leak

118
00:09:23,520 --> 00:09:30,160
into the bulk there are also extra dimensional theories where gravity becomes repulsive at large

119
00:09:30,160 --> 00:09:37,060
distances there are gr there are  theories of extra dimensions well all theories of extra dimensions have

120
00:09:37,070 --> 00:09:42,860
an infinite tower of excited states in the effective four-dimensional field theory this Kaluza-Klein

121
00:09:42,870 --> 00:09:50,300
tower and there are some possibilities where the first excited mode of the graviton is very

122
00:09:50,300 --> 00:09:57,740
light as the distance scale mass of smaller than an inverse gigaparsec so if we look

123
00:09:57,740 --> 00:10:04,080
at gravity on distances of solar systems less than a gigaparsec it's essentially a

124
00:10:04,080 --> 00:10:09,240
biometric theory but if you go to  very large distances the cosmological distance where

125
00:10:09,240 --> 00:10:16,780
the supernovae are coming to us it is a a  single metric then there is

126
00:10:16,780 --> 00:10:22,920
this idea of some colleagues of mine Sean Carrol and Mike Turner at Chicago who propose that they are

127
00:10:22,920 --> 00:10:28,300
smarter than Einstein and Hilbert and they realized that  Einstein and Hilbert got it wrong in

128
00:10:28,300 --> 00:10:33,600
nineteen fifteen the gravitational action is not the square root of G times R but it's the square

129
00:10:33,600 --> 00:10:39,240
root of G divided by R and then there's the idea that it's the back

130
00:10:39,240 --> 00:10:46,240
reaction of inhomogeneities and people here at CERN have have played a a big role in this one

131
00:10:46,240 --> 00:10:51,100
of the things that I think comes out of these attempts to modify gravity at

132
00:10:51,100 --> 00:10:57,160
large distances is the realization of how difficult it is to modify gravity at large

133
00:10:57,160 --> 00:11:05,540
distances while keeping consistency with observations on solar system scales precision test of gravity and also

134
00:11:05,540 --> 00:11:14,100
without leading to theoretical unpleasantness like ghosts or nonlinearities so you can't say that

135
00:11:14,100 --> 00:11:18,620
it can't be done and some of these people think that their particular model does it

136
00:11:18,620 --> 00:11:24,300
but it's not so easy to modify the left hand side of the Einstein equations

137
00:11:24,340 --> 00:11:28,460
so we don't know whether it's the right hand side of the left hand side

138
00:11:28,480 --> 00:11:33,480
so you might make the statement that nothing more can be done by the theorists

139
00:11:33,480 --> 00:11:38,520
in this matter it is only you the astronomers who can perform a simply invaluable

140
00:11:38,520 --> 00:11:45,500
service to theoretical physics by giving us more observational information about dark energy well this

141
00:11:45,500 --> 00:11:54,160
quote comes from a letter that Einstein wrote in nineteen thirteen to an astronomer at Berlin

142
00:11:54,210 --> 00:11:59,790
Freundlich encouraging him to mount an expedition to measure the deflection of light

143
00:11:59,790 --> 00:12:06,320
by the sun and of course astronomers are very naive when an astronomer hears

144
00:12:06,610 --> 00:12:12,100
something from a physicist they immediately think it's a good idea so in fact this

145
00:12:12,100 --> 00:12:16,580
poor guy decided to mount an expeditions to measure the deflection of light by

146
00:12:16,580 --> 00:12:23,360
the sun unfortunately he wanted to do it in nineteen fourteen and the eclipse was best seen

147
00:12:23,360 --> 00:12:28,860
in the Crimea and while he was there the first World War broke out

148
00:12:28,860 --> 00:12:32,960
he was imprisoned he never did the experiment was released a year later or so but

149
00:12:32,960 --> 00:12:39,900
it was very unpleasant for him so maybe astronomers should be a little cautious in following the

150
00:12:39,900 --> 00:12:46,360
advice of physicists also it's probably lucky for Einstein that this happened because in nineteen fourteen

151
00:12:46,840 --> 00:12:51,690
he had the wrong prediction for the deflection of light by the sun that wasn't completed

152
00:12:51,690 --> 00:12:57,380
until nineteen fifteen so he he would have gotten the wrong answer but Freundlich

153
00:12:57,380 --> 00:13:03,740
would've gotten maybe the correct answer but it would not have agreed with Einstein's prediction however

154
00:13:03,740 --> 00:13:09,240
one can make the case that we have all these theoretical ideas perhaps none

155
00:13:09,240 --> 00:13:13,280
of them are good perhaps none of them are right and what we need is some

156
00:13:13,280 --> 00:13:21,180
sort of experimental guidance some observational guidance to tell us  whether it's the left hand side

157
00:13:21,180 --> 00:13:27,760
or the right hand side or something different so in order to try to wrestle with this

158
00:13:27,760 --> 00:13:33,380
issue in the U.S. last year there was something known as the dark energy task

159
00:13:33,380 --> 00:13:37,980
force that I happen to be the chairman of and there was sort of a cool thing about an

160
00:13:38,290 --> 00:13:42,600
airplane when somebody would say why you're going to Washington and you say well I'm

161
00:13:42,600 --> 00:13:50,500
on the dark energy task force and then they always ask do you actually know

162
00:13:50,500 --> 00:13:59,740
Dick Cheney so what we tried to do is to suggest some sort ofs experimental

163
00:13:59,740 --> 00:14:05,940
strategy and the experimental strategy I think is a good idea the first thing

164
00:14:05,940 --> 00:14:11,520
we wanna  do is to determine as well as possible whether the accelerating expansion

165
00:14:11,520 --> 00:14:14,410
because really solve the problem

166
00:14:14,410 --> 00:14:18,640
so long ago a i people said this cannot be a good approach

167
00:14:18,700 --> 00:14:22,970
what's happening on the web is the realisation that in this approach

168
00:14:22,990 --> 00:14:26,860
you know there's something inherent about the web

169
00:14:27,860 --> 00:14:32,140
the web you don't get to judge whether all the other pages on the web

170
00:14:32,140 --> 00:14:34,780
as good or bad other i mean by some

171
00:14:34,810 --> 00:14:38,800
abstract notion of that you just when you create your web page

172
00:14:38,820 --> 00:14:42,960
link to the things that you know about and that you want to talk about

173
00:14:44,380 --> 00:14:46,700
and those people do the same thing

174
00:14:46,740 --> 00:14:49,120
and eventually you get now

175
00:14:49,220 --> 00:14:53,390
according to the latest numbers i've looked and this now a little more than a

176
00:14:53,390 --> 00:14:58,260
dozen web pages for each and every person on earth that doesn't mean describing each

177
00:14:58,270 --> 00:15:03,340
that and that's just the open web that doesn't include stuff behind firewalls and so

178
00:15:03,340 --> 00:15:07,700
there are now an order of magnitude more web pages and there are people

179
00:15:07,730 --> 00:15:11,900
and what pages and is increasing considerably faster than people

180
00:15:11,920 --> 00:15:15,880
OK so essentially the the amount of information out there

181
00:15:15,900 --> 00:15:21,420
just trying very very fast in a certain sense an uncontrolled growth thing rather and

182
00:15:21,420 --> 00:15:25,640
then how do we tell what's good information what's been information we use are human

183
00:15:25,640 --> 00:15:27,210
reasoning powers

184
00:15:27,220 --> 00:15:30,440
not the machine to do that so you google for something

185
00:15:30,450 --> 00:15:33,580
google gives you some suggestions using an algorithm

186
00:15:33,610 --> 00:15:36,680
if you look at the pages again gave you when you say i like this

187
00:15:37,960 --> 00:15:40,270
this one doesn't sound right to me

188
00:15:40,290 --> 00:15:42,140
who the heck is this guy

189
00:15:42,150 --> 00:15:45,400
all this is one of my friends i really trust that

190
00:15:45,410 --> 00:15:49,160
things are going to use a lot of your human flavor on top of what

191
00:15:49,160 --> 00:15:50,770
an algorithm gives you

192
00:15:50,820 --> 00:15:54,200
every single day on the web so this process

193
00:15:54,210 --> 00:15:58,110
could we take that same idea suppose we have a whole lot of different

194
00:15:58,120 --> 00:16:02,230
dataspaces i'm actually going to talk about this later in the afternoon

195
00:16:02,260 --> 00:16:05,330
and we link these things together

196
00:16:05,380 --> 00:16:06,480
right so

197
00:16:06,510 --> 00:16:10,380
just as an example the one top of musicbrainz has a lot of information about

198
00:16:10,380 --> 00:16:12,230
musical systems

199
00:16:12,250 --> 00:16:18,120
the one over here called DDTD is a representation of the link space wikipedia with

200
00:16:18,130 --> 00:16:24,090
things on wikipedia talk about which other things wikipedia geonames tells you about places so

201
00:16:24,090 --> 00:16:29,460
you can intuitively see that somehow you could put together those three data

202
00:16:29,490 --> 00:16:31,710
without boundaries without them

203
00:16:31,710 --> 00:16:36,720
using different representation to find some way to work between the different data

204
00:16:36,780 --> 00:16:39,950
approaches then you could you answer a question like

205
00:16:39,960 --> 00:16:44,370
o i'm looking at a musician who grew up near i where i did

206
00:16:44,390 --> 00:16:47,460
who else what other musicians grew up near where i

207
00:16:47,500 --> 00:16:52,060
and you would do something like use musicbrainz to figure out the musicians are

208
00:16:52,080 --> 00:16:55,330
then you go to the pedia to figure out which of who they are and

209
00:16:55,330 --> 00:16:56,500
where they grew up

210
00:16:56,510 --> 00:17:00,110
and then you go over to geonames to see which of those places are somewhere

211
00:17:00,110 --> 00:17:01,400
close to

212
00:17:01,440 --> 00:17:05,590
what someone say you know what famous musician grew up near where i did we

213
00:17:05,590 --> 00:17:08,560
now have the data set to do it but of course

214
00:17:08,590 --> 00:17:12,000
there's lots and lots of data about how do we find it back to the

215
00:17:12,000 --> 00:17:15,580
problem so now we put the human back in with the data

216
00:17:15,630 --> 00:17:19,910
so in this view the idea is that we break down the barriers between lots

217
00:17:19,910 --> 00:17:22,510
of different stuff the the way we broke it down

218
00:17:22,540 --> 00:17:23,770
in the web

219
00:17:23,780 --> 00:17:25,960
and then our hope is that

220
00:17:25,970 --> 00:17:27,460
we can somehow

221
00:17:27,480 --> 00:17:30,200
use that same stuff from the web

222
00:17:30,210 --> 00:17:32,030
to put it back together

223
00:17:32,060 --> 00:17:34,480
so in this view

224
00:17:34,530 --> 00:17:36,950
there's a lot of excitement

225
00:17:36,980 --> 00:17:40,860
because now we're talking about something that looks like the web

226
00:17:40,890 --> 00:17:43,960
and now we're talking about something that scales like the way

227
00:17:44,650 --> 00:17:48,710
the notion is there's lots and lots of data out there

228
00:17:48,720 --> 00:17:52,460
and there's very little semantics out there so if you just add a little bit

229
00:17:52,460 --> 00:17:56,550
of semantics to sort of find the right stuff at the right time

230
00:17:56,580 --> 00:17:58,290
you would have a big win

231
00:17:58,310 --> 00:18:02,950
and again i say find i don't necessarily mean search it's answer queries it it's

232
00:18:02,950 --> 00:18:05,390
find the right kind of papers for this

233
00:18:05,400 --> 00:18:07,670
for the problem you are trying to sell things like that so

234
00:18:07,690 --> 00:18:10,920
the idea is you want to declare very simple little

235
00:18:10,940 --> 00:18:17,160
relations that can be inferred apply these at scale heuristically perhaps to a very very

236
00:18:18,430 --> 00:18:20,380
data sets so

237
00:18:20,400 --> 00:18:25,380
again i have lots of information i can grab from different places about different people

238
00:18:26,070 --> 00:18:29,690
i see somebody named john dominic here and i see somebody with a similar name

239
00:18:29,690 --> 00:18:32,610
here i see somebody with a similar name it

240
00:18:32,610 --> 00:18:35,890
there's lots of people in the world with that name

241
00:18:35,900 --> 00:18:40,150
right well if i can start saying hey but you know these four are actually

242
00:18:40,150 --> 00:18:41,590
the same one

243
00:18:41,610 --> 00:18:43,630
that can be very powerful

244
00:18:43,670 --> 00:18:49,040
so this guy blogging this livejournal i have reason to believe is the same person

245
00:18:49,040 --> 00:18:52,330
who is being described in this wikipedia page

246
00:18:52,360 --> 00:18:55,870
who is the same person who came up in this google hit who is the

247
00:18:55,870 --> 00:18:57,020
same person

248
00:18:59,090 --> 00:19:03,360
you know is that this university now suddenly i can answer some very powerful queries

249
00:19:03,360 --> 00:19:07,950
that kind of answer from those individual dataset so i needed to do

250
00:19:07,960 --> 00:19:14,780
some fairly simple inferencing maybe the inferences if two people have the same email address

251
00:19:14,800 --> 00:19:19,480
consider that maybe they're the the same person or heuristically assume they're the same person

252
00:19:19,500 --> 00:19:20,860
will be correct

253
00:19:20,860 --> 00:19:23,050
not one hundred percent time that you're doing

254
00:19:23,080 --> 00:19:26,420
eighty ninety percent it's something the size of the web

255
00:19:26,430 --> 00:19:27,870
you're doing very well

256
00:19:27,890 --> 00:19:30,610
but the key thing here is that

257
00:19:30,620 --> 00:19:34,130
now completeness is no longer

258
00:19:36,030 --> 00:19:39,290
not only is it necessarily now what you're looking for

259
00:19:40,380 --> 00:19:44,700
not only is it not necessarily what you're looking for its necessarily not what you're

260
00:19:44,700 --> 00:19:46,520
looking for what i mean by that

261
00:19:46,540 --> 00:19:49,480
it is remember time is money so

262
00:19:49,480 --> 00:19:53,640
i saw a talk at an into internal google thing this was eight or nine

263
00:19:53,640 --> 00:19:56,810
years ago when they were first really getting it up so

264
00:19:56,830 --> 00:20:02,460
i can't really prove that this is today's math but they said at that time

265
00:20:02,470 --> 00:20:06,430
if they could get google results ten times better

266
00:20:06,440 --> 00:20:09,920
but it would take you seconds to answer a query

267
00:20:09,940 --> 00:20:12,310
google would go out of business

268
00:20:12,340 --> 00:20:15,950
right so if when you hit google you had to wait two two seconds but

269
00:20:15,950 --> 00:20:18,780
you've got a better results people wouldn't work

270
00:20:18,830 --> 00:20:22,670
furthermore if they could get it a hundred times is good

271
00:20:22,680 --> 00:20:27,470
in five seconds it wouldn't keep them in business they become an issue business they

272
00:20:27,470 --> 00:20:31,420
become the specialized search business

273
00:20:31,430 --> 00:20:35,630
so again on the web what you want to get some answers fast here's my

274
00:20:35,630 --> 00:20:37,490
favorite example of that

275
00:20:37,510 --> 00:20:42,430
this is from a website called wine made by a company called radarnetworks it was

276
00:20:42,430 --> 00:20:44,510
really maximises

277
00:20:45,580 --> 00:20:49,640
this is sort of the top this quantity expected return

278
00:20:49,690 --> 00:20:53,580
if from the next time step you you saying that the returns are going to

279
00:20:53,580 --> 00:20:55,320
be defined or given

280
00:20:56,130 --> 00:20:57,240
the function b

281
00:20:58,060 --> 00:21:01,820
so if you believe that those are the future of the difference

282
00:21:01,820 --> 00:21:04,630
this this is just the top unexpectedly turned

283
00:21:04,630 --> 00:21:08,950
so this is my expected return if i take action a

284
00:21:09,000 --> 00:21:14,290
and this is my top i expected return if i'm following policy pi

285
00:21:14,300 --> 00:21:18,390
and that should be closed and then i call policy pi greedy

286
00:21:19,710 --> 00:21:21,000
so that means

287
00:21:23,290 --> 00:21:25,490
the policy is

288
00:21:25,500 --> 00:21:32,140
is acting in a way that it tries to maximize this quantity right

289
00:21:32,150 --> 00:21:34,940
that's very simple

290
00:21:38,800 --> 00:21:47,010
and this figure one this fact

291
00:21:49,890 --> 00:21:50,670
and so

292
00:21:50,690 --> 00:21:54,330
we can conclude that he is a contraction

293
00:21:54,340 --> 00:21:59,670
so ceased is a contraction has a unique fixed point and the next pair and

294
00:21:59,670 --> 00:22:04,400
say that there is unique fixed point happens to be the optimal value function

295
00:22:04,420 --> 00:22:06,490
so this is no longer to right

296
00:22:06,540 --> 00:22:09,190
because from this

297
00:22:09,240 --> 00:22:12,480
already you can you can sort of feel that

298
00:22:12,580 --> 00:22:16,670
well if i plug in recent years

299
00:22:16,680 --> 00:22:23,390
then this is the way i should i should always try to maximize the put

300
00:22:23,390 --> 00:22:26,550
expected discounted return so if i shown that

301
00:22:26,560 --> 00:22:28,390
from time step one

302
00:22:28,400 --> 00:22:31,330
i will following optimal policy how should i

303
00:22:31,350 --> 00:22:33,560
a concept c

304
00:22:33,600 --> 00:22:36,080
if i should take a look at the

305
00:22:36,140 --> 00:22:38,240
all possible actions

306
00:22:38,300 --> 00:22:42,530
and other all possible actions the pope had expected

307
00:22:42,570 --> 00:22:47,820
this going return assuming that from timestamp one i'm following an optimal policy so the

308
00:22:47,830 --> 00:22:51,820
it turns out that is we start off at y

309
00:22:51,900 --> 00:22:54,200
and i should just maximize this quantity

310
00:22:55,160 --> 00:22:56,800
so it's it's

311
00:22:56,810 --> 00:23:00,430
it's not to be wondered at that you have a six-man expedition

312
00:23:06,660 --> 00:23:11,240
so this is just the mathematical constructions so that no women make that we know

313
00:23:11,240 --> 00:23:13,310
everything about the environment

314
00:23:13,320 --> 00:23:15,890
we don't care about learning at this stage

315
00:23:15,900 --> 00:23:17,430
we are not yet there

316
00:23:17,440 --> 00:23:21,940
so learning comes later i hope that well if you can finish to sing them

317
00:23:21,980 --> 00:23:22,780
i don't know

318
00:23:22,820 --> 00:23:24,240
learning to their today

319
00:23:24,270 --> 00:23:29,550
very very little learning but tumour

320
00:23:29,560 --> 00:23:31,320
you can learn the beach

321
00:23:32,440 --> 00:23:34,260
the snow two

322
00:23:38,900 --> 00:23:40,160
so to prove

323
00:23:40,450 --> 00:23:43,930
so i want to show this group because it's so nice

324
00:23:43,940 --> 00:23:46,300
and it's it's very simple as far

325
00:23:46,340 --> 00:23:47,820
so one thing that you

326
00:23:47,840 --> 00:23:48,930
you have to know

327
00:23:48,940 --> 00:23:51,690
like i'm using innovation again here

328
00:23:51,700 --> 00:23:55,810
it compares to operator something about that is more than equal to

329
00:23:55,820 --> 00:23:57,450
some other operator

330
00:23:57,460 --> 00:24:02,980
if applied this operators some back to apply the other operators some other vector

331
00:24:03,010 --> 00:24:07,690
but i always had that one is componentwise smaller than the other

332
00:24:07,730 --> 00:24:09,980
for the returns are smaller

333
00:24:09,990 --> 00:24:12,620
so that's the definition of this thing

334
00:24:12,670 --> 00:24:13,350
and there

335
00:24:13,530 --> 00:24:18,950
it's very easy to see that for these operators tf client even pi is just

336
00:24:18,950 --> 00:24:20,670
an arbitrary policy

337
00:24:20,680 --> 00:24:22,240
this inequality holds

338
00:24:22,260 --> 00:24:24,690
why because in you have max

339
00:24:24,690 --> 00:24:28,680
instead of selecting their actions in a specific way

340
00:24:28,710 --> 00:24:32,540
you're maximizing quantity so you should get something bigger

341
00:24:32,550 --> 00:24:35,650
so they just immediate from the definition of t

342
00:24:35,660 --> 00:24:37,220
if plug in

343
00:24:37,230 --> 00:24:39,370
maybe select set and

344
00:24:39,430 --> 00:24:42,770
the beach activations in the sand and

345
00:24:42,840 --> 00:24:47,810
if you just like in the definitions and you can verify using this inequality holds

346
00:24:47,850 --> 00:24:50,190
so this inequality holds

347
00:24:50,190 --> 00:24:52,590
then we can conclude that the start

348
00:24:52,610 --> 00:24:54,520
cannot be larger than b

349
00:24:57,990 --> 00:25:05,490
OK so

350
00:25:05,510 --> 00:25:07,440
so i mean

351
00:25:09,690 --> 00:25:12,040
four or

352
00:25:13,610 --> 00:25:15,320
keep i b

353
00:25:15,320 --> 00:25:20,020
it's smaller than TV

354
00:25:20,060 --> 00:25:21,260
and that

355
00:25:21,280 --> 00:25:23,610
this competition is componentwise

356
00:25:23,690 --> 00:25:25,480
so if you want

357
00:25:25,490 --> 00:25:27,410
this spell like

358
00:25:27,430 --> 00:25:29,740
even more

359
00:25:29,760 --> 00:25:32,940
and they should hold for all for all the

360
00:25:34,060 --> 00:25:37,600
but i see that he iceland he

361
00:25:37,840 --> 00:25:40,850
OK so

362
00:25:40,910 --> 00:25:44,390
these studies is not larger than the y

363
00:25:44,400 --> 00:25:45,690
because the

364
00:25:45,700 --> 00:25:50,920
we started so what do we know about the stuff so this is the maximum

365
00:25:50,930 --> 00:25:55,090
of the value functions at the states is just about to maximize resource that the

366
00:25:55,090 --> 00:25:56,480
policies right

367
00:25:56,540 --> 00:25:59,090
so the definition of is that we don't have

368
00:25:59,100 --> 00:26:02,310
any additional knowledge about it

369
00:26:03,260 --> 00:26:06,690
OK so what we know know as well is that

370
00:26:06,710 --> 00:26:09,240
if we keep applying t pi

371
00:26:09,260 --> 00:26:11,940
so let's speak any any policies

372
00:26:13,200 --> 00:26:15,550
and we keep applying t pi

373
00:26:15,590 --> 00:26:16,580
to some

374
00:26:20,200 --> 00:26:22,020
or the state space

375
00:26:22,060 --> 00:26:24,790
then this is called matching two

376
00:26:26,080 --> 00:26:28,950
is converging t

377
00:26:29,090 --> 00:26:32,680
we apply the value function of the policy

378
00:26:35,650 --> 00:26:36,960
we also know

379
00:26:36,970 --> 00:26:40,720
that t by is is not larger than c so if we

380
00:26:40,740 --> 00:26:43,080
apply instead of t pi

381
00:26:43,940 --> 00:26:45,320
k times

382
00:26:45,330 --> 00:26:48,140
then the again get something much

383
00:26:48,160 --> 00:26:49,290
so p

384
00:26:49,310 --> 00:26:52,410
applied k times to be zero

385
00:26:52,450 --> 00:26:54,170
returning to do something

386
00:26:55,610 --> 00:26:58,920
not smaller than if you apply here by i

387
00:26:58,930 --> 00:27:00,740
OK thanks to be zero

388
00:27:00,760 --> 00:27:06,450
and there is this guy converging t

389
00:27:06,510 --> 00:27:08,090
well it's committed to be

390
00:27:08,110 --> 00:27:09,400
the fixed point of t

391
00:27:09,410 --> 00:27:15,320
we don't know yet if we was stopped right

392
00:27:15,320 --> 00:27:22,510
the simplest maybe something called the escorts formulation where one simply decide that this thing

393
00:27:22,510 --> 00:27:30,180
has not only the regular space-time coordinates but also anti commuting coordinate sign your site

394
00:27:30,220 --> 00:27:32,870
fill them new left and right

395
00:27:32,890 --> 00:27:37,780
if you don't like these where think of them simply as a string that is

396
00:27:39,180 --> 00:27:45,030
it doesn't only have geometric motion space time it has some extra for munich

397
00:27:45,050 --> 00:27:50,410
degrees of freedom that avalanche around the training and will give it some extra properties

398
00:27:50,410 --> 00:27:53,370
and quantum numbers

399
00:27:54,490 --> 00:27:58,800
if you really do the calculations before they just go through essentially in the same

400
00:27:58,800 --> 00:28:04,180
way simply because they mention goes down to ten are that's better than twenty six

401
00:28:04,220 --> 00:28:08,990
but still not very good of course for particle physics and the rest of the

402
00:28:08,990 --> 00:28:15,680
analysis proceeds as before i want to discuss it anymore except for two crucial differences

403
00:28:15,680 --> 00:28:18,620
so that's the only place where things change

404
00:28:19,860 --> 00:28:22,720
this family owns that put in

405
00:28:22,740 --> 00:28:30,010
to accompany the space-time coordinates surface thing can be either periodic or and periodic after

406
00:28:30,010 --> 00:28:36,120
all the only observable are by linear unfair neurons in physics therefore if bailey linear

407
00:28:36,430 --> 00:28:38,720
periodic around this

408
00:28:38,740 --> 00:28:44,680
sorry around the closed training the families themselves can be either periodic or and periodic

409
00:28:44,720 --> 00:28:49,990
this thing these conditions are known as the what's and that's why this thing is

410
00:28:49,990 --> 00:28:53,510
called the pheromone schwarz string

411
00:28:54,570 --> 00:28:59,870
if you use periodic conditions then this means that sign you

412
00:28:59,910 --> 00:29:03,470
and i'm sorry so this for cross things

413
00:29:03,490 --> 00:29:09,660
if you translate the corresponding fact for open strings when he tells you that shine

414
00:29:09,660 --> 00:29:15,370
you cited them you will not have standing waves can be identified arbitrarily at one

415
00:29:15,370 --> 00:29:20,820
point you know you can only absorb signed into the definition of in one end

416
00:29:20,890 --> 00:29:25,370
but at the other end there is an arbitrary assignment game therefore this is the

417
00:29:25,370 --> 00:29:26,360
analogue of

418
00:29:26,360 --> 00:29:32,590
one of the swans boundary conditions was essentially an implication is that the frequencies of

419
00:29:32,590 --> 00:29:39,780
these modes are integers environment sector and half integer universe wants

420
00:29:39,800 --> 00:29:48,360
and i think in particular about sector sector who have is integer frequency for munich

421
00:29:48,360 --> 00:29:50,620
modes going around the string

422
00:29:50,700 --> 00:29:56,010
they have in particular is zero frequency counterpart and zero frequency means they don't give

423
00:29:56,010 --> 00:29:58,620
any more mass or energy to this string

424
00:29:58,640 --> 00:30:05,660
but this not commuting so the zero modes satisfy the canonical commutation relations

425
00:30:05,680 --> 00:30:10,390
and if you look at these this is exactly an algebra of the right gamma

426
00:30:11,570 --> 00:30:16,910
the article your data or of was say no zero easy you know that's the

427
00:30:16,910 --> 00:30:23,470
dirac algebra this has to be realized on pheromone states because these will become operators

428
00:30:23,470 --> 00:30:26,030
that don't change the must level of this thing

429
00:30:26,090 --> 00:30:33,450
and which satisfy dirac algebra therefore there one states have to be spaced fine spinners

430
00:30:33,490 --> 00:30:40,820
whereas i remember or look here we never introduced to start with spinoffs of space-time

431
00:30:40,840 --> 00:30:44,840
these were simply spinoffs on a two dimensional surface

432
00:30:44,840 --> 00:30:50,890
so that's how space-time far more the superstring from around one sectors

433
00:30:50,910 --> 00:30:57,010
there is a second crucial difference from the blue explaining it turns out to be

434
00:30:57,010 --> 00:30:59,640
consistent and necessary

435
00:30:59,640 --> 00:31:06,390
to impose definite family two of the world she to make this theory consistent and

436
00:31:06,390 --> 00:31:09,370
this is known as the archbishop called live

437
00:31:09,390 --> 00:31:13,620
projection minus two VF is minus one

438
00:31:13,640 --> 00:31:18,430
now i minus one i want to get three you can you have to get

439
00:31:18,430 --> 00:31:23,140
your hands a little bit dirty she one minus and plus but that's the correct

440
00:31:23,140 --> 00:31:28,470
i you have to impose now let's think about the spectrum of this object as

441
00:31:28,470 --> 00:31:32,390
before taking into account this to new ingredients

442
00:31:32,450 --> 00:31:34,950
so here is the open superstring

443
00:31:34,970 --> 00:31:38,590
that must level minus the have that's the acllon

444
00:31:38,620 --> 00:31:46,100
this is the ground state in the universe what sector were firmly serendipity object

445
00:31:46,120 --> 00:31:49,200
at the first excited level which happens to be

446
00:31:49,220 --> 00:31:50,470
zero mass

447
00:31:50,470 --> 00:31:57,990
that's what fixes the critical dimension you get as before vector like particle if photon

448
00:31:58,010 --> 00:32:05,660
excitation which is created by the half integer first half integer frequency of

449
00:32:05,660 --> 00:32:08,450
so here's the four times before

450
00:32:08,510 --> 00:32:12,240
how about that one sector well there in one sector of the family and some

451
00:32:12,240 --> 00:32:14,700
of the boys and instead both periodic

452
00:32:14,760 --> 00:32:23,870
they have integer multiple frequencies that zero energy fluctuations cancel out by two-dimensional supersymmetry therefore

453
00:32:23,870 --> 00:32:26,800
there are one viking was directly massless

454
00:32:26,860 --> 00:32:31,910
it doesn't have a document on the the other hand remember it realizes this dirac

455
00:32:31,910 --> 00:32:36,800
algebra so it is a spinoff and it is actually gaugino it turns out

456
00:32:36,840 --> 00:32:42,990
but it is a high dimensional gaugino lives in ten dimensions and in ten dimensions

457
00:32:43,070 --> 00:32:45,990
digital can be both hierarchy majorana

458
00:32:46,010 --> 00:32:52,700
and therefore that the spectrum of the lowest level of an open superstring it's basically

459
00:32:52,720 --> 00:32:59,680
this super fall in ten dimensions actual is the bacteria is erased by the judicial

460
00:32:59,680 --> 00:33:06,280
projection because it has even for municipality and one of the two chirality is erased

461
00:33:06,280 --> 00:33:11,410
by the GSO projection and the reason is that minus two VF t commutes with

462
00:33:11,410 --> 00:33:17,120
all the gamma matrices remember the zero modes of shy therefore it's exactly the chirality

463
00:33:17,120 --> 00:33:22,620
operator and kills one of the two chirality so therefore dimensions you get the photon

464
00:33:22,620 --> 00:33:27,620
you get divided by your and gaugino you can count physical degrees of freedom there

465
00:33:27,620 --> 00:33:33,800
are eight of each of these theories supersymmetric and we'll come back to later

466
00:33:33,950 --> 00:33:39,320
you can do the same exercise for closed strings it gets a bit more

467
00:33:39,370 --> 00:33:44,840
missing because you have independent left and right sectors but let me now

468
00:33:44,860 --> 00:33:49,370
got a bit more faster well at the master's level it comes from the what's

469
00:33:49,370 --> 00:33:56,140
sector on both sides you get as before it graviton dilaton and some antisymmetric tensor

470
00:33:58,220 --> 00:34:02,180
then there are these mixed sector is one of the left moving for me and

471
00:34:02,180 --> 00:34:06,490
some of you are going to get the article right more than once set periodic

472
00:34:06,510 --> 00:34:08,410
or vice versa

473
00:34:09,510 --> 00:34:15,410
spinoffs from one side vector from the other side and they can be

474
00:34:15,410 --> 00:34:23,320
actually exactly identified the supersymmetric partners of the gravity gravity need in ten dimensions

475
00:34:23,320 --> 00:34:28,370
there is only one little freedom the chirality they can be of the same autofocus

476
00:34:29,660 --> 00:34:35,030
and in these two cases are known as type two and type to be

477
00:34:35,050 --> 00:34:41,410
this is the finance sector namely were both left and right moving fermions are periodic

478
00:34:41,410 --> 00:34:46,260
there you have to realize two dirac algebra so it's been spin or in ten

479
00:34:46,260 --> 00:34:51,970
dimensions so it's not fair municipal only has to spin in this is and this

480
00:34:51,990 --> 00:34:53,840
if you analyse it

481
00:34:53,860 --> 00:34:58,990
by using gamma matrix algebra you find that it really corresponds to yet another bunch

482
00:34:58,990 --> 00:35:02,950
of the antisymmetric tensor fields which are referred to as or more and more about

483
00:35:02,950 --> 00:35:05,990
and we going to be rejected

484
00:35:07,720 --> 00:35:09,220
is it OK

485
00:35:12,510 --> 00:35:14,910
a nondeterministic automata

486
00:35:15,470 --> 00:35:20,300
there is no deterministic automaton

487
00:35:20,340 --> 00:35:21,670
that can be

488
00:35:23,400 --> 00:35:24,860
define two

489
00:35:24,910 --> 00:35:31,130
if you assume assuming deterministic is is not the the language is specified initially also

490
00:35:37,220 --> 00:35:42,740
when to accept the automaton except treat accepts tree if it has an accepting

491
00:35:47,400 --> 00:35:49,200
so the languages

492
00:35:49,490 --> 00:35:54,630
thirteen runs the language is a set of trees for which the system has

493
00:35:59,030 --> 00:36:03,200
so all the acceptance condition i skipped here

494
00:36:04,570 --> 00:36:06,550
all of them

495
00:36:06,900 --> 00:36:09,530
it's frustrating to parity condition

496
00:36:11,530 --> 00:36:12,860
so you should not

497
00:36:13,360 --> 00:36:20,320
during the columns this is also about expressiveness expressive than parity

498
00:36:22,900 --> 00:36:27,950
and so on so it's

499
00:36:34,240 --> 00:36:35,900
so that the things

500
00:36:38,680 --> 00:36:42,340
my hope was that by telling for example the

501
00:36:44,340 --> 00:36:46,910
not so much on infinite words

502
00:36:49,700 --> 00:36:52,400
finally connected components so OK

503
00:36:52,470 --> 00:36:54,360
if true is given

504
00:36:56,110 --> 00:36:57,860
by a finite

505
00:36:58,050 --> 00:37:04,670
that represents the tree when you're probably

506
00:37:06,150 --> 00:37:09,200
say if you're as the automaton is finite

507
00:37:09,290 --> 00:37:15,240
only have trace much it is much more complicated than jets computing

508
00:37:18,260 --> 00:37:22,260
but you have to compute some some

509
00:37:23,470 --> 00:37:26,110
because of the automaton is addressed

510
00:37:26,200 --> 00:37:29,740
and also so that you can

511
00:37:30,970 --> 00:37:36,410
derive from this of exhibits a successful run on some of people

512
00:37:37,680 --> 00:37:39,150
so what is see

513
00:37:40,470 --> 00:37:42,090
the support

514
00:37:42,110 --> 00:37:44,200
we use games

515
00:37:49,280 --> 00:37:50,950
you know the

516
00:37:52,280 --> 00:37:53,910
because of the

517
00:37:53,970 --> 00:37:56,990
one of the lectures to show there is a bit and if you win it

518
00:37:58,340 --> 00:38:00,030
then you will

519
00:38:00,070 --> 00:38:02,990
you have found the weight of some

520
00:38:04,220 --> 00:38:06,490
even colour

521
00:38:09,610 --> 00:38:10,970
callers that are

522
00:38:12,570 --> 00:38:17,410
yes it is complicated problem

523
00:38:17,430 --> 00:38:20,470
as objects of finite

524
00:38:21,530 --> 00:38:25,340
it's very very easy to see whether you have it

525
00:38:26,650 --> 00:38:27,670
now you can

526
00:38:27,680 --> 00:38:30,820
things would be great if you want to know

527
00:38:32,880 --> 00:38:34,970
cholera occurs infinitely often

528
00:38:35,630 --> 00:38:38,900
i think i have to say you look inside your cycle when you look at

529
00:38:38,900 --> 00:38:41,970
the core of the what was

530
00:38:44,240 --> 00:38:46,320
the presentation

531
00:38:46,340 --> 00:38:49,820
it's a question mark

532
00:38:51,200 --> 00:38:55,700
so so we answer immediately parity games

533
00:38:57,010 --> 00:38:59,400
which is and so the question

534
00:39:00,720 --> 00:39:07,510
again used two-person games two players

535
00:39:08,800 --> 00:39:10,200
so all these things

536
00:39:11,410 --> 00:39:13,680
what do they play with the we in

537
00:39:13,680 --> 00:39:14,930
what is happening

538
00:39:16,240 --> 00:39:19,630
so now in a very simple thing it's graph

539
00:39:19,670 --> 00:39:23,300
with this so let me show you the pictures right away because

540
00:39:24,380 --> 00:39:26,740
and run out of game

541
00:39:28,050 --> 00:39:30,130
so that two players

542
00:39:30,130 --> 00:39:33,170
circle is the same as the best

543
00:39:35,300 --> 00:39:36,490
in general

544
00:39:36,590 --> 00:39:40,550
you will use the tools will derive games

545
00:39:41,800 --> 00:39:45,990
from the time of the one because has a winning strategy then the automaton has

546
00:39:45,990 --> 00:39:46,740
a model

547
00:39:49,820 --> 00:39:53,130
this is the game raising when you're in squared

548
00:39:53,170 --> 00:39:56,740
player is zero player one

549
00:39:58,170 --> 00:40:00,530
the way to this scenario

550
00:40:01,930 --> 00:40:03,990
i don't need to tell you

551
00:40:04,030 --> 00:40:06,260
formerly with it is

552
00:40:07,430 --> 00:40:09,680
so what can we we have here we can play

553
00:40:09,820 --> 00:40:14,240
so i can say here

554
00:40:15,760 --> 00:40:19,180
so it is the want to play all he has no choice he has to

555
00:40:19,180 --> 00:40:20,360
be here

556
00:40:21,570 --> 00:40:25,400
so we have two choices so you can see here or here

557
00:40:26,590 --> 00:40:29,630
three again it is his turn to play

558
00:40:31,990 --> 00:40:40,360
he plays tracy place here and this one sticks to playing this with solar cells

559
00:40:42,700 --> 00:40:45,740
so in thing

560
00:40:47,030 --> 00:40:50,590
after some historians you mention here

561
00:40:51,990 --> 00:40:55,990
can just two or three here but you can choose to play what here

562
00:40:57,110 --> 00:40:58,950
and the other here

563
00:40:58,950 --> 00:41:02,150
it can be able to generate

564
00:41:03,570 --> 00:41:05,910
and the strategy is

565
00:41:06,450 --> 00:41:11,650
for one player is the choice it makes she makes

566
00:41:13,720 --> 00:41:15,200
some point

567
00:41:15,240 --> 00:41:17,510
so strategies like you

568
00:41:18,910 --> 00:41:22,510
around trail s two of the right play

569
00:41:22,720 --> 00:41:26,360
that's another one so the strategy the function

570
00:41:27,610 --> 00:41:30,180
which a

571
00:41:31,650 --> 00:41:35,050
two propositional it's your turn because what to play next

572
00:41:36,450 --> 00:41:41,090
and of course what to play next may depend on

573
00:41:44,050 --> 00:41:47,440
so now i have to tell you how to win the game because now it's

574
00:41:47,440 --> 00:41:50,860
just want how to play with doing is raising

575
00:41:53,470 --> 00:41:55,090
have collars

576
00:41:55,090 --> 00:42:00,700
when we play at your talk

577
00:42:00,740 --> 00:42:03,970
so here if i have this thing here

578
00:42:05,360 --> 00:42:06,630
i would have been

579
00:42:06,720 --> 00:42:07,970
as you will

580
00:42:09,380 --> 00:42:11,800
not be able to play any more

581
00:42:11,820 --> 00:42:16,110
so to play where we reach a position when the one who was supposed to

582
00:42:19,780 --> 00:42:21,510
this may happen in the west from it

583
00:42:21,550 --> 00:42:23,110
no outgoing edges

584
00:42:25,650 --> 00:42:28,180
well one loses the one who can play

585
00:42:28,220 --> 00:42:31,990
and the loop is the one at the end of the play

586
00:42:38,280 --> 00:42:41,360
and i look at the core of

587
00:42:41,360 --> 00:42:46,090
this is not only a course for english majors

588
00:42:46,100 --> 00:42:48,450
but for other majors two

589
00:42:48,500 --> 00:42:50,300
the poets will be reading

590
00:42:50,310 --> 00:42:57,030
well they they knew about science music politics economics and they presumed to talk about

591
00:42:57,030 --> 00:42:58,620
those things

592
00:42:58,900 --> 00:43:02,120
in their poetry and out of the poetry too

593
00:43:02,140 --> 00:43:06,320
my lectures are going to presume no special

594
00:43:07,370 --> 00:43:09,370
and you're part

595
00:43:09,390 --> 00:43:15,700
i i see this as a course that's an introduction to the literature of the

596
00:43:17,070 --> 00:43:19,340
to modern poetry

597
00:43:19,360 --> 00:43:30,740
the will be studying several poets in some detail the presumption as well that they

598
00:43:30,740 --> 00:43:32,920
they all

599
00:43:32,970 --> 00:43:37,530
reward and demand a certain amount of close reading

600
00:43:37,750 --> 00:43:42,340
at the same time i do mean to give you some

601
00:43:42,350 --> 00:43:43,380
sense of the

602
00:43:43,390 --> 00:43:48,150
period in which they are writing some sense of modernism

603
00:43:48,160 --> 00:43:54,970
as is the field as one of the richest fields in the english language

604
00:43:56,370 --> 00:44:03,130
finally though this really is a course and poetry plain and simple

605
00:44:03,150 --> 00:44:07,470
i mean to introduce you to particular parts

606
00:44:07,490 --> 00:44:10,110
to give you ways to

607
00:44:10,150 --> 00:44:12,980
possessed them enjoy them

608
00:44:13,000 --> 00:44:17,330
be puzzled or frustrated by them too

609
00:44:18,920 --> 00:44:20,450
learn something from them

610
00:44:20,460 --> 00:44:23,490
care about them into

611
00:44:23,570 --> 00:44:27,490
carry them with you as you as you go forward

612
00:44:27,540 --> 00:44:29,390
after this class

613
00:44:30,510 --> 00:44:32,370
that's the sense what

614
00:44:32,420 --> 00:44:35,840
i want to accomplish these lectures

615
00:44:35,860 --> 00:44:38,280
it will mean reading a lot of poems

616
00:44:38,300 --> 00:44:41,160
in writing about them some

617
00:44:41,180 --> 00:44:42,950
the syllabus

618
00:44:42,970 --> 00:44:48,140
you'll see notes the general topic of each lecture in the reading that i want

619
00:44:48,140 --> 00:44:51,730
you to have done for that day

620
00:44:51,740 --> 00:44:57,860
there's mid-term that will be a short answer test that's intended to give you a

621
00:44:57,860 --> 00:45:03,130
chance to show how diligently you've been reading an incoming class

622
00:45:03,140 --> 00:45:10,530
the the final will include both the short answer component and then some as a

623
00:45:12,330 --> 00:45:14,010
there are two papers

624
00:45:14,070 --> 00:45:18,840
a shorter and slightly longer one

625
00:45:18,850 --> 00:45:23,730
the first paper is going to ask you to write about one short poem the

626
00:45:23,730 --> 00:45:25,670
second will ask you to write about

627
00:45:25,720 --> 00:45:33,200
two or more poems or poems perhaps by two authors or perhaps upon and some

628
00:45:33,200 --> 00:45:36,890
other kind of text or image

629
00:45:36,910 --> 00:45:42,250
the teaching fellows from this course i'm lucky to work with them and you youtube

630
00:45:42,880 --> 00:45:47,310
they are trained and have have an interest in modern poetry

631
00:45:47,330 --> 00:45:53,400
and this is a happy collaboration for me with them

632
00:45:53,410 --> 00:45:57,080
as a so will start to get

633
00:45:57,130 --> 00:46:02,170
our discussion sections organised on monday and they should be set i hope by the

634
00:46:03,190 --> 00:46:05,520
lecture next week

635
00:46:05,640 --> 00:46:08,880
i want you to come to lecture on time she wanted to that started on

636
00:46:09,590 --> 00:46:14,580
i don't always do that but i'd like to and i can if you come

637
00:46:14,670 --> 00:46:16,060
at eleven thirty

638
00:46:16,110 --> 00:46:18,710
bring your books i'm going to be talking

639
00:46:18,750 --> 00:46:21,680
about the text and i hope you'll have open

640
00:46:21,700 --> 00:46:27,300
in the a course you will come to your discussion sections in the same state

641
00:46:30,170 --> 00:46:33,600
as i say the the syllabuses

642
00:46:33,800 --> 00:46:38,240
should be accessible on the class classes v two server

643
00:46:38,260 --> 00:46:41,970
however i've had problems with that in the past and should please let me know

644
00:46:41,970 --> 00:46:44,030
if it's not

645
00:46:44,050 --> 00:46:46,510
there are just two books for the course

646
00:46:46,530 --> 00:46:52,890
both labyrinth one is the first volume of the norton

647
00:46:52,930 --> 00:46:55,850
anthology of modern and contemporary poetry

648
00:46:55,880 --> 00:46:57,210
third edition

649
00:46:57,220 --> 00:47:00,420
edited by johann ramezani

650
00:47:00,430 --> 00:47:03,020
formerly a teaching fellow in this course

651
00:47:03,040 --> 00:47:04,540
there's also

652
00:47:04,820 --> 00:47:07,040
elizabeth bishop's collected poems

653
00:47:07,060 --> 00:47:16,730
there will be a packet that you can order from RAS that's gathers if you

654
00:47:16,750 --> 00:47:19,310
supplementary readings

655
00:47:20,120 --> 00:47:24,930
there will be the visual images that going talk about in lecture and then i

656
00:47:24,930 --> 00:47:28,210
will make accessible to you on the class server

657
00:47:28,260 --> 00:47:32,600
there are also audio recordings of poets

658
00:47:32,650 --> 00:47:35,940
that we will be reading that come from stirling

659
00:47:36,350 --> 00:47:41,460
and you can get to on the centre for language study website

660
00:47:41,470 --> 00:47:50,240
all those things we can talk about more as the semester develops

661
00:47:50,250 --> 00:47:52,820
and i hope you will talk to me

662
00:47:52,830 --> 00:47:57,630
you can do that on email you can do that in my office

663
00:47:57,900 --> 00:48:03,140
which is downstairs on the first floor of this building and LC one o nine

664
00:48:03,190 --> 00:48:07,250
you can catch me after lecture or before

665
00:48:07,260 --> 00:48:15,110
we can have large all sorts of opportunities for talking and i hope you'll take

666
00:48:15,110 --> 00:48:16,850
advantage of it

667
00:48:19,120 --> 00:48:20,250
four monday

668
00:48:20,260 --> 00:48:25,920
we're going to start talking about robert frost and and i'd like you to

669
00:48:25,970 --> 00:48:31,660
pay special attention to respond mowing in in the RAS packet

670
00:48:31,800 --> 00:48:34,430
into his palm birches in the norton

671
00:48:34,450 --> 00:48:36,350
as you read

672
00:48:37,030 --> 00:48:40,600
special attention to images of tools

673
00:48:43,690 --> 00:48:47,290
read frosts

674
00:48:47,370 --> 00:48:49,650
in short

675
00:48:49,660 --> 00:48:56,090
poetic statement prose poetic statements in the in the norton called

676
00:48:56,110 --> 00:48:59,240
the figure palm x

677
00:49:00,260 --> 00:49:03,690
the norton anthology this but

678
00:49:03,710 --> 00:49:08,890
this heavy book i ordered as as a way to

679
00:49:08,900 --> 00:49:13,190
well reduce your expenses here is just one

680
00:49:13,240 --> 00:49:18,810
big book to buy it also provides needed annotation

681
00:49:18,860 --> 00:49:21,680
modern poetry is in need of annotation

682
00:49:21,730 --> 00:49:25,460
this this new edition of this old

683
00:49:25,470 --> 00:49:29,000
the book is an excellent one

684
00:49:29,010 --> 00:49:32,130
you should read johann relies on his introduction

685
00:49:32,350 --> 00:49:36,040
readers prose notes you know that

686
00:49:36,090 --> 00:49:39,450
the preface his various selections

687
00:49:39,460 --> 00:49:46,460
having said that there's really nothing said that is the norton anthology

688
00:49:46,480 --> 00:49:54,250
you know or or ponderous and you know i i do agree with with a

689
00:49:54,250 --> 00:49:56,710
little well

690
00:49:56,710 --> 00:50:03,800
it is way

691
00:50:03,840 --> 00:50:04,860
OK so

692
00:50:04,880 --> 00:50:07,170
what i'm saying here is

693
00:50:07,230 --> 00:50:08,960
we specializing two

694
00:50:08,980 --> 00:50:14,020
limited class of motion the particle has a different activation a

695
00:50:15,710 --> 00:50:20,020
in every situation where the body is an acceleration in the location has to have

696
00:50:20,020 --> 00:50:21,820
this for

697
00:50:21,840 --> 00:50:24,380
we this number is where it was initially

698
00:50:24,480 --> 00:50:27,820
this was the initial blast

699
00:50:27,820 --> 00:50:30,800
so when it through the thing up and you can't

700
00:50:30,820 --> 00:50:32,920
what you're doing mentally

701
00:50:32,940 --> 00:50:37,710
i was immediately figuring out where it started and what's

702
00:50:37,800 --> 00:50:39,000
that was the initial

703
00:50:40,420 --> 00:50:43,110
then in your mind without realizing it

704
00:50:43,110 --> 00:50:47,920
you follow the trajectory at all future times

705
00:50:49,500 --> 00:50:50,960
that is one other

706
00:50:51,020 --> 00:50:54,230
celebrate formula that goes with this i'm going to find that

707
00:50:54,280 --> 00:50:55,210
then we

708
00:50:55,210 --> 00:51:00,630
i give you an example

709
00:51:00,630 --> 00:51:03,300
now i'm fully aware that

710
00:51:03,340 --> 00:51:05,150
this is not the flashiest

711
00:51:05,210 --> 00:51:08,800
example in physics but i'm not worried about the right now will have you see

712
00:51:08,820 --> 00:51:10,940
enough things that will come for you

713
00:51:10,960 --> 00:51:12,360
right now

714
00:51:12,380 --> 00:51:14,050
i want to demonstrate

715
00:51:14,050 --> 00:51:15,820
a simple paradigm of

716
00:51:15,840 --> 00:51:19,150
what it means to know the present and what it it means to say

717
00:51:19,150 --> 00:51:23,150
this is what the future behaviour and want to do that the simplest context then

718
00:51:23,150 --> 00:51:26,800
we can make the example more and more complicated but the full phenomenon will be

719
00:51:26,800 --> 00:51:28,230
the same

720
00:51:28,230 --> 00:51:30,590
so what we have found so far

721
00:51:31,150 --> 00:51:35,920
and purposely going from x to y because i want you to know

722
00:51:35,980 --> 00:51:40,050
the unknown variables can be colin x can be called the y

723
00:51:40,050 --> 00:51:42,920
it doesn't matter

724
00:51:42,940 --> 00:51:47,050
as long as the second derivative is it that's the answer

725
00:51:47,070 --> 00:51:49,820
now that's the second formula one derived from

726
00:51:49,900 --> 00:51:52,270
that's probably know that two from here

727
00:51:52,610 --> 00:51:56,070
this is the day care i want to write the formula and put it up

728
00:51:56,130 --> 00:51:57,420
and we can see

729
00:51:57,440 --> 00:51:59,860
how we use the second formula

730
00:51:59,920 --> 00:52:02,340
price to relate the final velocity

731
00:52:02,340 --> 00:52:03,920
at some time t

732
00:52:03,980 --> 00:52:05,590
initial velocity

733
00:52:05,650 --> 00:52:09,820
and the distance traveled with no difference in time

734
00:52:09,840 --> 00:52:12,610
so the trick is to eliminate high

735
00:52:12,630 --> 00:52:13,980
from this equation

736
00:52:14,000 --> 00:52:16,540
so let's see how we can eliminate time

737
00:52:16,610 --> 00:52:19,190
you know that if you look at the end of it what is

738
00:52:19,230 --> 00:52:20,900
finally of t

739
00:52:20,940 --> 00:52:22,820
is the zero

740
00:52:22,900 --> 00:52:26,750
was eighty

741
00:52:26,750 --> 00:52:28,770
what that means is

742
00:52:28,780 --> 00:52:32,480
if you know the last year the given time and you know the initial velocity

743
00:52:32,480 --> 00:52:33,960
you know what time it is

744
00:52:33,980 --> 00:52:35,750
time in fact the

745
00:52:35,840 --> 00:52:38,150
when the zero or

746
00:52:38,190 --> 00:52:41,820
they don't show you any argument for the mean to be at time t

747
00:52:41,880 --> 00:52:45,070
and the subsequent zero means we have zero

748
00:52:45,090 --> 00:52:47,270
so what this is

749
00:52:47,320 --> 00:52:51,420
you can measure time by having own clock clock tells you what time is

750
00:52:51,440 --> 00:52:54,590
we can also say what time it is by seeing how fast the particle is

751
00:52:55,800 --> 00:52:57,960
because you know it started with some speed

752
00:52:57,980 --> 00:53:02,210
it gaining speed summary a so speed was so and so now

753
00:53:02,210 --> 00:53:05,800
and the time had to be time can be indirectly inferred

754
00:53:05,840 --> 00:53:09,000
from these quantities

755
00:53:09,050 --> 00:53:11,550
then you take that formula here

756
00:53:11,570 --> 00:53:14,420
and you put it here that i would see at time t

757
00:53:14,440 --> 00:53:16,420
you put this expression

758
00:53:16,440 --> 00:53:20,770
what really get again expression in which there is no t he has been banished

759
00:53:22,000 --> 00:53:25,190
i'm not going to waste your time by asking what happens if you put it

760
00:53:25,420 --> 00:53:26,880
just tell you what happens

761
00:53:26,880 --> 00:53:30,040
what happens is you will find the square

762
00:53:30,090 --> 00:53:32,150
article visitors where

763
00:53:32,170 --> 00:53:33,840
list two a

764
00:53:33,860 --> 00:53:38,540
claims x minus x u

765
00:53:38,540 --> 00:53:39,980
how many people have seen this

766
00:53:40,000 --> 00:53:43,960
o thing before

767
00:53:44,020 --> 00:53:48,130
OK that's alot look i know you've seen this

768
00:53:48,130 --> 00:53:50,900
at the moment i have to go through

769
00:53:50,920 --> 00:53:55,400
some of the more standard material before we go to the more non-standard material

770
00:53:55,400 --> 00:53:57,110
this part of it easy for you

771
00:53:57,130 --> 00:53:58,610
there's not much

772
00:53:58,670 --> 00:54:00,420
i can do right now

773
00:54:00,440 --> 00:54:02,320
so let me drop box

774
00:54:02,380 --> 00:54:04,210
trying to box

775
00:54:04,210 --> 00:54:05,460
you guys means

776
00:54:06,670 --> 00:54:11,480
these are the two important things

777
00:54:11,550 --> 00:54:13,300
i claim no

778
00:54:13,420 --> 00:54:16,440
remember i want understand one thing

779
00:54:16,480 --> 00:54:19,820
how much of this should you memorized was you've never seen this in high school

780
00:54:19,820 --> 00:54:21,940
and with supposed to memory

781
00:54:21,940 --> 00:54:24,170
i would say keep that to minimum

782
00:54:24,420 --> 00:54:26,860
because what the first formula tells you

783
00:54:26,860 --> 00:54:29,210
b so into that you don't have to cry

784
00:54:29,250 --> 00:54:32,170
with the particles of constant acceleration

785
00:54:32,270 --> 00:54:35,380
it's going to take two derivatives i want to get a

786
00:54:35,420 --> 00:54:37,940
then you should know enough calculus and it has to be

787
00:54:37,980 --> 00:54:42,150
something like it is where have come from taking to the evidence

788
00:54:42,170 --> 00:54:44,550
the other two you know our stuff you can act

789
00:54:44,550 --> 00:54:45,840
and you know what

790
00:54:45,840 --> 00:54:47,420
well adding those things

791
00:54:47,440 --> 00:54:48,670
because the particle

792
00:54:48,690 --> 00:54:53,420
as the head start is good initial position even at quality zero

793
00:54:53,420 --> 00:54:55,050
has an initial velocity

794
00:54:55,070 --> 00:54:58,360
even without any acceleration it would be moving

795
00:54:58,360 --> 00:55:02,820
from why not the when was vt acceleration gives you an extra stuff

796
00:55:02,860 --> 00:55:05,110
quadratic time

797
00:55:05,150 --> 00:55:09,130
once you get got that one that would give you the velocity

798
00:55:09,170 --> 00:55:12,860
and in the crimes you can eliminate the input in this form

799
00:55:12,980 --> 00:55:17,480
but most people and up memorizing these two because you use it so many times

800
00:55:17,500 --> 00:55:21,040
UN statistics in you but you should try to memorize every

801
00:55:21,050 --> 00:55:22,770
so we're not going to do

802
00:55:23,610 --> 00:55:26,090
standard problem where

803
00:55:26,150 --> 00:55:28,170
we will convince ourselves

804
00:55:28,210 --> 00:55:30,110
we can do

805
00:55:30,130 --> 00:55:35,780
we can apply the formulas and predict the future given the press

806
00:55:35,800 --> 00:55:38,170
the problem i wanted do

807
00:55:38,250 --> 00:55:41,070
there are many things you can do but i just pick one

808
00:55:41,110 --> 00:55:44,550
this is the one with the all numbers so i can do to calculate

809
00:55:44,550 --> 00:55:46,110
here's the problem

810
00:55:46,130 --> 00:55:49,400
that is this building

811
00:55:49,440 --> 00:55:54,040
and it's going to be fifteen meters high

812
00:55:54,050 --> 00:55:56,020
i'm going to try something

813
00:55:56,070 --> 00:55:58,270
and it's going to go up and come down

814
00:55:58,280 --> 00:55:59,960
something i grew up

815
00:56:00,000 --> 00:56:01,630
as the initial speed

816
00:56:01,650 --> 00:56:11,320
of ten meters per second

817
00:56:12,270 --> 00:56:15,340
we have to ask now know that my claim is

818
00:56:15,400 --> 00:56:18,860
you can ask me any question you want what this part

819
00:56:18,900 --> 00:56:20,540
and i can see

820
00:56:20,550 --> 00:56:21,590
and ask me

821
00:56:21,630 --> 00:56:23,320
where b

822
00:56:23,320 --> 00:56:26,550
nine seconds from now it's taken from all of us will be moving

823
00:56:26,550 --> 00:56:28,960
i can answer anything at all

824
00:56:29,000 --> 00:56:31,920
but what i needed to do it to do this problem

825
00:56:31,980 --> 00:56:35,550
i was to find these two unknowns you got to get used the notion of

826
00:56:35,550 --> 00:56:36,820
what will be given

827
00:56:36,820 --> 00:56:40,170
in general and what is tailor made for the occasion

828
00:56:40,170 --> 00:56:42,210
so we know in this example

829
00:56:42,210 --> 00:56:44,980
initial height should be fifteen meters

830
00:56:45,000 --> 00:56:46,800
an initial velocity

831
00:56:46,820 --> 00:56:48,020
the ten

832
00:56:48,070 --> 00:56:49,670
and what acceleration

833
00:56:49,690 --> 00:56:51,440
i'm going to use minus g

834
00:56:51,440 --> 00:56:56,300
a learning algorithm or method of mapping that takes us from training data to functions

835
00:56:56,300 --> 00:57:00,780
and we don't care what those functions i

836
00:57:01,670 --> 00:57:04,270
right so so in particular you know have have here

837
00:57:04,280 --> 00:57:08,150
it just takes as input x one y one through x and y and and

838
00:57:09,550 --> 00:57:11,110
the function l

839
00:57:11,130 --> 00:57:14,880
it's just some mapping to functions defined on x

840
00:57:14,900 --> 00:57:19,280
OK doesn't and no longer restricted to choose f from some class it could be

841
00:57:21,690 --> 00:57:24,070
you could use any any kind of functions

842
00:57:26,480 --> 00:57:29,500
let's define the minimax estimation error in this

843
00:57:29,510 --> 00:57:30,380
you know

844
00:57:30,380 --> 00:57:32,210
n delta kind of way

845
00:57:32,210 --> 00:57:34,250
we have a sample size and we have this

846
00:57:34,270 --> 00:57:35,980
this company parameter delta

847
00:57:36,010 --> 00:57:38,000
this is the best we can do

848
00:57:38,860 --> 00:57:42,130
functions l over the algorithms or methods

849
00:57:42,150 --> 00:57:45,070
right of the smallest error

850
00:57:45,090 --> 00:57:46,230
we can hope for

851
00:57:46,280 --> 00:57:48,480
and at delta level

852
00:57:48,480 --> 00:57:51,070
OK so what does that mean that means

853
00:57:51,070 --> 00:57:52,730
you know it is the

854
00:57:52,750 --> 00:57:55,630
is this an excellent for which we can get

855
00:57:57,070 --> 00:57:58,170
the risk

856
00:57:58,210 --> 00:58:00,820
all this had the the function returned by

857
00:58:00,860 --> 00:58:02,150
this method

858
00:58:02,210 --> 00:58:05,030
within epsilon of the best

859
00:58:05,050 --> 00:58:07,420
in the class capital

860
00:58:07,440 --> 00:58:12,320
OK so here the criterion involves the class capital have not restricted choosing functions from

861
00:58:12,900 --> 00:58:14,690
right for any method

862
00:58:14,710 --> 00:58:15,730
any method here

863
00:58:15,750 --> 00:58:17,400
what's the best method

864
00:58:17,440 --> 00:58:19,150
whether tis from f or not

865
00:58:19,190 --> 00:58:22,570
in the sense of minimising the deviation between

866
00:58:23,840 --> 00:58:26,320
and minimal risk over that classifier

867
00:58:26,370 --> 00:58:28,090
right with high probability

868
00:58:28,090 --> 00:58:33,070
OK we don't want the deviation to exceed that's with probability more than delta

869
00:58:33,070 --> 00:58:37,070
and this is going to be true all probability distributions

870
00:58:37,130 --> 00:58:39,570
OK so whatever problem we faced with

871
00:58:39,590 --> 00:58:44,710
we want to have the probability that we have we have worse than epsilon gap

872
00:58:44,710 --> 00:58:46,000
between the best

873
00:58:46,050 --> 00:58:50,230
we could for using f and the thing that our l returns

874
00:58:50,250 --> 00:58:52,360
the risk of that if at

875
00:58:52,400 --> 00:58:57,400
we want that probability to be small

876
00:58:57,420 --> 00:59:00,630
OK so

877
00:59:00,650 --> 00:59:05,030
for the sample error minimisation algorithm that shows that that this minimum

878
00:59:05,030 --> 00:59:06,920
is no worse than

879
00:59:06,940 --> 00:59:12,280
you know some constant times the square to the VC dimension of an essentially

880
00:59:12,320 --> 00:59:16,920
right because in particular if we choose l as the element of capital f that

881
00:59:16,920 --> 00:59:19,630
minimizes the sample average of loss

882
00:59:19,650 --> 00:59:23,280
minimizes the empirical risk on the start of then we get

883
00:59:23,360 --> 00:59:27,570
you know the steps along the deviation between the risk and the best in the

884
00:59:28,630 --> 00:59:32,440
is of order something like this two-dimensional over in the square root of the at

885
00:59:38,230 --> 00:59:40,000
that's for

886
00:59:40,000 --> 00:59:46,690
sampler minimisation that that gives us this upper bound it turns out that that's the

887
00:59:46,690 --> 00:59:51,380
that's essentially optimal in the minimax minimax sense

888
00:59:51,400 --> 00:59:52,380
it's the

889
00:59:52,380 --> 00:59:53,900
the right right anyway

890
00:59:53,900 --> 00:59:57,230
so as long as his is theorem as long as the

891
00:59:57,230 --> 01:00:06,880
confidence isn't it parameters is not too large can be treated as constant then

892
01:00:06,940 --> 01:00:11,920
for a class that has VC dimension released the

893
01:00:15,610 --> 01:00:19,320
is the least as big as something of order VC dimension of a skirt of

894
01:00:19,320 --> 01:00:20,940
this invention over

895
01:00:20,960 --> 01:00:23,610
we get we get exactly the right thing

896
01:00:23,610 --> 01:00:26,280
and i put a minimum here because you know you can

897
01:00:26,300 --> 01:00:30,170
come up with silly things when the sample size is small

898
01:00:30,190 --> 01:00:33,230
to make this article not true

899
01:00:33,280 --> 01:00:36,320
so this is this essential was not something to worry about everything just about the

900
01:00:36,960 --> 01:00:42,070
OK so this this shows that the

901
01:00:42,070 --> 01:00:44,400
sample error minimisation algorithm

902
01:00:44,440 --> 01:00:49,590
if this is this upper bound minimax estimation error and that upper bound

903
01:00:49,610 --> 01:00:50,770
can be

904
01:00:50,820 --> 01:00:55,150
right so as long as you know the class your your comparing yourself to has

905
01:00:55,150 --> 01:00:56,750
VC dimension at least d

906
01:00:57,920 --> 01:01:01,320
this is the mesh minimax estimation error has to be at least as big as

907
01:01:01,320 --> 01:01:03,440
grid mention of

908
01:01:03,690 --> 01:01:07,000
going of square the average

909
01:01:07,130 --> 01:01:09,590
should dr guess

910
01:01:09,590 --> 01:01:11,320
OK that's

911
01:01:11,380 --> 01:01:17,050
remember we're working you know we've got to perform uniformly well across all probability distributions

912
01:01:17,090 --> 01:01:27,920
all right that's crucial in improving the converse result for the minimax right

913
01:01:27,940 --> 01:01:33,730
we work with so so the idea of the proof of this lower bound

914
01:01:33,750 --> 01:01:35,920
it is based on the probabilistic method we we

915
01:01:37,110 --> 01:01:40,570
a probability distribution that makes life hard for

916
01:01:40,610 --> 01:01:42,650
learning algorithm l

917
01:01:43,880 --> 01:01:46,880
and we do that in a random way so that you know if if i

918
01:01:46,880 --> 01:01:50,900
were to propose the probability distribution you could easily design an algorithm that would work

919
01:01:50,900 --> 01:01:56,340
well for that particular distribution what we show is that if you choose the distribution

920
01:01:56,340 --> 01:01:58,360
randomly from from some

921
01:01:58,380 --> 01:02:00,840
from some set

922
01:02:02,440 --> 01:02:06,670
then under the random choice you you do badly somewhere

923
01:02:06,670 --> 01:02:10,070
right so there must exist the probability distribution makes life

924
01:02:11,590 --> 01:02:15,340
right and so in particular when we look at the worst case of probability distributions

925
01:02:15,340 --> 01:02:16,820
then you'll be doing

926
01:02:16,820 --> 01:02:18,170
that badly

927
01:02:18,190 --> 01:02:18,940
OK so

928
01:02:18,960 --> 01:02:23,070
the set of distributions you know this this is all

929
01:02:23,130 --> 01:02:25,480
classical that's quite nice

930
01:02:25,480 --> 01:02:29,460
second process distributions concentrates

931
01:02:29,480 --> 01:02:33,750
x on a shattered set and then chooses the wise with probability

932
01:02:33,780 --> 01:02:35,340
you know it's possible minus one

933
01:02:35,360 --> 01:02:40,340
so a possible plus one with probably half plus a little bit or probably half

934
01:02:40,340 --> 01:02:43,050
months little little bit you choose randomly

935
01:02:43,050 --> 01:02:45,090
right whether it's possible minus one here

936
01:02:45,250 --> 01:02:48,280
plus one discipline over two

937
01:02:48,300 --> 01:02:49,210
and you can show

938
01:02:49,300 --> 01:02:51,460
in that case that for whatever

939
01:02:51,480 --> 01:02:53,690
learning algorithm you have

940
01:02:53,710 --> 01:02:55,550
the expected value

941
01:02:55,550 --> 01:02:57,650
all of the excess risk

942
01:02:57,690 --> 01:03:03,420
OK expected value expectation is now under the random choice of this probability distribution the

943
01:03:03,420 --> 01:03:06,380
expected value of this excess risk is large

944
01:03:06,400 --> 01:03:08,130
larger than its long

945
01:03:08,770 --> 01:03:09,960
and therefore

946
01:03:09,980 --> 01:03:12,440
there must be some probability distribution

947
01:03:12,460 --> 01:03:16,820
that causes this difference to be bigger than epsilon

948
01:03:16,840 --> 01:03:23,070
OK so this is the probabilistic method showing that under some distribution the expectation of

949
01:03:23,070 --> 01:03:24,340
utilize the

950
01:03:24,350 --> 01:03:27,720
parallel immersion learning algorithms

951
01:03:27,740 --> 01:03:30,440
we were a a hundred performers

952
01:03:30,500 --> 01:03:31,990
power efficiency

953
01:03:34,320 --> 01:03:36,100
hard to do that

954
01:03:36,530 --> 01:03:39,560
here's some interesting facts that lead to our

955
01:03:39,710 --> 01:03:41,750
solutions approaches

956
01:03:41,800 --> 01:03:49,550
the first think about the google datacenters the missions we have sufficient computing power for

957
01:03:50,370 --> 01:03:51,510
of the

958
01:03:53,680 --> 01:03:59,690
and each enterprise or individual only replicas that we care

959
01:03:59,700 --> 01:04:00,920
for example and its

960
01:04:00,940 --> 01:04:06,510
guys around the michelin there was some sort of the computations and the google around

961
01:04:06,520 --> 01:04:10,650
the web search for the next thing and some kind of data mining

962
01:04:12,630 --> 01:04:15,130
the general purpose computing want

963
01:04:15,140 --> 01:04:19,570
designed for this specific task

964
01:04:19,600 --> 01:04:25,480
four ethics that is specially designed hardware

965
01:04:25,490 --> 01:04:27,110
it can lead to

966
01:04:27,120 --> 01:04:28,320
the cells and

967
01:04:28,330 --> 01:04:29,790
performance power

968
01:04:31,000 --> 01:04:33,000
but it's very expensive

969
01:04:33,050 --> 01:04:35,020
nine days they are more than

970
01:04:36,760 --> 01:04:42,690
we want people to chip who created the mosque it me spend tens of

971
01:04:42,690 --> 01:04:46,560
millions of dollars for the when past

972
01:04:46,610 --> 01:04:49,250
but if you feel you when beta

973
01:04:49,320 --> 01:04:50,580
that money will

974
01:04:50,640 --> 01:04:53,270
will become neuhaus

975
01:04:55,420 --> 01:04:57,980
collecting with this facts

976
01:04:58,000 --> 01:05:00,190
OK let's move back

977
01:05:00,200 --> 01:05:02,170
another is

978
01:05:03,390 --> 01:05:10,310
application specific integrated circuits also suffers from the product design productivity

979
01:05:12,990 --> 01:05:14,570
some design problems

980
01:05:14,640 --> 01:05:17,760
a common solution is to have one

981
01:05:17,830 --> 01:05:19,390
programming framework

982
01:05:20,050 --> 01:05:25,430
four machine learning and data mining there's a very successful programming framework called mapreduce

983
01:05:25,440 --> 01:05:28,170
there's many many work has been done here

984
01:05:28,210 --> 01:05:30,920
so here comes our approach first

985
01:05:30,930 --> 01:05:32,270
we will create

986
01:05:32,310 --> 01:05:34,750
a supercomputer in a box

987
01:05:34,820 --> 01:05:37,610
that is a combination of

988
01:05:37,650 --> 01:05:42,260
actually g and suffused produce reaper when

989
01:05:42,310 --> 01:05:48,680
got it is also semiconductor devices then you can create second was that i will

990
01:05:48,680 --> 01:05:49,830
introduce more

991
01:05:49,850 --> 01:05:51,250
the story

992
01:05:52,440 --> 01:05:53,560
and the second

993
01:05:53,560 --> 01:05:55,440
with this simple computer

994
01:05:57,190 --> 01:05:58,650
have to design

995
01:05:58,670 --> 01:06:03,000
a framework for programming it that from we can easily

996
01:06:03,060 --> 01:06:09,540
programme the which in many ways of the money and get the palette that parallel

997
01:06:09,540 --> 01:06:11,320
of this them automatically

998
01:06:11,480 --> 01:06:17,710
you can hear the big picture left side is the programming side

999
01:06:17,760 --> 01:06:19,730
we'll have applications

1000
01:06:19,740 --> 01:06:22,130
you designed the

1001
01:06:22,310 --> 01:06:28,000
the mappers or reducers i i think everybody here is very familiar with mapreduce and

1002
01:06:28,620 --> 01:06:32,130
with the members and to reduce you can utilize

1003
01:06:32,180 --> 01:06:34,500
the highway what this is cool

1004
01:06:34,500 --> 01:06:35,680
that's cool

1005
01:06:35,690 --> 01:06:39,130
can take the three also plus

1006
01:06:39,140 --> 01:06:41,270
programs as input

1007
01:06:41,320 --> 01:06:43,790
and the hardware circuit

1008
01:06:44,700 --> 01:06:46,420
so this is hard

1009
01:06:46,500 --> 01:06:49,300
the hardware the circuit will be programmed

1010
01:06:49,350 --> 01:06:52,080
so the so the circuit

1011
01:06:52,100 --> 01:06:53,500
will automatically

1012
01:06:55,380 --> 01:06:57,520
and the functionality we all

1013
01:06:58,100 --> 01:06:59,900
you can

1014
01:06:59,950 --> 01:07:03,260
implement many of the mappers reducers on percent

1015
01:07:08,690 --> 01:07:14,600
and also you can improve some user constant to let the system ran we'll have

1016
01:07:14,600 --> 01:07:17,880
some knowledge about the application

1017
01:07:17,890 --> 01:07:22,110
the right side is the picture side that's the supercomputer in a box

1018
01:07:22,150 --> 01:07:27,320
that's heterogeneous architecture refers to fuse and peaches

1019
01:07:27,440 --> 01:07:29,500
in the all the FPT we have

1020
01:07:29,560 --> 01:07:31,000
some pre

1021
01:07:31,020 --> 01:07:32,990
the final redesigned

1022
01:07:33,000 --> 01:07:34,120
building blocks

1023
01:07:34,130 --> 01:07:37,540
that's the interconnection network which

1024
01:07:37,580 --> 01:07:39,680
and that all the mappers and reduce errors

1025
01:07:39,690 --> 01:07:42,900
also on track on chip scheduler that can

1026
01:07:42,960 --> 01:07:50,310
scheduled the different task and they can manage management middle management module

1027
01:07:50,370 --> 01:07:53,440
and can fetch high and the

1028
01:07:53,510 --> 01:07:55,240
for the input and output

1029
01:07:55,250 --> 01:07:58,500
so this is the big picture

1030
01:07:58,510 --> 01:08:03,130
let's see why we select actually

1031
01:08:03,140 --> 01:08:05,520
first few the definition

1032
01:08:05,530 --> 01:08:08,100
five refugees field programmable

1033
01:08:08,140 --> 01:08:11,830
iterate is can changes functionality

1034
01:08:11,840 --> 01:08:13,730
after you you place

1035
01:08:13,740 --> 01:08:16,460
cheap on two young second bt

1036
01:08:16,470 --> 01:08:18,020
we sense

1037
01:08:18,080 --> 01:08:21,120
components you can create arbitrary logic

1038
01:08:21,130 --> 01:08:22,380
with guitarist

1039
01:08:22,470 --> 01:08:27,720
so what is guitarist is islands of recoverable logic

1040
01:08:27,740 --> 01:08:31,320
in the sea of reconfigurable interconnect

1041
01:08:31,330 --> 01:08:33,140
i have pictures for that

1042
01:08:33,230 --> 01:08:35,790
this is the basic

1043
01:08:35,800 --> 01:08:38,000
structure of music

1044
01:08:38,010 --> 01:08:39,800
logic block

1045
01:08:39,830 --> 01:08:41,050
for the PC

1046
01:08:41,080 --> 01:08:43,970
it is very

1047
01:08:43,980 --> 01:08:45,380
it's very easy

1048
01:08:45,390 --> 01:08:47,380
work but i want to

1049
01:08:47,500 --> 01:08:49,760
show that they had to look up people

1050
01:08:52,130 --> 01:08:54,530
there's a lookup table you can

1051
01:08:54,580 --> 01:08:58,910
design arbitrary functions all for your input model

1052
01:08:58,930 --> 01:09:04,290
also you can store the result in flip-flop

1053
01:09:04,290 --> 01:09:09,660
are we also need a way to express the relationship between members of these different

1054
01:09:10,440 --> 01:09:13,120
for instance one we like to say

1055
01:09:13,210 --> 01:09:19,180
that whenever an artist does something you create a piece of art

1056
01:09:19,210 --> 01:09:24,040
OK so is this property that relates every artist with a piece of art which

1057
01:09:24,040 --> 01:09:27,230
is named creates and you see out

1058
01:09:27,230 --> 01:09:31,270
you've write this down so you say creates is appropriate

1059
01:09:31,270 --> 01:09:37,330
if you think it in first order logic which are sainsbury's binary predicate

1060
01:09:37,410 --> 01:09:43,750
that links basically to intervene one which is named create cases

1061
01:09:43,810 --> 01:09:50,250
yes yes saying look very is this possibility to relate member of two different sets

1062
01:09:50,310 --> 01:09:56,040
and you can use any server then creates case

1063
01:09:56,060 --> 01:10:00,830
is basically a way to say that road in wikis are related by create

1064
01:10:00,850 --> 01:10:01,750
in four

1065
01:10:04,390 --> 01:10:05,460
one can do more

1066
01:10:05,480 --> 01:10:09,140
which is a kind of difficult to understand if you

1067
01:10:09,190 --> 01:10:11,480
comes from object orientation

1068
01:10:11,500 --> 01:10:16,560
if you comes from a variety of object oriented languages this is not possible you

1069
01:10:16,560 --> 01:10:20,520
normally can not to do is some type of the property

1070
01:10:20,560 --> 01:10:23,410
or at least you can do it but i mean it is not what we

1071
01:10:23,410 --> 01:10:28,250
normally think about in terms of property in RDF is possible so you can say

1072
01:10:28,250 --> 01:10:31,600
that there is a specific way to use the predicate

1073
01:10:31,620 --> 01:10:35,120
but also for a subset of all

1074
01:10:35,120 --> 01:10:38,790
the members of the previous property sue painting

1075
01:10:38,810 --> 01:10:42,960
being sick is a specific way to create a you can write like this

1076
01:10:43,040 --> 01:10:49,160
you can see painter is a subpropertyof creates any basically means that for each page

1077
01:10:49,250 --> 01:10:55,890
that are connected by being then you can claim that they are connected by create

1078
01:10:57,460 --> 01:11:01,100
good on this thing you can do for scones

1079
01:11:01,330 --> 01:11:06,120
a subpropertyof creates an blah blah

1080
01:11:08,140 --> 01:11:13,640
there is one last thing which is very important which is out you to connect

1081
01:11:14,890 --> 01:11:18,540
two sets so so far we have a year key

1082
01:11:18,580 --> 01:11:25,330
of sets any year kill properties but not really connected them together i mean creates

1083
01:11:25,330 --> 01:11:29,250
is still something that can be done by everybody if i want to say that

1084
01:11:29,250 --> 01:11:33,710
is not possible to create unless you are an artist i have to use these

1085
01:11:35,040 --> 01:11:42,600
the property that comes from rdfs it means that creates use is done by artist

1086
01:11:42,640 --> 01:11:45,600
and when he's done produce piece

1087
01:11:48,870 --> 01:11:51,040
so in this way i'm

1088
01:11:51,040 --> 01:11:57,190
basically stating that whenever you create something you are artist and whatever you create is

1089
01:11:57,190 --> 01:11:59,290
it masterpiece

1090
01:11:59,350 --> 01:12:02,600
and you can do the same for the subproperty so

1091
01:12:02,660 --> 01:12:07,250
a painter creates paints and scott to create scores

1092
01:12:08,000 --> 01:12:09,390
i mean can be wrong

1093
01:12:09,410 --> 01:12:15,180
but in a way and trying to represent these constraints about the data

1094
01:12:15,190 --> 01:12:18,520
and here i have representation in form but i mean

1095
01:12:18,540 --> 01:12:20,850
i don't want to go into them

1096
01:12:21,830 --> 01:12:25,890
what's the resulting ontology what we did so what we did was really

1097
01:12:25,910 --> 01:12:31,100
drawing an ontology is a very simple one but it's not that easy about the

1098
01:12:31,100 --> 01:12:38,040
world of arts we first of all property which is this create then using domain

1099
01:12:38,040 --> 01:12:39,710
and range

1100
01:12:39,710 --> 01:12:41,350
we use

1101
01:12:41,370 --> 01:12:47,910
to connected to artist so creates applied to artists and generates masterpieces

1102
01:12:47,920 --> 01:12:53,680
and then we have all the subproperties so we have the painters spain spain

1103
01:12:53,690 --> 01:12:56,270
and sculptor sculpt course

1104
01:12:58,850 --> 01:13:01,810
now i'm getting to explain why i

1105
01:13:01,830 --> 01:13:05,040
keep talking about all this stuff now

1106
01:13:05,040 --> 01:13:13,040
this is inference in rdfs these rules so that basically i used to derive the

1107
01:13:13,040 --> 01:13:16,120
knowledge from the knowledge that you put in RDF

1108
01:13:16,120 --> 01:13:19,210
and i put them in this way because they believe that it's pretty easy to

1109
01:13:19,210 --> 01:13:24,310
understand so if you have something like except that class of

1110
01:13:24,310 --> 01:13:26,890
it's and you have something a

1111
01:13:26,890 --> 01:13:32,270
which is of type peaks and then you can say that a is that keeps

1112
01:13:32,310 --> 01:13:35,440
what does it mean if you have

1113
01:13:38,940 --> 01:13:41,830
is an artist a u

1114
01:13:42,940 --> 01:13:47,560
then is it painter which is not true story then

1115
01:13:47,710 --> 01:13:50,440
then is also an artist

1116
01:13:50,440 --> 01:13:55,080
OK so this is away from a very simple no its ever tried to derive

1117
01:13:55,140 --> 01:13:59,600
new tribe which is exactly meaningful because of the ontology

1118
01:13:59,750 --> 01:14:03,870
you can do similar things so if you have a chain of subgraphs offer you

1119
01:14:03,870 --> 01:14:08,500
can unfold data and do the same but you did before so i mean we

1120
01:14:08,500 --> 01:14:14,640
don't have a chain of properties but one can image to have come from the

1121
01:14:14,640 --> 01:14:19,640
flemish painter or to have other kind of painters and then you have all these

1122
01:14:19,640 --> 01:14:24,560
different going on the same applies to properties and the these nice things that applies

1123
01:14:24,560 --> 01:14:26,060
to the main types so

1124
01:14:26,230 --> 01:14:27,580
if you have a

1125
01:14:27,580 --> 01:14:33,770
that are then scott wikis you know that the scots

1126
01:14:33,810 --> 01:14:36,690
is producing sorry

1127
01:14:37,140 --> 01:14:39,100
done by sculptor then

1128
01:14:39,120 --> 01:14:44,520
from just these then scored wiki so you can deduce better than is this guy

1129
01:14:44,560 --> 01:14:46,140
without any the average

1130
01:14:46,180 --> 01:14:47,810
which is also

1131
01:14:47,850 --> 01:14:49,440
and the same applies to range

1132
01:14:51,770 --> 01:14:53,710
imagine that you have his ontology

1133
01:14:55,040 --> 01:14:57,210
which is the one that produced so far

1134
01:14:57,270 --> 01:14:59,580
and you made that you transmitter

1135
01:15:01,310 --> 01:15:05,830
recipient to another these tribal so yes saying that from

1136
01:15:08,080 --> 01:15:10,350
two years

1137
01:15:10,420 --> 01:15:16,420
you're sending something like subject predicate object nothing more than that OK

1138
01:15:20,410 --> 01:15:23,100
the recipient

1139
01:15:23,120 --> 01:15:26,770
can always answer this query

1140
01:15:27,370 --> 01:15:28,310
but then

1141
01:15:30,140 --> 01:15:33,560
does it all that there is a sculptor

1142
01:15:33,660 --> 01:15:36,710
sorry the better than scott wikis

1143
01:15:36,730 --> 01:15:40,500
this is always possible because syntax right so i mean

1144
01:15:40,520 --> 01:15:43,460
i sent these

1145
01:15:43,460 --> 01:15:48,460
and i'm writing down some query that involve only these knowledge

1146
01:15:48,770 --> 01:15:52,080
so and this plain syntactic word and it works

1147
01:15:52,120 --> 01:15:53,420
but look

1148
01:15:53,440 --> 01:15:57,500
if the two recipients share the same ontology then you don't have to

1149
01:15:58,040 --> 01:16:00,140
to send all the information

1150
01:16:00,140 --> 01:16:04,750
but all the other queries down here can be answered so you can ask

1151
01:16:04,790 --> 01:16:09,370
is there then a sculptor and that's yes because you know that if somebody's got

1152
01:16:09,390 --> 01:16:12,060
something then is discussed

1153
01:16:12,080 --> 01:16:14,480
is it an artist yes

1154
01:16:14,520 --> 01:16:17,370
because if it is the skull is also an artist

1155
01:16:17,410 --> 01:16:20,100
and you can go to painter is a painter

1156
01:16:20,140 --> 01:16:22,140
well actually it is not be

1157
01:16:22,180 --> 01:16:25,980
because i mean it given that is the sculpture and given that we are making

1158
01:16:25,980 --> 01:16:30,830
some important things like unique name assumption

1159
01:16:30,890 --> 01:16:34,600
if it is the sculpture idea instead of it is a painter van is just

1160
01:16:35,730 --> 01:16:37,410
OK and so forth

1161
01:16:37,440 --> 01:16:39,890
this is what we call semantics

1162
01:16:39,890 --> 01:16:44,620
so i mean you over in pores over that tell a year which is RDF

1163
01:16:44,770 --> 01:16:50,580
constraints and when you transmit one element of the data more than you can derive

1164
01:16:50,580 --> 01:16:53,790
all the knowledge that were shared between the two semantic models

1165
01:16:53,850 --> 01:16:55,850
OK so you have a recipient

1166
01:16:55,890 --> 01:17:00,690
two recipients one shared more than the transmitter is simple tripod but you derive all

1167
01:17:00,710 --> 01:17:03,850
the implicit knowledge which is carried by the truckload

1168
01:17:03,850 --> 01:17:05,860
so it begins to swing back and forth

1169
01:17:05,890 --> 01:17:09,940
becomes completely chaotic you can no longer see what's happening

1170
01:17:09,990 --> 01:17:12,040
and it just so happened that about

1171
01:17:12,110 --> 01:17:13,740
six months ago dave

1172
01:17:13,740 --> 01:17:16,290
i had been was professor they've trooper

1173
01:17:17,170 --> 01:17:21,000
i explained to him that it is just unfortunate that you can never really show

1174
01:17:21,000 --> 01:17:24,910
it that you jump of the people have about scale and which and see that

1175
01:17:24,930 --> 01:17:27,250
way go down to zero in free

1176
01:17:27,250 --> 01:17:28,610
and he said

1177
01:17:29,600 --> 01:17:30,880
i can do that

1178
01:17:30,990 --> 01:17:33,340
he says i can make use scale

1179
01:17:33,380 --> 01:17:35,010
which has a response time

1180
01:17:35,020 --> 01:17:37,030
well maybe ten seconds

1181
01:17:37,080 --> 01:17:39,210
so when you jump off the table

1182
01:17:39,310 --> 01:17:42,500
in ten seconds you will see that things go down

1183
01:17:42,510 --> 01:17:43,890
two zero

1184
01:17:45,440 --> 01:17:46,650
he delivered

1185
01:17:46,660 --> 01:17:47,890
he came through

1186
01:17:47,900 --> 01:17:49,890
he built his wonderful device

1187
01:17:50,970 --> 01:17:52,190
united going to

1188
01:17:52,210 --> 01:17:53,970
demonstrate through

1189
01:17:54,100 --> 01:17:55,210
the first

1190
01:17:55,230 --> 01:17:57,620
give you some

1191
01:17:57,640 --> 01:18:01,110
reasonable light for this

1192
01:18:01,120 --> 01:18:04,550
and i would like to show you

1193
01:18:04,600 --> 01:18:06,920
on the scale there

1194
01:18:08,330 --> 01:18:13,120
this scale that he built is indicated here is the scale

1195
01:18:13,240 --> 01:18:15,070
i have it in my hands

1196
01:18:15,080 --> 01:18:19,110
and on top of this scale is a little platform just like on your scale

1197
01:18:19,160 --> 01:18:22,650
this platform ways four-and-a-half months

1198
01:18:22,710 --> 01:18:25,110
you can see that

1199
01:18:25,160 --> 01:18:27,460
it's about four and a half

1200
01:18:27,540 --> 01:18:28,920
you will say

1201
01:18:28,970 --> 01:18:31,630
i wouldn't want that kind of bathroom scale

1202
01:18:31,650 --> 01:18:34,990
if i want to see my bathroom scale once see is zero for i want

1203
01:18:34,990 --> 01:18:37,830
to go up i'm happy enough all by myself i don't want to get another

1204
01:18:37,840 --> 01:18:40,250
four-and-a-half pounds

1205
01:18:40,250 --> 01:18:43,480
the manufacturer has simply zero that scale for

1206
01:18:43,490 --> 01:18:48,020
but obviously also your bathroom scale as a cover on it

1207
01:18:48,070 --> 01:18:53,520
once you've seen these demonstrations you will be able to answer for yourself why we

1208
01:18:53,520 --> 01:18:58,310
don't zero this we really believe this to be four-and-a-half that's the actual

1209
01:18:58,360 --> 01:19:02,650
mass is on top of the spring what is not really is spring

1210
01:19:02,670 --> 01:19:04,020
it is the pressure gauge

1211
01:19:04,040 --> 01:19:06,150
but i think of it as the spring

1212
01:19:06,170 --> 01:19:08,790
four and a half pounds

1213
01:19:09,920 --> 01:19:12,380
we have a great

1214
01:19:12,460 --> 01:19:14,500
which is above their weight

1215
01:19:14,520 --> 01:19:18,460
which is ten pounds

1216
01:19:19,060 --> 01:19:23,770
this from one of your children that they are doing it yourself

1217
01:19:23,790 --> 01:19:26,170
ten parts

1218
01:19:26,190 --> 01:19:29,170
report on top here

1219
01:19:29,190 --> 01:19:30,250
what do you see

1220
01:19:30,270 --> 01:19:33,290
roughly fourteen and-a-half months

1221
01:19:33,340 --> 01:19:38,920
all right we're going to take it down

1222
01:19:38,940 --> 01:19:43,360
there we go

1223
01:19:43,400 --> 01:19:45,360
i'm going to drop it

1224
01:19:45,400 --> 01:19:48,090
from about one i have two meters

1225
01:19:48,130 --> 01:19:50,060
and we drop it in here

1226
01:19:50,070 --> 01:19:56,230
well cushioned because we don't want to break this beautiful device

1227
01:19:56,250 --> 01:19:58,880
when we drop with

1228
01:19:58,940 --> 01:20:02,000
the responses so fast that you will see indeed

1229
01:20:02,000 --> 01:20:03,900
that point to go to zero

1230
01:20:03,940 --> 01:20:06,480
october mind when he hits

1231
01:20:07,980 --> 01:20:10,250
that the way will go up

1232
01:20:10,250 --> 01:20:14,290
for now i wanted to concentrate only on the thing going to zero and not

1233
01:20:14,290 --> 01:20:15,520
what comes later

1234
01:20:15,540 --> 01:20:16,810
we'll deal with that

1235
01:20:16,810 --> 01:20:18,230
within a minute

1236
01:20:25,880 --> 01:20:29,360
fourteen and-a-half parts

1237
01:20:29,440 --> 01:20:32,310
you know what i think is actually jiggling back and forth

1238
01:20:32,330 --> 01:20:34,290
i can hold exactly still

1239
01:20:34,290 --> 01:20:35,170
and so

1240
01:20:35,210 --> 01:20:40,560
i slightly accelerated upwards and downwards and when accelerated slightly upward sideways a little more

1241
01:20:40,560 --> 01:20:43,040
when accelerated downward sideways less

1242
01:20:43,040 --> 01:20:45,400
interesting you can see on the surface

1243
01:20:45,480 --> 01:20:47,840
it is my nervous tension be there

1244
01:20:51,020 --> 01:20:52,790
look at don't look at me now

1245
01:20:52,790 --> 01:20:54,210
look at that

1246
01:20:54,250 --> 01:20:55,110
o point to

1247
01:20:55,130 --> 01:20:58,480
three two one zero

1248
01:20:58,540 --> 01:21:00,000
you see go to zero

1249
01:21:00,020 --> 01:21:03,170
all the way to zero

1250
01:21:04,590 --> 01:21:07,540
come something even more remarkable

1251
01:21:07,540 --> 01:21:09,500
he said to me

1252
01:21:09,560 --> 01:21:11,710
i can also

1253
01:21:11,730 --> 01:21:14,790
make the students c

1254
01:21:14,810 --> 01:21:18,420
the response on a timescale of about

1255
01:21:18,460 --> 01:21:21,310
the fraction of a second by the way this is the hero who made all

1256
01:21:21,310 --> 01:21:22,670
this stuff

1257
01:21:31,420 --> 01:21:32,960
i can show you

1258
01:21:33,000 --> 01:21:34,000
the weights

1259
01:21:34,980 --> 01:21:37,630
on electronic scale

1260
01:21:37,630 --> 01:21:39,790
and this weight

1261
01:21:39,840 --> 01:21:41,730
you will see as a function of time

1262
01:21:45,500 --> 01:21:47,650
ten pounds back on again

1263
01:21:47,710 --> 01:21:50,540
take the little

1264
01:21:53,330 --> 01:21:56,670
so the level that you see no

1265
01:21:57,360 --> 01:22:01,660
fourteen and-a-half months this is fourteen and have found and this is zero these markets

1266
01:22:04,810 --> 01:22:07,110
i'm going to

1267
01:22:07,190 --> 01:22:09,590
holding my hand

1268
01:22:09,670 --> 01:22:17,690
notice if i can hold it still you back to fourteen and-a-half months

1269
01:22:17,730 --> 01:22:20,590
now i'm going to drop it

1270
01:22:20,630 --> 01:22:23,290
you will see you go down to zero

1271
01:22:23,310 --> 01:22:25,330
it will hit the floor

1272
01:22:26,130 --> 01:22:31,650
it will get an acceleration upwards it will become way heavier than it was before

1273
01:22:31,750 --> 01:22:33,290
and then it will even be

1274
01:22:33,340 --> 01:22:36,670
bounce back up in the air goes again into free fall

1275
01:22:36,670 --> 01:22:38,250
we will freeze that for you

1276
01:22:38,250 --> 01:22:39,560
you will be able

1277
01:22:39,710 --> 01:22:41,860
we will be able to analyse them

1278
01:22:41,880 --> 01:22:44,980
after that all happens

1279
01:22:47,560 --> 01:22:49,630
fourteen and-a-half pounds

1280
01:22:50,860 --> 01:22:54,360
one zero

1281
01:22:54,360 --> 01:22:57,250
and now professor to freezing it for you look at this

1282
01:22:57,250 --> 01:23:00,070
look at this incredible picture

1283
01:23:00,250 --> 01:23:01,690
this is truly

1284
01:23:01,750 --> 01:23:06,330
i open for me when i saw the the physics in here is unbelievable

1285
01:23:06,520 --> 01:23:09,520
issue fourteen and-a-half months

1286
01:23:09,560 --> 01:23:12,270
take marks from here to here i half seconds

1287
01:23:12,290 --> 01:23:14,540
it was the second in free fall

1288
01:23:14,560 --> 01:23:17,400
and it goes to zero that no way

1289
01:23:17,460 --> 01:23:19,360
now it hits the floor the cushions

1290
01:23:19,360 --> 01:23:20,920
and its weight goes up

1291
01:23:20,940 --> 01:23:23,210
in something like tens of seconds

1292
01:23:23,250 --> 01:23:24,880
look this is about one

1293
01:23:24,880 --> 01:23:28,630
two three it's about three and a half times its weight now

1294
01:23:28,650 --> 01:23:32,210
o to fourteen and-a-half has to be multiplied by three and a half before which

1295
01:23:32,210 --> 01:23:35,540
is exactly what we predicted that it would be much higher

1296
01:23:35,560 --> 01:23:36,840
but now it's being

1297
01:23:36,840 --> 01:23:41,000
it bounces off because it's very nice because throws it back up so it goes

1298
01:23:41,000 --> 01:23:44,230
back into the air so goes immediately to weightlessness again

1299
01:23:44,250 --> 01:23:46,460
and then it oscillates back and forth

1300
01:23:46,480 --> 01:23:47,750
and then here

1301
01:23:47,750 --> 01:23:52,770
you would expect that this level fourteen and-a-half problems will be the same as this

1302
01:23:52,860 --> 01:23:56,590
the only reason why that's not the case because little cable that fell with it

1303
01:23:56,900 --> 01:24:00,130
which is pushing a little bit up on the upper

1304
01:24:00,190 --> 01:24:04,150
on the upper this this this was making the life

1305
01:24:04,150 --> 01:24:06,790
isn't it incredible you see in front of you

1306
01:24:06,810 --> 01:24:08,130
the weightlessness

1307
01:24:08,130 --> 01:24:10,840
and you see the actual weight when it hits

1308
01:24:10,840 --> 01:24:12,790
and again followed by

1309
01:24:14,880 --> 01:24:20,000
eight plus you pass the course

1310
01:24:20,020 --> 01:24:22,770
there is great interest

1311
01:24:22,790 --> 01:24:27,500
in doing experiments under weightless conditions

1312
01:24:27,500 --> 01:24:30,110
massa was very interested in it

1313
01:24:31,690 --> 01:24:33,460
if you would jump

1314
01:24:33,480 --> 01:24:35,570
a hundred meters up in the sky

1315
01:24:35,590 --> 01:24:37,480
you would only be nine seconds out

1316
01:24:37,540 --> 01:24:40,630
you wouldn't even be weightless because of drag

1317
01:24:40,670 --> 01:24:42,460
however if you could jump up

1318
01:24:43,270 --> 01:24:45,090
the at the top of the atmosphere

1319
01:24:45,130 --> 01:24:47,310
ready and raag is negligible

1320
01:24:48,340 --> 01:24:49,480
you would be weightless

1321
01:24:49,500 --> 01:24:50,920
for quite some time

1322
01:24:50,980 --> 01:24:51,920
and that is

1323
01:24:51,940 --> 01:24:53,730
what people have been doing

1324
01:24:53,770 --> 01:24:58,460
for the past few decades professor young professor oldman here at the

1325
01:24:58,460 --> 01:25:00,460
aeronautics department

1326
01:25:00,480 --> 01:25:04,210
i have done what they call zero gravity experiments

1327
01:25:04,210 --> 01:25:08,080
i keep getting phone calls in the middle of the night

1328
01:25:08,090 --> 01:25:09,710
we have but i don't

1329
01:25:11,840 --> 01:25:20,480
like his six goals

1330
01:25:20,530 --> 01:25:24,750
i know

1331
01:25:32,600 --> 01:25:34,010
OK so

1332
01:25:34,050 --> 01:25:39,150
i'm really get ahead but just just decided to bring about more of graphical models

1333
01:25:39,230 --> 01:25:42,440
because i will use them a little bit in the process of

1334
01:25:42,510 --> 01:25:46,150
like this all be on the same page

1335
01:25:47,230 --> 01:25:50,870
one example that so there's three cases you need to

1336
01:25:51,980 --> 01:25:57,510
keep in mind

1337
01:25:57,540 --> 01:26:00,120
so we see in this case

1338
01:26:00,120 --> 01:26:01,880
we've seen case one

1339
01:26:03,330 --> 01:26:06,190
if it's cloudy

1340
01:26:06,200 --> 01:26:07,740
it rained

1341
01:26:07,760 --> 01:26:10,710
and you get wet

1342
01:26:11,540 --> 01:26:15,150
and independence here being that

1343
01:26:20,050 --> 01:26:22,990
of course clouds given that it's right

1344
01:26:23,010 --> 01:26:28,130
if you know it's raining you don't need to know whether it's class

1345
01:26:28,510 --> 01:26:34,210
another case there's two more cases you need to know about

1346
01:26:34,230 --> 01:26:38,050
of conditional independencies

1347
01:26:38,100 --> 01:26:44,050
and then there's a way of formalizing all this

1348
01:26:44,690 --> 01:26:50,900
we saw this case we've actually seen both of those cases included in the graph

1349
01:26:51,710 --> 01:26:57,930
in this case here say

1350
01:26:57,940 --> 01:27:00,090
well is rain

1351
01:27:00,130 --> 01:27:02,600
or a shower

1352
01:27:02,620 --> 01:27:04,520
you could get wet

1353
01:27:04,760 --> 01:27:11,790
this is NOT causality but again i emphasise that these are correlations the dependencies

1354
01:27:11,820 --> 01:27:18,450
OK for the causality immediate counterfactual you need to consider the other possible thing

1355
01:27:23,710 --> 01:27:29,210
suppose you're trying to decide whether it's raining or not

1356
01:27:29,900 --> 01:27:30,770
and you know

1357
01:27:30,800 --> 01:27:32,620
you observe that it's wet

1358
01:27:32,670 --> 01:27:35,420
so we shape indicating some search

1359
01:27:35,430 --> 01:27:38,430
so you know the state of what

1360
01:27:39,900 --> 01:27:42,930
you happen to be wet covered in water

1361
01:27:42,930 --> 01:27:44,450
is the rain

1362
01:27:45,250 --> 01:27:50,150
it would help to know whether they are in the shower or not

1363
01:27:51,180 --> 01:27:53,250
in this case

1364
01:27:53,400 --> 01:27:56,430
i started this should be independent

1365
01:27:56,430 --> 01:27:59,180
in this case you say that range

1366
01:27:59,220 --> 01:28:01,800
is dependent on shower

1367
01:28:01,840 --> 01:28:04,680
given what

1368
01:28:04,680 --> 01:28:08,400
OK and then the last example was a thing of the frogs

1369
01:28:08,420 --> 01:28:09,920
and wet

1370
01:28:09,960 --> 01:28:12,580
and range

1371
01:28:12,620 --> 01:28:16,560
and that basically says that

1372
01:28:16,580 --> 01:28:17,960
where it is

1373
01:28:17,960 --> 01:28:19,770
whether you went to not

1374
01:28:19,830 --> 01:28:22,300
does not depend on

1375
01:28:22,300 --> 01:28:24,430
the state of frogs out there

1376
01:28:24,470 --> 01:28:34,620
if you know it's raining

1377
01:28:34,680 --> 01:28:38,460
held perpendicular as

1378
01:28:40,650 --> 01:28:42,470
i'm just going to

1379
01:28:42,490 --> 01:28:46,050
read the find the symbols the symbols mean

1380
01:28:46,050 --> 01:28:51,560
a word is just the word in the words of

1381
01:28:53,490 --> 01:28:56,180
this meant independent

1382
01:28:56,300 --> 01:28:58,120
well that's true correct

1383
01:28:58,120 --> 01:28:59,900
and you know what i mean

1384
01:28:59,920 --> 01:29:01,450
that is

1385
01:29:01,460 --> 01:29:04,530
given are these guys are independent

1386
01:29:04,550 --> 01:29:07,580
given are these two guys are independent

1387
01:29:07,640 --> 01:29:11,680
but watch out for this case because this case is different

1388
01:29:16,970 --> 01:29:18,780
this find something called

1389
01:29:18,830 --> 01:29:23,420
and here is more like a typical graphical model which has many nodes

1390
01:29:23,470 --> 01:29:25,130
and it indicates

1391
01:29:25,150 --> 01:29:30,090
you know people construe strike this huge graphical models to tell you how variables dependent

1392
01:29:30,090 --> 01:29:34,020
variables and you learn the structure of this graphical models

1393
01:29:34,020 --> 01:29:37,350
and hopefully next features here but that you can learn the structure of the graphical

1394
01:29:37,350 --> 01:29:40,340
models to learn all the conditional probability tables

1395
01:29:40,400 --> 01:29:42,510
and you learn all of these things

1396
01:29:42,600 --> 01:29:47,400
and fulfil sort of for this type of network you can all be learned this

1397
01:29:47,400 --> 01:29:51,580
type of size larger networks you need to use some of the first techniques like

1398
01:29:51,580 --> 01:29:55,170
some of the ones mentioned here today

1399
01:29:55,170 --> 01:29:59,600
the training data and the training error what can we say about

1400
01:29:59,620 --> 01:30:02,900
and one of the search

1401
01:30:02,910 --> 01:30:06,300
the most amazing results in

1402
01:30:06,330 --> 01:30:11,030
you know the theory of computer science and applied statistics is the result that yes

1403
01:30:11,030 --> 01:30:12,960
you can actually say something

1404
01:30:12,970 --> 01:30:14,680
the test error

1405
01:30:14,680 --> 01:30:19,390
if you just have to training under the assumption that the training has come from

1406
01:30:19,390 --> 01:30:21,170
the same distribution

1407
01:30:21,360 --> 01:30:27,960
so that phenomenal should result in applied statistics that goes all the way back you

1408
01:30:27,960 --> 01:30:30,800
know how to do that

1409
01:30:30,820 --> 01:30:32,520
and even earlier and

1410
01:30:32,560 --> 01:30:36,410
i think that that's really you know started off

1411
01:30:36,420 --> 01:30:37,740
people thinking

1412
01:30:37,770 --> 01:30:40,410
along these lines the

1413
01:30:40,410 --> 01:30:43,190
concept here is what called generalisation

1414
01:30:43,210 --> 01:30:47,930
OK so what's danger danger in the above setup is that we're going to do

1415
01:30:47,930 --> 01:30:52,680
really well on the training data but very poorly on the testing and that

1416
01:30:52,730 --> 01:30:56,730
do well in the training program i is called overfitting

1417
01:30:56,740 --> 01:31:02,670
so for example there's something called the sure thing hypotheses for the mccarthy is credited

1418
01:31:02,740 --> 01:31:04,370
with coming this case

1419
01:31:04,370 --> 01:31:05,730
sure thing i

1420
01:31:05,780 --> 01:31:08,240
you memorize the training data

1421
01:31:08,250 --> 01:31:11,180
any produce garbage on the test

1422
01:31:11,180 --> 01:31:13,760
but i want to conclude that

1423
01:31:13,790 --> 01:31:16,210
you might want to go through

1424
01:31:16,210 --> 01:31:19,220
now that function is kind of ridiculous

1425
01:31:19,260 --> 01:31:22,310
right i need to memorize the is of course

1426
01:31:22,320 --> 01:31:26,610
the result of training data it ran out of the supplies terror test

1427
01:31:26,740 --> 01:31:30,200
so why should we care about is one thing that's scary about this function has

1428
01:31:30,220 --> 01:31:31,980
very low

1429
01:31:32,000 --> 01:31:34,260
los test time time

1430
01:31:34,270 --> 01:31:39,970
so if we are only guided thinking functions functions that have the training loss that

1431
01:31:39,970 --> 01:31:44,650
function should hypothesis and this will be confuse us and cause it to become very

1432
01:31:44,650 --> 01:31:46,750
stupid learning so

1433
01:31:48,240 --> 01:31:53,490
the idea here is that you can learn anything about the world without making assumptions

1434
01:31:53,540 --> 01:31:59,770
so this is formalized in many many ways including the so-called no free lunch theorems

1435
01:31:59,780 --> 01:32:01,300
by walter

1436
01:32:01,320 --> 01:32:04,150
was series have their problems

1437
01:32:04,730 --> 01:32:10,190
they indicate an important point which is that if you really believe april any function

1438
01:32:10,190 --> 01:32:13,760
in the world possible you've never learn anything

1439
01:32:13,790 --> 01:32:19,510
so the ability to achieve small loss on test data is called generalisation and that's

1440
01:32:19,530 --> 01:32:20,530
the real goal

1441
01:32:20,550 --> 01:32:24,230
is to build a machine that that good generalisation we can estimate the training very

1442
01:32:24,580 --> 01:32:27,920
important so

1443
01:32:27,930 --> 01:32:33,030
in order to be able to say something stronger guarantee the ability to generalize to

1444
01:32:33,030 --> 01:32:34,800
get low walls test there

1445
01:32:34,820 --> 01:32:37,630
we have to control something called the capacity

1446
01:32:37,630 --> 01:32:39,860
which is the complexity of the hypothesis

1447
01:32:39,880 --> 01:32:45,270
so learning is really searched hypothesis learning is all about trying to find the magic

1448
01:32:45,270 --> 01:32:51,470
setting about not fade which gives you know you lost it test

1449
01:32:51,580 --> 01:32:56,350
well how we evaluate test loss on test data so we can make this sampling

1450
01:32:56,490 --> 01:32:59,720
the test come from the same distribution

1451
01:32:59,880 --> 01:33:06,890
OK so here is the inductive learning hypothesis generalisation is possible

1452
01:33:06,890 --> 01:33:10,860
that's a very strong state

1453
01:33:10,860 --> 01:33:12,200
that statement said

1454
01:33:12,890 --> 01:33:17,740
you make assumptions about the world you can sometimes actually improve that you're not going

1455
01:33:17,740 --> 01:33:20,220
to that understand

1456
01:33:21,190 --> 01:33:24,390
so in a in words what inductive learning

1457
01:33:24,630 --> 01:33:27,540
this tells us that if machine performs well on most

1458
01:33:27,780 --> 01:33:35,830
training data and it's not too complex it will probably do well on similar

1459
01:33:37,120 --> 01:33:39,100
this statement can break down

1460
01:33:39,110 --> 01:33:41,170
in many ways

1461
01:33:41,190 --> 01:33:44,000
the is we can bring is you can fail

1462
01:33:44,020 --> 01:33:45,330
the first condition

1463
01:33:45,490 --> 01:33:51,020
you can trigger classifier and it can be really bad training data when they really

1464
01:33:51,020 --> 01:33:53,440
should be that surprised at that time

1465
01:33:53,440 --> 01:33:55,300
and if you do well the training data

1466
01:33:55,940 --> 01:33:57,850
that's the old people

1467
01:33:57,860 --> 01:34:01,820
most people trying to correct this problem but there's two more things you need to

1468
01:34:01,820 --> 01:34:06,210
correct if you want protect one is you need to make sure that it's not

1469
01:34:06,210 --> 01:34:07,710
too too complex

1470
01:34:08,570 --> 01:34:14,070
the sure thing hypothesis which memorized the entire training was very complicated right memorize all

1471
01:34:14,070 --> 01:34:16,020
the training sets of training

1472
01:34:16,130 --> 01:34:17,540
we can back to this

1473
01:34:17,550 --> 01:34:19,140
classifier trained

1474
01:34:19,140 --> 01:34:24,360
three strategic types so very complicated beast so so that's why i'm sure hypothesis is

1475
01:34:24,360 --> 01:34:30,530
that because it's not a simple and this is the third assumption have to satisfy

1476
01:34:30,530 --> 01:34:32,490
the test data has to be similar

1477
01:34:32,520 --> 01:34:35,930
you will be amazed how many people go to the world

1478
01:34:35,980 --> 01:34:38,650
the the world this and machine learning in the collective

1479
01:34:38,670 --> 01:34:39,630
training data

1480
01:34:39,640 --> 01:34:41,760
and they trained classifier make sure

1481
01:34:41,780 --> 01:34:43,350
very complicated and

1482
01:34:43,360 --> 01:34:48,130
training and then applied to test it's completely different than the training data

1483
01:34:48,150 --> 01:34:51,360
and then it doesn't work in this sort of stuff

1484
01:34:51,410 --> 01:34:56,080
the inductive learning hypothesis protecting what is going on well what's going

1485
01:34:56,180 --> 01:35:02,400
you're going to totally different aspects so make sure please when you go human beings

1486
01:35:03,190 --> 01:35:08,460
you actually need to train the classifier can do that too big but is not

1487
01:35:08,460 --> 01:35:11,710
too complicated you have to control and three

1488
01:35:11,720 --> 01:35:16,560
do a sanity check that you're testing there is somewhat similar to training data before

1489
01:35:16,650 --> 01:35:23,250
start playing these things but the amazing fact if you satisfy all these then you

1490
01:35:23,250 --> 01:35:27,360
right than the ones that don't matter so the best solution is the one that's

1491
01:35:27,730 --> 01:35:30,090
the ones that matter right i mean these

1492
01:35:30,150 --> 01:35:35,110
large i mean that if the best solution had failed

1493
01:35:35,160 --> 01:35:40,420
it would have been an entirely different result altogether these large

1494
01:35:40,470 --> 01:35:43,220
returns here

1495
01:35:43,270 --> 01:35:46,020
which tau them sort of got right

1496
01:35:46,060 --> 01:35:48,610
it has basically move the sharpe ratio up

1497
01:35:49,390 --> 01:35:52,870
that's the best solution we get

1498
01:36:05,050 --> 01:36:11,080
at the moment i do not know could well general regression analysis and stuff like

1499
01:36:11,080 --> 01:36:14,440
this there's the issue of spurious regressions

1500
01:36:14,460 --> 01:36:18,320
so it could be that i mean i think one thing we need to consider

1501
01:36:18,320 --> 01:36:19,600
as well

1502
01:36:20,050 --> 01:36:25,260
which which is basically a subject to future research is

1503
01:36:27,500 --> 01:36:31,060
that our two minus thirty five full-time

1504
01:36:32,530 --> 01:36:36,500
eight just random you know i just spurious

1505
01:36:55,080 --> 01:36:57,400
but what is

1506
01:37:00,860 --> 01:37:03,950
basically will even all

1507
01:37:03,970 --> 01:37:07,240
we everything is an illusionary

1508
01:37:07,260 --> 01:37:10,350
so what we do and as you have

1509
01:37:10,370 --> 01:37:12,770
a series of integers strings

1510
01:37:12,780 --> 01:37:15,160
you have a set of rules

1511
01:37:15,180 --> 01:37:17,460
which is like a grammar

1512
01:37:19,560 --> 01:37:22,670
you're trying to find the right sequence of integers strings

1513
01:37:22,680 --> 01:37:26,960
that maps to solutions like this

1514
01:37:26,970 --> 01:37:31,720
so in the article in its evolution basically have a modular structure

1515
01:37:31,760 --> 01:37:35,460
you have the space but search space you're working on the search space

1516
01:37:35,540 --> 01:37:37,910
two in crossover and mutation

1517
01:37:37,930 --> 01:37:39,660
and you have the solution space

1518
01:37:39,670 --> 01:37:42,300
so in the so suppose you got into strings

1519
01:37:42,350 --> 01:37:45,680
you produce integer representations of solutions

1520
01:37:45,690 --> 01:37:48,490
you work on integer representations trying to

1521
01:37:48,500 --> 01:37:50,450
find that

1522
01:37:50,460 --> 01:37:54,720
the production rules are basically is basically a lookup table

1523
01:37:54,730 --> 01:37:58,710
and you're trying to find the right sequence of integers that tells you

1524
01:37:58,760 --> 01:38:01,310
how to learn how to go through that lookup table

1525
01:38:01,330 --> 01:38:03,830
and produced that

1526
01:38:03,840 --> 01:38:06,820
which is like in the solution space

1527
01:38:07,850 --> 01:38:12,020
natural selection happens in the solution space

1528
01:38:13,620 --> 01:38:18,250
crossover and mutation and everything happens in the search space

1529
01:38:18,260 --> 01:38:22,090
so i mean the main ingredients are just returns we assuming here that we can

1530
01:38:22,090 --> 01:38:24,040
actually use past returns

1531
01:38:24,090 --> 01:38:27,210
and projects in the future it has which is well

1532
01:38:28,080 --> 01:38:31,820
most AI models and econometric models sort of

1533
01:38:31,870 --> 01:38:34,720
assume that there's some sort of autocorrelation

1534
01:38:35,910 --> 01:38:41,320
between past returns in future tense

1535
01:38:41,520 --> 01:38:47,070
what we see what we see here is basically a comparison between the accumulated return

1536
01:38:47,080 --> 01:38:48,460
achieved by

1537
01:38:48,600 --> 01:38:51,650
the best solution which i just showed you

1538
01:38:51,700 --> 01:38:55,990
and by the but buy-and-hold strategy in our model

1539
01:38:56,010 --> 01:39:00,740
and like i said what you'll notice this

1540
01:39:01,830 --> 01:39:05,460
know the solution produced by GE

1541
01:39:05,480 --> 01:39:09,840
i happen to be you know fit because it managed to predict

1542
01:39:09,850 --> 01:39:11,360
all these large

1543
01:39:14,540 --> 01:39:17,020
like right

1544
01:39:17,070 --> 01:39:18,310
all right

1545
01:39:18,330 --> 01:39:27,500
i'm not sure if i make myself

1546
01:39:27,520 --> 01:39:29,160
sharpe ratio

1547
01:39:29,210 --> 01:39:31,370
which is

1548
01:39:33,210 --> 01:39:41,050
which is basically the

1549
01:39:41,100 --> 01:39:44,290
so if are at our team

1550
01:39:44,310 --> 01:39:45,590
is the

1551
01:39:45,610 --> 01:39:47,910
right you know the actual return

1552
01:39:47,950 --> 01:39:50,190
this time interval

1553
01:39:50,210 --> 01:39:51,990
where prediction

1554
01:39:52,000 --> 01:39:53,270
our CEO

1555
01:39:53,280 --> 01:39:54,470
it's basically

1556
01:39:54,500 --> 01:39:57,170
the prediction from model

1557
01:39:58,260 --> 01:40:00,580
based on that production

1558
01:40:00,590 --> 01:40:05,260
if the prediction is greater than zero if we projects in return higher than zero

1559
01:40:05,280 --> 01:40:06,820
we go along

1560
01:40:06,840 --> 01:40:09,270
has you know the positive side

1561
01:40:09,310 --> 01:40:12,110
else we go short

1562
01:40:12,120 --> 01:40:16,570
so you know if if we if we if we get the prediction right then

1563
01:40:16,620 --> 01:40:18,530
will make positive returns

1564
01:40:18,580 --> 01:40:20,310
if it's enough the goes up

1565
01:40:20,350 --> 01:40:24,600
and we predict predicted go along you get positive return it goes down

1566
01:40:24,620 --> 01:40:28,380
you get negative return we predicted to go ashore short

1567
01:40:28,430 --> 01:40:31,570
negative negative cancels out you get positive

1568
01:40:39,870 --> 01:40:46,120
the graph g

1569
01:40:46,130 --> 01:40:48,570
this area

1570
01:40:55,670 --> 01:40:58,080
this is the best solution produced by g

1571
01:40:58,490 --> 01:41:00,590
what i mean here

1572
01:41:00,640 --> 01:41:04,500
what i did not mention this we assume and it's basically

1573
01:41:04,670 --> 01:41:11,560
the individuals must trade at every time interval which is like five ninety minute intervals

1574
01:41:11,560 --> 01:41:16,850
was human that we're in in a frictionless environment no slippage no transaction costs

1575
01:41:16,860 --> 01:41:19,780
what we try to do is we're trying to work from

1576
01:41:19,840 --> 01:41:25,520
skeleton and put some flesh on skeleton than have maybe something that actually works

1577
01:41:25,530 --> 01:41:29,210
would probably work in the real world environment

1578
01:41:29,230 --> 01:41:33,990
so i mean maybe maybe maybe like you

1579
01:41:34,040 --> 01:41:35,550
the sampler of the earlier

1580
01:41:35,560 --> 01:41:38,190
maybe sometimes the best

1581
01:41:38,210 --> 01:41:40,360
strategy is not to train

1582
01:41:40,770 --> 01:41:45,470
what we have is simplistic rules so maybe i mean the future research what we

1583
01:41:45,470 --> 01:41:49,950
might want to consider is subject in the decision rule to evolution as well so

1584
01:41:49,950 --> 01:41:51,670
you're coevolving to model

1585
01:41:51,690 --> 01:41:58,230
and the decision rule which is something we presented like somewhere else so the decision

1586
01:41:59,330 --> 01:42:02,170
is evolved as well as the model

1587
01:42:02,170 --> 01:42:05,460
so rather than say the return greater than zero

1588
01:42:07,180 --> 01:42:12,260
the return less than zero short maybe your subjective decision or a decision rule to

1589
01:42:12,260 --> 01:42:15,700
evolution as well so such that you've all something

1590
01:42:15,710 --> 01:42:16,860
that works well

1591
01:42:16,870 --> 01:42:17,470
you know

1592
01:42:17,470 --> 01:42:24,150
process prior as a prior over phi star vector this infinite length vector and a

1593
01:42:24,150 --> 01:42:30,070
prior over the parameters theta and we know that the phi star vector is given by this

1594
01:42:30,070 --> 01:42:35,990
GEM distribution our stick breaking construction and the thetas are basically IID draws from our

1595
01:42:35,990 --> 01:42:46,010
base distribution H and given phi and theta we know that G is of this

1596
01:42:46,010 --> 01:42:51,450
yeah G is of this form right and what that states that if we sample

1597
01:42:51,470 --> 01:42:56,430
a value from G that value will be equal to theta star K with probability

1598
01:42:56,430 --> 01:43:02,850
phi star K so we can think of this phi star K as basically the mixing proportion  of cluster

1599
01:43:02,850 --> 01:43:09,070
K and phi star K and theta star K is the parameters of cluster K

1600
01:43:09,070 --> 01:43:16,710
okay so you if we now come back to this if we sample ZI from

1601
01:43:16,710 --> 01:43:22,540
phi star K according to a distribute discrete distribution over phi star K so basically ZI

1602
01:43:22,550 --> 01:43:30,390
equals to value K with probability phi star K and data item XI given that

1603
01:43:30,390 --> 01:43:35,230
we know that it be that it belongs to  cluster ZI is gonna

1604
01:43:35,230 --> 01:43:42,250
be sample from a distribution parameterized by that parameter so this gives us our stick breaking

1605
01:43:42,270 --> 01:43:47,810
representation of the DP mixture model and this kind of makes explicit that this DP mixture

1606
01:43:47,810 --> 01:43:54,510
model is really just a mixture model but with the infinite number of components now

1607
01:43:54,550 --> 01:44:00,950
and so this will lead to kind of different sorts of samplers which were explored by

1608
01:44:00,950 --> 01:44:07,050
people down here  which are called conditional samplers and is conditional in the sense

1609
01:44:07,110 --> 01:44:15,120
that is basically standard Gibbs  sampling where we just you know compute the conditional probability of each

1610
01:44:15,120 --> 01:44:20,910
variable given the other variables sample from that conditional and then we sample we compute

1611
01:44:20,910 --> 01:44:25,290
the conditional distribution of phi star given Z and then we sample from phi star and

1612
01:44:25,290 --> 01:44:30,090
so forth we're always computing conditional distributions the nice thing with this is that it's

1613
01:44:30,090 --> 01:44:33,710
actually quite easy to work with non conjugate priors in the case of the

1614
01:44:33,710 --> 01:44:38,310
Chinese restaurant process because of the way that we need to deal with introducing new

1615
01:44:38,310 --> 01:44:43,050
clusters into the Chinese restaurant process it turns out to be a bit more complicated

1616
01:44:43,050 --> 01:44:48,270
if you need to deal with non conjugate priors okay in the case of a stick breaking representations

1617
01:44:48,270 --> 01:44:55,370
is basically very straightforward but for the sampler to work well we need to mix we

1618
01:44:55,370 --> 01:45:02,070
need to introduce moves for permuting the clusters okay so basically once

1619
01:45:02,070 --> 01:45:06,570
we use the stick breaking construction we have basically limited ourselves to a particular way of

1620
01:45:06,570 --> 01:45:13,530
ordering the clusters and we need to mix over the orderings for the sampler to for

1621
01:45:13,530 --> 01:45:19,570
the sampler to mix there's kind of another approach which is I cannot call it the explicit G

1622
01:45:19,570 --> 01:45:28,350
sampler which basically represents G explicitly okay so basically we alternatively sample

1623
01:45:28,350 --> 01:45:33,410
the set of so we just directly look at the this DP mixture model and we

1624
01:45:33,410 --> 01:45:38,350
sample and we we derive an algorithm that was sampled the theta given G

1625
01:45:38,710 --> 01:45:46,010
and G given theta for the theta given G it straightforward because

1626
01:45:46,010 --> 01:45:52,050
of the Polya urn scheme in the case but for the conditional distribution for G

1627
01:45:52,050 --> 01:45:56,680
given theta that's also straight-forward because we know that that conditional distribution is DP

1628
01:45:56,680 --> 01:46:03,570
is again a Dirichlet process because of conjugacy and we can work out that

1629
01:46:03,570 --> 01:46:07,810
this Dirichlet process here can always be written up can we always be written in this

1630
01:46:07,810 --> 01:46:13,590
form where phi zero and phi one to phi K this vector here is

1631
01:46:13,590 --> 01:46:20,090
Dirichlet distributed with parameters given by this and G prime is independent sample from the same

1632
01:46:20,100 --> 01:46:29,870
Dirichlet process in the prior okay for both the stick breaking representation and this explicit

1633
01:46:29,870 --> 01:46:39,030
G sampler one issue is that this G prime here you know this is

1634
01:46:39,030 --> 01:46:44,090
a atomic distribution with an infinite number of atoms so it  can't represent all infinitely

1635
01:46:44,090 --> 01:46:47,670
many atoms in our  computer so there has to be some way of kind of

1636
01:46:47,800 --> 01:46:52,410
doing this truncate  of truncating the number of clusters to a finite number to

1637
01:46:52,410 --> 01:46:56,810
be able to represent it on our computer and there are kind of a few ways of doing this

1638
01:46:56,810 --> 01:47:05,830
truncation  yeah which I it's kind of described in this papers anyways

1639
01:47:06,050 --> 01:47:10,450
there're of course lots of different inference algorithms for Dirichlet processes there's split-merge algorithms

1640
01:47:10,930 --> 01:47:17,350
where you look at the clustering and then you introduce Metropolis Hastings moves

1641
01:47:17,350 --> 01:47:22,310
that take a cluster splitting into two or take two clusters and merging it into one and

1642
01:47:22,310 --> 01:47:30,990
this sampler tends to if you combine this with the Gibbs sampling algorithm it tends to

1643
01:47:30,990 --> 01:47:37,190
converge a lot faster okay and this is close quite quite close in spirit to the reversible jump

1644
01:47:37,190 --> 01:47:42,770
CMC methods that Peter Green may have talked about last week there are also sequential

1645
01:47:42,770 --> 01:47:49,050
Monte Carlo samplers which I guess Arnold  talked about as well alright which is this

1646
01:47:49,050 --> 01:47:53,550
is this are basically iterative algorithms where you just have one single pass through your

1647
01:47:53,550 --> 01:47:59,450
data set and  each  each step of the algorithm you introduce a new

1648
01:47:59,450 --> 01:48:03,990
data item into into the model and then you assign it to one of the clusters

1649
01:48:03,990 --> 01:48:09,950
and you move on to the next data item there are also variational and kind expectation propagation

1650
01:48:09,950 --> 01:48:24,410
algorithms right any questions about yes personally

1651
01:48:24,410 --> 01:48:28,850
I think that the sampling algorithms in the case of the Dirichlet process actually

1652
01:48:28,850 --> 01:48:36,380
works really well  the variational algorithms the nice thing is that you could

1653
01:48:36,410 --> 01:48:40,390
it will converge to sum mode and then you could analyse that mode you don't have to

1654
01:48:40,390 --> 01:48:46,170
worry about the problem of label switching where the label of the clusters gets switched around

1655
01:48:46,170 --> 01:48:50,550
you don't have to worry about convergence because you know exactly when it converges when

1656
01:48:50,550 --> 01:48:55,670
the variation of parameters stopped moving so in that sense is kind of easy to

1657
01:48:55,670 --> 01:49:02,230
analyze but I find that when you look at predictive the predictive

1658
01:49:02,230 --> 01:49:07,910
ability of variational algorithms as opposed to sampling algorithms the samplers the sampling algorithms tend

1659
01:49:07,910 --> 01:49:14,050
to to do better  they're kind of easier to analyse but how the but worse at doing

1660
01:49:14,050 --> 01:49:30,010
predictions that's variational yeah yes that's right yes

1661
01:49:30,010 --> 01:49:37,490
well I guess you could try it on

1662
01:49:37,490 --> 01:49:41,530
kind of syntactic data sets where you know the clustering and then look at

1663
01:49:41,530 --> 01:49:45,970
the harmonies how long does it take to converge to the kind of to the neighborhood

1664
01:49:45,970 --> 01:49:50,870
of the true clusters you can also do things like kind of more

1665
01:49:50,870 --> 01:49:58,410
predictive sort of things right so give it analysis which is a clustering of your

1666
01:49:58,430 --> 01:50:01,750
data set you can then take that clustering and then do something else with that

1667
01:50:01,750 --> 01:50:11,570
so in the case of for example of  object recognition people often do clustering

1668
01:50:11,570 --> 01:50:17,010
it might have zero entries that means those two notes are not connected

1669
01:50:17,020 --> 01:50:19,570
and the entries are guaranteed to be nonnegative

1670
01:50:20,130 --> 01:50:25,140
so that's your w and the you create a diagonal matrix d d where they

1671
01:50:25,150 --> 01:50:32,100
element on the diagonal PII is simply the sum of the weights connected to node

1672
01:50:32,100 --> 01:50:37,510
i so that's the degree of node i see of this matrix and then the

1673
01:50:37,510 --> 01:50:39,970
laplacian in which i

1674
01:50:39,980 --> 01:50:43,680
you start here but i will also use l

1675
01:50:43,700 --> 01:50:46,460
OK so the laplacian matrix is the man to stop you

1676
01:50:46,480 --> 01:50:52,110
and the interesting thing is well in the graph energy we wanted to minimize is

1677
01:50:52,400 --> 01:50:55,720
have transposed w delta f

1678
01:50:55,770 --> 01:50:58,860
that's the quadratic form of it

1679
01:50:58,920 --> 01:51:01,850
and then we can write our

1680
01:51:02,570 --> 01:51:08,410
objective function as follows so again that funny infinity is really just constrained and then

1681
01:51:08,410 --> 01:51:11,610
you want to minimize the graph in

1682
01:51:14,010 --> 01:51:16,330
so the solution is simple

1683
01:51:16,340 --> 01:51:18,060
a matter of linear algebra

1684
01:51:18,490 --> 01:51:19,460
if we

1685
01:51:19,490 --> 01:51:21,510
arrange the notes

1686
01:51:21,530 --> 01:51:23,000
in such a way

1687
01:51:23,020 --> 01:51:29,290
so that the first l are labeled nodes and the remaining you are unable notes

1688
01:51:29,310 --> 01:51:35,040
i sort my nose this way that you can partition your matrix into four parts

1689
01:51:35,440 --> 01:51:40,350
to do have this label part and the one unable part and so on

1690
01:51:41,450 --> 01:51:44,570
then it's easy to see that

1691
01:51:44,570 --> 01:51:47,250
you're the solution

1692
01:51:47,270 --> 01:51:50,000
the factor on the unlabelled data portion is

1693
01:51:51,080 --> 01:51:55,600
which involves this block here but inverted

1694
01:51:55,620 --> 01:51:59,350
that block and then this is short given labels

1695
01:52:03,940 --> 01:52:06,130
you can also use

1696
01:52:06,170 --> 01:52:10,720
other forms of not bloodletting in place of this down here so you can use

1697
01:52:10,720 --> 01:52:12,400
the normalized version

1698
01:52:12,450 --> 01:52:18,950
or you can raise it some is power

1699
01:52:24,390 --> 01:52:30,320
there's another variation called local and global consistency yes

1700
01:52:31,920 --> 01:52:35,620
it has something to do with the

1701
01:52:35,810 --> 01:52:40,810
this is called the harmonic i think it's called harmonic property

1702
01:52:40,830 --> 01:52:43,350
that the

1703
01:52:45,050 --> 01:52:51,840
the function value is a weighted average of its neighbors

1704
01:52:57,850 --> 01:52:59,790
so let's look at this

1705
01:52:59,790 --> 01:53:01,390
this variation

1706
01:53:01,420 --> 01:53:06,270
the only difference is

1707
01:53:06,280 --> 01:53:10,860
that now we don't have this infinity here that we have some tuning parameter lambda

1708
01:53:12,010 --> 01:53:17,610
so this model was the fact that maybe you're given

1709
01:53:17,630 --> 01:53:21,260
labels on your small labels that it's not always

1710
01:53:21,270 --> 01:53:26,100
one hundred percent correct so you want to allow for some label noise

1711
01:53:28,140 --> 01:53:33,580
you want to say that now let's allow my prediction labelled data to be different

1712
01:53:33,580 --> 01:53:36,360
from the given label but i'm going to penalize it

1713
01:53:36,380 --> 01:53:47,420
OK at the same time i also want to minimize the graph energy

1714
01:53:48,680 --> 01:53:50,130
so far

1715
01:53:50,150 --> 01:53:52,870
these methods they work

1716
01:53:52,920 --> 01:53:57,010
but they are all trasductive

1717
01:53:57,010 --> 01:54:03,770
that means my as is only defined on this graph because it's just factor rate

1718
01:54:03,790 --> 01:54:09,170
is not defined at any point p and my given training set

1719
01:54:09,190 --> 01:54:13,460
so therefore if i want to predict new

1720
01:54:13,510 --> 01:54:17,610
that's a unseen test point i can not do that

1721
01:54:17,630 --> 01:54:19,600
i cannot do that easily

1722
01:54:19,660 --> 01:54:25,310
one way is to create a new graph by adding that new test point

1723
01:54:25,320 --> 01:54:28,670
you have to create some edges and then

1724
01:54:28,690 --> 01:54:30,200
some of the system

1725
01:54:30,210 --> 01:54:32,200
or you some heuristics

1726
01:54:32,260 --> 01:54:39,560
but so for example look at the test points neighbors but that's beyond what we

1727
01:54:39,560 --> 01:54:42,700
discussed so we want to find

1728
01:54:44,470 --> 01:54:47,610
inductive semi supervised

1729
01:54:47,690 --> 01:54:53,920
graph solution and manifold regularisation is one way to do that

1730
01:54:53,930 --> 01:54:56,610
OK so

1731
01:54:56,620 --> 01:55:02,930
here we start with an as that's define everywhere so here it defining some kernel

1732
01:55:03,020 --> 01:55:09,750
reproducing kernel space but don't worry about the details basically start from some after define

1733
01:55:09,790 --> 01:55:11,350
over the whole space

1734
01:55:12,140 --> 01:55:17,640
and then you also want to view this graph has already instantiation of

1735
01:55:18,130 --> 01:55:20,890
some underlying manifold

1736
01:55:22,510 --> 01:55:24,090
then what you do is this

1737
01:55:24,110 --> 01:55:27,450
turn this the prediction of your

1738
01:55:27,550 --> 01:55:34,320
a function on the labeled and unlabeled set look at that factor to minimize this

1739
01:55:39,560 --> 01:55:43,200
eventually you want to do something like this so OK so i'm going to explain

1740
01:55:43,200 --> 01:55:46,070
this later don't worry about that that's the hinge loss

1741
01:55:46,090 --> 01:55:50,360
but you could just be square loss or any other loss so this is the

1742
01:55:50,930 --> 01:55:52,500
labelled data fitting

1743
01:55:52,500 --> 01:55:57,590
term how well your efforts here labelled data and then you have

1744
01:55:57,610 --> 01:55:59,000
some norm

1745
01:55:59,050 --> 01:56:01,680
that regularizes your functions

1746
01:56:01,680 --> 01:56:07,290
when we make the decision of which class this image comes from which cluster of

1747
01:56:07,490 --> 01:56:12,830
comes from we just do it according to the bayes rule that the

1748
01:56:12,890 --> 01:56:19,200
the poster is proportional to the likelihood times the prior which is in general just

1749
01:56:19,290 --> 01:56:20,740
uniform distribution

1750
01:56:20,810 --> 01:56:26,160
and then we make the maximum decision on this on this

1751
01:56:26,180 --> 01:56:30,080
on this poster so that's the basic setup of her mother

1752
01:56:30,160 --> 01:56:36,370
it's very very simple but it's tested on how meaning can always think seven

1753
01:56:36,390 --> 01:56:42,000
seven categories and it actually was quite successful in terms of

1754
01:56:42,020 --> 01:56:44,990
how how well they do it

1755
01:56:45,020 --> 01:56:46,970
so what we do

1756
01:56:46,970 --> 01:56:54,680
two or more sophisticated model here we have several flavors we have the error corrected

1757
01:56:54,680 --> 01:56:56,890
told you this is really the

1758
01:56:56,930 --> 01:57:05,260
the starting point of all these latent topic models is the hoffman latent semantic analysis

1759
01:57:05,260 --> 01:57:11,470
model we call it pls and then we'll see the how related to originally allocation

1760
01:57:12,260 --> 01:57:15,910
that's a visual object categorisation

1761
01:57:16,740 --> 01:57:22,690
here i'll take this image ICCV o five paper has an case study were they

1762
01:57:22,690 --> 01:57:30,910
use the probabilistic latent semantic analysis model where here all describe to you the visual

1763
01:57:30,910 --> 01:57:36,930
meaning of these variables so d is a document response correspond to an image as

1764
01:57:36,930 --> 01:57:42,080
it here is the latent topic the they put it into

1765
01:57:42,080 --> 01:57:44,240
so test face versus bicycle

1766
01:57:44,260 --> 01:57:50,830
and then of course w here are patches of this of this image another flavor

1767
01:57:50,830 --> 01:57:58,520
is what i proposed using latent dirichlet allocation because of the weakness of this model

1768
01:57:58,520 --> 01:58:02,810
in my opinion is that course you have images like this

1769
01:58:02,870 --> 01:58:05,770
you can not assume there is only one topic

1770
01:58:05,770 --> 01:58:06,660
for this

1771
01:58:06,660 --> 01:58:13,620
for this for this image especially objects are embedded in backgrounds so in this model

1772
01:58:13,620 --> 01:58:21,930
where we actually know we we pooling contributions of different topics to specific category so

1773
01:58:21,930 --> 01:58:27,660
what we have here as in this paper in ICCV o five paper we used

1774
01:58:27,660 --> 01:58:33,580
it in a natural scene categorisation setting so it's a little different from objects but

1775
01:58:33,830 --> 01:58:40,700
very similar where the categorical among the label c is at the image level so

1776
01:58:40,700 --> 01:58:47,930
it's outside of the patch and then we have these parameters i think this is

1777
01:58:47,930 --> 01:58:51,350
the theta parameter in eric's lights we we

1778
01:58:51,350 --> 01:58:57,560
just label the pi which is mixing proportion of the topics the potential topics that

1779
01:58:57,560 --> 01:59:02,490
are in this image so we can say there's thirty percent water or twenty percent

1780
01:59:02,490 --> 01:59:07,290
rock and eighty percent sky o it has to add up to a hundred or

1781
01:59:07,290 --> 01:59:12,310
one and then each each patch

1782
01:59:12,330 --> 01:59:18,830
get to pick its own topic according to some kind of this distribution and of

1783
01:59:18,830 --> 01:59:21,100
course w is the fifth

1784
01:59:21,120 --> 01:59:30,620
actual patches this is the LDA model used for cattle categorizing visual images and then

1785
01:59:30,620 --> 01:59:34,200
the PLSA model is some some just going to show you

1786
01:59:34,620 --> 01:59:36,100
i think it's

1787
01:59:36,330 --> 01:59:45,100
and how we formulated here it's pretty simple we have for every for every document

1788
01:59:45,350 --> 01:59:49,930
we want to know the probability of the probability of this

1789
01:59:50,020 --> 01:59:55,580
image or document is the image sorry and it is actually

1790
01:59:57,270 --> 02:00:04,470
like eric it's basically a mixing makes mixture model of the topics so we have

1791
02:00:04,470 --> 02:00:10,660
the probability of the particular topic in this in this document or image and then

1792
02:00:10,660 --> 02:00:18,790
we have the probability of the words words in this a given this topic and

1793
02:00:18,810 --> 02:00:22,930
and they can be viewed in a very nice linear algebra way and on getting

1794
02:00:22,930 --> 02:00:24,540
into the details of this

1795
02:00:25,870 --> 02:00:27,580
and then what we want

1796
02:00:27,600 --> 02:00:33,000
o what the savage and his colleagues he did is to look at what of

1797
02:00:33,000 --> 02:00:40,120
the probabilities of different topics within this document and select the most probable one

1798
02:00:40,120 --> 02:00:46,100
and you can choose to do inference you can write down the likelihood of all

1799
02:00:46,100 --> 02:00:52,200
the training images and all the patches in this training pool and then use maximum

1800
02:00:52,200 --> 02:00:56,640
likelihood and then you can just use the same expectation maximisation

1801
02:00:56,910 --> 02:01:00,560
methods to iterate

1802
02:01:00,600 --> 02:01:07,140
between the parameters and arrive at at you're on values

1803
02:01:07,160 --> 02:01:08,770
so OK so

1804
02:01:08,770 --> 02:01:13,550
order will be never be visited twice so this one is an example of a

1805
02:01:13,550 --> 02:01:15,170
directed graph

1806
02:01:15,800 --> 02:01:21,540
this one i sorry directed acyclic graph and this is an example of a directed

1807
02:01:21,540 --> 02:01:26,490
acyclic graph because we added this link here from x four two x five so

1808
02:01:26,490 --> 02:01:29,290
for example the

1809
02:01:29,300 --> 02:01:31,100
the park x five

1810
02:01:31,120 --> 02:01:38,110
x seven x four x five will be the norm x five twice

1811
02:01:38,160 --> 02:01:43,680
OK so we made it to define our for this type of graphical model namely

1812
02:01:43,910 --> 02:01:45,120
the belief network

1813
02:01:45,240 --> 02:01:49,060
relief mentor is a directed acyclic graph in which

1814
02:01:49,070 --> 02:01:53,430
each node has associated the probability distribution of

1815
02:01:53,470 --> 02:01:55,230
the conditional distributions

1816
02:01:55,240 --> 02:01:59,330
of that node given its parents so for example if you look at the graph

1817
02:02:01,040 --> 02:02:05,930
and we do not eat it as aspiring to be and c and the four

1818
02:02:05,930 --> 02:02:11,620
these will have associated the conditional distribution of a given b and c

1819
02:02:11,620 --> 02:02:17,070
and a joint distribution of all the nodes in the graph is simply obtained by

1820
02:02:17,080 --> 02:02:20,190
taking the problem this condition

1821
02:02:24,170 --> 02:02:28,300
OK i give you a little bit of an example of all to use ability

1822
02:02:28,360 --> 02:02:32,680
make talking let's assume that sally

1823
02:02:32,690 --> 02:02:34,990
a burglar alarm is sounding

1824
02:02:34,990 --> 02:02:42,380
was the butler or was the alarm triggered by an earthquake sally to the radio

1825
02:02:42,380 --> 02:02:44,870
on for news of two

1826
02:02:44,890 --> 02:02:51,260
so in this problem we have four random variable a b e and are

1827
02:02:51,280 --> 02:02:57,750
representing the lower the burglar earthquake and radio and if we start writing down the

1828
02:02:57,750 --> 02:02:59,950
joint distribution of these

1829
02:03:02,370 --> 02:03:08,530
we consistently conditions the distribution they on the order side what you want to apply

1830
02:03:08,550 --> 02:03:12,790
by the joint distribution of in the is

1831
02:03:12,820 --> 02:03:14,990
we learn how to do it

1832
02:03:17,340 --> 02:03:19,730
similarly for this to you

1833
02:03:19,740 --> 02:03:23,250
we can condition of our on a and b

1834
02:03:23,270 --> 02:03:24,620
and multiply by

1835
02:03:24,640 --> 02:03:27,040
the distribution of a and b

1836
02:03:27,050 --> 02:03:31,600
and similarly for the last and so we obtain a factorisation of our

1837
02:03:31,660 --> 02:03:36,290
initial distribution and the belief network presentation of these is

1838
02:03:36,350 --> 02:03:38,580
even here in which for example

1839
02:03:38,590 --> 02:03:41,230
a despondent he

1840
02:03:41,240 --> 02:03:42,940
we are

1841
02:03:42,960 --> 02:03:45,330
and this corresponds to the standard

1842
02:03:45,350 --> 02:03:46,420
we see here

1843
02:03:46,440 --> 02:03:48,980
are assessed by e

1844
02:03:49,010 --> 02:03:54,160
and b this correspond to stand here and so i don't know if you think

1845
02:03:54,160 --> 02:03:59,090
about this problem with more carefully realized that

1846
02:03:59,110 --> 02:04:03,380
the large certaintly is not causing any

1847
02:04:03,390 --> 02:04:08,990
any broadcasting on on the radio and therefore we can say that

1848
02:04:10,660 --> 02:04:13,120
the radio is not causing

1849
02:04:14,230 --> 02:04:17,370
the lower and you know the reward

1850
02:04:17,380 --> 02:04:23,490
this distribution here is independent is independent of all of

1851
02:04:24,900 --> 02:04:27,920
and therefore we can remove this link

1852
02:04:28,240 --> 02:04:29,790
from our graphs

1853
02:04:29,820 --> 02:04:32,140
the delete from two to a

1854
02:04:32,160 --> 02:04:34,590
similarly this in a week

1855
02:04:37,400 --> 02:04:39,620
understand that

1856
02:04:39,630 --> 02:04:41,220
the burglary

1857
02:04:41,230 --> 02:04:42,170
is not

1858
02:04:43,870 --> 02:04:47,080
the iranian therefore we cannot remove these

1859
02:04:49,550 --> 02:04:56,280
valuable here and the associated link in the graph and finally also the burglar is

1860
02:04:57,120 --> 02:05:02,360
directly causing the quake and therefore we can remove it

1861
02:05:02,370 --> 02:05:07,620
the final link from p two p and we end up with what much simpler

1862
02:05:07,620 --> 02:05:11,130
type of believe it or

1863
02:05:11,480 --> 02:05:13,760
that they wrote down here

1864
02:05:13,770 --> 02:05:16,160
now we can use these

1865
02:05:16,170 --> 02:05:18,620
matter to make inference

1866
02:05:18,630 --> 02:05:21,040
for example we can ask

1867
02:05:21,050 --> 02:05:24,760
before actually doing that we need to specify

1868
02:05:24,830 --> 02:05:30,300
what if there were entries are for each conditional distribution that we have in our

1869
02:05:32,720 --> 02:05:36,870
so so for example we have to to define what the probability of a human

1870
02:05:36,870 --> 02:05:38,250
being the ease

1871
02:05:38,260 --> 02:05:40,860
and so then delete if they

1872
02:05:40,940 --> 02:05:44,010
but they but i was in the house and the walls

1873
02:05:44,150 --> 02:05:46,270
an quick then their lower

1874
02:05:46,320 --> 02:05:49,750
the probability that a lot of these something is very high

1875
02:05:49,750 --> 02:05:53,680
for everybody so there's no CU which tells you what is the right loss function

1876
02:05:53,710 --> 02:05:58,920
so this is really application dependent or user dependent

1877
02:05:59,210 --> 02:06:00,810
specify the loss function

1878
02:06:00,820 --> 02:06:02,370
and your goal is

1879
02:06:02,840 --> 02:06:04,690
to minimize loss

1880
02:06:04,740 --> 02:06:08,010
or if you use a negative sign maximise profit

1881
02:06:08,800 --> 02:06:11,170
you take your past observations

1882
02:06:11,350 --> 02:06:13,710
compute the loss

1883
02:06:13,780 --> 02:06:15,100
the expected loss

1884
02:06:15,110 --> 02:06:17,130
and try to minimize that

1885
02:06:17,150 --> 02:06:22,390
so what you can show is that in this predictive setting where your decision does

1886
02:06:22,390 --> 02:06:24,210
not influence

1887
02:06:24,210 --> 02:06:25,150
the future

1888
02:06:25,150 --> 02:06:27,950
this greedy minimisation of

1889
02:06:28,000 --> 02:06:33,670
the lost in the next time instances optimal so you don't need to look ahead

1890
02:06:34,020 --> 02:06:38,780
but it's important that the decision to influence environment

1891
02:06:38,800 --> 02:06:40,740
so here are some examples

1892
02:06:41,120 --> 02:06:43,770
of more generic loss functions

1893
02:06:44,050 --> 02:06:47,850
and the relation to the estimate for instance if you start with the square loss

1894
02:06:47,860 --> 02:06:49,540
and you do this minimisation

1895
02:06:49,560 --> 02:06:51,480
it's just the mean

1896
02:06:51,490 --> 02:06:53,470
not the absolute loss

1897
02:06:53,490 --> 02:06:54,420
then is

1898
02:06:54,420 --> 02:06:55,710
you get the media

1899
02:06:55,800 --> 02:06:58,440
if you start with the zero one loss

1900
02:06:58,480 --> 02:06:59,770
you get the mode

1901
02:06:59,850 --> 02:07:08,310
so there's a close relationship between some loss functions and some standard statistical estimators

1902
02:07:11,340 --> 02:07:16,500
if you actions influence environment so you have now this action here

1903
02:07:16,520 --> 02:07:18,400
and influence environment then you

1904
02:07:18,410 --> 02:07:22,880
o operations so these are the axis and these are the x and y

1905
02:07:22,890 --> 02:07:24,190
and then you

1906
02:07:24,350 --> 02:07:27,210
get into the enforcement learning

1907
02:07:28,240 --> 02:07:31,510
and things get much much much more complicated

1908
02:07:31,520 --> 02:07:36,390
because now

1909
02:07:36,390 --> 02:07:41,450
you need to be farsighted you can't really maximise your reward in the next time

1910
02:07:42,260 --> 02:07:44,470
this would lead to very bad behavior

1911
02:07:46,450 --> 02:07:50,410
very simple example is the bandit problem

1912
02:07:50,420 --> 02:07:54,240
there you have to go to casino

1913
02:07:55,590 --> 02:07:59,380
these bands of slot machines and you can put on the probability and you have

1914
02:07:59,380 --> 02:08:03,210
two of them and many probability is different

1915
02:08:03,280 --> 02:08:07,710
could i mean pull each then look for you when i mean

1916
02:08:07,740 --> 02:08:11,640
wait until you the first and then you look at one and then always take

1917
02:08:11,640 --> 02:08:12,490
this army

1918
02:08:12,540 --> 02:08:16,960
and it's clearly i mean not good study what you should used first explore some

1919
02:08:16,960 --> 02:08:20,730
political both arms makers statistics and then

1920
02:08:20,740 --> 02:08:24,920
ideally gradually switched to the slot machines which

1921
02:08:24,930 --> 02:08:26,690
seems better

1922
02:08:26,770 --> 02:08:30,540
once told that

1923
02:08:30,560 --> 02:08:35,760
research grants committee and looked at me and i can't sell that in this case

1924
02:08:35,760 --> 02:08:41,160
sorry to medical treatments treatment a or b you can either sector two more you

1925
02:08:41,160 --> 02:08:45,040
can read the two more and you have some success probabilities

1926
02:08:45,060 --> 02:08:47,570
and you don't know them and you want to lose as

1927
02:08:47,590 --> 02:08:51,450
if you like possible and but you have to explore a little bit so what

1928
02:08:51,450 --> 02:08:53,300
is the optimal way of doing that

1929
02:08:53,320 --> 02:08:56,470
and he was much much happier

1930
02:08:56,480 --> 02:09:03,500
but mathematically i mean it's exactly the same

1931
02:09:04,150 --> 02:09:08,240
yes i mean this simple case i mean it sounds really really simple

1932
02:09:08,260 --> 02:09:12,850
but there's a closed form solution but it's still a whole book

1933
02:09:12,880 --> 02:09:14,810
there's a whole book about it

1934
02:09:15,290 --> 02:09:18,490
and if it gets slightly more complex

1935
02:09:18,510 --> 02:09:20,770
you have to approximate the solution

1936
02:09:20,810 --> 02:09:24,390
and the solution turns out to be really i mean explore both at the beginning

1937
02:09:24,390 --> 02:09:27,440
equally and then you gradually shifted to the

1938
02:09:27,710 --> 02:09:29,170
two there are

1939
02:09:29,180 --> 02:09:33,480
well to the treatment which seems better

1940
02:09:34,140 --> 02:09:36,050
so that's the famous

1941
02:09:36,100 --> 02:09:40,900
exploration and exploitation problem

1942
02:09:41,040 --> 02:09:46,440
OK you some agent with some more details inside

1943
02:09:46,540 --> 02:09:48,210
i think

1944
02:09:53,720 --> 02:09:55,910
florida board

1945
02:09:58,540 --> 02:10:02,070
OK the previous slide explicitly specify the rewards

1946
02:10:02,110 --> 02:10:06,470
so you they more explicit so you have these agent influence environment you get some

1947
02:10:06,470 --> 02:10:10,610
observations but occasionally you get some reward signal

1948
02:10:11,910 --> 02:10:15,570
i mean he's not every time step but it can be zero and sometimes you

1949
02:10:15,570 --> 02:10:18,670
get reward points you win at chess game

1950
02:10:18,680 --> 02:10:21,520
for every one of the negative ones

1951
02:10:21,540 --> 02:10:24,760
and sri forced learning is concerned

1952
02:10:24,810 --> 02:10:28,710
with how an agent ought to take actions in an environment

1953
02:10:28,740 --> 02:10:34,150
so as to maximize its long term reward

1954
02:10:34,190 --> 02:10:40,940
if your environment is finite state MDP

1955
02:10:40,970 --> 02:10:42,990
and you know the MDP

1956
02:10:43,040 --> 02:10:44,270
i mean then

1957
02:10:44,290 --> 02:10:46,490
you can find solution easily

1958
02:10:46,540 --> 02:10:48,000
if you go beyond that

1959
02:10:48,010 --> 02:10:50,140
then it

1960
02:10:50,810 --> 02:10:54,920
nearly all cases gets already so even in the MDP case if you don't know

1961
02:10:54,920 --> 02:10:59,630
the MDP make some bayesian model and you make an average over mdps an MDP

1962
02:10:59,630 --> 02:11:04,480
anymore throughout the framework and it's difficult

1963
02:11:06,160 --> 02:11:08,660
i think it even more difficult

1964
02:11:08,920 --> 02:11:10,450
if you're

1965
02:11:10,470 --> 02:11:16,130
if you model i mean if you sort of passively multimedia environment some probability distributions

1966
02:11:16,140 --> 02:11:19,410
which may depend on the action but it's still not

1967
02:11:19,420 --> 02:11:21,790
really symmetric but in games

1968
02:11:22,120 --> 02:11:26,400
your opponent is also play also learns

1969
02:11:26,450 --> 02:11:31,400
well phrased differently is also quite complex probability distribution your own

1970
02:11:34,050 --> 02:11:37,170
the new layer can learn through self play

1971
02:11:37,210 --> 02:11:41,110
o point is like and became an autosomal chequer programme but

1972
02:11:41,310 --> 02:11:47,310
there's not even a theoretical optimal solution i mean the people don't even know what

1973
02:11:47,490 --> 02:11:48,510
it means

1974
02:11:48,520 --> 02:11:50,940
i mean this the concept of nash equilibria

1975
02:11:50,940 --> 02:11:54,250
but in this equilibrium can be quite bad

1976
02:11:54,270 --> 02:11:55,700
for instance

1977
02:11:55,700 --> 02:11:57,460
in the prisoner's dilemma

1978
02:11:58,320 --> 02:11:59,890
so you have to

1979
02:12:00,320 --> 02:12:01,720
victims who

1980
02:12:01,770 --> 02:12:03,890
rob the bank or so

1981
02:12:03,950 --> 02:12:05,400
and they got caught

1982
02:12:06,170 --> 02:12:07,700
they could either

