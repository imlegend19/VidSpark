1
00:00:00,000 --> 00:00:06,990
affects the tried to was to suppressed by adding the signals to the confounders and

2
00:00:07,130 --> 00:00:09,090
had to report the

3
00:00:10,600 --> 00:00:15,040
the KT and how to get clean EEG but also has an influence on the

4
00:00:15,130 --> 00:00:17,470
from right so we also use it in that respect

5
00:00:19,050 --> 00:00:24,310
now this is 1 of the subjects that are assumed to be applied all the

6
00:00:24,310 --> 00:00:29,430
methods so do you very correlated EEG and the from right and you see here

7
00:00:29,430 --> 00:00:31,190
that the subject correlation is

8
00:00:31,670 --> 00:00:38,740
larger than boards 3 and actually this is not what I should use more statistically

9
00:00:38,740 --> 00:00:44,500
statistical threshold but this this correlation means that the the the false detection rate very

10
00:00:44,500 --> 00:00:50,780
very low and this year the occipital lobe was highly correlated and also the parietal

11
00:00:54,250 --> 00:00:59,250
if we a look at all the other subjects we see that the amount of

12
00:00:59,250 --> 00:01:06,430
correlation varies a lot of here again I used this threshold of important to throw

13
00:01:06,430 --> 00:01:08,290
in some subjects that this this

14
00:01:08,300 --> 00:01:13,040
thresholds in the whole brain is correlated at some other subjects there is not so

15
00:01:13,040 --> 00:01:18,980
much the correlation of forces the correlation is not a measure which all these constantly

16
00:01:19,000 --> 00:01:26,740
compared to what we should do is to use false detection and to have a

17
00:01:26,740 --> 00:01:31,220
lot to do with the same false detection rates for these these images could be

18
00:01:31,220 --> 00:01:36,320
compared with this slide is only to show you that there's a very large variations

19
00:01:36,650 --> 00:01:41,780
of subject if we look at the amount of correlation between alphabet

20
00:01:43,010 --> 00:01:50,610
and from right here and in the light of day this beach and so full

21
00:01:50,750 --> 00:01:56,230
all of of these holes in the lobby of various lived experience that we have

22
00:01:56,230 --> 00:02:01,750
used is the average of all the occiput lead and we have explored also the

23
00:02:02,180 --> 00:02:09,530
reconstructed reconstructed source for using of elites but that doesn't make very much a difference

24
00:02:09,990 --> 00:02:16,030
nowadays season even video it of all people are will show you something that that

25
00:02:16,030 --> 00:02:21,090
that's very robust and even if you do something completely different you still get the

26
00:02:21,090 --> 00:02:27,590
same thing now what you can do because we have estimated the hemodynamic responses from

27
00:02:27,590 --> 00:02:32,620
the data itself instead of using standard family response functions we can also studied and

28
00:02:32,620 --> 00:02:37,850
remembered responses so this part of the brain is the representing the thalamus we see

29
00:02:37,850 --> 00:02:42,530
that the government response look like this so it's positive and negative and goes to

30
00:02:42,530 --> 00:02:49,370
0 these are the non-causal correlation so that you see that there not significant if

31
00:02:49,370 --> 00:02:55,650
you tested more formally then indeed you find that that these points are not significant

32
00:02:55,750 --> 00:03:00,290
and you see for more adept at the periphery of the occipital lobe the

33
00:03:00,480 --> 00:03:02,870
and remember responses or negative starting the

34
00:03:03,060 --> 00:03:09,210
effect at the center responses are negative the implies that the more alpha rhythm is

35
00:03:09,210 --> 00:03:13,580
produced locally the less energy is locally consumed and that the tool for the whole

36
00:03:13,580 --> 00:03:18,080
cortex except for the thalamus that's the other way around so you could think of

37
00:03:18,080 --> 00:03:18,970
that these new

38
00:03:19,060 --> 00:03:25,720
generator of the algorithm is that the thalamus and uh that the regions of the

39
00:03:25,720 --> 00:03:31,450
cortex mm the follows the use of these oscillations

40
00:03:31,480 --> 00:03:39,720
it's not a very good explanation actually but actually I think nobody understands what are

41
00:03:39,720 --> 00:03:48,720
probabilities of what the collision is for the negative or positive

42
00:03:50,230 --> 00:03:55,690
well we also analyze that for all subjects instantly so now we will look at

43
00:03:55,690 --> 00:04:02,450
for predefined points left right warned right parental portal triple point and that's elements there

44
00:04:02,450 --> 00:04:05,770
were there were selected them

45
00:04:07,270 --> 00:04:12,310
the that that ball those points the the the false detection rate was determined if

46
00:04:12,310 --> 00:04:17,450
that will smaller than 1 % then the recent there is and there is an

47
00:04:17,450 --> 00:04:24,450
activity going on there and if that also about that threshold then we put now

48
00:04:24,450 --> 00:04:30,310
here's itself off to 16 subjects you find correlation of the occipital point and that

49
00:04:30,310 --> 00:04:36,890
of the common sense of this stuff the left and right throughout the points also

50
00:04:39,650 --> 00:04:44,270
and if you look at the hemodynamic responses of different subjects that we see despite

51
00:04:44,280 --> 00:04:49,410
the fact that the amount of correlation the total correlation patterns very very much of

52
00:04:49,410 --> 00:04:54,930
a subject we see that that if you look at the acceptable born the all

53
00:04:54,930 --> 00:05:01,130
of the government responses to traders and also the telomeres thereby face extensive involved is

54
00:05:01,130 --> 00:05:05,090
1 subject was a little bit more wild and see also that the left and

55
00:05:05,090 --> 00:05:14,030
right frontal cortex the members forms also look very similar there's only 1 exception that's

56
00:05:14,040 --> 00:05:21,150
the subjects who has a positive their correlations at the cortex and that subject have

57
00:05:21,170 --> 00:05:26,230
recorded another time and again we found this this positive correlation

58
00:05:27,330 --> 00:05:33,870
so I think there still is some antiseptics variability and priorities have government responses is

59
00:05:33,870 --> 00:05:39,050
positive instead of negative that is that the what don't know

60
00:05:42,180 --> 00:05:46,610
not the sort of more interest to give it a bit more detail about what

61
00:05:46,960 --> 00:05:51,210
the the shape of the response if you have if you consider the government response

62
00:05:51,220 --> 00:05:56,910
itself as we have determined it that you can only study at this time resolution

63
00:05:57,290 --> 00:06:04,290
of 3 seconds because that's the acquisition of the of the memorize and to interpolate

64
00:06:04,750 --> 00:06:12,770
the functions we used new parametrization so deposit this parameterization restricted to the to the

65
00:06:12,770 --> 00:06:13,950
hemodynamic responses

66
00:06:15,430 --> 00:06:17,930
not to directly on the

67
00:06:18,110 --> 00:06:21,050
component the model

68
00:06:22,210 --> 00:06:25,350
and then we get to the top of this kind of

69
00:06:25,580 --> 00:06:30,890
examples so this is the thalamus received modification of signal and now we can do

70
00:06:30,890 --> 00:06:36,820
much more precisely the determine the peak times of the year of the hemodynamic responses

71
00:06:38,590 --> 00:06:44,830
now if you compare for all these 4 points of interest all the subjects of

72
00:06:45,090 --> 00:06:50,450
the peak times of the occipital in the and the prior to and 4 and

73
00:06:50,450 --> 00:06:52,990
introducing actually in new approach

74
00:06:52,990 --> 00:06:56,200
that learns an outlier us come filter

75
00:06:56,210 --> 00:07:02,930
and this is work done in collaboration with the bank of doris and stefan schaal

76
00:07:02,960 --> 00:07:06,460
also at USC

77
00:07:06,510 --> 00:07:10,100
so just to start this is the outline of my talk first going to start

78
00:07:10,100 --> 00:07:11,830
by motivating

79
00:07:12,280 --> 00:07:16,780
why were interested in producing an outlier robust come filter

80
00:07:16,830 --> 00:07:18,740
i'll quickly

81
00:07:18,760 --> 00:07:22,120
review what the common filter is briefly

82
00:07:22,130 --> 00:07:26,090
and then i will introduce our method which is essentially

83
00:07:26,150 --> 00:07:30,080
a new way of doing robust coming filtering by learning

84
00:07:30,410 --> 00:07:32,500
beige weights

85
00:07:32,510 --> 00:07:34,830
and i'll finish up by

86
00:07:34,840 --> 00:07:43,620
presenting our results on an experimental data set and conclude

87
00:07:45,150 --> 00:07:47,400
we consider

88
00:07:47,810 --> 00:07:54,420
real time applications where storing data samples may not be a viable option either due

89
00:07:55,110 --> 00:07:57,890
the high frequency of incoming sensory data

90
00:07:58,730 --> 00:08:03,010
insufficient or limited memory resources that you would have

91
00:08:03,060 --> 00:08:05,840
so for example in systems where

92
00:08:05,890 --> 00:08:10,720
the quality of sensory data needs to be high

93
00:08:10,750 --> 00:08:12,700
reliable and robust

94
00:08:12,720 --> 00:08:18,530
detection of outliers and removal of outliers is pretty essential for optimal performance

95
00:08:18,530 --> 00:08:24,390
so for example i can give you an in illustrating applications in robotics which is

96
00:08:24,390 --> 00:08:28,140
what our lab deals with and if you want to do

97
00:08:28,190 --> 00:08:30,330
legged locomotion

98
00:08:30,360 --> 00:08:33,750
and you'll see this in the last few slides the top we're actually show you

99
00:08:33,750 --> 00:08:36,730
results on a robot dog

100
00:08:37,940 --> 00:08:41,700
and undetected outlier in your incoming sensory data

101
00:08:41,700 --> 00:08:46,750
can potentially disturb the balance control that you have on the robotic dog

102
00:08:46,780 --> 00:08:51,580
such that the dog loses stability which is something that you don't want

103
00:08:51,590 --> 00:08:56,950
so in such scenarios where real time tracking is necessary to come filter

104
00:08:56,970 --> 00:08:58,090
is a

105
00:08:58,150 --> 00:09:00,470
very simple method for doing this

106
00:09:00,940 --> 00:09:02,620
it's in fact the

107
00:09:02,650 --> 00:09:08,470
the optimal linear estimator for a gushing system that gives the minimum mean squared error

108
00:09:08,470 --> 00:09:10,870
and it's often used to

109
00:09:10,900 --> 00:09:15,280
projects are estimated what the hidden states are

110
00:09:15,340 --> 00:09:16,870
given noisy measurements

111
00:09:16,870 --> 00:09:23,410
so you can use hidden state estimates to then and also calculate what the measurement

112
00:09:23,470 --> 00:09:25,410
do you should be

113
00:09:25,470 --> 00:09:29,870
now the problem of the color filter though it it's not robust to outliers

114
00:09:29,910 --> 00:09:35,790
and in fact it integrates its performance degrades in the presence of outliers

115
00:09:35,810 --> 00:09:40,090
so let's look at what previous methods have have tried to do too two

116
00:09:40,120 --> 00:09:42,150
addresses this problem

117
00:09:42,560 --> 00:09:45,150
so i just missed really briefly

118
00:09:45,160 --> 00:09:46,840
and the top half of the fly

119
00:09:46,850 --> 00:09:50,410
some of the matter out there that have been developed in the past forty fifty

120
00:09:51,690 --> 00:09:54,680
and the issues that we have with them

121
00:09:54,690 --> 00:10:00,910
so first of all for example you can their methods that consider nongaussian distributions to

122
00:10:00,910 --> 00:10:06,220
model the random variables so the reason why non gas distribution to use it because

123
00:10:06,310 --> 00:10:08,090
they tend to be heavy tailed

124
00:10:08,100 --> 00:10:11,500
so long red sites and

125
00:10:11,560 --> 00:10:14,040
hence less sensitive to

126
00:10:14,060 --> 00:10:16,590
you know the mean squared error

127
00:10:16,600 --> 00:10:22,400
another option is to model your noise your state norms and you observed notice as

128
00:10:22,400 --> 00:10:25,340
nongaussian his tail distributions as well

129
00:10:25,370 --> 00:10:29,310
so an example of such a distribution p the student t distribution

130
00:10:29,310 --> 00:10:33,400
with a little bit flatter has heavier tails

131
00:10:33,410 --> 00:10:38,990
other methods also use sampling techniques for integration techniques and outside the few pieces of

132
00:10:38,990 --> 00:10:41,020
work by kitty to go up

133
00:10:41,030 --> 00:10:45,430
and another approach which is a little bit different uses

134
00:10:45,780 --> 00:10:47,990
a weighted least squares approach

135
00:10:49,690 --> 00:10:53,780
this is something that we you see robustly squares and the idea is quite simple

136
00:10:53,780 --> 00:10:59,500
you basically take each sample that you observe and associate away with it

137
00:10:59,530 --> 00:11:05,440
and you know most techniques would actually model this weighted sum the error function

138
00:11:05,750 --> 00:11:08,130
for example the q function i say that

139
00:11:09,850 --> 00:11:12,160
basically this we associate with

140
00:11:12,200 --> 00:11:16,460
each data sample indicates sort of its contribution to it

141
00:11:16,470 --> 00:11:18,910
it's how much of an outlier this

142
00:11:18,970 --> 00:11:19,890
to the

143
00:11:19,900 --> 00:11:21,530
to the filter so

144
00:11:21,530 --> 00:11:26,580
a lower windsor zero which indicate that it's more of about outlier in higher weight

145
00:11:26,580 --> 00:11:27,660
i mean

146
00:11:27,680 --> 00:11:28,940
so start

147
00:11:28,990 --> 00:11:30,590
do you have an announcement

148
00:11:43,510 --> 00:11:44,730
so here is now

149
00:11:44,740 --> 00:11:45,660
you know

150
00:11:45,750 --> 00:11:48,390
a slightly more interesting model

151
00:11:48,410 --> 00:11:51,310
to come up with this probability estimates OK

152
00:11:51,700 --> 00:11:53,420
to do this smoothing

153
00:11:53,440 --> 00:11:54,620
OK so

154
00:11:56,990 --> 00:12:00,680
what we will do in this model and the probabilistic latent semantic analysis model is

155
00:12:00,680 --> 00:12:03,510
you know we have all these documents we have all these terms

156
00:12:03,520 --> 00:12:08,010
we should use an intermediate layer if you like

157
00:12:08,030 --> 00:12:09,880
of latent concepts

158
00:12:10,880 --> 00:12:13,390
and will then try to estimate for each

159
00:12:13,410 --> 00:12:18,060
document a distribution over the latent concepts

160
00:12:18,080 --> 00:12:19,620
OK so say

161
00:12:19,630 --> 00:12:21,720
however we need to

162
00:12:21,750 --> 00:12:22,610
refer to

163
00:12:23,440 --> 00:12:27,630
as of parameter you know we just call it either OK but it's really just

164
00:12:27,690 --> 00:12:33,540
in saturated probability distribution case for every document every concept there will be approach you

165
00:12:33,540 --> 00:12:37,400
know i'm number nonnegative number which corresponds to probability

166
00:12:37,540 --> 00:12:41,110
and then for the terms are concerned you know we can

167
00:12:41,120 --> 00:12:45,000
private like this doesn't really matter but you know there is a probability

168
00:12:45,020 --> 00:12:47,850
that particular word will express

169
00:12:47,860 --> 00:12:51,740
the concept is the same because this

170
00:12:51,750 --> 00:12:54,340
you know these parameters pi

171
00:12:54,380 --> 00:12:58,320
OK and so the idea is something that

172
00:12:58,340 --> 00:13:02,200
you know in order to deal with variations in the level of the terms

173
00:13:02,210 --> 00:13:07,190
that you know there are these concepts you like trade right so this is one

174
00:13:07,190 --> 00:13:10,530
concept we could figure out that the document is with the concept trades

175
00:13:10,550 --> 00:13:13,470
and then figure out the trade

176
00:13:13,470 --> 00:13:17,570
can be expressed you know that concept by the the term trade but also by

177
00:13:17,580 --> 00:13:22,490
things like imports and economic and knows what all these things actually relates to trade

178
00:13:22,490 --> 00:13:23,750
so if we see

179
00:13:23,780 --> 00:13:25,210
something like imports

180
00:13:25,220 --> 00:13:29,130
it's kind of an indication for this concept

181
00:13:32,220 --> 00:13:36,420
so then you know so that's basically the structure of the model

182
00:13:37,800 --> 00:13:43,490
now these concept expression probabilities right on the right we can estimate based on all

183
00:13:43,490 --> 00:13:47,870
documents that are dealing with a particular concept right so we can use the whole

184
00:13:47,870 --> 00:13:51,510
document collection to do that and

185
00:13:51,520 --> 00:13:53,710
and in a way you can also think of this as kind of an odd

186
00:13:53,740 --> 00:13:55,170
mixing up with them

187
00:13:55,180 --> 00:14:00,080
where each document is kind of composed of multiple topics are concepts

188
00:14:00,090 --> 00:14:03,590
right and sort of we trying to figure out you know what are these pure

189
00:14:03,590 --> 00:14:06,050
concepts like to what we observe

190
00:14:06,060 --> 00:14:11,020
in real life is always something that a mixture of multiple concepts but now we

191
00:14:11,040 --> 00:14:12,300
we assume that

192
00:14:12,460 --> 00:14:16,430
we can somehow you know we have a million of these mixes that somehow we

193
00:14:16,430 --> 00:14:22,330
can perhaps find a couple of hundred per couple of thousands of these pure concepts

194
00:14:22,360 --> 00:14:26,770
and then explain the the documents that we see is a mix of these

195
00:14:26,960 --> 00:14:28,610
pure content

196
00:14:28,610 --> 00:14:33,740
so if we write down algebraically

197
00:14:34,520 --> 00:14:36,020
we can write it like this

198
00:14:36,050 --> 00:14:41,460
the only thing is that actually i switch status and the policy of

199
00:14:41,480 --> 00:14:43,340
it's not a big deal

200
00:14:43,670 --> 00:14:47,640
so this is i think what i would use this in the following so

201
00:14:47,650 --> 00:14:49,230
it's a mixture model

202
00:14:49,240 --> 00:14:52,620
right so the probability of word given document

203
00:14:53,200 --> 00:14:58,730
we write as sum over the concepts and the probability of that concert given the

204
00:14:58,730 --> 00:15:02,270
document the probability of the word given the concepts rights

205
00:15:02,300 --> 00:15:05,760
so that's what graphically we mean by being in the middle right so and then

206
00:15:05,760 --> 00:15:06,540
we some out

207
00:15:06,950 --> 00:15:08,680
that latent variable

208
00:15:08,700 --> 00:15:11,790
it's like in concert very right so we have the document language model to the

209
00:15:11,790 --> 00:15:17,400
left the latent concept is expression probability that the document specific mixture proportions if you

210
00:15:18,920 --> 00:15:24,080
so every document has specific mixture proportions in

211
00:15:24,870 --> 00:15:26,670
these probabilities

212
00:15:26,700 --> 00:15:31,680
we have to estimate some model fitting like

213
00:15:31,900 --> 00:15:37,040
and let me come back to what we saw before the break in terms of

214
00:15:37,040 --> 00:15:39,890
matrix factorizations

215
00:15:39,920 --> 00:15:42,630
right you can actually rewrite

216
00:15:42,860 --> 00:15:48,920
you could you can kind of take that model and also introduces probability for each

217
00:15:48,920 --> 00:15:50,840
document is basically

218
00:15:51,010 --> 00:15:52,680
the document length

219
00:15:53,050 --> 00:15:57,180
in that case here and you can actually write it in the symmetric form can

220
00:15:57,180 --> 00:16:02,410
just by reversing this probability here using bayes rule so you get this

221
00:16:02,430 --> 00:16:04,450
and this actually

222
00:16:04,480 --> 00:16:07,620
you know this farm you can actually think

223
00:16:07,630 --> 00:16:12,860
of this as as a matrix so it's whatever we have and documents a and

224
00:16:12,860 --> 00:16:16,870
q concept it would be an n by q matrix and this you would be

225
00:16:17,420 --> 00:16:23,620
and MIQ matrix m terms and again q concepts and so

226
00:16:23,680 --> 00:16:29,610
you know it corresponds to the composition where you know this this thing matrix here

227
00:16:29,730 --> 00:16:35,670
a kind of the document probabilities and this over here would be termed probabilities

228
00:16:36,610 --> 00:16:38,010
one entry here

229
00:16:38,030 --> 00:16:39,590
right would be

230
00:16:39,610 --> 00:16:45,340
concept hugh what is the probability of a particular term expressing that concept

231
00:16:45,370 --> 00:16:50,180
and then you can see that this mixture model here really just for you know

232
00:16:50,180 --> 00:16:53,530
you can just write it if you if you encode things as matrices just the

233
00:16:53,530 --> 00:16:55,200
matrix multiplication and you have

234
00:16:55,400 --> 00:16:59,200
basically this a diagonal matrix in the middle which is the

235
00:16:59,220 --> 00:17:05,600
the concept probability something different concept might have different probabilities of occurring prior probability

236
00:17:06,740 --> 00:17:11,670
and so there's actually relationship also which i can really show in detail here with

237
00:17:11,670 --> 00:17:17,340
what's called a nonnegative matrix factorisation algorithm by on which

238
00:17:17,360 --> 00:17:22,280
kind of independently developed of our work because of course you know if you look

239
00:17:22,280 --> 00:17:23,440
at the the entries

240
00:17:24,540 --> 00:17:28,240
you know these are all nonnegative entries because these probabilities

241
00:17:28,310 --> 00:17:34,170
right so in particular this is not the solution is not given by something like

242
00:17:34,170 --> 00:17:36,610
a singular value decomposition or anything like that

243
00:17:37,280 --> 00:17:42,130
you know we have we have different constraints here's a probabilistic

244
00:17:45,810 --> 00:17:47,370
so now how do we

245
00:17:48,540 --> 00:17:50,130
the models

246
00:17:50,130 --> 00:17:56,390
and you said no in the observational regime we could have independent dependence between the

247
00:17:56,390 --> 00:18:05,130
treatment and the extra variables and that that is the problem of compact confound basically

248
00:18:05,160 --> 00:18:10,130
so remember that there is that we can introduce the potential response the response to

249
00:18:10,130 --> 00:18:12,140
be supplied treatment f

250
00:18:12,170 --> 00:18:17,880
it is just y to define this way if i think about interventional study

251
00:18:17,930 --> 00:18:22,450
i'm just gonna look at the very i you will have whatever distribution it's got

252
00:18:22,450 --> 00:18:26,520
and this will give me distribution for y as justified so y one f if

253
00:18:26,540 --> 00:18:30,800
one of you know everyone and you will have to work out its distribution

254
00:18:30,810 --> 00:18:35,890
so i got the two distributions no problem what actually go but i haven't done

255
00:18:35,890 --> 00:18:42,260
experimental study done observational study so i observe t and y

256
00:18:42,270 --> 00:18:44,720
what is the distribution of y

257
00:18:44,720 --> 00:18:46,880
in the say given t equals one

258
00:18:46,890 --> 00:18:52,590
so here is that it's one of the distribution of this function when i tell

259
00:18:52,590 --> 00:18:56,670
you that it was one or first of all i can obviously change t into

260
00:18:58,180 --> 00:19:02,680
but secondly and conditioning on was one and that fixed value so i to the

261
00:19:02,730 --> 00:19:06,220
conditional distribution of you guarantee was one

262
00:19:07,520 --> 00:19:11,710
so the observation of this rich y given tickles t would generally have this form

263
00:19:11,710 --> 00:19:16,170
but with a different distribution for you instead of the marginal distribution for you which

264
00:19:16,170 --> 00:19:19,170
is relevant to of interventional regime

265
00:19:19,220 --> 00:19:24,040
i would have to look at the conditional distribution in the observation narration the disk

266
00:19:24,090 --> 00:19:28,340
and they won't necessarily be the same one case there will be the same as

267
00:19:28,340 --> 00:19:30,880
when that conditioning doesn't make any difference

268
00:19:31,800 --> 00:19:36,330
this will be problematic in the case that t and u are actually independent

269
00:19:36,330 --> 00:19:38,770
the conditioning makes no difference

270
00:19:39,750 --> 00:19:45,160
this is a nice thing to hold because then the distribution i get the observation

271
00:19:45,160 --> 00:19:46,920
regime but y

272
00:19:47,380 --> 00:19:52,260
among those patients who happened to be assigned treatment one will be the same as

273
00:19:52,260 --> 00:19:56,220
the desired distribution in the intervention regime

274
00:19:56,230 --> 00:19:59,620
so that's an important condition t independent

275
00:19:59,620 --> 00:20:03,350
that will make i can just that will allow me to jump over the barrier

276
00:20:03,350 --> 00:20:08,930
between the two regimes without making any change to anything we can express the graphically

277
00:20:11,770 --> 00:20:16,540
there's two variables UNC team so you have some distribution

278
00:20:16,540 --> 00:20:20,230
may or may not know what it is team could be linked to you how

279
00:20:20,230 --> 00:20:23,840
the treatment is assigned could depend on your principle

280
00:20:23,850 --> 00:20:30,920
that this arrow like that means the probabilistic dependence on our what i would have

281
00:20:31,120 --> 00:20:34,510
blocked field in it means of functional dependence

282
00:20:34,630 --> 00:20:39,960
so this is wise function write it in in there it is

283
00:20:40,080 --> 00:20:48,680
we have a question is can we measure the conditional independence that we have to

284
00:20:48,680 --> 00:20:51,220
assume that the answer is mostly we have to assume it

285
00:20:51,230 --> 00:20:53,290
because we can't observe u

286
00:20:53,310 --> 00:20:54,580
o point

287
00:20:54,590 --> 00:20:56,630
and that's one of the problems

288
00:20:56,630 --> 00:21:02,790
OK so so what we basically said i mean so this is just a graphical

289
00:21:02,790 --> 00:21:08,830
representation of exactly what i said already we get if we delete an arrow from

290
00:21:09,800 --> 00:21:14,190
that represents the marginal independence of t in the

291
00:21:14,570 --> 00:21:20,380
and the treatment is regarded as being assigned summer independent unit characteristic which is or

292
00:21:20,380 --> 00:21:26,850
at least mimics a randomized experiment were somehow the treatment assignment is done by also

293
00:21:26,970 --> 00:21:29,840
corner whatever which completely unrelated to

294
00:21:32,040 --> 00:21:35,100
the there was used to point to randomisation

295
00:21:35,120 --> 00:21:41,010
randomisation will bring about this rather desirable state of affairs that allow us to make

296
00:21:41,010 --> 00:21:44,250
this link between the two regimes

297
00:21:44,270 --> 00:21:50,000
no it's not if and only if

298
00:21:50,010 --> 00:21:54,230
that doesn't follow from what i've said

299
00:21:54,250 --> 00:21:59,540
but it certainly sufficient not necessary

300
00:22:02,590 --> 00:22:04,640
randomisation but you

301
00:22:05,750 --> 00:22:08,220
males and females in the world

302
00:22:10,640 --> 00:22:12,420
you do

303
00:22:12,660 --> 00:22:20,300
that's get into more complicated experimental designs and so the question is really the randomisation

304
00:22:20,300 --> 00:22:23,970
and so but the definitely so equal numbers in two groups there are all sorts

305
00:22:23,970 --> 00:22:31,790
of complicated randomisation schemes and that so all theories experimental design of i won't get

306
00:22:31,790 --> 00:22:33,790
into that you know

307
00:22:33,790 --> 00:22:41,510
the simple level of independence and identical behavior for every unit they are talking about

308
00:22:41,610 --> 00:22:44,580
this for years

309
00:22:44,920 --> 00:22:47,960
the median age

310
00:22:49,300 --> 00:22:52,080
you know so the question is do i need to toss the coin now i

311
00:22:52,080 --> 00:22:56,840
don't but i need to somehow justify this assumption if i'm going to use it

312
00:22:56,840 --> 00:23:03,540
doesn't require pretty convincing way justifying the assumption that there may be other way

313
00:23:03,550 --> 00:23:09,910
now are room i remember that a special case of the functional model was the

314
00:23:09,910 --> 00:23:14,880
potential response model where this sort of mathematical fiction if you like for variable u

315
00:23:14,880 --> 00:23:19,510
which is by bivariate quantity the two things on the two tablets of stone with

316
00:23:19,510 --> 00:23:22,720
some bivariate distribution

317
00:23:22,750 --> 00:23:25,840
and it's the same story because

318
00:23:26,670 --> 00:23:30,670
again we will make the equality we will be able to use this framework and

319
00:23:30,670 --> 00:23:33,020
say that

320
00:23:33,040 --> 00:23:38,130
it's OK to go from the observation regime to the intervention regime so long as

321
00:23:38,130 --> 00:23:39,660
it is independent of y

322
00:23:40,220 --> 00:23:42,880
and this is so this is a common

323
00:23:42,920 --> 00:23:50,870
important in analyses people who like potential responses they say they scored ignore ignore all

324
00:23:50,880 --> 00:23:58,470
treatment x assignment no confounding and

325
00:23:58,470 --> 00:24:03,880
OK i was there with his wife everybody's got their why doesn't matter how you

326
00:24:03,890 --> 00:24:09,290
you get treated all one regime and the no confounding or ignorable treatment

327
00:24:09,690 --> 00:24:16,730
condition is that the way you get the same treatment is independent of the two

328
00:24:16,730 --> 00:24:19,550
values engraved on tablet to stone

329
00:24:20,330 --> 00:24:21,300
which by the way

330
00:24:21,330 --> 00:24:26,040
and most one can never be observed and at the moment probably

331
00:24:26,040 --> 00:24:27,150
there's no it is

332
00:24:27,170 --> 00:24:29,400
this part is reserved for three years

333
00:24:29,400 --> 00:24:31,670
and this part is reserved for

334
00:24:31,730 --> 00:24:32,650
you can check

335
00:24:32,670 --> 00:24:36,880
that's really what this array means let's finish running the algorithm so that was the

336
00:24:36,880 --> 00:24:38,590
last element so

337
00:24:38,630 --> 00:24:43,450
across enough but we've done that now look at the next to last element that's

338
00:24:44,290 --> 00:24:50,270
four position five my four here position five and decrement the counter

339
00:24:50,320 --> 00:24:52,570
next i look at another three

340
00:24:52,590 --> 00:24:54,710
three is now the position to

341
00:24:54,730 --> 00:24:56,730
so the because here

342
00:24:56,750 --> 00:25:00,770
and then i don't comment that counter i want actually use that can more realistic

343
00:25:00,770 --> 00:25:02,770
comment because that's what it says

344
00:25:02,790 --> 00:25:07,750
look at the previous element that so one ones composition one

345
00:25:07,770 --> 00:25:08,960
but here

346
00:25:08,980 --> 00:25:11,980
and can that counts

347
00:25:12,000 --> 00:25:13,860
and finally i have another four

348
00:25:14,360 --> 00:25:18,770
and for competition for now position for his here

349
00:25:18,820 --> 00:25:23,110
and i recommend that count

350
00:25:23,130 --> 00:25:24,460
OK so that's

351
00:25:24,460 --> 00:25:29,290
counting sort and you notice that all the elements appear and they appear in order

352
00:25:29,960 --> 00:25:31,150
the other

353
00:25:31,150 --> 00:25:36,290
that was the running time counting sort

354
00:25:37,400 --> 00:25:38,750
is an upper bound

355
00:25:38,750 --> 00:25:40,810
it's a little bit better than that

356
00:25:40,820 --> 00:25:44,150
actually quite a bit better

357
00:25:44,170 --> 00:25:49,170
it requires some

358
00:25:52,900 --> 00:25:56,020
going back to the top

359
00:25:56,020 --> 00:26:01,020
how much time does the static

360
00:26:02,340 --> 00:26:06,750
how much time this is static

361
00:26:08,500 --> 00:26:15,960
how much time does this static

362
00:26:17,070 --> 00:26:21,150
each of these operations in the for loop taking constantine time

363
00:26:21,170 --> 00:26:23,810
this is just what is the length of that

364
00:26:23,820 --> 00:26:27,130
how many iterations of that for the father

365
00:26:27,150 --> 00:26:28,810
and finally

366
00:26:28,810 --> 00:26:30,520
this step takes

367
00:26:34,070 --> 00:26:37,130
so the total running time

368
00:26:37,150 --> 00:26:42,980
of counting sort is

369
00:26:43,040 --> 00:26:44,170
OK plus

370
00:26:44,190 --> 00:26:54,730
OK and this is great algorithm if k is relatively small like at most and

371
00:26:54,750 --> 00:26:59,710
k is big like and square or to to the and whatever this is not

372
00:26:59,710 --> 00:27:01,310
such a good outcome

373
00:27:01,320 --> 00:27:04,500
but if k because order and

374
00:27:04,710 --> 00:27:06,040
this is great

375
00:27:10,070 --> 00:27:11,170
and we get our

376
00:27:11,210 --> 00:27:13,020
linear time sorting out

377
00:27:13,040 --> 00:27:17,000
so not only do we need the assumption that numbers are integers but we need

378
00:27:17,000 --> 00:27:20,820
the range of the integers is pretty small for the seven to work of all

379
00:27:20,820 --> 00:27:25,570
the numbers are most and are between one in order and then we get a

380
00:27:25,570 --> 00:27:28,920
linear time algorithm but as soon as they are up to n log n

381
00:27:28,940 --> 00:27:31,190
four toaster back to organ sort

382
00:27:31,210 --> 00:27:32,670
that's great

383
00:27:32,690 --> 00:27:36,670
so you could write a combination of that well if k

384
00:27:36,690 --> 00:27:39,920
is bigger than n log and then i'll just use merge sort

385
00:27:39,980 --> 00:27:42,610
and it's less than n log n o use counting sort

386
00:27:42,630 --> 00:27:45,170
that would work but we can do better

387
00:27:47,710 --> 00:27:54,040
thanks so time

388
00:27:54,040 --> 00:27:57,650
it's worth noting

389
00:27:57,820 --> 00:28:01,770
we've been around but only assuming they were outside the comparison models we we're really

390
00:28:01,770 --> 00:28:03,750
contradicted the original theorem

391
00:28:03,770 --> 00:28:05,340
we're just changing the model

392
00:28:05,480 --> 00:28:08,550
so it's good question what you're allowed to do any

393
00:28:08,610 --> 00:28:11,840
scenario any problem scenario

394
00:28:11,860 --> 00:28:15,230
in say some practical scenarios this would be great

395
00:28:15,250 --> 00:28:18,400
if the numbers are dealing with say by law

396
00:28:18,420 --> 00:28:23,750
then k is only to the eight which is two hundred fifty six seeing exhilarating

397
00:28:23,790 --> 00:28:28,250
size to fifty six and you know this is really fast fifty six plus and

398
00:28:28,250 --> 00:28:31,520
no matter how big n is is linear and so if you know your numbers

399
00:28:31,520 --> 00:28:32,980
are small great

400
00:28:33,000 --> 00:28:34,940
the four numbers are bigger say

401
00:28:34,940 --> 00:28:39,320
you still other integers but they fit in like thirty two bit words then

402
00:28:39,340 --> 00:28:41,360
life is not so easy because

403
00:28:41,440 --> 00:28:45,290
OK is n two to the thirty two which is four point two

404
00:28:45,310 --> 00:28:47,880
a billion or so

405
00:28:47,900 --> 00:28:52,750
right which is pretty big and you need to see julia ray of four point

406
00:28:52,750 --> 00:28:57,440
two billion words which is probably like sixteen giga bytes so you just need to

407
00:28:57,440 --> 00:29:01,170
initialize an array before you can even get started so let's and is like

408
00:29:01,190 --> 00:29:03,310
much much more than four billion

409
00:29:03,320 --> 00:29:09,040
and you really you have sixteen devices storage just throw away which had i don't

410
00:29:09,040 --> 00:29:13,400
even have any machines with sixteen megabytes of RAM

411
00:29:13,420 --> 00:29:17,520
this is not such a great out to just give feels good numbers are really

412
00:29:18,340 --> 00:29:22,570
what we're gonna do next is come up with the fancier out of that uses

413
00:29:22,570 --> 00:29:26,170
this as a subroutine on small numbers

414
00:29:26,190 --> 00:29:31,310
and combines this algorithm to get to handle larger numbers

415
00:29:31,320 --> 00:29:35,810
that algorithm is called radix sort

416
00:29:35,860 --> 00:29:39,960
but we need one important property of counting sort before we can

417
00:29:40,020 --> 00:29:42,520
go there

418
00:29:42,630 --> 00:29:51,440
and the important property instability

419
00:29:51,460 --> 00:29:59,960
so stable sorting algorithms

420
00:30:05,020 --> 00:30:06,000
the order

421
00:30:06,020 --> 00:30:07,860
the core elements

422
00:30:07,880 --> 00:30:11,650
so the relative order

423
00:30:23,170 --> 00:30:29,860
so this is a itself because usually we think of elements just as numbers and

424
00:30:30,040 --> 00:30:32,610
we have a couple freezing we had a couple flaws

425
00:30:32,610 --> 00:30:35,610
it turns out if you look at the order of those three is on the

426
00:30:35,610 --> 00:30:40,150
order of those force we we kept them in order to the last three

427
00:30:40,170 --> 00:30:41,480
and we put it here

428
00:30:41,500 --> 00:30:45,050
and we took the next last three we put it like that we're always documenting

429
00:30:45,050 --> 00:30:46,110
our counter

430
00:30:46,130 --> 00:30:47,290
and moving from

431
00:30:47,320 --> 00:30:50,630
so the end from the end of the rate at the beginning of the era

432
00:30:50,630 --> 00:30:52,040
so no matter

433
00:30:52,250 --> 00:30:55,630
how we do that the orders of the three to preserve the orders the force

434
00:30:56,670 --> 00:31:00,860
space like a relatively simple thing but if you look at the other for sorting

435
00:31:00,860 --> 00:31:04,440
out we've seen not all of them are stable

436
00:31:04,460 --> 00:31:06,480
this is an exercise

437
00:31:06,500 --> 00:31:21,190
exercises figure out which other sorting algorithms that we've seen are stable which are

438
00:31:21,400 --> 00:31:30,520
anchor to to work that out because this the sort of thing we ask and

439
00:31:33,820 --> 00:31:38,670
for now all we need is the counting sort is stable

440
00:31:38,690 --> 00:31:42,400
OK and i will prove this but it should be pretty obvious from the output

441
00:31:45,090 --> 00:31:49,290
we it's talk about radix sort

442
00:31:49,320 --> 00:32:07,460
radius is going to work for a much larger range of numbers in linear time

443
00:32:07,460 --> 00:32:10,790
kind of thing so you can see a model for you probably can't see that

444
00:32:10,790 --> 00:32:13,330
there a line around the jordan for it

445
00:32:13,330 --> 00:32:16,190
which is where the models stocks to the person

446
00:32:16,230 --> 00:32:19,440
the transition is hard enough to see

447
00:32:19,480 --> 00:32:21,750
OK so that's kind of model that

448
00:32:21,770 --> 00:32:22,750
that works

449
00:32:22,770 --> 00:32:25,650
pretty well it's a little bit delicate to do this fitting

450
00:32:25,670 --> 00:32:28,400
so i would say this is

451
00:32:28,420 --> 00:32:32,170
the initialisation of these models is problematic so it's not really and i think that

452
00:32:32,830 --> 00:32:37,090
can be robust in all circumstances and like the previous ones with there's no initialisation

453
00:32:37,090 --> 00:32:43,670
just look in your database and see what looks similar

454
00:32:43,730 --> 00:32:46,110
OK so you can get even more

455
00:32:46,110 --> 00:32:48,190
model based build

456
00:32:48,230 --> 00:32:51,380
and articulate body model either in two d or three d

457
00:32:51,380 --> 00:32:56,440
and instead trying to track these kinds of models when so much about the that's

458
00:32:56,440 --> 00:33:00,590
not really learning once you start doing things like motion models for from these things

459
00:33:00,840 --> 00:33:02,400
in the component

460
00:33:02,420 --> 00:33:05,520
what you start learning appearances well it can be learning

461
00:33:05,540 --> 00:33:10,170
associated with that model but a little bit lose their models which are basically kind

462
00:33:10,170 --> 00:33:11,440
of springs in this

463
00:33:11,460 --> 00:33:16,420
springs and positions so far and i might have a thing that finds i plates

464
00:33:16,420 --> 00:33:19,860
in the image and this kind of model

465
00:33:19,880 --> 00:33:20,690
which is

466
00:33:20,710 --> 00:33:22,380
tells you roughly how much

467
00:33:22,400 --> 00:33:24,230
the i should be apart

468
00:33:24,250 --> 00:33:28,060
OK if you can formulate that is a graphical model of course so

469
00:33:28,150 --> 00:33:32,070
when you're trying to use these models it tends to reduce the graphical model inference

470
00:33:32,090 --> 00:33:35,070
and these things usually continuous state models

471
00:33:35,090 --> 00:33:38,790
we can either happen continuously in discretized positions

472
00:33:38,980 --> 00:33:42,770
so so the inference in a graphical model which is continuous state which is hard

473
00:33:42,920 --> 00:33:47,210
or in a graphical model which has many discrete values which is also hard

474
00:33:47,400 --> 00:33:52,610
so these interesting inference problems the other thing is that often you won't find features

475
00:33:52,610 --> 00:33:55,460
because you're you're basically low-level vision wasn't strong enough

476
00:33:55,560 --> 00:33:59,980
well something was included or something so you can assume have the whole always on

477
00:33:59,980 --> 00:34:04,610
on its these models when you're doing inference

478
00:34:05,090 --> 00:34:13,590
so so the arguments for these things are basically the popular graphical model inference methods

479
00:34:13,770 --> 00:34:17,310
dynamical programming is a good thing to do if you've got a graph that can

480
00:34:17,310 --> 00:34:20,460
be reduced to singly connected graph

481
00:34:20,730 --> 00:34:24,440
which is very often want to be able to do

482
00:34:24,480 --> 00:34:26,940
so these kinds of models

483
00:34:26,940 --> 00:34:29,110
you can search for

484
00:34:29,130 --> 00:34:33,340
the basic parts of the model in the image with primitive detectors and then try

485
00:34:33,340 --> 00:34:36,290
to string together and enter into an articulated model

486
00:34:36,340 --> 00:34:38,670
this is a singly connected

487
00:34:38,710 --> 00:34:41,330
model and that if we node

488
00:34:41,340 --> 00:34:42,360
this the change

489
00:34:42,380 --> 00:34:43,170
from from

490
00:34:43,190 --> 00:34:46,540
coming from united so you can do rapid inference and this can be done in

491
00:34:46,630 --> 00:34:48,590
programming types solutions

492
00:34:48,590 --> 00:34:50,790
this is where SVM

493
00:34:50,790 --> 00:34:54,310
based detector this was just coloured status

494
00:34:54,360 --> 00:34:57,880
similar models from that

495
00:34:58,250 --> 00:35:00,110
you can also do that was

496
00:35:00,130 --> 00:35:04,940
the same kind of thing with things like superpixels that i mentioned before so over

497
00:35:04,940 --> 00:35:07,590
segmentations of the image and then you try to string

498
00:35:07,610 --> 00:35:09,480
pieces of those together

499
00:35:09,500 --> 00:35:12,420
into the memes that you going to make a model

500
00:35:12,460 --> 00:35:17,460
like some kind of inference process

501
00:35:17,710 --> 00:35:20,650
so that's all very well

502
00:35:20,670 --> 00:35:24,190
but if you represent the person as a graphical model

503
00:35:25,000 --> 00:35:26,440
everything is

504
00:35:26,480 --> 00:35:31,690
well nothing is connected so there's no multiple connections you run into problems

505
00:35:31,750 --> 00:35:32,980
which is that

506
00:35:33,000 --> 00:35:36,270
both legs of the model for one link in the image

507
00:35:36,290 --> 00:35:40,170
OK and the reason for this is the graphical model doesn't have anything telling it

508
00:35:40,170 --> 00:35:41,520
to not do that

509
00:35:41,560 --> 00:35:46,710
because the legs independent and inference in graphical models because they are independent

510
00:35:46,730 --> 00:35:49,020
OK so what happens is that both

511
00:35:49,040 --> 00:35:52,940
both legs of model tend to fit the bass lake in the image

512
00:35:53,210 --> 00:35:57,020
and so many but things like that because it's not very good

513
00:35:57,380 --> 00:35:59,520
the way to get around that

514
00:35:59,540 --> 00:36:00,380
which is

515
00:36:00,540 --> 00:36:04,420
in an interesting trick in graphical models is to introduce

516
00:36:04,440 --> 00:36:06,060
latent node

517
00:36:06,110 --> 00:36:10,920
which says something about essentially the state of the lakes without really explicitly saying that

518
00:36:10,920 --> 00:36:14,940
saying something about the state of the makes so the be of the discrete variable

519
00:36:14,960 --> 00:36:20,000
introduced which might have a couple of values in which forces of one for the

520
00:36:20,000 --> 00:36:21,480
other be backwards

521
00:36:21,520 --> 00:36:25,190
and they for stocks the lake having the same position

522
00:36:25,270 --> 00:36:29,040
and then you can do inference in that model because all you have done

523
00:36:29,090 --> 00:36:34,060
in the central node is increase the dimension of the state of which doesn't to

524
00:36:34,150 --> 00:36:36,090
the inference too much model

525
00:36:36,110 --> 00:36:40,020
so so so in some sense it's like kind of dimensionality reduced version of the

526
00:36:40,020 --> 00:36:41,630
full graphical model

527
00:36:41,650 --> 00:36:45,690
you have taken a particular types of behavior the you try to suppress and you

528
00:36:45,690 --> 00:36:46,790
encode them

529
00:36:46,810 --> 00:36:48,290
and nodes

530
00:36:48,290 --> 00:36:51,290
the to the central server so

531
00:36:51,310 --> 00:36:52,860
things can go wrong

532
00:36:52,920 --> 00:36:55,360
further down the chain

533
00:36:55,380 --> 00:36:56,790
you can then these models

534
00:36:57,590 --> 00:36:58,360
this course

535
00:36:58,360 --> 00:36:59,840
images as well

536
00:37:02,540 --> 00:37:04,330
just a little bit on learning

537
00:37:04,380 --> 00:37:08,880
o auto regressive dynamics to this is the piecewise learning method for that images should

538
00:37:08,900 --> 00:37:10,150
video that

539
00:37:10,440 --> 00:37:11,730
it's fairly

540
00:37:14,460 --> 00:37:25,130
it's fairly straightforward machine in twenty two we can and things like tens walks things

541
00:37:25,290 --> 00:37:27,840
so this is something else

542
00:37:27,840 --> 00:37:28,940
a little bit

543
00:37:28,940 --> 00:37:32,480
the more interesting so this is what we did a couple of years ago

544
00:37:32,840 --> 00:37:38,210
we want to we've been doing human tracking with this model based approach

545
00:37:38,250 --> 00:37:40,380
and things always went wrong

546
00:37:40,420 --> 00:37:44,330
you always the model and this track was fitting on piece of the image

547
00:37:44,340 --> 00:37:47,210
so what we wanted to do is look back and say OK what can we

548
00:37:48,380 --> 00:37:50,020
if we try to

549
00:37:50,040 --> 00:37:52,290
just use learning based approach

550
00:37:53,250 --> 00:37:57,540
so we started with so it's wanted to calculate some kind of robust shape descriptor

551
00:37:58,920 --> 00:38:01,900
and use that to predict the three d pose

552
00:38:02,980 --> 00:38:07,650
and so what we use to me and the relevance vector machine regression sparse bayesian

553
00:38:09,020 --> 00:38:12,610
it doesn't matter too much you can also use an SVM greece or even linear

554
00:38:12,610 --> 00:38:17,790
model different models the IVM is good because it's possible faster i

555
00:38:17,840 --> 00:38:19,940
maybe that's the main difference

556
00:38:19,940 --> 00:38:24,230
so how do we cope these things well we took its

557
00:38:24,230 --> 00:38:27,940
who knows if this sentence makes sense all read out loud loosely this can be

558
00:38:27,940 --> 00:38:33,040
thought of as softening the strict definition of co occurrence in a mixture model OK

559
00:38:33,770 --> 00:38:36,110
so here's what i meant by that

560
00:38:36,150 --> 00:38:37,830
and this is very loose

561
00:38:39,380 --> 00:38:41,440
that's the first thing that lives

562
00:38:41,470 --> 00:38:46,360
the second thing that is that so the mixture model we are asserting definitively all

563
00:38:46,360 --> 00:38:49,060
these words in this document are co occurring so

564
00:38:49,480 --> 00:38:54,130
you model better respect that and get give me topics that respect that all of

565
00:38:54,130 --> 00:38:56,660
these words in this document occur

566
00:38:57,390 --> 00:39:00,370
alternatively we could say

567
00:39:00,380 --> 00:39:02,890
as we increase alpha look

568
00:39:02,930 --> 00:39:06,990
some of these words the words about one topic are co occurring within that topic

569
00:39:07,330 --> 00:39:10,760
and the words about another topic are co occurring within that topic

570
00:39:10,820 --> 00:39:16,090
but if i the document that's about baseball and statistics what i'm saying

571
00:39:16,110 --> 00:39:17,400
is that

572
00:39:17,410 --> 00:39:21,800
the words about baseball are all co occurring on the words about statistics are all

573
00:39:21,800 --> 00:39:25,920
co occurring but the words about baseball in the words about statistics are not co

574
00:39:26,860 --> 00:39:31,710
OK so we don't see guassian process and you know

575
00:39:31,720 --> 00:39:36,180
oh my god i don't know anything about baseball pitch

576
00:39:36,220 --> 00:39:40,450
you don't see guassian process and pitch

577
00:39:40,470 --> 00:39:43,330
as a co occurrence whereas you would in the mixture model

578
00:39:43,390 --> 00:39:44,150
can you

579
00:39:44,160 --> 00:39:45,470
can you

580
00:39:45,490 --> 00:39:47,810
can you dig this

581
00:39:47,830 --> 00:39:51,690
when you increase out there and you allow for two topics to be activated in

582
00:39:51,690 --> 00:39:56,010
one document then suddenly guassian process and

583
00:39:56,010 --> 00:39:59,440
better or

584
00:39:59,440 --> 00:40:01,070
what's at home run

585
00:40:01,090 --> 00:40:07,690
very good that's what i want better home runs are guassian process batman are co

586
00:40:07,690 --> 00:40:10,530
occurring guassian process in home and are not caucus

587
00:40:10,570 --> 00:40:11,740
and so

588
00:40:11,780 --> 00:40:13,920
in that sense

589
00:40:13,940 --> 00:40:18,210
we can think of a mixed membership model as softening this strict definition of co

590
00:40:18,210 --> 00:40:23,800
occurrence in a mixture model and what that leads to are sets of terms that

591
00:40:23,800 --> 00:40:28,230
somehow more tightly co occurrence of terms that we're used to seeing together all the

592
00:40:29,840 --> 00:40:35,360
that's that's my instinct for the answer to that question about posterior

593
00:40:36,360 --> 00:40:40,050
during the

594
00:40:40,280 --> 00:40:49,380
the preface to any slaying comment all

595
00:40:52,240 --> 00:40:57,800
one of the four classes were known

596
00:40:58,590 --> 00:41:03,110
to explore the whole

597
00:41:03,820 --> 00:41:06,630
what you want

598
00:41:06,800 --> 00:41:08,780
one of is

599
00:41:12,420 --> 00:41:14,230
and what you

600
00:41:15,570 --> 00:41:25,170
right that's a great question so and we're going to get to something

601
00:41:25,170 --> 00:41:27,670
will get to that a little well we're not going to get the marketing but

602
00:41:27,670 --> 00:41:28,490
we're going

603
00:41:28,510 --> 00:41:31,920
we could have gone to it and what in some of the slides

604
00:41:33,010 --> 00:41:34,880
later but

605
00:41:34,920 --> 00:41:36,240
i don't believe you

606
00:41:36,240 --> 00:41:38,760
is the answer to the question i don't believe

607
00:41:42,780 --> 00:41:45,360
are used as sparsely as

608
00:41:45,360 --> 00:41:46,630
co occurring

609
00:41:46,630 --> 00:41:48,590
topical words

610
00:41:50,490 --> 00:41:53,110
that's just my intuition but

611
00:41:53,110 --> 00:41:56,740
that i think that that's the answer to your question that the reason for this

612
00:41:56,740 --> 00:42:00,860
is the somehow subject to alpha the sparsest

613
00:42:00,860 --> 00:42:08,570
challenging but not only because of technical issues but also because of communication mismatches

614
00:42:08,580 --> 00:42:10,780
also on the human level

615
00:42:10,850 --> 00:42:14,620
medical ontologies have specific characteristics

616
00:42:14,630 --> 00:42:16,420
typically there are large

617
00:42:16,450 --> 00:42:25,220
complex civil park semantics talk to us at least engineers and this requires special treatment

618
00:42:25,240 --> 00:42:28,190
expert knowledge is not always available

619
00:42:28,200 --> 00:42:32,650
in the medical domain because the doctors are almost always

620
00:42:32,660 --> 00:42:34,560
in lack of time

621
00:42:35,410 --> 00:42:42,010
this requires finding alternative solutions to acquire expert knowledge

622
00:42:42,030 --> 00:42:47,840
like we try to address with address with the query and aviation technique

623
00:42:47,840 --> 00:42:52,820
and medical image semantics requires integration

624
00:42:52,870 --> 00:42:55,170
of information

625
00:42:55,200 --> 00:42:57,920
across three dimensions which are

626
00:42:58,920 --> 00:43:02,070
anatomy radiology and diseases

627
00:43:02,070 --> 00:43:04,400
or pathology

628
00:43:04,410 --> 00:43:08,260
and maybe one final issues that i didn't really bring up

629
00:43:08,320 --> 00:43:09,720
so far is that

630
00:43:09,740 --> 00:43:15,510
actually conditions on radiologists are not if they are not aware of the technologies that

631
00:43:16,170 --> 00:43:18,090
available to them

632
00:43:18,100 --> 00:43:20,250
one important step is

633
00:43:20,310 --> 00:43:25,370
probably to inform them about all what they can do MAP because they are working

634
00:43:25,370 --> 00:43:26,330
on their

635
00:43:26,340 --> 00:43:29,120
on the current workstations how they are

636
00:43:29,140 --> 00:43:31,200
on the databases on their

637
00:43:31,330 --> 00:43:37,260
restricted query how they are they have been doing this for all these many years

638
00:43:37,370 --> 00:43:39,680
and they don't really know that they could

639
00:43:39,710 --> 00:43:43,060
change the situation that they could ask for more

640
00:43:43,100 --> 00:43:47,350
so maybe it's a good point to make them aware

641
00:43:47,360 --> 00:43:50,080
of all what is that their disposal

642
00:43:50,100 --> 00:43:57,680
so that they can exploit the candyman more we can do the work for the

643
00:43:57,700 --> 00:44:04,340
there are also dissemination activities already there's a planned to it's going to be enormous

644
00:44:04,350 --> 00:44:07,110
conference and there are publications

645
00:44:07,480 --> 00:44:09,870
the talk about the technical details

646
00:44:09,920 --> 00:44:15,710
also we would like to thank our project partners to DFKI language technology lab and

647
00:44:15,710 --> 00:44:17,380
the houses kaiserslautern

648
00:44:17,420 --> 00:44:25,370
knowledge management group also university of london for their valuable contributions so far

649
00:44:25,370 --> 00:44:31,920
and if you have any questions if you would like to talk discuss more please

650
00:44:31,920 --> 00:44:33,110
contact us

651
00:44:33,150 --> 00:44:37,710
send this email alerts were more than happy to talk to you about it

652
00:44:37,730 --> 00:44:40,660
about medical and the technology

653
00:44:40,680 --> 00:44:50,460
and thank you

654
00:44:50,510 --> 00:44:55,160
thank you very much for this excellent speech and

655
00:44:55,200 --> 00:44:59,760
we are still a couple of minutes for your questions

656
00:44:59,770 --> 00:45:02,060
we have one

657
00:45:25,480 --> 00:45:34,970
well actually as i mentioned in the beginning medical in the first place and are

658
00:45:34,970 --> 00:45:40,300
much intended for health care workers to fulfill their

659
00:45:40,300 --> 00:45:42,390
feel free to address their challenges

660
00:45:42,400 --> 00:45:44,030
but once the start

661
00:45:45,060 --> 00:45:48,340
there is no next system is going to be data

662
00:45:50,420 --> 00:45:54,310
to the public so to say so many should be also

663
00:45:54,340 --> 00:45:58,710
at the disposal of the citizens of

664
00:45:58,930 --> 00:46:00,960
out the of the citizens so

665
00:46:00,970 --> 00:46:05,310
it should be available on our laptops sort

666
00:46:05,420 --> 00:46:08,620
computers so that he

667
00:46:08,630 --> 00:46:12,910
maybe on the web basis so that you can search stand out your

668
00:46:12,920 --> 00:46:16,960
semantic queries on the web and maybe

669
00:46:16,970 --> 00:46:18,800
even standard picture

670
00:46:18,810 --> 00:46:24,140
and then you got from your doctor and you say OK this is my new

671
00:46:24,150 --> 00:46:24,940
this is the

672
00:46:24,970 --> 00:46:29,390
new on x-ray that i have had and you stand in their way

673
00:46:29,980 --> 00:46:35,610
and then you get other similar pictures from other patients and then you can maybe

674
00:46:35,610 --> 00:46:43,410
look into on the related forums and exchange experiences and this type of like very

675
00:46:43,420 --> 00:46:48,030
fast materials so i think these are going to be the future challenges but i

676
00:46:48,030 --> 00:46:53,160
don't really know and they are i mean when it's going to come that far

677
00:47:04,200 --> 00:47:05,160
i believe that

678
00:47:05,170 --> 00:47:09,130
the sounds very interesting for the future

679
00:47:09,130 --> 00:47:11,190
remember so that

680
00:47:11,210 --> 00:47:14,460
well you consumers australia also started

681
00:47:15,520 --> 00:47:17,660
to kind of initiative

682
00:47:19,520 --> 00:47:22,640
semantic systems and life sciences

683
00:47:22,720 --> 00:47:24,170
and the

684
00:47:24,240 --> 00:47:32,160
reply conference last year in graz we tried to create a platform for more

685
00:47:32,160 --> 00:47:37,370
the interests interested parties in this area that means

686
00:47:40,860 --> 00:47:43,340
like siemens medical

687
00:47:43,340 --> 00:47:45,890
if everyone behaves randomly so

688
00:47:46,760 --> 00:47:50,860
now there is no no no plot no conspiracy behind

689
00:47:53,840 --> 00:47:58,050
just simple data mining will detect anything suspicious and not so these is not the questions so

690
00:47:58,800 --> 00:47:59,640
by intuition so

691
00:48:00,160 --> 00:48:01,450
what would you say uh

692
00:48:02,610 --> 00:48:03,550
how many people would be

693
00:48:04,050 --> 00:48:05,180
how many pairs of people

694
00:48:05,930 --> 00:48:07,240
would be fine which they

695
00:48:08,510 --> 00:48:10,950
it is twice in the same could there not knowing each other

696
00:48:12,140 --> 00:48:12,910
we have an intuition

697
00:48:19,200 --> 00:48:32,990
one hour

698
00:48:33,070 --> 00:48:33,950
so not knowing

699
00:48:34,510 --> 00:48:37,410
each other this is also uh uh

700
00:48:39,820 --> 00:48:41,470
so this is an be a monitoring now

701
00:48:42,110 --> 00:48:46,030
billion people just the small groups which meet here and there okay

702
00:48:46,990 --> 00:48:52,450
but so the answer is so this is completely random to people who don't know each other with appear in

703
00:48:54,380 --> 00:48:55,340
who knows what's

704
00:48:55,780 --> 00:48:56,530
in three years

705
00:48:57,530 --> 00:48:57,890
so it's

706
00:48:58,430 --> 00:49:00,200
two hundred fifty thousand pairs

707
00:49:02,390 --> 00:49:04,610
so this is just simple probability that given distance

708
00:49:08,470 --> 00:49:11,010
there's just too many combinations to combinations

709
00:49:11,570 --> 00:49:12,840
to check far let's say

710
00:49:13,740 --> 00:49:16,410
intelligence services just expensive so

711
00:49:17,120 --> 00:49:18,110
what do have

712
00:49:19,910 --> 00:49:24,360
we need to have some additional evidence within the data so that we reduce this

713
00:49:24,360 --> 00:49:27,120
number down to something which is meaningful languages

714
00:49:28,240 --> 00:49:31,380
very hot verifiable at least degree that

715
00:49:31,820 --> 00:49:32,990
what we find is

716
00:49:33,700 --> 00:49:34,470
suspicious people

717
00:49:36,490 --> 00:49:37,120
so this is kind of

718
00:49:37,550 --> 00:49:40,380
interesting examples so that doesn't happen in the past you know when

719
00:49:41,030 --> 00:49:42,970
we had such a small data so

720
00:49:44,050 --> 00:49:47,550
of course this was hard find now with these huge data is very easy to

721
00:49:47,550 --> 00:49:49,430
find very interesting stuff just by

722
00:49:50,320 --> 00:49:51,140
purely yeah

723
00:49:52,660 --> 00:49:55,740
probabilistic simple from the probability would tell you that

724
00:49:56,160 --> 00:49:58,110
in the this waste the

725
00:49:58,510 --> 00:50:03,180
the vast amount of patterns which are generated by out of the data

726
00:50:03,950 --> 00:50:06,300
now you will always find some advances as well

727
00:50:06,930 --> 00:50:08,030
which are completely meaningless

728
00:50:08,820 --> 00:50:11,110
but they exist they are because they start

729
00:50:11,840 --> 00:50:12,660
allowed to detect

730
00:50:14,140 --> 00:50:18,720
so obviously here we would need to know a little bit more about these people than just that they appeared

731
00:50:19,110 --> 00:50:20,090
twice in the same cluster

732
00:50:22,360 --> 00:50:22,800
so this is

733
00:50:23,340 --> 00:50:26,010
i get this example just to give you an intuition that

734
00:50:27,360 --> 00:50:29,760
we need to be careful what you find out of big data

735
00:50:32,910 --> 00:50:34,030
and this is good

736
00:50:34,470 --> 00:50:41,820
area for conspiracy theory people there is always find conspiracies in big data in the small they those hard-to-find

737
00:50:46,340 --> 00:50:46,970
okay so

738
00:50:47,620 --> 00:50:49,590
everyone go to the to deep into the

739
00:50:51,280 --> 00:50:53,510
o operations are the

740
00:50:54,890 --> 00:50:55,950
methods or techniques

741
00:50:56,550 --> 00:50:57,660
but just to give you all

742
00:51:00,320 --> 00:51:01,200
flavour of

743
00:51:02,860 --> 00:51:06,700
type of techniques which are not connected follows a small data

744
00:51:07,200 --> 00:51:08,340
but now we need to be

745
00:51:09,300 --> 00:51:12,180
bare them middle east we need to be able to deal with this

746
00:51:12,970 --> 00:51:19,450
so one is smart sampling of data so this is something which in a small dataset world didn't exist through

747
00:51:20,470 --> 00:51:21,930
so smart sampling means

748
00:51:22,840 --> 00:51:26,910
if you have a huge amounts of data so how to reduce this huge amount

749
00:51:26,910 --> 00:51:31,430
to small smaller size without losing information content from the

750
00:51:32,360 --> 00:51:32,860
for some

751
00:51:33,390 --> 00:51:34,300
problems this is easy

752
00:51:36,320 --> 00:51:41,700
do random sampling can it's already enough just for simple counting car estimating simple

753
00:51:44,780 --> 00:51:49,530
just random sampling would be enough but let's see if we have graphs instance

754
00:51:50,050 --> 00:51:50,820
social networks

755
00:51:51,220 --> 00:51:52,970
the social networks usually have

756
00:51:53,470 --> 00:51:56,030
is power law distribution and so on and we cannot

757
00:51:56,490 --> 00:51:57,720
sampling graphs is

758
00:51:58,260 --> 00:52:02,950
nontrivial you need to be aware of the problem which are solving otherwise you'll basically

759
00:52:02,950 --> 00:52:05,410
if you just take some random links in the graph

760
00:52:05,880 --> 00:52:09,090
basically wouldn't get the structure which you want to mine afterwards

761
00:52:11,050 --> 00:52:12,910
smart sampling can be very hard

762
00:52:13,300 --> 00:52:15,510
for some problems and for some data types

763
00:52:17,300 --> 00:52:19,610
another simple sampling sort of simple

764
00:52:20,450 --> 00:52:25,240
i mean simple nowadays but let's say in early nineties we were not simple far as

765
00:52:26,340 --> 00:52:27,720
how to simple picture so

766
00:52:28,490 --> 00:52:30,140
jetpack is a nice example of

767
00:52:30,930 --> 00:52:32,700
smart sampling cover the picture so

768
00:52:33,510 --> 00:52:34,570
we have this huge image

769
00:52:35,340 --> 00:52:36,780
every pixel inspectorate i mean

770
00:52:37,390 --> 00:52:39,570
that is what the camera produced

771
00:52:40,220 --> 00:52:43,180
but the person doesn't need this to get the message so

772
00:52:43,990 --> 00:52:47,390
think was seeing the discrete cosine transform basically just

773
00:52:48,340 --> 00:52:51,780
the transformed image into frequency space we reduce this

774
00:52:54,990 --> 00:52:56,410
significant frequencies

775
00:52:58,090 --> 00:52:58,610
so this was

776
00:52:59,090 --> 00:52:59,950
compression then we

777
00:53:00,360 --> 00:53:02,340
came back from the most important frequencies

778
00:53:02,800 --> 00:53:04,660
back to the image and the person

779
00:53:06,030 --> 00:53:08,110
watching the image was the message was the same

780
00:53:08,680 --> 00:53:11,120
so this is a nice example what's possible with the image

781
00:53:11,990 --> 00:53:16,470
and the same is ambac formats and so on for the movies is pretty much the same

782
00:53:16,970 --> 00:53:20,700
but not all data are as friendly but still even for that image was not

783
00:53:20,700 --> 00:53:23,430
clear how far in eighties we didn't have this

784
00:53:24,220 --> 00:53:24,660
back so

785
00:53:25,970 --> 00:53:28,780
we have to deal with not too large files but still was

786
00:53:30,660 --> 00:53:31,840
compression was not easy

787
00:53:33,930 --> 00:53:35,300
it was just run length

788
00:53:38,200 --> 00:53:41,220
similar sequence of of of bits and bytes

789
00:53:41,640 --> 00:53:43,410
it was compressed and this was the only one

790
00:53:46,970 --> 00:53:48,110
six seven year minutes but

791
00:53:48,570 --> 00:53:49,010
you can

792
00:53:52,470 --> 00:53:55,340
so this is pretty much the last slide on

793
00:53:56,140 --> 00:53:57,300
analytical operators so

794
00:53:58,740 --> 00:54:01,240
on the top of this previous operators

795
00:54:02,550 --> 00:54:05,180
the same not i didn't go through

796
00:54:07,140 --> 00:54:10,840
so smart sampling is one important intuition which we need to be aware of

797
00:54:10,840 --> 00:54:12,210
crossword puzzle

798
00:54:12,330 --> 00:54:16,430
and the benefit of that is all the images not blurred all the high spatial

799
00:54:16,430 --> 00:54:18,880
frequencies actually is

800
00:54:19,030 --> 00:54:23,860
so from that you can take a photo you may have missed focused and then

801
00:54:23,930 --> 00:54:25,630
the blurb software

802
00:54:26,800 --> 00:54:28,930
all high spatial frequency

803
00:54:28,980 --> 00:54:30,850
so you can deliver sloppy

804
00:54:30,860 --> 00:54:32,760
in taking this for us

805
00:54:32,810 --> 00:54:35,670
and you can even get the link in the

806
00:54:35,680 --> 00:54:39,520
so this is a footnote example from just once in the fourth

807
00:54:39,670 --> 00:54:41,240
so now we certainly

808
00:54:41,330 --> 00:54:44,760
introduced by a little bit of computation in the physical domain

809
00:54:44,900 --> 00:54:47,220
enough capabilities and or software

810
00:54:47,230 --> 00:54:52,050
to recover those frequencies that we

811
00:54:53,410 --> 00:54:55,020
blocking light in time

812
00:54:55,040 --> 00:54:57,860
we can preserve information for motion

813
00:54:57,880 --> 00:54:59,670
blocking light in space

814
00:54:59,680 --> 00:55:01,350
we can

815
00:55:01,540 --> 00:55:04,880
we can preserve information in case of defocus blur

816
00:55:05,000 --> 00:55:06,610
and this seems

817
00:55:06,620 --> 00:55:09,240
contrary due to bad blocking light

818
00:55:09,250 --> 00:55:10,520
in time or space

819
00:55:10,530 --> 00:55:12,400
preserving more information

820
00:55:12,410 --> 00:55:13,470
less light

821
00:55:13,530 --> 00:55:15,200
is more information

822
00:55:17,670 --> 00:55:22,030
and it's not surprising because and i should like driving on the street and is

823
00:55:22,030 --> 00:55:23,300
clear from the sun

824
00:55:23,310 --> 00:55:24,950
retired abruptly

825
00:55:25,180 --> 00:55:27,680
so that's another example where less like

826
00:55:27,690 --> 00:55:31,900
it's more information we can look at history of the sunlight distrubing we don't know

827
00:55:31,900 --> 00:55:33,030
what's going on b

828
00:55:33,060 --> 00:55:35,690
we're trying to block it or time to see if we can see more

829
00:55:35,710 --> 00:55:40,400
so what can we think of it in the more more scientific way

830
00:55:40,410 --> 00:55:41,590
and think about

831
00:55:41,610 --> 00:55:45,110
very simple simplest possible

832
00:55:45,120 --> 00:55:48,890
audience the simplest possible visual systems

833
00:55:48,910 --> 00:55:49,950
like a war

834
00:55:49,970 --> 00:55:52,890
which has a single pixel right

835
00:55:52,910 --> 00:55:55,000
it's just a single pixel

836
00:55:55,010 --> 00:55:56,920
it also seems to have

837
00:55:56,970 --> 00:55:58,920
this very intriguing factor

838
00:55:58,940 --> 00:56:03,990
in its visual field has the shooting big man looks very much similar to our

839
00:56:04,000 --> 00:56:05,360
crossword puzzles here

840
00:56:05,400 --> 00:56:08,900
and the reason apparently is that

841
00:56:08,910 --> 00:56:10,120
if this

842
00:56:11,700 --> 00:56:17,050
i was just open hemispherical a there very very difficult for this war

843
00:56:17,100 --> 00:56:21,130
to maintain its orientation with respect to one strong like maybe this

844
00:56:21,150 --> 00:56:22,560
but this

845
00:56:22,580 --> 00:56:25,240
start behaving very much like a motion detector

846
00:56:25,470 --> 00:56:29,840
i would like to find the mask in front so that if the war changes

847
00:56:29,840 --> 00:56:34,300
the body direction large UCI spike some kind of a sinusoid

848
00:56:38,610 --> 00:56:43,550
so so that allows us this one maintain the body orientation

849
00:56:43,570 --> 00:56:46,120
and if you go to a more complex

850
00:56:47,150 --> 00:56:50,070
they might have four or five for directors

851
00:56:50,160 --> 00:56:55,730
but they still have this shielding so something funny is going on

852
00:56:55,740 --> 00:56:59,570
so in the end the money

853
00:56:59,590 --> 00:57:02,670
my presentation in going to talk war several concepts

854
00:57:02,680 --> 00:57:05,320
where we essentially blocked light

855
00:57:05,360 --> 00:57:07,260
in time and space

856
00:57:07,270 --> 00:57:08,210
in light

857
00:57:08,220 --> 00:57:09,980
in their land and so on

858
00:57:10,120 --> 00:57:15,720
and then i'll talk about this concept of whole computational photography is more than two

859
00:57:15,730 --> 00:57:19,060
different things

860
00:57:19,100 --> 00:57:22,510
so this is of computational photography i feel it's

861
00:57:22,530 --> 00:57:26,960
it's more into three phases and computational photography is a

862
00:57:27,000 --> 00:57:31,880
so what emerging field that combines computer graphics and vision

863
00:57:31,890 --> 00:57:35,380
geometry optics signal processing

864
00:57:36,270 --> 00:57:38,590
electronic hardware

865
00:57:38,590 --> 00:57:41,070
online photo sharing and all that

866
00:57:42,310 --> 00:57:48,000
the first phase i like to call it epsilon photography was similar to epsilon geometry

867
00:57:48,000 --> 00:57:49,700
from computational geometry

868
00:57:49,980 --> 00:57:51,440
where we want to

869
00:57:51,460 --> 00:57:53,930
create an estimate of pixel

870
00:57:53,940 --> 00:57:56,010
with very good samaritan which

871
00:57:56,020 --> 00:57:59,090
we want to just estimate the pixel that's all i can want

872
00:57:59,100 --> 00:58:01,210
and that's been done

873
00:58:01,210 --> 00:58:02,770
typically by

874
00:58:02,780 --> 00:58:05,410
if you want to get a high dynamic range for

875
00:58:05,410 --> 00:58:07,480
i can estimate the pixel value

876
00:58:07,500 --> 00:58:12,950
in case of saturation are under exposure i just bracket my exposure to multiple photos

877
00:58:14,120 --> 00:58:17,330
if we can estimate the value of a pixel because starts in the field of

878
00:58:18,170 --> 00:58:21,920
then you take multiple photos and stitched panorama

879
00:58:22,130 --> 00:58:23,400
and so on

880
00:58:23,410 --> 00:58:25,600
so if you can think of of this

881
00:58:25,630 --> 00:58:31,820
techniques as if you think about the parameter space for camera is six four times

882
00:58:31,870 --> 00:58:33,270
for a few

883
00:58:33,290 --> 00:58:35,410
frame all these parameters

884
00:58:35,410 --> 00:58:37,490
we're just waiting it

885
00:58:37,500 --> 00:58:43,040
in the apse long-range and trying to get a better estimate of forty six

886
00:58:43,060 --> 00:58:45,820
and this is what

887
00:58:45,840 --> 00:58:47,810
computational photography was

888
00:58:47,820 --> 00:58:50,710
for the last several years our so last fifteen years

889
00:58:50,720 --> 00:58:53,920
or so since i work in the late nineties

890
00:58:54,120 --> 00:58:59,690
but that this is not as interesting anymore and moving into

891
00:58:59,710 --> 00:59:00,970
the second phase

892
00:59:00,980 --> 00:59:05,260
which i like to call recorded photography but we don't really care about

893
00:59:05,280 --> 00:59:06,900
a good as up

894
00:59:06,900 --> 00:59:08,940
good modernization of pixel

895
00:59:08,950 --> 00:59:11,700
but i really care about some mid level cues

896
00:59:11,710 --> 00:59:14,530
regions boundaries

897
00:59:14,770 --> 00:59:17,830
decomposition problems and so on

898
00:59:18,000 --> 00:59:20,780
and it's worth taking a lot of photos by bracketing

899
00:59:20,840 --> 00:59:23,740
we want to take one are just a few photos

900
00:59:23,740 --> 00:59:27,260
that reversibly encode information about the scene

901
00:59:27,270 --> 00:59:28,990
and i give you a couple of examples

902
00:59:28,990 --> 00:59:30,050
how much

903
00:59:30,070 --> 00:59:31,960
blocking light in time or space

904
00:59:31,960 --> 00:59:36,770
we can reversibly encode the motion blur or defocused

905
00:59:37,060 --> 00:59:41,600
and lots of other related projects even stupid of cameras

906
00:59:41,720 --> 00:59:46,500
is an example of court photography because they're trying to preserve the treaty information that

907
00:59:46,500 --> 00:59:48,510
would have been lost if you just took

908
00:59:48,590 --> 00:59:50,490
a photo from a single camera

909
00:59:50,600 --> 00:59:55,770
but more sophisticated examples are light field capture which i will talk about the next

910
00:59:56,010 --> 00:59:59,010
detecting the

911
00:59:59,020 --> 01:00:02,770
distinguishing that edges from reflectance edges

912
01:00:02,780 --> 01:00:05,770
foreground background segmentation

913
01:00:05,810 --> 01:00:09,660
engineering the point spread function i give you a couple of examples

914
01:00:09,700 --> 01:00:12,170
and also the focus

915
01:00:12,560 --> 01:00:15,490
also this is an interesting problem

916
01:00:15,490 --> 01:00:19,700
the sum of the information from the measurement and the information from the

917
01:00:19,710 --> 01:00:22,870
associated with the prior

918
01:00:22,970 --> 01:00:25,440
but there's something a bit funny there and

919
01:00:25,450 --> 01:00:28,320
information weighted mean i mean would look something like this

920
01:00:28,330 --> 01:00:30,340
i seem to be trying to add together

921
01:00:30,350 --> 01:00:34,620
in compatible matrices this disguise two by two matrix isn't it with rank one and

922
01:00:34,620 --> 01:00:35,430
this guy

923
01:00:35,480 --> 01:00:39,300
it is it's a six by six matrix and the find space as we have

924
01:00:39,300 --> 01:00:40,770
here so there's a sort of i

925
01:00:40,790 --> 01:00:42,030
you know the impedance

926
01:00:45,090 --> 01:00:48,300
you know why trumpets work much better than penny whistles is because the

927
01:00:48,350 --> 01:00:52,100
the bell of a trumpet does the kind of impedance match between thin tube and

928
01:00:52,110 --> 01:00:56,160
the trumpet and the and sort fat air out there and i guess they're gonna

929
01:00:56,160 --> 01:01:00,000
impedance mismatch and we need an element to like the trumpet we're never going to

930
01:01:00,000 --> 01:01:03,170
get this this bell from two kind of

931
01:01:04,550 --> 01:01:07,610
make the matrices fit together

932
01:01:07,660 --> 01:01:10,350
so what we want to be got in the toolkit we've already seen that we

933
01:01:10,350 --> 01:01:12,440
could use to

934
01:01:12,450 --> 01:01:16,270
this is a know like a method of dimensions sort of argument for the right

935
01:01:16,270 --> 01:01:18,880
answer but i promise you the answer will be right

936
01:01:18,900 --> 01:01:21,160
what's the missing element

937
01:01:21,240 --> 01:01:25,120
we can use for

938
01:01:25,170 --> 01:01:30,930
as our old friend the observation matrix which HMS remember that which miraculously is the

939
01:01:30,940 --> 01:01:38,160
two by six matrix sends sounds promising might to bridge the gap between the

940
01:01:38,180 --> 01:01:44,550
a mismatch so there's the mismatch OK now let's put it right by putting nature

941
01:01:44,550 --> 01:01:47,780
versus in all in all the right places there's only one way i can use

942
01:01:47,780 --> 01:01:48,540
them in each

943
01:01:48,550 --> 01:01:53,090
in each case to make the dimensions work right this is a method of dimensions

944
01:01:53,090 --> 01:01:57,590
argument doesn't quite amount to improve but i promise you the answer is

945
01:01:57,610 --> 01:01:59,660
right and all the

946
01:02:01,530 --> 01:02:03,590
matrix mentions mashup and so now

947
01:02:03,610 --> 01:02:08,300
what we have is a kind of a way of coupling these low dimensional

948
01:02:08,350 --> 01:02:12,860
observations of features into the high dimensional state space

949
01:02:13,650 --> 01:02:14,660
that works

950
01:02:14,670 --> 01:02:20,100
both of the new information and for the that's this getting new covariance the

951
01:02:20,140 --> 01:02:23,270
posterior and for the new me in the past it so that's all

952
01:02:23,690 --> 01:02:25,240
all works nicely now

953
01:02:26,140 --> 01:02:28,540
OK so now i can show you pictures of

954
01:02:28,590 --> 01:02:32,500
a system that actually tracking moving hand using

955
01:02:32,510 --> 01:02:33,870
this idea

956
01:02:35,940 --> 01:02:38,110
so you see to hand is going through

957
01:02:38,120 --> 01:02:41,930
it's fine space repertoire

958
01:02:42,570 --> 01:02:48,180
tilting and rotating and its training in scale and so on and now if i

959
01:02:48,180 --> 01:02:49,230
do something

960
01:02:49,260 --> 01:02:53,430
which is not in the alpine space taken from out then because the contour refuses

961
01:02:53,440 --> 01:02:59,540
to follow because that was never included in this simple model

962
01:03:01,080 --> 01:03:05,120
now another idea which is you know very well known in

963
01:03:07,810 --> 01:03:14,210
conventional tracking you even used in radar kalman filters in real systems is validation gating

964
01:03:15,110 --> 01:03:20,660
this is the idea of testing measurements to see if there are plausible so you

965
01:03:20,660 --> 01:03:24,750
remember from that earlier picture that we had with all of those normals cast out

966
01:03:25,110 --> 01:03:32,000
around the guy's profile and typically along each normal there are many responses so that's

967
01:03:32,000 --> 01:03:35,670
a bit of a problem the ambiguity of you know we're not knowing which is

968
01:03:35,670 --> 01:03:38,230
the valid observation so

969
01:03:38,280 --> 01:03:42,030
in in the sort of fifteen minutes something i'm going to to look at a

970
01:03:42,030 --> 01:03:46,800
more radical approach to dealing with that but a very reasonably tasteful attack

971
01:03:46,810 --> 01:03:51,850
it is the the following idea the validation gate this is really outside the gas

972
01:03:52,020 --> 01:03:53,150
framework now

973
01:03:53,170 --> 01:03:58,490
ruined the exactness of the gaussians filter by introducing this relation gate but still

974
01:03:58,580 --> 01:04:02,290
people do it because it's a natural thing to do it you will bring in

975
01:04:02,590 --> 01:04:04,410
a switch into the

976
01:04:04,430 --> 01:04:08,880
filter hypothesis test so the idea is that if the

977
01:04:08,890 --> 01:04:10,220
measurement you are

978
01:04:10,230 --> 01:04:11,370
about to see

979
01:04:11,530 --> 01:04:18,370
has a an observation distribution of this form it's unbiased so the main observation is

980
01:04:18,370 --> 01:04:19,620
corresponds to the

981
01:04:19,670 --> 01:04:24,350
o point actually on the curve and it has a following covariance and we'll forget

982
01:04:24,350 --> 01:04:28,870
for a moment about the general case where we can write down the covariance so

983
01:04:28,870 --> 01:04:32,700
let's just think about corners for now but this cannot be done for tangent observations

984
01:04:32,700 --> 01:04:34,100
perfectly nicely as well

985
01:04:34,110 --> 01:04:35,410
so now

986
01:04:35,440 --> 01:04:38,190
we can write down what the

987
01:04:38,210 --> 01:04:44,980
distribution is for expected response expected observation zns and it's just a normal distribution

988
01:04:44,990 --> 01:04:51,070
whose mean is just is taken directly from the prediction and whose variance

989
01:04:51,130 --> 01:04:55,720
is made up of two components one is the covariance associated with the observation itself

990
01:04:55,770 --> 01:05:00,470
and the other is the sort of inherited covariance from the prior distribution with the

991
01:05:00,470 --> 01:05:04,050
ages all in the right places so we can meaningfully at those things together so

992
01:05:04,050 --> 01:05:09,550
concepts for transfer transportation so steering a road vehicle

993
01:05:09,570 --> 01:05:13,490
transportation events generally a tipping over

994
01:05:13,550 --> 01:05:19,280
so when you try to acquire too quickly that supply transportation event and and what

995
01:05:20,780 --> 01:05:24,130
incurring damage colliding currents in vehicle accidents

996
01:05:24,150 --> 01:05:27,090
transport across events bring a vehicle

997
01:05:28,050 --> 01:05:32,030
most of the things that you want to talk about we're talking about transfer is

998
01:05:32,030 --> 01:05:37,070
logical vocabulary for talking about the sorts of things that you can do logical or

999
01:05:37,070 --> 01:05:40,970
about know you can do logical inference about everything

1000
01:05:41,950 --> 01:05:47,800
it's useful to do it and often the terms already available and again already

1001
01:05:47,820 --> 01:05:50,360
i cannot find out as a

1002
01:05:50,380 --> 01:05:53,450
as a knowledge base

1003
01:05:54,760 --> 01:05:58,030
relating events in their participants so in

1004
01:05:58,150 --> 01:06:03,420
it's a very important class of concepts situation parts

1005
01:06:03,440 --> 01:06:12,440
so and then types a specialisation situation so for giving a tutorial

1006
01:06:12,510 --> 01:06:13,860
is a

1007
01:06:13,880 --> 01:06:16,920
the event type therefore the situation time

1008
01:06:16,920 --> 01:06:24,090
and therefore we know that our role in such a situation types their subevents

1009
01:06:24,150 --> 01:06:25,920
relationships between some of it

1010
01:06:25,940 --> 01:06:31,170
situations and relationships between some some situations there are participants

1011
01:06:31,190 --> 01:06:37,470
x is this there props things which are involved which are sort of used by

1012
01:06:37,470 --> 01:06:43,880
the participants and their predicates reduced to relate these together so far is

1013
01:06:44,050 --> 01:06:45,530
it's pretty good

1014
01:06:45,530 --> 01:06:51,030
inputsdestroyed so what if i am eating eating is the kind of event inside the

1015
01:06:52,940 --> 01:06:58,800
instance of anything event michael witbrock is launched on the twenty seventh of

1016
01:06:59,070 --> 01:07:01,030
october two two thousand eight

1017
01:07:01,050 --> 01:07:03,690
that's a that's eating event

1018
01:07:03,690 --> 01:07:08,240
inputsdestroyed in the evening event were bullshit

1019
01:07:09,190 --> 01:07:15,030
so there were some instances of membership for some particular instance of

1020
01:07:15,030 --> 01:07:17,050
the whole system

1021
01:07:17,110 --> 01:07:21,680
the benefits to the beneficiary of that evening event was either me or the restaurant

1022
01:07:21,680 --> 01:07:27,650
depending on how happy i was in new york

1023
01:07:27,670 --> 01:07:30,440
the outputscreated and that if you have info

1024
01:07:30,450 --> 01:07:35,940
and the sign triphosphate i guess ultimately and in in g

1025
01:07:35,940 --> 01:07:37,860
in nothing

1026
01:07:37,880 --> 01:07:38,780
in fact

1027
01:07:38,800 --> 01:07:45,470
the device used in any event was a piece of

1028
01:07:45,510 --> 01:07:50,380
great to transport of chopped tomatoes to my mouth

1029
01:07:54,630 --> 01:07:59,380
in the going to a restaurant event was also used in order to have a

1030
01:07:59,380 --> 01:08:04,400
computer network computer device used to get take the money

1031
01:08:04,440 --> 01:08:07,380
there wasn't a vehicle there wasn't anything damage

1032
01:08:07,440 --> 01:08:15,530
it was the providerofmotiveforce already the rule on eating events we say that the

1033
01:08:15,550 --> 01:08:17,210
the consumer

1034
01:08:17,220 --> 01:08:22,820
of the earth encounter meeting event is the providerofmotiveforce for the meeting events you don't

1035
01:08:22,820 --> 01:08:25,470
have to say that there are over

1036
01:08:25,530 --> 01:08:31,760
four hundred more of these relationships if you're of patient and you remember

1037
01:08:31,820 --> 01:08:33,590
friends and slots

1038
01:08:33,690 --> 01:08:41,910
these are the sorts of relations these sorts of relationships which were typically solvents inference

1039
01:08:42,800 --> 01:08:45,280
don't stuff

1040
01:08:45,300 --> 01:08:47,800
yes idea sort of

1041
01:08:47,820 --> 01:08:49,900
the frame system

1042
01:08:49,920 --> 01:08:52,190
in fact if not in

1043
01:08:53,440 --> 01:09:00,280
these these tend to be very very useful predicates

1044
01:09:00,320 --> 01:09:05,420
eleven thousand types of cutting knows singing

1045
01:09:05,440 --> 01:09:10,740
the training stage devices

1046
01:09:11,000 --> 01:09:13,550
giving something

1047
01:09:13,570 --> 01:09:17,320
back OK view of virus when you find the

1048
01:09:17,570 --> 01:09:22,710
the USP computer

1049
01:09:23,110 --> 01:09:26,440
carving cracking buying

1050
01:09:26,490 --> 01:09:27,880
changing the temperature

1051
01:09:28,000 --> 01:09:30,420
change site

1052
01:09:30,440 --> 01:09:37,050
almost anything you can say i have been type four or more importantly

1053
01:09:37,170 --> 01:09:41,400
almost everything of there's an event type in reach of

1054
01:09:42,300 --> 01:09:51,320
say i represent i know know finding treasure because of its we interested in

1055
01:09:51,360 --> 01:09:54,010
sunken ships

1056
01:09:54,010 --> 01:09:56,240
if you start from scratch

1057
01:09:56,240 --> 01:09:59,430
classifier that answers your

1058
01:10:00,990 --> 01:10:06,760
right so the younger you're supervised classification algorithm will give you

1059
01:10:07,320 --> 01:10:11,510
a classifier map access the actions and that's what you're going to use to make

1060
01:10:16,740 --> 01:10:21,560
you can analyse

1061
01:10:21,560 --> 01:10:26,970
get the statement like this so your policy regret

1062
01:10:26,990 --> 01:10:29,560
well the bounded by four key

1063
01:10:29,580 --> 01:10:32,060
times the the binary regret

1064
01:10:32,120 --> 01:10:35,930
the adagrad of the classifier

1065
01:10:35,950 --> 01:10:40,600
on the classification problem

1066
01:10:40,620 --> 01:10:44,560
so there's no square root dependence but there is the dependence on the number of

1067
01:11:34,330 --> 01:11:43,290
and there's a link on the tutorial web page

1068
01:11:43,310 --> 01:11:44,810
to of the paper that

1069
01:11:44,930 --> 01:11:47,620
she was how to do that

1070
01:11:47,620 --> 01:11:51,640
one that

1071
01:11:57,510 --> 01:12:02,180
you can use any supervised

1072
01:12:02,200 --> 01:12:04,450
a regression approach

1073
01:12:04,470 --> 01:12:08,430
so you so you can use the same graph you can use the same rejection

1074
01:12:08,430 --> 01:12:15,740
sampling to get rid of the importance is in the regression approach

1075
01:12:21,160 --> 01:12:34,410
so if you remember is greater than the importance weight

1076
01:12:34,430 --> 01:12:37,220
the new you through

1077
01:12:37,280 --> 01:12:42,370
and this is make sure you you just the distribution so it's

1078
01:12:42,370 --> 01:12:46,310
it is if you draw from

1079
01:12:46,350 --> 01:12:49,530
this is this paper that we work with younger

1080
01:12:49,530 --> 01:12:50,890
and the killing

1081
01:12:51,830 --> 01:12:53,510
scrapped that

1082
01:12:53,510 --> 01:12:58,970
make sure to draw your learning three distribution which optimize overall

1083
01:13:07,160 --> 01:13:13,290
why would they use these as importance weight

1084
01:13:13,310 --> 01:13:16,450
so if it was a rare actions

1085
01:13:16,470 --> 01:13:20,260
then you boost the reward

1086
01:13:20,310 --> 01:13:25,410
because you divide by small numbers so you boost the word the observed word the

1087
01:13:25,410 --> 01:13:28,560
freighter actions

1088
01:13:28,580 --> 01:13:29,640
and that is

1089
01:13:29,660 --> 01:13:38,870
as as this or this gives you this unbiased estimate of the word

1090
01:13:38,940 --> 01:13:43,850
but the post the link to this paper the john mentioned

1091
01:13:43,850 --> 01:13:46,180
the algorithm is called

1092
01:13:46,390 --> 01:13:52,990
cost and

1093
01:13:53,060 --> 01:14:15,120
no it's

1094
01:14:15,160 --> 01:14:16,990
so it's a different

1095
01:14:17,050 --> 01:14:18,830
algorithm is just

1096
01:14:18,830 --> 01:14:20,530
it gives you a

1097
01:14:20,560 --> 01:14:22,620
of transform in

1098
01:14:22,640 --> 01:14:25,970
the multiclass classifier into policy

1099
01:14:27,180 --> 01:14:31,200
you partially built problems

1100
01:14:47,600 --> 01:14:57,740
so this is an important point so it doesn't tell you how to choose actions

1101
01:14:58,550 --> 01:15:01,870
so this is what i talked about the previous slide

1102
01:15:01,870 --> 01:15:03,260
right you can either

1103
01:15:04,280 --> 01:15:06,680
randomly and then exploit

1104
01:15:06,740 --> 01:15:08,450
or you can

1105
01:15:08,450 --> 01:15:10,200
do EXP before

1106
01:15:10,220 --> 01:15:16,910
so this tells you how to explore how to choose actions to make optimal decisions

1107
01:15:16,930 --> 01:15:19,330
and here

1108
01:15:19,350 --> 01:15:20,830
i'm giving your way

1109
01:15:21,620 --> 01:15:26,370
use exploration the city half exploration the this for

1110
01:15:26,390 --> 01:15:27,600
you can apply

1111
01:15:27,620 --> 01:15:30,390
any supervised learning algorithm

1112
01:15:30,420 --> 01:15:34,370
to learn a policy

1113
01:15:34,410 --> 01:15:47,330
that will help you make decisions in the future

1114
01:15:47,930 --> 01:15:51,640
there's really of for orthogonal angles

1115
01:15:52,870 --> 01:15:55,260
so the first was

1116
01:15:55,280 --> 01:15:59,510
to that there is a regression problem so you already have data of the form

1117
01:16:00,810 --> 01:16:01,970
it's a feature

1118
01:16:01,990 --> 01:16:06,740
features action taken the words the probability of taking that action

1119
01:16:06,890 --> 01:16:11,760
and you can treat it as a regression problem

1120
01:16:11,760 --> 01:16:14,600
or you can see that there is an importance weighted

1121
01:16:14,600 --> 01:16:17,390
classification problems

1122
01:16:18,200 --> 01:16:20,200
you can do something better

1123
01:16:20,220 --> 01:16:22,550
using the offset the

1124
01:16:22,580 --> 01:16:24,930
and that i will describe next

1125
01:16:24,990 --> 01:16:28,160
and this is sort of a better way

1126
01:16:28,180 --> 01:16:31,680
of using this exploration the

1127
01:16:31,700 --> 01:16:33,870
to learn a slightly better

1128
01:16:35,870 --> 01:16:37,850
OK so you have

1129
01:16:38,080 --> 01:16:43,410
let's just say that i have two actions for simplicity there are two actions were

1130
01:16:43,410 --> 01:16:44,680
chosen from

1131
01:16:44,700 --> 01:16:47,780
and this is a trick

1132
01:16:47,830 --> 01:16:49,470
that will give you

1133
01:16:49,490 --> 01:16:51,740
a better

1134
01:16:51,760 --> 01:16:57,970
performance that will improve your performance and we'll see why

1135
01:16:57,990 --> 01:16:59,970
so that the cos two

1136
01:16:59,990 --> 01:17:06,580
so you were is larger than one house so you have this example

1137
01:17:06,580 --> 01:17:12,870
coming from so this is your exploration example acts action they can reward probability of

1138
01:17:12,870 --> 01:17:14,280
taking that action

1139
01:17:14,280 --> 01:17:18,140
you do with this higher than one half

1140
01:17:18,160 --> 01:17:23,470
then you say that was a good action to take so you form an

1141
01:17:23,490 --> 01:17:27,160
an example classification example acts

1142
01:17:27,180 --> 01:17:28,530
common e

1143
01:17:28,560 --> 01:17:31,790
so you want your classifier to predict the

1144
01:17:31,830 --> 01:17:34,660
on x and this is an important

1145
01:17:35,760 --> 01:17:39,030
right so this is the observed reward

1146
01:17:39,060 --> 01:17:40,830
minus one half

1147
01:17:40,850 --> 01:17:43,990
so this is the distance from one half

1148
01:17:44,010 --> 01:17:45,310
divided by p

1149
01:17:45,330 --> 01:17:49,240
so instead of importance weighted by our over p

1150
01:17:49,260 --> 01:17:55,140
you importance weighted by those offset from one half

1151
01:17:55,160 --> 01:17:57,660
by this distance from one half

1152
01:17:57,660 --> 01:17:59,050
OK so

1153
01:17:59,060 --> 01:18:03,410
now let me talk about least squares with linear constraints

1154
01:18:03,430 --> 01:18:05,640
there's a bunch of techniques that

1155
01:18:05,650 --> 01:18:07,280
one can use for that

1156
01:18:08,130 --> 01:18:11,680
one of these ideas to use the branch multipliers

1157
01:18:11,700 --> 01:18:15,580
and you get a system that looks like this is a transposing a looks like

1158
01:18:15,580 --> 01:18:16,890
normal equations

1159
01:18:16,960 --> 01:18:17,780
and then

1160
01:18:17,790 --> 01:18:22,000
this this matrix c is sitting here and here

1161
01:18:22,040 --> 01:18:24,630
and you you have to solve in this

1162
01:18:24,780 --> 01:18:30,690
formulation not only for the unknowns but for the lagrange multipliers so you've actually increase

1163
01:18:30,710 --> 01:18:32,810
the dimensionality of the problem

1164
01:18:33,670 --> 01:18:38,650
if you had an variables and p constraints now you have to solve for ten

1165
01:18:38,650 --> 01:18:46,250
plus p anyway that's not a good idea ocean because you increase the dimensionality really

1166
01:18:46,250 --> 01:18:52,560
constraints reduces the dimensionality and you should reduce the complexity

1167
01:18:52,580 --> 01:18:59,310
the problem and often you make it better condition by eliminating the constraints

1168
01:19:06,980 --> 01:19:07,790
o is an idea

1169
01:19:07,870 --> 01:19:11,830
finish my discussion singular systems i'm sorry i got so carried away here

1170
01:19:12,240 --> 01:19:16,670
a i talked about the QR factorizations just a moment ago

1171
01:19:16,680 --> 01:19:22,330
here's something what that you can do if the matrix is exactly are not only

1172
01:19:22,330 --> 01:19:23,570
can you performed

1173
01:19:23,590 --> 01:19:27,350
orthogonal transformations on the left but on the right also

1174
01:19:27,360 --> 01:19:32,300
and so we have this complete orthogonal factorizations

1175
01:19:32,320 --> 01:19:36,540
and others what's known as the pseudo inverse of a matrix

1176
01:19:36,550 --> 01:19:40,970
it can be constructed just from this decomposition is just like an inverse of this

1177
01:19:41,820 --> 01:19:45,370
one of the full rank you would get exactly the inverse of the matrix

1178
01:19:45,420 --> 01:19:53,780
so here's the definition of the pseudo inverse of a matrix satisfies these four properties

1179
01:19:53,800 --> 01:19:57,070
and sometimes called the more penrose pseudoinverse

1180
01:19:58,040 --> 01:20:01,960
it in solving the least squares problem if you

1181
01:20:01,980 --> 01:20:04,930
construct the pseudoinverse you find out

1182
01:20:04,950 --> 01:20:10,530
that's not only do you minimize the residual vector but you minimize the length of

1183
01:20:10,530 --> 01:20:16,200
the solution vector so that's the the great use of the pseudo inverse it gives

1184
01:20:16,910 --> 01:20:23,230
but the the minimum residual plus the minimum length of the solution

1185
01:20:24,020 --> 01:20:28,160
that may or may not be desirable you may want solution the minimum length or

1186
01:20:28,160 --> 01:20:31,270
you may have some other constraint that you want to get out of all these

1187
01:20:31,270 --> 01:20:33,440
possible solutions

1188
01:20:33,450 --> 01:20:37,450
OK here we go to least squares with linear constraints

1189
01:20:37,520 --> 01:20:42,410
and here i have this augmented system which is now an plus the by implies

1190
01:20:42,420 --> 01:20:46,370
p and this comes up in so many different applications

1191
01:20:46,490 --> 01:20:54,810
the people work in math programming they call the KKT karush kuhn tucker that they

1192
01:20:54,810 --> 01:21:02,140
were the first ones in in the math programming to to use this so extensively

1193
01:21:02,150 --> 01:21:06,510
OK well the idea that i outlined a little procedure where you

1194
01:21:06,530 --> 01:21:11,400
south course for the unconstrained system providing you can

1195
01:21:11,400 --> 01:21:15,800
and then when she was out for the unconstrained you south of the grunge multipliers

1196
01:21:15,800 --> 01:21:17,500
given by these equations here

1197
01:21:17,980 --> 01:21:19,570
and then you modify

1198
01:21:19,570 --> 01:21:22,010
the least squares solution you would have gone

1199
01:21:22,040 --> 01:21:24,700
without the constrained by

1200
01:21:24,730 --> 01:21:30,090
adding that this modification and then you get the solution to the least squares problem

1201
01:21:30,160 --> 01:21:34,900
so that's sometimes very useful in statistics often you have the situation

1202
01:21:34,910 --> 01:21:38,950
you want to know what the solution would be without the constraints and then you

1203
01:21:38,950 --> 01:21:43,330
want to know what the solution would look like after you impose constraints and this

1204
01:21:43,330 --> 01:21:51,650
is a technique for getting the two solutions providing they both exist

1205
01:21:51,720 --> 01:21:57,560
OK let me just say a bit more about this is the some ongoing work

1206
01:21:57,740 --> 01:22:01,900
these matrices come up and as i say in so many applications of particular and

1207
01:22:01,900 --> 01:22:04,100
solving partial differential equations

1208
01:22:04,340 --> 01:22:10,410
one situation is where this matrix a transpose a is singular or almost singular

1209
01:22:10,430 --> 01:22:12,660
so what you do is

1210
01:22:12,660 --> 01:22:15,440
you have a dispute over here

1211
01:22:15,450 --> 01:22:21,330
so a transpose a plus CWC transpose sea trout transpose x by this condition here

1212
01:22:21,330 --> 01:22:22,580
is equal to zero

1213
01:22:23,180 --> 01:22:28,100
you sort of increase the this

1214
01:22:28,560 --> 01:22:33,760
the increase the condition number are well actually decrease the condition number of the system

1215
01:22:33,760 --> 01:22:37,320
over here that can be almost always shown

1216
01:22:37,450 --> 01:22:41,130
this matrix w is

1217
01:22:41,160 --> 01:22:42,750
matrix that you

1218
01:22:44,080 --> 01:22:48,910
there are all kinds of possibilities it doesn't have to be a multiple of the

1219
01:22:48,910 --> 01:22:51,340
identity matrix talk about that

1220
01:22:51,380 --> 01:22:56,270
here i set w of multiple the identity matrix but it can be chosen just

1221
01:22:56,270 --> 01:22:59,400
to choose a few of the

1222
01:22:59,420 --> 01:23:05,490
the columns of the matrix c and just enough to make a transpose a nonsingular

1223
01:23:05,570 --> 01:23:08,790
and of course you want to do this with some choice

1224
01:23:10,070 --> 01:23:14,470
it may be that a transfer a has a sparse structure by adding in the

1225
01:23:15,590 --> 01:23:19,690
you screwed up the entire structure so you may only want to choose a few

1226
01:23:19,710 --> 01:23:27,180
of the columns of c rather than all the columns of c

1227
01:23:27,190 --> 01:23:31,350
OK well is just a small amusing detail

1228
01:23:31,350 --> 01:23:33,970
if you take the inverse of this matrix

1229
01:23:34,000 --> 01:23:38,960
it's the inverse of the matrix without this part minus this quantity here

1230
01:23:38,970 --> 01:23:43,990
so this is a this is how the inverse changes when you add in this

1231
01:23:44,190 --> 01:23:49,890
matrix c wc transpose and what that leads to is the following

1232
01:23:49,930 --> 01:23:54,990
but the condition number of this new system is bounded by the condition number of

1233
01:23:55,000 --> 01:24:00,680
the original system plus some additional terms which means actually that the condition number of

1234
01:24:01,670 --> 01:24:06,890
larger system may actually increase by adding this part portion here

1235
01:24:06,890 --> 01:24:12,240
nevertheless some the one so this is interesting the one block becomes

1236
01:24:12,270 --> 01:24:20,240
that condition but the entire matrix becomes may possibly be more field conditions

1237
01:24:20,250 --> 01:24:25,900
and in fact we have done some iterations schemes and sometimes there's a remarkable change

1238
01:24:25,950 --> 01:24:27,810
by just adding that very

1239
01:24:28,110 --> 01:24:34,130
by adding the quality of the larger gamma is the boreal condition the matrix is

1240
01:24:34,140 --> 01:24:39,840
the convergence baby fast methodist college in rats and it's a very effective way for

1241
01:24:39,840 --> 01:24:45,920
solving nonsense symmetric or indefinite systems of equations

1242
01:24:54,850 --> 01:24:58,390
here is a very useful technique

1243
01:24:58,590 --> 01:25:02,840
where you can use the technology that you already have

1244
01:25:02,890 --> 01:25:08,110
as the following so here we have this constraint problem already and now let's add

1245
01:25:08,110 --> 01:25:12,940
in this weighting function square after all and citrus lexical zero

1246
01:25:12,970 --> 01:25:14,420
and this goes away

1247
01:25:14,440 --> 01:25:20,180
and we and the constraint is satisfied so the idea is

1248
01:25:20,190 --> 01:25:26,060
augment the original matrix a by mute times the transpose so that just it says

1249
01:25:26,060 --> 01:25:31,000
if you see trams are more observations in the problem and you want to solve

1250
01:25:31,000 --> 01:25:31,840
this new

1251
01:25:31,840 --> 01:25:33,400
least squares problems

1252
01:25:34,050 --> 01:25:38,800
OK and for large and the solution x had new of the unconstrained problem should

1253
01:25:38,800 --> 01:25:41,300
be a good approximation to the solution

1254
01:25:46,750 --> 01:25:48,950
my colleague charlie

1255
01:25:48,960 --> 01:25:53,100
he actually show that this action the for mu going to infinity

1256
01:25:53,100 --> 01:25:59,500
this then does go towards the solution of the least constrained least squares problem what

1257
01:25:59,500 --> 01:26:04,470
he uses for this was called the generalized singular value decomposition this is a very

1258
01:26:04,470 --> 01:26:05,930
body centered cubic

1259
01:26:05,940 --> 01:26:07,360
and we go down at all

1260
01:26:07,370 --> 01:26:08,580
one one

1261
01:26:08,660 --> 01:26:10,370
simple cubic reflects

1262
01:26:10,380 --> 01:26:15,270
body centered cubic reflects would face centered cubic doesn't reflect so it's you can't look

1263
01:26:15,270 --> 01:26:16,800
in any single line

1264
01:26:16,820 --> 01:26:19,470
you have to look at the entire pattern

1265
01:26:19,510 --> 01:26:20,820
and so on

1266
01:26:20,870 --> 01:26:24,220
and there's a simple way of of kind of flying all of this

1267
01:26:24,300 --> 01:26:26,630
simple cubic all

1268
01:26:26,660 --> 01:26:27,810
all of the

1269
01:26:27,840 --> 01:26:30,310
indices are reflecting

1270
01:26:30,320 --> 01:26:35,140
in body centered cubic if you go through this pattern what we're looking at is

1271
01:26:35,170 --> 01:26:39,110
the sum of eight squares plus case where it applies elsewhere and that gives you

1272
01:26:39,110 --> 01:26:42,480
a nice sequence one two three four five and so on and what you see

1273
01:26:42,480 --> 01:26:45,180
with BCC is you have the sequence two

1274
01:26:45,200 --> 01:26:47,680
four six eight

1275
01:26:48,720 --> 01:26:51,520
the sum of age plus k plus al is even

1276
01:26:51,580 --> 01:26:54,790
so that's the way of capturing what's going on in this

1277
01:26:54,870 --> 01:26:55,870
the series

1278
01:26:55,880 --> 01:27:01,220
and for face centered cubic it turns out that the planes that reflect are either

1279
01:27:02,500 --> 01:27:03,720
even numbers

1280
01:27:03,730 --> 01:27:08,270
or all odd numbers so for example face centered cubic EC one one one

1281
01:27:08,320 --> 01:27:12,990
these are all i had to all zero is termed in this

1282
01:27:13,020 --> 01:27:18,950
calculation is being an even-numbered so twos even zero even in zero is even so

1283
01:27:18,950 --> 01:27:21,200
three and four reflect

1284
01:27:21,240 --> 01:27:23,180
five and six do not

1285
01:27:23,180 --> 01:27:28,150
eight does that's two two o and so on eight eleven twelve sixteen so this

1286
01:27:28,150 --> 01:27:30,860
is the set of selection criteria

1287
01:27:30,910 --> 01:27:32,160
that have been

1288
01:27:34,360 --> 01:27:35,810
your convenience

1289
01:27:35,830 --> 01:27:37,320
now we can do

1290
01:27:37,340 --> 01:27:40,860
now we can do is go into some example

1291
01:27:40,860 --> 01:27:43,980
using this technique in order to index the crystal

1292
01:27:44,000 --> 01:27:45,520
so i want to show you

1293
01:27:45,780 --> 01:27:48,530
two different techniques

1294
01:27:48,620 --> 01:27:50,410
that allow us to

1295
01:27:50,410 --> 01:27:52,710
determine the crystal structure

1296
01:27:52,800 --> 01:28:02,510
the first one is called the fractal geometry

1297
01:28:02,540 --> 01:28:07,270
the first one is called the different

1298
01:28:07,280 --> 01:28:13,730
and since it's done with x-rays it's also known as x-ray diffraction XRD

1299
01:28:13,740 --> 01:28:15,770
and the characteristics of

1300
01:28:15,780 --> 01:28:20,410
x-ray diffraction are we use radiation a fixed wavelength

1301
01:28:21,260 --> 01:28:23,880
we very the angle

1302
01:28:23,890 --> 01:28:26,330
variable angle so

1303
01:28:26,390 --> 01:28:28,270
look at this cartoon here

1304
01:28:28,290 --> 01:28:31,770
if we have fixed wavelength for varying the angle

1305
01:28:31,810 --> 01:28:34,960
i'm going to be moving the crystal this is i'm going to get very angle

1306
01:28:34,970 --> 01:28:37,510
what i'm saying is variable angle of incidence

1307
01:28:37,620 --> 01:28:41,340
now the words i want to probe all the incident angles and and build the

1308
01:28:41,360 --> 01:28:48,320
library of responses for a given incidence angle what's the intensity of the reflected radiation

1309
01:28:48,320 --> 01:28:50,110
and the way i do that is to

1310
01:28:50,120 --> 01:28:55,420
part being and rotate the specimen and ultimately i end up with a plot that

1311
01:28:55,430 --> 01:28:57,030
looks like this

1312
01:28:57,080 --> 01:29:01,670
what looks like this intensity versus angle

1313
01:29:01,670 --> 01:29:02,810
this is angle

1314
01:29:07,880 --> 01:29:09,780
so now we're going to decide

1315
01:29:10,540 --> 01:29:13,570
the pattern we observe conforms to

1316
01:29:13,610 --> 01:29:16,300
this set of rules up here

1317
01:29:16,320 --> 01:29:20,080
and by making that comparison we can deduce

1318
01:29:20,120 --> 01:29:23,670
which of these three crystal structures the

1319
01:29:23,720 --> 01:29:27,940
crystal belongs to and i'm keeping this is a very narrow set i'm saying this

1320
01:29:27,940 --> 01:29:29,720
three nine one so we

1321
01:29:29,740 --> 01:29:33,580
confine ourselves to the universe of cubic crystals if you want to get

1322
01:29:33,630 --> 01:29:34,990
more general than

1323
01:29:35,020 --> 01:29:38,280
it could be any one of the crystals that comes out of the fourteen brevity

1324
01:29:38,280 --> 01:29:39,820
lattices plus

1325
01:29:39,890 --> 01:29:42,400
a range of bases

1326
01:29:42,400 --> 01:29:45,440
so there's a huge database out there that has

1327
01:29:45,460 --> 01:29:48,670
values of intensity versus

1328
01:29:51,300 --> 01:29:53,490
thousands and thousands

1329
01:29:53,500 --> 01:29:56,480
of metals and compounds

1330
01:29:56,550 --> 01:29:59,720
huge amount of mineralogy in this database

1331
01:29:59,720 --> 01:30:02,450
so when you bring a specimen of unknown

1332
01:30:03,880 --> 01:30:05,990
we are able to use this technique

1333
01:30:06,010 --> 01:30:07,390
and infer

1334
01:30:09,000 --> 01:30:10,580
the constituents are

1335
01:30:10,590 --> 01:30:14,200
so first of all let's take a look at what the what the specimen looks

1336
01:30:14,200 --> 01:30:19,220
like i can take a few atoms we use a powdered specimens

1337
01:30:19,230 --> 01:30:20,780
powdered specimens

1338
01:30:20,790 --> 01:30:24,840
so we grind the material into a fine powder

1339
01:30:24,860 --> 01:30:27,870
and we put in the silicate

1340
01:30:27,920 --> 01:30:30,610
a silicate glass

1341
01:30:33,440 --> 01:30:36,880
and silicate glass holder looks something like this

1342
01:30:37,780 --> 01:30:39,720
it's very narrow

1343
01:30:39,720 --> 01:30:41,590
this is not to scale

1344
01:30:41,640 --> 01:30:44,820
definitely not to scale this dimension here

1345
01:30:44,860 --> 01:30:48,470
is about a tenth of a millimeter

1346
01:30:48,470 --> 01:30:51,540
very narrow to pretend to the millimetre made of

1347
01:30:51,570 --> 01:30:53,790
silicate glass

1348
01:30:53,840 --> 01:30:55,620
it's an oxide glasses

1349
01:30:55,620 --> 01:30:57,300
cousin of window glass

1350
01:30:57,320 --> 01:30:58,800
why do we choose it

1351
01:30:58,810 --> 01:31:02,780
not because it's transparent to visible light we don't care were shooting x-rays through this

1352
01:31:02,780 --> 01:31:07,160
thing for heaven sakes why do we choose this because it amorphous and it doesn't

1353
01:31:07,160 --> 01:31:09,190
have a crystal structure of its own

1354
01:31:09,260 --> 01:31:11,960
we're trying to radiate the contents of this

1355
01:31:11,960 --> 01:31:12,870
and the

1356
01:31:12,880 --> 01:31:16,910
the tube itself has the crystal structure how we got to sort out

1357
01:31:16,930 --> 01:31:22,300
what's the contents from the container so this is amorphous no long range order and

1358
01:31:22,300 --> 01:31:26,570
inside we put little

1359
01:31:26,630 --> 01:31:28,410
grains of powder in here

1360
01:31:28,450 --> 01:31:30,810
OK so the powder

1361
01:31:30,840 --> 01:31:34,720
the powder dimension is on the order of about o

1362
01:31:34,740 --> 01:31:37,130
fifty micrometers

1363
01:31:37,170 --> 01:31:39,160
fifty micrometers

1364
01:31:39,240 --> 01:31:45,220
and all the wall thickness i mention that that's one one-hundredth of a millimetre

1365
01:31:45,250 --> 01:31:49,400
saying it's paper-thin is an exaggeration it's one hundredth of a millimetre

1366
01:31:49,400 --> 01:31:51,530
and this distance is about old

1367
01:31:51,550 --> 01:31:53,970
two centimetres

1368
01:31:54,010 --> 01:31:58,250
so you growing and more personal drop the powder in here and then this goes

1369
01:31:58,250 --> 01:32:00,270
into the diffractive element

1370
01:32:00,310 --> 01:32:01,790
and is irradiated

1371
01:32:01,880 --> 01:32:04,240
and we're going to turn this

1372
01:32:04,300 --> 01:32:08,170
in order to present as many different angles as possible we have many grains and

1373
01:32:08,170 --> 01:32:09,430
here each grain

1374
01:32:09,470 --> 01:32:12,570
the the theory is if we grind is fine enough

1375
01:32:12,590 --> 01:32:16,080
each one of these can act as a single crystal

1376
01:32:16,130 --> 01:32:16,980
so now

1377
01:32:17,020 --> 01:32:21,380
light is coming in reflecting off this but it's cousin is sitting at a slightly

1378
01:32:21,380 --> 01:32:25,670
different angle and so i'm going to try to get through the mix of power

1379
01:32:25,670 --> 01:32:29,990
and the rotation of the specimen try to get all of the different incident angle

1380
01:32:29,990 --> 01:32:34,680
so that i will get as many reflected angles as possible

1381
01:32:34,690 --> 01:32:37,460
i i worked with this so my phd thesis

1382
01:32:37,510 --> 01:32:39,750
and it was the material was

1383
01:32:39,770 --> 01:32:44,110
i was sensitive to moisture in the air had to work in glovebox

1384
01:32:44,110 --> 01:32:47,010
these big thick rubber gloves on and and

1385
01:32:47,040 --> 01:32:50,600
holding something like this only it's really in real life it's about this big you're

1386
01:32:50,600 --> 01:32:53,510
inside the glovebox and its toronto and it's august

1387
01:32:53,530 --> 01:32:58,560
perspiration sports just post crying the perspiration is pouring down my face and then i

1388
01:32:58,560 --> 01:33:01,490
have to i have to get the power to fall in here

1389
01:33:01,500 --> 01:33:05,030
and i'm going to flick this little bit right look at one more time and

1390
01:33:05,030 --> 01:33:06,770
then it breaks off

1391
01:33:06,830 --> 01:33:10,470
these are you left now it's not funny

1392
01:33:10,470 --> 01:33:14,910
or they will be more if the coin has been cast yet and it could

1393
01:33:14,910 --> 01:33:16,940
have been already tossed conceal

1394
01:33:16,950 --> 01:33:18,940
why would that be

1395
01:33:18,960 --> 01:33:22,190
it might be this just some intuitive sense

1396
01:33:23,750 --> 01:33:25,780
that i that i can

1397
01:33:25,790 --> 01:33:27,550
i don't know i i can

1398
01:33:27,610 --> 01:33:31,580
i have some magical forces and here i can change things

1399
01:33:31,620 --> 01:33:34,840
so the idea of probability theory

1400
01:33:34,860 --> 01:33:39,970
is that no you can't change things these objective laws of probability out there that

1401
01:33:39,970 --> 01:33:41,070
guide everything

1402
01:33:41,120 --> 01:33:43,430
most languages around the world

1403
01:33:43,480 --> 01:33:45,050
i have

1404
01:33:45,060 --> 01:33:46,660
a different

1405
01:33:49,300 --> 01:33:50,710
word for

1406
01:33:51,620 --> 01:33:53,300
and the rest

1407
01:33:53,310 --> 01:33:55,840
or luck and fortune

1408
01:33:55,960 --> 01:34:01,370
luck seems to mean something about you i'm lucky person

1409
01:34:01,380 --> 01:34:04,990
i don't know what that means like the god or god's favor me him so

1410
01:34:04,990 --> 01:34:07,920
i'm lucky this is my lucky day

1411
01:34:07,940 --> 01:34:12,250
probability theory is really a movement away from that

1412
01:34:12,260 --> 01:34:15,680
so we then have a mathematical rigorous

1413
01:34:15,690 --> 01:34:19,110
discipline so

1414
01:34:19,130 --> 01:34:22,390
now i'm going to go through some of the

1415
01:34:22,400 --> 01:34:25,190
some of the terms of probability here

1416
01:34:27,090 --> 01:34:31,030
this will be review for many of you

1417
01:34:31,800 --> 01:34:34,050
it would be something that we're going to use

1418
01:34:34,100 --> 01:34:37,040
in the

1419
01:34:37,090 --> 01:34:39,460
so i use

1420
01:34:39,510 --> 01:34:41,010
the symbol p

1421
01:34:41,020 --> 01:34:43,810
or i can sometimes read out its probability

1422
01:34:43,890 --> 01:34:46,160
to represent a problem in probability

1423
01:34:47,190 --> 01:34:50,820
it is always the number that lies between

1424
01:34:50,870 --> 01:34:55,480
zero and one or between zero percent and a hundred percent

1425
01:34:56,250 --> 01:34:57,420
percent means

1426
01:34:57,430 --> 01:35:01,410
divided by hundred in that right so hundred percent is one

1427
01:35:01,420 --> 01:35:02,700
OK and so

1428
01:35:02,720 --> 01:35:07,760
this is the probability if the probability is zero that means the events

1429
01:35:07,770 --> 01:35:09,140
can help

1430
01:35:09,150 --> 01:35:11,470
if the probability is one

1431
01:35:11,490 --> 01:35:13,890
it means it certain to happen

1432
01:35:13,950 --> 01:35:17,260
if the probability is everyone see this from over there

1433
01:35:18,700 --> 01:35:22,200
and if i can model can probably move can i

1434
01:35:22,220 --> 01:35:24,290
just like

1435
01:35:24,620 --> 01:35:30,160
you you know you're the most disadvantaged person you can see it

1436
01:35:34,290 --> 01:35:37,250
that's the basic idea

1437
01:35:37,370 --> 01:35:42,920
now one of the first principles of probability is the idea of independence

1438
01:35:49,570 --> 01:35:51,920
the idea is that the probability

1439
01:35:52,000 --> 01:35:53,890
is the

1440
01:35:57,490 --> 01:35:59,180
likelihood of some

1441
01:35:59,190 --> 01:36:03,640
outcome that say the outcome of an experiment okay like tossing a coin

1442
01:36:03,650 --> 01:36:08,110
you might say the probability that the particle and it comes up heads is

1443
01:36:08,160 --> 01:36:12,210
i have OK because equally likely to heads and tails

1444
01:36:12,270 --> 01:36:19,490
independent experiments are experiments that occur without relation to each other

1445
01:36:19,510 --> 01:36:21,670
so if you toss a coin twice

1446
01:36:21,670 --> 01:36:26,430
and the first experiment doesn't influence the second we say the independent

1447
01:36:26,460 --> 01:36:29,460
and so the there's no relation between the two

1448
01:36:29,470 --> 01:36:36,050
and one of the first principles of probability theory is called the multiplication

1449
01:36:41,350 --> 01:36:44,650
and that says if you have independent probabilities

1450
01:36:44,670 --> 01:36:46,900
the probability of two events

1451
01:36:46,930 --> 01:36:51,170
is equal to the product of the probabilities so the probability

1452
01:36:53,450 --> 01:36:56,190
and the

1453
01:36:56,260 --> 01:36:58,670
is equal to the probability

1454
01:36:58,720 --> 01:37:00,240
of a

1455
01:37:01,830 --> 01:37:06,300
the probability of being

1456
01:37:09,240 --> 01:37:13,580
that wouldn't hold if they are not independent

1457
01:37:13,620 --> 01:37:18,550
the theory of insurance is that ideally an insurance company wants to

1458
01:37:19,440 --> 01:37:21,910
independent events OK

1459
01:37:21,950 --> 01:37:25,400
so ideally life insurance is insurance people

1460
01:37:25,400 --> 01:37:31,150
or fire insurance is ensuring people against independent events it's not the fire of london

1461
01:37:31,160 --> 01:37:33,440
it's the for its the problem that

1462
01:37:33,500 --> 01:37:37,710
sometimes people knock over an oil lamp in the home

1463
01:37:37,760 --> 01:37:40,090
and they bring their own house down

1464
01:37:40,090 --> 01:37:43,780
and so it's not going to burn any other houses down this is just completely

1465
01:37:43,780 --> 01:37:45,700
independent of anything else

1466
01:37:45,730 --> 01:37:51,760
so the probability that the whole city burns down is infinitesimally small right

1467
01:37:51,810 --> 01:37:56,350
this this will generalize to probability of a and b and c

1468
01:37:56,390 --> 01:38:00,090
is the probability of a times the probability of the improbable times the probability of

1469
01:38:00,090 --> 01:38:01,370
c and so on

1470
01:38:01,390 --> 01:38:05,670
so if there a probability is one thousand that house burns down

1471
01:38:05,720 --> 01:38:10,350
and there's a thousand houses the probability that they all bring down is probably is

1472
01:38:11,420 --> 01:38:13,930
one thousand to thousand power

1473
01:38:13,950 --> 01:38:15,560
which is virtually zero

1474
01:38:15,560 --> 01:38:17,910
so our insurance companies that

1475
01:38:17,920 --> 01:38:23,310
basically if they write a lot of policies they have virtually no risk

1476
01:38:23,350 --> 01:38:27,020
and so that is the

1477
01:38:27,050 --> 01:38:28,970
the the fundamental idea

1478
01:38:28,990 --> 01:38:31,530
which may seem simple and obvious

1479
01:38:32,230 --> 01:38:36,670
but it certainly wasn't that when the idea first came up

1480
01:38:36,690 --> 01:38:40,380
so one of the

1481
01:38:40,430 --> 01:38:44,060
it's that we have a problem set which i wanted to start today and we

1482
01:38:44,940 --> 01:38:48,000
not a week this time because we have

1483
01:38:48,650 --> 01:38:50,750
martin luther king day

1484
01:38:50,780 --> 01:38:53,480
coming up but it will be do

1485
01:38:53,780 --> 01:38:56,230
the monday following that

1486
01:39:01,850 --> 01:39:07,810
if you follow through from the independent variables

1487
01:39:07,820 --> 01:39:12,260
one of the basic relations in probability theory

1488
01:39:12,280 --> 01:39:15,770
it's called the binomial distribution

1489
01:39:16,700 --> 01:39:17,660
i'm not going to spend

1490
01:39:17,660 --> 01:39:24,700
spend a whole lot of time on this

1491
01:39:24,720 --> 01:39:27,630
but it gives the probability of

1492
01:39:27,670 --> 01:39:29,710
x successes

1493
01:39:29,740 --> 01:39:33,850
in n trials or in the case of insurance

1494
01:39:34,800 --> 01:39:36,710
if you're unsure against an accident

1495
01:39:36,740 --> 01:39:39,760
the probability that you get x accidents

1496
01:39:40,460 --> 01:39:42,000
and trials

1497
01:39:43,390 --> 01:39:47,390
it binomial distribution gives the probability

1498
01:39:47,430 --> 01:39:49,390
as a function of x

1499
01:39:49,410 --> 01:39:51,910
and is given by the formula

1500
01:39:51,930 --> 01:39:55,490
where p is the probability of the accident

1501
01:39:55,530 --> 01:39:57,750
peter the acts

1502
01:39:57,800 --> 01:40:00,510
one minus the

1503
01:40:00,530 --> 01:40:03,670
today and minus axis

1504
01:40:03,670 --> 01:40:06,620
times and factorial

1505
01:40:08,090 --> 01:40:11,480
and minus x

1506
01:40:13,650 --> 01:40:16,840
and so that's the formula

1507
01:40:16,840 --> 01:40:18,840
do this do this again

1508
01:40:18,880 --> 01:40:22,590
so now what the cheapest thing to extract

1509
01:40:22,640 --> 01:40:25,370
this guy here right

1510
01:40:25,370 --> 01:40:27,620
so we'll take him out

1511
01:40:27,640 --> 01:40:31,380
OK and now we update all his neighbours

1512
01:40:31,390 --> 01:40:34,010
so this guy gets five

1513
01:40:34,060 --> 01:40:35,710
this guy

1514
01:40:35,720 --> 01:40:37,370
gets twelve

1515
01:40:37,430 --> 01:40:40,640
this guy gets nine

1516
01:40:40,670 --> 01:40:44,490
this guy we don't update

1517
01:40:45,100 --> 01:40:48,050
we don't update and because he's no longer

1518
01:40:48,050 --> 01:40:49,390
in the queue

1519
01:40:49,420 --> 01:40:50,660
the priority queue

1520
01:40:50,710 --> 01:40:54,680
in all these guys now we make a point

1521
01:40:54,700 --> 01:40:57,220
the way they supposed to point to

1522
01:40:57,270 --> 01:40:59,230
and we're done with that step

1523
01:40:59,250 --> 01:41:04,040
now we find the cheapest one was the cheapest one now

1524
01:41:04,060 --> 01:41:07,460
the five over here good so we can out

1525
01:41:07,460 --> 01:41:10,520
OK we update

1526
01:41:10,550 --> 01:41:11,750
the neighbours

1527
01:41:12,770 --> 01:41:14,300
yeah that goes to

1528
01:41:14,310 --> 01:41:16,450
o to six now

1529
01:41:16,460 --> 01:41:18,170
and we have

1530
01:41:18,210 --> 01:41:19,670
that pointer

1531
01:41:19,720 --> 01:41:22,550
and this guy we don't do because it's not in there

1532
01:41:22,630 --> 01:41:23,760
this guy

1533
01:41:23,760 --> 01:41:25,800
becomes fourteen

1534
01:41:25,810 --> 01:41:29,920
and this guy here becomes

1535
01:41:32,160 --> 01:41:34,080
we update

1536
01:41:34,090 --> 01:41:36,810
that making b

1537
01:41:38,040 --> 01:41:39,330
i do this the right way

1538
01:41:44,520 --> 01:41:47,700
because the because pi is a function from the sky

1539
01:41:47,710 --> 01:41:49,770
so basically this thing then

1540
01:41:49,800 --> 01:41:51,830
this appears

1541
01:41:51,840 --> 01:41:54,960
i have another one that i missed

1542
01:41:55,020 --> 01:41:58,950
twelve yes good gets removed

1543
01:41:58,960 --> 01:42:02,230
OK because pi is just function

1544
01:42:02,410 --> 01:42:04,960
and now i'm OK

1545
01:42:04,990 --> 01:42:08,620
OK so now what do i do i pick

1546
01:42:08,660 --> 01:42:12,390
OK so now my set a consists of these three things

1547
01:42:12,410 --> 01:42:16,020
and now i want the cheapest and i know it's in minimum spanning trees on

1548
01:42:16,100 --> 01:42:18,620
just greedily pick OK

1549
01:42:18,630 --> 01:42:22,200
so what's the cheapest thing now

1550
01:42:22,290 --> 01:42:25,790
the uh this guy appear

1551
01:42:25,800 --> 01:42:28,090
six we take it

1552
01:42:28,130 --> 01:42:28,770
the goal

1553
01:42:28,840 --> 01:42:33,460
update these things and it doesn't matter nothing matters here

1554
01:42:33,460 --> 01:42:37,210
OK nothing changes to these guys are already in a

1555
01:42:37,210 --> 01:42:41,060
OK so now the cheapest one is

1556
01:42:41,080 --> 01:42:42,230
eight here

1557
01:42:43,040 --> 01:42:44,970
take it out

1558
01:42:44,990 --> 01:42:46,120
we update

1559
01:42:46,130 --> 01:42:47,950
there's nothing to be done

1560
01:42:47,970 --> 01:42:49,590
there's nothing to be done

1561
01:42:49,660 --> 01:42:51,760
this nothing oh no this one

1562
01:42:51,770 --> 01:42:54,010
there are fourteen we can make this p three

1563
01:42:54,160 --> 01:42:56,880
we get rid of that corner

1564
01:42:56,890 --> 01:43:00,950
make it point that way

1565
01:43:00,950 --> 01:43:03,750
now three is the cheapest thing

1566
01:43:03,800 --> 01:43:05,080
so we take it out

1567
01:43:05,120 --> 01:43:07,730
in the course there's nothing to be done over there

1568
01:43:07,790 --> 01:43:09,800
and now last i take

1569
01:43:11,230 --> 01:43:14,020
and it's done in fifteen

1570
01:43:14,050 --> 01:43:15,170
it's gone

1571
01:43:15,170 --> 01:43:17,430
the goal and why

1572
01:43:17,450 --> 01:43:21,880
because it's the outer product of three vectors

1573
01:43:21,900 --> 01:43:24,690
and then you just add up a bunch of those ten

1574
01:43:24,710 --> 01:43:26,740
now if i give you enough tensors

1575
01:43:26,760 --> 01:43:29,070
then you can add up to make any sense you like

1576
01:43:29,080 --> 01:43:32,900
any through anything you like if i give you a limited number usually make things

1577
01:43:32,900 --> 01:43:37,830
have sort of regular structure this is like PCA but for three three-dimensional things into

1578
01:43:43,180 --> 01:43:44,680
so as an equation

1579
01:43:45,640 --> 01:43:51,300
we turned the three-way weight into a product of three

1580
01:43:51,350 --> 01:43:53,330
factor specific weights

1581
01:43:53,350 --> 01:43:57,490
and then we can factorize that top line equation and we see that the energy

1582
01:43:57,490 --> 01:44:00,240
contributed by given factor

1583
01:44:00,360 --> 01:44:02,230
it's just the product

1584
01:44:02,250 --> 01:44:05,180
one of three weighted sums

1585
01:44:05,190 --> 01:44:09,710
for each group of units your connecting you take a weighted sum so you apply

1586
01:44:09,720 --> 01:44:10,990
linear filter

1587
01:44:11,020 --> 01:44:13,120
and then the energy is the product

1588
01:44:13,120 --> 01:44:16,870
of all those three manifold

1589
01:44:16,890 --> 01:44:21,690
so now for hidden unit h if you wanted to know whether i should try

1590
01:44:21,810 --> 01:44:23,440
to turn on

1591
01:44:23,450 --> 01:44:26,710
you need to know the difference in the energy of the whole system as a

1592
01:44:26,710 --> 01:44:29,010
function of whether ages on or off

1593
01:44:29,030 --> 01:44:33,800
so just considering the energy contributed by factor f what is fact and need to

1594
01:44:33,800 --> 01:44:38,220
tell hidden unit which in order for a to decide how to adopt to state

1595
01:44:38,940 --> 01:44:40,910
give the system the image

1596
01:44:40,960 --> 01:44:44,920
h needs to know the difference in the energy the the whole system

1597
01:44:44,960 --> 01:44:47,190
as function with its on or off

1598
01:44:47,200 --> 01:44:51,680
which is the sum of the energies from all factors the energy contributed by factor

1599
01:44:51,690 --> 01:44:56,190
so in fact that one of the difference in energy contributed by factor

1600
01:44:56,200 --> 01:44:58,130
when h is on and off

1601
01:44:58,180 --> 01:45:00,920
so take the right hand side of the top line

1602
01:45:00,940 --> 01:45:04,220
and yes differentiating with respect to sh

1603
01:45:04,250 --> 01:45:06,950
to give you the right chance even though something thing to do

1604
01:45:08,210 --> 01:45:11,510
what you get is that the whf comes up front

1605
01:45:11,540 --> 01:45:13,000
so you see that what

1606
01:45:13,020 --> 01:45:15,730
hidden units h needs to see

1607
01:45:15,760 --> 01:45:20,460
is the product of the other two weighted sum

1608
01:45:20,470 --> 01:45:25,010
multiplied by the weight on the connection between the fact and h

1609
01:45:25,030 --> 01:45:29,480
so we can draw the picture

1610
01:45:29,540 --> 01:45:33,960
the fact that each of its three vertices computer weighted sum

1611
01:45:33,980 --> 01:45:36,000
and what h needs to see

1612
01:45:36,040 --> 01:45:38,330
is the product of the weighted sums

1613
01:45:38,440 --> 01:45:40,100
the bottom two vertices

1614
01:45:40,120 --> 01:45:43,510
and that needs to be sent to each and on its way to change it

1615
01:45:43,510 --> 01:45:45,900
needs to be multiplied by w

1616
01:45:45,980 --> 01:45:50,800
thirty connectionist way all the hidden units need to see that same message but each

1617
01:45:50,800 --> 01:45:56,670
weighted by the weight on the connection

1618
01:45:56,690 --> 01:45:59,180
if you look at learning

1619
01:45:59,960 --> 01:46:03,350
that message that goes from the fact that che

1620
01:46:03,360 --> 01:46:09,150
is what you need for doing that and the learning algorithm balls machine learning outcomes

1621
01:46:09,150 --> 01:46:11,400
stays just the same

1622
01:46:11,410 --> 01:46:16,120
the learning basically says i want to lower the energy

1623
01:46:16,150 --> 01:46:16,910
when i

1624
01:46:17,350 --> 01:46:18,600
looking at data

1625
01:46:18,610 --> 01:46:23,030
and i want to raise the energy one generation fantasies from the model

1626
01:46:23,050 --> 01:46:27,300
and the derivative of the energy respect to work when i'm looking at data

1627
01:46:27,350 --> 01:46:29,230
it's just the product

1628
01:46:29,300 --> 01:46:33,730
of the state of hidden unit and the message that comes straight from the factory

1629
01:46:33,750 --> 01:46:37,490
the curvature of the energy when you from the model

1630
01:46:37,550 --> 01:46:40,610
is that same product but when you generate from the model

1631
01:46:40,610 --> 01:46:43,640
so the difference of those two parties what you need in order to learn the

1632
01:46:45,910 --> 01:46:48,460
one hundred thirty that's great so

1633
01:46:48,500 --> 01:46:55,160
you get exactly the same pairwise learning so we took these three way interactions we

1634
01:46:55,160 --> 01:47:00,280
factored everything and everything becomes pairwise with these messages products

1635
01:47:00,300 --> 01:47:05,350
of weighted sums and that's where the most gets into the act

1636
01:47:05,360 --> 01:47:08,270
so now i'm going to show you an example for translating images

1637
01:47:08,290 --> 01:47:09,980
we get the preimage

1638
01:47:09,990 --> 01:47:14,210
we are propose to image and we get trained up lots of pairs of images

1639
01:47:14,210 --> 01:47:15,900
and to begin with the images

1640
01:47:15,940 --> 01:47:20,990
will just be random dot sparse random dots and all is just translate to random

1641
01:47:20,990 --> 01:47:23,650
dots will have a number of different translations

1642
01:47:23,730 --> 01:47:26,350
and that's the training data

1643
01:47:26,360 --> 01:47:28,050
and we learn

1644
01:47:28,070 --> 01:47:29,960
the weights to the factors

1645
01:47:29,970 --> 01:47:33,180
well the whole bunch of active learn latent factor and the weights being vaccinated you

1646
01:47:33,430 --> 01:47:36,940
will learn everything we start with small random weights and just like it

1647
01:47:36,980 --> 01:47:40,310
and after its learned what i'm going to show you is

1648
01:47:40,740 --> 01:47:46,360
the weights the factor has to the preimage and weights factor has to postage

1649
01:47:46,370 --> 01:47:50,820
that is these two filters and you have multiplied together center in fact

1650
01:47:50,850 --> 01:47:54,900
and of course is going to be a lot of factors assigned show the weights

1651
01:47:54,900 --> 01:47:58,210
for the pre and post image full the factors so here's the weights to the

1652
01:47:58,210 --> 01:48:00,380
preimage of all the factors

1653
01:48:01,190 --> 01:48:04,770
for those who know about translation and fourier analysis and stuff like this this should

1654
01:48:05,310 --> 01:48:06,990
not a surprise

1655
01:48:07,010 --> 01:48:10,160
and here's the weights to the post image

1656
01:48:11,150 --> 01:48:12,680
postage preimage

1657
01:48:12,690 --> 01:48:14,180
postage preimage

1658
01:48:14,210 --> 01:48:15,040
first image

1659
01:48:15,060 --> 01:48:17,280
it's learning basically

1660
01:48:17,320 --> 01:48:21,640
the fourier basis and this learning has gratings that are about ninety degrees out of

1661
01:48:22,480 --> 01:48:26,930
and we seem like a sensible thing to do and can represent translations very nicely

1662
01:48:26,940 --> 01:48:29,880
is that translating images we could rotate images

1663
01:48:29,900 --> 01:48:33,820
so we take the same random dot patterns and we take

1664
01:48:33,820 --> 01:48:36,380
i just

1665
01:48:36,540 --> 01:48:39,150
formulating the iterative methods

1666
01:48:39,190 --> 01:48:44,020
and at each iteration you just replace if by a quadratic approximation for just the

1667
01:48:44,020 --> 01:48:46,520
second order taylor expansion

1668
01:48:46,540 --> 01:48:51,520
and then you solve this quadratic approximation of second order approximation subject to the constraints

1669
01:48:51,670 --> 01:48:54,250
and that gives you an update x

1670
01:48:54,310 --> 01:48:58,630
is equal to the current value x had this step that you are the fission

1671
01:48:58,630 --> 01:48:59,730
is the newton step

1672
01:49:00,360 --> 01:49:03,340
so the kinetic approximation would be at x had to be there

1673
01:49:03,790 --> 01:49:05,150
mention value

1674
01:49:05,150 --> 01:49:07,320
in the product with the gradients

1675
01:49:07,380 --> 01:49:10,080
and then the quadratic form it has shown

1676
01:49:10,290 --> 01:49:14,540
so it requires first and second derivatives

1677
01:49:14,560 --> 01:49:18,520
and then you get a quadratic function subject to equality constraints

1678
01:49:18,540 --> 01:49:23,130
you know the optimality conditions are of the quadratic problem from the previous page

1679
01:49:23,170 --> 01:49:28,880
so that's how you can compute the optimal value of this quadratic approximation or is

1680
01:49:28,900 --> 01:49:33,940
expressed in terms of the difference between the optimal value and the current value

1681
01:49:33,980 --> 01:49:36,480
it's your system and delta x and

1682
01:49:36,480 --> 01:49:38,670
the college multipliers

1683
01:49:38,690 --> 01:49:45,540
so that's the definition of the newton step at x had for convex function f

1684
01:49:45,560 --> 01:49:49,590
and this sort of problem with no constraints would be there

1685
01:49:50,040 --> 01:49:56,820
the interpretation of it so we have a convex function f locally make quadratic approximation

1686
01:49:56,880 --> 01:49:58,290
you minimize the local

1687
01:49:58,310 --> 01:50:02,540
the quadratic approximation to get the next value except as the newton step

1688
01:50:02,750 --> 01:50:06,520
so this suggests that the method work very well if you are close to the

1689
01:50:07,560 --> 01:50:12,360
because close to the minimum the functional actually look almost exactly quadratic

1690
01:50:12,380 --> 01:50:13,810
it doesn't really

1691
01:50:13,810 --> 01:50:19,270
tell you whether it works for values of x hats far from the minimum

1692
01:50:19,320 --> 01:50:22,590
and it turns out that actually in general doesn't work for

1693
01:50:22,590 --> 01:50:24,130
x not close to the mean

1694
01:50:24,230 --> 01:50:25,770
the four simple

1695
01:50:25,790 --> 01:50:27,400
convex functions

1696
01:50:27,400 --> 01:50:30,730
because if you do this and it's not close to the mean

1697
01:50:30,770 --> 01:50:37,360
if you keep repeating replacing experts expressed this new step x might diverge or

1698
01:50:37,420 --> 01:50:40,690
it doesn't converge in general

1699
01:50:40,770 --> 01:50:44,840
and it's easy promises to fix the answer is that in general you have to

1700
01:50:44,840 --> 01:50:47,560
add in a line search so in

1701
01:50:49,750 --> 01:50:53,940
algorithm with line search you would actually first try this from step

1702
01:50:53,940 --> 01:50:56,110
of replacing the current it through x

1703
01:50:56,110 --> 01:50:57,790
with expressed in units that

1704
01:50:57,810 --> 01:51:02,480
if that results in function value that's actually larger than the current value

1705
01:51:02,500 --> 01:51:06,400
you want divide and instead by for example two

1706
01:51:06,420 --> 01:51:09,210
and then again compared with the current function value

1707
01:51:09,250 --> 01:51:12,520
nicky backtracking or dividing the newton step

1708
01:51:12,540 --> 01:51:14,540
until you find one

1709
01:51:14,560 --> 01:51:18,860
function value that's actually faced less than the current value

1710
01:51:18,860 --> 01:51:25,190
and for convex functions you can show that actually is sufficient for physical convergence

1711
01:51:25,440 --> 01:51:29,820
so then newton's algorithm would look like this you have each iteration you solve this

1712
01:51:29,820 --> 01:51:31,480
new system

1713
01:51:31,500 --> 01:51:34,790
if gradient to threats the haitian

1714
01:51:34,840 --> 01:51:36,810
we compute the newton step

1715
01:51:36,840 --> 01:51:39,080
you have the termination condition

1716
01:51:39,110 --> 01:51:42,880
and then you updates the next value of x is x the current value plus

1717
01:51:42,880 --> 01:51:45,060
a multiple of the newton step

1718
01:51:45,080 --> 01:51:48,980
and should close to the optimum you would like to use to use one

1719
01:51:49,000 --> 01:51:54,190
because that's the idea of the methods but in general you need to step size

1720
01:51:54,190 --> 01:51:57,270
that might be less might have to take on and

1721
01:51:57,310 --> 01:52:00,210
to have a global convergence

1722
01:52:00,230 --> 01:52:04,500
or guarantee that it's descendant the function value keeps decreasing

1723
01:52:04,750 --> 01:52:10,320
so an example that's what typically looks like if you this is for a convex

1724
01:52:10,320 --> 01:52:12,900
function it's quite

1725
01:52:12,920 --> 01:52:17,690
complex number of variables and terms in this

1726
01:52:17,730 --> 01:52:21,170
but you see it converges and a small number of iterations and that's typical of

1727
01:52:21,170 --> 01:52:22,310
newton's method

1728
01:52:22,360 --> 01:52:28,340
you also see two two regions so there is the first region where it converges

1729
01:52:28,380 --> 01:52:30,750
but not necessarily very quickly

1730
01:52:30,770 --> 01:52:31,980
and also

1731
01:52:32,110 --> 01:52:37,480
you need to step size less than one to make it converge actually and i

1732
01:52:37,480 --> 01:52:41,710
want to get close enough to the solution it accelerates and you get fast asymptotic

1733
01:52:43,110 --> 01:52:47,270
so there's actually typical for newton's method the number of iterations is not usually not

1734
01:52:47,270 --> 01:52:48,520
very high

1735
01:52:48,540 --> 01:52:54,110
but each iteration can be quite expensive because you have to solve this system of

1736
01:52:54,110 --> 01:52:58,270
equations to find you have to find the derivative second derivatives and to solve the

1737
01:52:58,290 --> 01:53:00,630
system to compute that x

1738
01:53:03,020 --> 01:53:07,460
and there is some convergence theory that actually explains this and that we can probably

1739
01:53:08,500 --> 01:53:13,730
but it's interesting because the convergence analysis of newton's method actually explains

1740
01:53:13,770 --> 01:53:16,960
this qualitatively this two regimes

1741
01:53:17,040 --> 01:53:18,440
so in the first

1742
01:53:18,460 --> 01:53:22,310
so the typical summary of the standard convergence analysis is that it is the first

1743
01:53:23,210 --> 01:53:26,360
but the gradients very far from the optimum

1744
01:53:26,380 --> 01:53:30,650
and this is the result established the function value keep create decreases

1745
01:53:30,730 --> 01:53:33,790
at least by some constant come up might not be very

1746
01:53:33,840 --> 01:53:36,840
large but at least it shows that decreases

1747
01:53:36,880 --> 01:53:42,770
and once the gradient becomes smaller close to the optimum it converges quadratically because then

1748
01:53:42,770 --> 01:53:47,090
the result is that the gradient this career at each iteration

1749
01:53:47,090 --> 01:53:50,480
and that explains the fast local convergence

1750
01:53:51,840 --> 01:53:55,540
so there are qualitatively it explains the these two regions

1751
01:53:55,560 --> 01:54:00,020
the bound you obtain not very useful because they're very conservative

1752
01:54:00,020 --> 01:54:04,360
and they also depend on some constants that you typically don't know in practice but

1753
01:54:04,360 --> 01:54:06,540
it explains some of the

1754
01:54:06,660 --> 01:54:09,520
the behaviour you see in general

1755
01:54:09,540 --> 01:54:14,310
and something so the classical analysis all it goes back to at least the nineteen

1756
01:54:15,920 --> 01:54:21,920
and then in around nineteen ninety nesterov and ask actually developed a new convergence theory

1757
01:54:22,670 --> 01:54:24,650
newton's method

1758
01:54:24,670 --> 01:54:29,080
that is actually for a specific class of functions

1759
01:54:29,080 --> 01:54:35,200
he said well it's all questions

1760
01:54:35,210 --> 01:54:40,670
and then we can

1761
01:54:40,690 --> 01:54:43,230
so in nineteen seventy four

1762
01:54:43,270 --> 01:54:45,620
what do they use

1763
01:54:45,630 --> 01:54:46,770
they so

1764
01:54:46,930 --> 01:54:49,490
so you have two levels of

1765
01:54:49,510 --> 01:54:55,530
first was more than one can be saved me one and the second by the

1766
01:54:56,410 --> 01:55:02,560
processes the people always for example you could call viruses propagate so of the social

1767
01:55:02,560 --> 01:55:04,120
networks so

1768
01:55:04,170 --> 01:55:08,650
but before that i would just like to use my thesis committee right cases for

1769
01:55:08,720 --> 01:55:13,510
this is my advisor and then there is every blum john lafferty jon kleinberg from

1770
01:55:16,740 --> 01:55:22,040
network strength network for these data so we go and collect this data in many

1771
01:55:22,040 --> 01:55:26,480
or in various domains so for example we can have friendship networks where nodes represent

1772
01:55:26,480 --> 01:55:29,160
people and there is an edge if two people who know each other or if

1773
01:55:29,160 --> 01:55:34,280
the people interact then we can have like web citation networks where nodes are web

1774
01:55:34,280 --> 01:55:35,600
pages or

1775
01:55:35,870 --> 01:55:41,400
papers and there is a directed edge between the two nodes if one links or

1776
01:55:41,400 --> 01:55:46,050
cite each other right and then we can have for example sexual networks where there

1777
01:55:46,050 --> 01:55:50,090
are links between the people if the people who had some kind of interaction

1778
01:55:50,270 --> 01:55:56,330
we can have internet networks meaning which should all this talk to reach or we

1779
01:55:56,330 --> 01:56:01,930
can have genetic regulatory networks but here every every

1780
01:56:01,950 --> 01:56:06,120
every node is now protein that is the edge between the media interact

1781
01:56:07,740 --> 01:56:11,960
more specifically what we are interested in large networks right so we we want to

1782
01:56:11,960 --> 01:56:14,310
do is to make statements about how

1783
01:56:14,330 --> 01:56:19,000
like networks evolve and for here i have just a few examples of networks that

1784
01:56:19,000 --> 01:56:22,680
i've been mentioned during my talk right so for example we have an instant messenger

1785
01:56:22,680 --> 01:56:26,470
network where people talk to each other it's pretty much the whole world talking so

1786
01:56:26,470 --> 01:56:30,140
we have almost two hundred million people in one point three billion edges

1787
01:56:30,160 --> 01:56:33,840
we have like blog networks where we have blogs and blog posts linking to each

1788
01:56:33,840 --> 01:56:35,220
other and again we have

1789
01:56:35,230 --> 01:56:40,270
an order of millions of nodes and edges become product recommendation networks again

1790
01:56:40,280 --> 01:56:43,090
millions of nodes and tens of millions of edges

1791
01:56:43,110 --> 01:56:46,880
and the questions we ask in this these are the following

1792
01:56:47,760 --> 01:56:51,530
we ask how do how do networks grow following patterns

1793
01:56:51,540 --> 01:56:56,650
then once we observe the patterns we ask can generate can build generative models of

1794
01:56:56,650 --> 01:56:58,800
networks that we observe this type of

1795
01:56:58,810 --> 01:57:03,930
patterns and then for cascades we ask how does this influence spread over the network

1796
01:57:03,930 --> 01:57:07,360
right does it spread like three or start or some other kind of pattern and

1797
01:57:07,360 --> 01:57:11,850
then once we do these observations we can use this to identify and select nodes

1798
01:57:11,850 --> 01:57:14,910
that to detect cascades which

1799
01:57:16,530 --> 01:57:21,240
and more generally what the research is about it's about analysing mother and modeling the

1800
01:57:22,430 --> 01:57:27,440
evolution and dynamics of large real world networks and this has two separate parts the

1801
01:57:27,470 --> 01:57:31,830
first part is about the evolution of the networks and then the second part is

1802
01:57:31,830 --> 01:57:36,120
about the cascades so the processes that take place on the networks

1803
01:57:37,210 --> 01:57:39,460
both of these two history part three

1804
01:57:39,470 --> 01:57:41,580
so first the first question is

1805
01:57:41,630 --> 01:57:46,860
what are interesting statistical and structural properties of the network to measure so the question

1806
01:57:46,860 --> 01:57:50,270
is what should be go and measure how can we characterize the topology of the

1807
01:57:50,270 --> 01:57:51,210
network rail

1808
01:57:51,220 --> 01:57:52,880
and once we have done

1809
01:57:52,930 --> 01:57:56,690
we can we can go and say what would be the models that that would

1810
01:57:56,690 --> 01:57:58,920
model this type of thing our behavior

1811
01:57:58,930 --> 01:58:02,250
and once we have the properties and once we can go and use these models

1812
01:58:02,260 --> 01:58:05,900
to do some tasks better design new algorithms and so on

1813
01:58:09,070 --> 01:58:12,400
so this is how our work can be structure that we have this two setting

1814
01:58:13,100 --> 01:58:17,810
network evolution and the processes that take place in networks and then for each one

1815
01:58:17,810 --> 01:58:18,600
of them

1816
01:58:18,680 --> 01:58:22,970
we first asked what are the characteristics that happen in your real data

1817
01:58:22,990 --> 01:58:25,520
then we ask how can you build the model

1818
01:58:25,540 --> 01:58:30,420
that would explain this type of phenomenon and then we use and exploit models to

1819
01:58:30,420 --> 01:58:32,910
make predictions forecasts and so on

1820
01:58:32,930 --> 01:58:34,870
and we have

1821
01:58:36,470 --> 01:58:40,530
in all of them and publish them in pretty much all major machine learning and

1822
01:58:40,530 --> 01:58:43,300
data mining conferences and journals

1823
01:58:43,810 --> 01:58:47,220
so why would you want to do that why why is this important

1824
01:58:47,240 --> 01:58:50,160
so i just want to spend one's life

1825
01:58:50,180 --> 01:58:51,860
and the first

1826
01:58:51,870 --> 01:58:56,000
o thing is for example the structural properties that if you know how

1827
01:58:56,010 --> 01:58:57,570
graphs look like

1828
01:58:57,610 --> 01:59:00,750
or how normal grassroots group then you can go and say

1829
01:59:00,770 --> 01:59:03,840
try to find a lot of graphs of not among nodes in the network and

1830
01:59:03,840 --> 01:59:08,550
this can be like spam can be that spanned some other types of things right

1831
01:59:08,640 --> 01:59:12,350
then once you have the model you can use this model for example to generate

1832
01:59:12,350 --> 01:59:14,480
synthetic graphs which means

1833
01:59:14,780 --> 01:59:19,860
in the net let's say in system setting where you have a new routing protocol

1834
01:59:19,860 --> 01:59:23,400
or something and you would like to see later over the network

1835
01:59:23,430 --> 01:59:27,760
you can generate a synthetic network and then run simulations so it and this way

1836
01:59:27,760 --> 01:59:31,780
you will be able to understand how design of the characteristic of the network influence

1837
01:59:31,780 --> 01:59:35,960
the performance of you and then you can use these things also for sampling

1838
01:59:35,980 --> 01:59:37,520
well the idea is

1839
01:59:37,570 --> 01:59:40,300
i think we have a the graph that it's like on the work with so

1840
01:59:40,310 --> 01:59:44,320
the idea is let's let's get a smaller graph that we have similar properties than

