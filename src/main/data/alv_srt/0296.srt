1
00:00:00,000 --> 00:00:03,670
and one what's here's our hashtable

2
00:00:03,730 --> 00:00:07,600
and what we're going to do is we're going use universal hashing at the first

3
00:00:09,790 --> 00:00:13,960
OK so we find a universal hash functions figure hash function at random

4
00:00:15,000 --> 00:00:16,870
and what we do is will

5
00:00:16,920 --> 00:00:19,290
the hash into that level

6
00:00:19,310 --> 00:00:22,520
and then we'll do is keep track of

7
00:00:25,190 --> 00:00:28,810
of two things one is what the size of the hash table is at the

8
00:00:28,810 --> 00:00:30,080
next level

9
00:00:30,120 --> 00:00:35,650
so in this case the size of the hash table uses enormous losses going for

10
00:00:35,710 --> 00:00:37,870
we're also going to keep a separate

11
00:00:37,900 --> 00:00:39,350
hash key

12
00:00:39,370 --> 00:00:40,620
the second level

13
00:00:40,620 --> 00:00:43,600
so each slot will have its own

14
00:00:43,650 --> 00:00:46,400
hash function for the second level

15
00:00:46,420 --> 00:00:47,620
so for example

16
00:00:47,650 --> 00:00:52,290
this one might have here thirty one that is a random number

17
00:00:52,310 --> 00:00:54,330
he is here

18
00:00:54,350 --> 00:00:55,560
these are there there

19
00:00:55,580 --> 00:00:57,000
it is up there

20
00:00:57,020 --> 00:01:00,100
OK so that's going to be the basis of my hash function the key with

21
00:01:00,100 --> 00:01:02,120
which germany hash

22
00:01:02,140 --> 00:01:05,810
one a is six and let's say this

23
00:01:05,850 --> 00:01:10,380
and then we have a pointer to the hash tables

24
00:01:10,440 --> 00:01:12,810
this is a a s one

25
00:01:14,620 --> 00:01:16,210
four slots

26
00:01:16,230 --> 00:01:18,540
fourteen twenty seven

27
00:01:18,540 --> 00:01:20,770
two slots are empty

28
00:01:23,650 --> 00:01:25,940
and this one for example

29
00:01:30,270 --> 00:01:33,250
what are

30
00:01:50,250 --> 00:01:54,710
so the idea here is that in this case if we look over all our

31
00:01:54,710 --> 00:01:56,620
top level hash function

32
00:01:56,640 --> 00:01:58,420
which are just call h

33
00:01:58,440 --> 00:02:01,020
has that h

34
00:02:02,850 --> 00:02:05,250
is equal to the age of

35
00:02:05,310 --> 00:02:06,600
twenty seven

36
00:02:06,620 --> 00:02:08,600
think one

37
00:02:08,650 --> 00:02:11,690
because we're in slot one

38
00:02:11,730 --> 00:02:15,120
so these two both past to the same

39
00:02:15,190 --> 00:02:20,650
slot in the level one hash table this is level one

40
00:02:20,670 --> 00:02:22,750
this is level two here

41
00:02:26,690 --> 00:02:33,500
so level one hashing fourteen twenty seven collided they went into the same slot here

42
00:02:33,690 --> 00:02:36,210
level two

43
00:02:36,310 --> 00:02:38,020
they got

44
00:02:38,100 --> 00:02:42,000
has to different places and hash function i use is going to be indexed by

45
00:02:42,000 --> 00:02:46,210
whatever the random numbers are that i chose found for this and i'll show you

46
00:02:46,210 --> 00:02:47,900
how we find those

47
00:02:48,960 --> 00:02:51,350
we have then h

48
00:02:51,350 --> 00:02:52,850
thirty one

49
00:02:56,420 --> 00:02:58,710
is equal to one

50
00:02:58,960 --> 00:03:02,900
thirty one r twenty seven is equal to two

51
00:03:02,940 --> 00:03:08,150
level two

52
00:03:09,170 --> 00:03:10,940
so i go

53
00:03:10,960 --> 00:03:13,710
passion here find the

54
00:03:13,730 --> 00:03:14,690
use this

55
00:03:14,710 --> 00:03:18,290
as the basis my hash function hash into whatever table

56
00:03:18,310 --> 00:03:20,620
got here

57
00:03:20,670 --> 00:03:22,690
and so if there are no

58
00:03:22,710 --> 00:03:26,210
if i can guarantee that there are no collisions at level two

59
00:03:26,310 --> 00:03:29,580
this is going to cost me order one time in the worst case to look

60
00:03:29,580 --> 00:03:30,540
something up

61
00:03:30,580 --> 00:03:32,810
how do i look it up take the value

62
00:03:32,980 --> 00:03:35,460
i apply each to it

63
00:03:35,520 --> 00:03:38,350
that takes me to some slot

64
00:03:38,400 --> 00:03:40,250
then i looked to see what the

65
00:03:40,270 --> 00:03:43,560
he is for this hash function apply that hash functions

66
00:03:43,620 --> 00:03:45,960
that takes me to another slot

67
00:03:48,250 --> 00:03:49,250
OK and that

68
00:03:49,250 --> 00:03:53,500
it took me basically two applications of hash functions plus some look up

69
00:03:53,500 --> 00:03:56,400
because who knows what minor amount of bookkeeping

70
00:04:00,670 --> 00:04:03,790
the the reason or have no

71
00:04:03,850 --> 00:04:06,370
collisions at this level

72
00:04:07,310 --> 00:04:10,330
is the following if there n

73
00:04:10,370 --> 00:04:11,670
so i

74
00:04:20,350 --> 00:04:22,520
a level

75
00:04:22,540 --> 00:04:24,500
one slot

76
00:04:28,670 --> 00:04:30,100
then we're going to you

77
00:04:32,420 --> 00:04:33,880
m so by

78
00:04:33,900 --> 00:04:35,850
which is equal to

79
00:04:35,920 --> 00:04:38,980
and so by square

80
00:04:41,150 --> 00:04:45,600
in the level two hash table

81
00:04:51,190 --> 00:04:52,040
OK so

82
00:04:52,060 --> 00:04:55,730
i should mention here this is going to be survive

83
00:04:55,750 --> 00:04:59,710
the size of the hash table and this is going to be my

84
00:05:00,640 --> 00:05:01,460
they survive

85
00:05:07,230 --> 00:05:08,580
so many use

86
00:05:08,600 --> 00:05:14,790
so basically i'm going to hash n so things into answer by square would

87
00:05:14,870 --> 00:05:16,560
locations here

88
00:05:16,600 --> 00:05:21,960
so this is going to be incredibly spores

89
00:05:21,980 --> 00:05:26,330
this can be quadratic and size

90
00:05:26,380 --> 00:05:30,600
OK so what i'm going to show is that under those circumstances

91
00:05:30,650 --> 00:05:35,770
it's easy for me to find hash function such that there are no collisions

92
00:05:35,810 --> 00:05:37,580
that's the name of the game

93
00:05:37,640 --> 00:05:41,520
case figure out how can i make these hash functions so there are no collisions

94
00:05:41,560 --> 00:05:43,810
that's why draw this with so few

95
00:05:43,830 --> 00:05:44,850
elements here

96
00:05:44,870 --> 00:05:49,190
so here for example i have two elements and i have a hash table size

97
00:05:49,190 --> 00:05:53,580
four here three elements i need hash table of size nine

98
00:05:53,580 --> 00:05:56,940
OK are hundred elements i need to hash table of size

99
00:05:57,000 --> 00:05:58,560
ten thousand

100
00:05:58,850 --> 00:06:01,650
i'm not going to pick something so it's likely that there is a lot of

101
00:06:01,650 --> 00:06:03,060
that size

102
00:06:03,120 --> 00:06:05,560
and the fact that this actually works

103
00:06:05,580 --> 00:06:09,880
this gives us all the properties the one that's sparked the analysis

104
00:06:09,920 --> 00:06:14,460
so everybody that the stakes order one worst-case time and what the basic structure of

105
00:06:14,460 --> 00:06:15,980
it is

106
00:06:15,980 --> 00:06:19,500
OK know these things by the way and not in this case prime

107
00:06:19,540 --> 00:06:22,690
i can always pick crimes that were close to the side do that in this

108
00:06:22,690 --> 00:06:28,080
you can use this faction that may be totally irrelevant for the problem

109
00:06:28,110 --> 00:06:33,570
and if you know some properties of the distribution in terms of partitioning the space

110
00:06:33,580 --> 00:06:38,200
instead of working in the whole space you can walk one block of the partition

111
00:06:38,200 --> 00:06:42,420
at the time and just put all the blocks together at the end because you

112
00:06:42,420 --> 00:06:49,070
move left in each block you get a much smaller violence at the

113
00:06:51,340 --> 00:06:52,450
so nineties

114
00:06:52,500 --> 00:06:56,060
that if if you know of any group actions

115
00:06:56,080 --> 00:06:59,110
then you can implement your space

116
00:06:59,310 --> 00:07:04,400
but it's not necessarily to to your problem for f is not environs by group

117
00:07:08,150 --> 00:07:13,050
during one simulation you do all the all the orbit of your simulation by the

118
00:07:13,930 --> 00:07:20,910
which is provably finite and there's one simulation you get an improved estimation compared with

119
00:07:20,910 --> 00:07:22,730
one point

120
00:07:22,740 --> 00:07:24,030
this is done

121
00:07:24,980 --> 00:07:26,840
and if you're not lucky

122
00:07:26,880 --> 00:07:29,900
what you'll end up is with an orbit

123
00:07:29,920 --> 00:07:32,780
that is just one significant point

124
00:07:32,790 --> 00:07:36,670
and if you're lucky enough if the action has something to do with your problem

125
00:07:36,670 --> 00:07:42,740
or if you just your function is spread well enough in terms of this group

126
00:07:42,740 --> 00:07:47,160
action then you get improvement

127
00:07:48,720 --> 00:07:54,160
if you look in in in in in hypercube in own dimension you can

128
00:07:54,180 --> 00:07:55,370
you can flip

129
00:07:55,370 --> 00:07:58,720
according to all the symmetries

130
00:07:58,740 --> 00:08:02,120
of of this this i hypercube prince

131
00:08:02,220 --> 00:08:06,990
if you stimulate the point here can you can change either first component of the

132
00:08:06,990 --> 00:08:08,340
second component

133
00:08:08,370 --> 00:08:10,660
all the company

134
00:08:10,820 --> 00:08:13,600
but of course

135
00:08:13,620 --> 00:08:16,640
this is principle which were only if

136
00:08:16,660 --> 00:08:17,390
you are

137
00:08:17,430 --> 00:08:22,700
target distribution is not a small clump in europe q otherwise you just end up

138
00:08:22,700 --> 00:08:26,370
putting more zeros your estimation

139
00:08:26,390 --> 00:08:31,010
always importance weight you get from that are very small

140
00:08:31,030 --> 00:08:36,870
there a recent paper in discussion paper in the last genesis

141
00:08:36,890 --> 00:08:39,510
by making and about that

142
00:08:39,550 --> 00:08:42,530
that's trying to make a principle of the

143
00:08:42,720 --> 00:08:45,390
the point

144
00:08:45,410 --> 00:08:47,620
that was

145
00:08:47,640 --> 00:08:53,840
i'm trying to prepare the way for instances of correlation is something that would be

146
00:08:53,840 --> 00:09:02,780
obvious and MCMC simulation rather blackwellisation is say that we will definitely use in MCMC

147
00:09:02,800 --> 00:09:07,010
and it's called the creation because that is thinking of this work robert but if

148
00:09:07,010 --> 00:09:12,600
you and that tells you that if you condition of the sufficient statistic you improved

149
00:09:13,930 --> 00:09:20,370
don't need sufficient statistics because some would you know every everything is sufficient

150
00:09:20,390 --> 00:09:23,890
can be sufficient in the sense that if you

151
00:09:23,910 --> 00:09:27,010
x y

152
00:09:27,030 --> 00:09:30,280
and if you start with st mary doesn't over x

153
00:09:30,300 --> 00:09:33,080
if you can compute this new st mary

154
00:09:33,100 --> 00:09:34,620
in downtown

155
00:09:34,640 --> 00:09:38,320
of x given y which only depends on y

156
00:09:38,350 --> 00:09:40,930
obviously the violence will decrease

157
00:09:40,990 --> 00:09:44,720
and both estimators of the same expectations

158
00:09:44,760 --> 00:09:46,850
therefore we are getting closer

159
00:09:46,870 --> 00:09:49,200
to the true value

160
00:09:50,930 --> 00:09:54,600
so if we can do is implement that in

161
00:09:54,620 --> 00:09:56,120
problems were

162
00:09:56,140 --> 00:09:58,010
you other joint simulation

163
00:09:58,030 --> 00:10:00,370
of x and y

164
00:10:00,390 --> 00:10:05,620
and if you stimulate your by simulating joint x and y just like in the

165
00:10:05,620 --> 00:10:09,740
stochastic illegitimate you simulate the phi

166
00:10:09,740 --> 00:10:12,470
and sigma m estimating in addition

167
00:10:12,490 --> 00:10:14,200
all this xt

168
00:10:14,220 --> 00:10:18,030
so you just have problems we need to supplement

169
00:10:18,050 --> 00:10:19,930
to get the simulation

170
00:10:19,930 --> 00:10:21,680
it's like way

171
00:10:21,700 --> 00:10:23,930
well if you go and if you do that

172
00:10:23,950 --> 00:10:27,950
then you have to try to produce an

173
00:10:27,970 --> 00:10:34,200
got rather than the average of the age of x i have to try to

174
00:10:34,260 --> 00:10:42,180
producer expectation of this process to matter by integrating out something carnegie otherwise for instance

175
00:10:42,180 --> 00:10:48,950
you can do that either exactly or approximately you will get a smaller volumes

176
00:10:48,970 --> 00:10:52,820
compared with his i had

177
00:10:52,950 --> 00:11:00,200
for example is is fairly city but just to give you a flavor

178
00:11:00,220 --> 00:11:03,660
i think function where i know

179
00:11:03,680 --> 00:11:08,300
the conditional expectation so i want to compute expectation of

180
00:11:08,550 --> 00:11:10,890
exponential of minus six square

181
00:11:12,740 --> 00:11:15,760
it's just student with

182
00:11:16,490 --> 00:11:21,320
degrees of freedom and i do that because seem interested in new

183
00:11:21,350 --> 00:11:23,740
i first met in all

184
00:11:24,660 --> 00:11:27,890
on the fly square so have a couple

185
00:11:28,800 --> 00:11:32,180
x y to produce much simulation from x

186
00:11:32,200 --> 00:11:34,930
and just throwing away the otherwise

187
00:11:34,930 --> 00:11:39,260
i can try to implement rao representation and here

188
00:11:39,300 --> 00:11:43,970
because estimate made this despair it's one x y one of two x and y

189
00:11:43,970 --> 00:11:46,220
and rather than you think

190
00:11:46,320 --> 00:11:48,120
one of n

191
00:11:48,290 --> 00:11:52,300
the average of the exponential minus xj square

192
00:11:52,300 --> 00:11:55,260
i could in that case i can compute

193
00:11:55,260 --> 00:11:59,030
the conditional expectation of six months my six where

194
00:11:59,120 --> 00:12:00,300
he one

195
00:12:01,220 --> 00:12:03,030
which is one of our

196
00:12:03,050 --> 00:12:04,580
two sigma square y

197
00:12:04,600 --> 00:12:06,100
plus one

198
00:12:07,720 --> 00:12:10,570
because they chose an example where it works really well

199
00:12:10,570 --> 00:12:12,010
you can see here

200
00:12:12,890 --> 00:12:14,890
realisation of ten

201
00:12:16,320 --> 00:12:18,660
iteration so it is

202
00:12:18,680 --> 00:12:21,300
standard average of the exponential

203
00:12:22,220 --> 00:12:24,740
six j square

204
00:12:24,760 --> 00:12:28,160
and here is so blackwell estimate

205
00:12:28,180 --> 00:12:34,530
and of course again and again the violence is much much smaller

206
00:12:35,490 --> 00:12:40,410
just a few words to to

207
00:12:40,430 --> 00:12:44,300
in one sampling perspective is with bayesian

208
00:12:47,120 --> 00:12:52,010
of course the problem when you want to compute an expedition against the posterior

209
00:12:52,030 --> 00:12:57,280
it's just a regular integration problems so you may apply

210
00:12:57,300 --> 00:12:58,280
the whole

211
00:12:58,280 --> 00:13:01,120
the toolbox that

212
00:13:02,260 --> 00:13:04,390
particular choices that are

213
00:13:04,410 --> 00:13:06,700
of interest

214
00:13:06,720 --> 00:13:08,740
i think if you kind

215
00:13:08,760 --> 00:13:10,320
user prior

216
00:13:11,070 --> 00:13:12,620
your importance sampling

217
00:13:14,320 --> 00:13:17,080
the ratio

218
00:13:20,340 --> 00:13:22,640
already important is just likely

219
00:13:22,660 --> 00:13:26,080
so if your prior distribution is proper

220
00:13:26,080 --> 00:13:28,750
one of them

221
00:14:03,090 --> 00:14:11,810
one of the

222
00:14:16,270 --> 00:14:20,960
one of the

223
00:14:52,220 --> 00:14:53,640
when you were

224
00:14:53,840 --> 00:14:56,550
or even more

225
00:14:56,720 --> 00:14:59,130
one of the

226
00:15:07,940 --> 00:15:10,940
i but

227
00:15:12,160 --> 00:15:17,220
when they were called

228
00:15:18,360 --> 00:15:22,580
one labelled her

229
00:15:22,700 --> 00:15:26,640
this than

230
00:15:40,970 --> 00:15:43,720
they are

231
00:15:50,720 --> 00:15:55,230
first of all

232
00:15:55,250 --> 00:15:57,900
o four o

233
00:16:00,310 --> 00:16:03,150
how is

234
00:16:03,160 --> 00:16:07,780
we're a

235
00:16:20,780 --> 00:16:23,470
a lot of

236
00:17:32,500 --> 00:17:35,540
you know

237
00:17:35,590 --> 00:17:38,530
and you are here

238
00:17:38,890 --> 00:17:41,020
the role of

239
00:17:55,660 --> 00:17:58,240
there is

240
00:17:58,430 --> 00:18:00,890
nine of

241
00:18:11,910 --> 00:18:14,090
on where are

242
00:18:17,640 --> 00:18:18,480
can be used

243
00:18:21,910 --> 00:18:23,030
i mean

244
00:18:29,840 --> 00:18:32,160
the only

245
00:18:34,600 --> 00:18:38,780
he appeared

246
00:18:38,840 --> 00:18:42,700
you want

247
00:19:01,340 --> 00:19:04,620
part of

248
00:19:04,620 --> 00:19:08,760
OK so now we don't need to have now the size of this page tables

249
00:19:08,760 --> 00:19:12,640
is much smaller right there are no longer to thirty now there through the twentieth

250
00:19:12,650 --> 00:19:19,690
bits which is some small number of megabytes big and they and but we still

251
00:19:19,690 --> 00:19:23,400
have this this we have avoided this problem we had before we have some flexibility

252
00:19:23,400 --> 00:19:27,800
in terms of where how these physical how each page maps into the physical memory

253
00:19:27,910 --> 00:19:32,330
so i can create a third page here i can allocate a third page to

254
00:19:32,770 --> 00:19:37,500
process and i can map that in any four thousand ninety six block by block

255
00:19:37,500 --> 00:19:41,690
fits in memory is currently unused i flexibility i don't have this problem setting with

256
00:19:41,690 --> 00:19:44,300
a where a c were colliding with each other

257
00:19:58,060 --> 00:20:01,860
this is sort of the outline of how virtual memory works but what i haven't

258
00:20:01,860 --> 00:20:03,430
yet described to you

259
00:20:03,430 --> 00:20:09,490
it is how it is that we can actually go about creating these these different

260
00:20:09,490 --> 00:20:14,330
address spaces that are allocated to the different modules and how we can switch between

261
00:20:14,700 --> 00:20:18,250
different modules using this this p m a r register so

262
00:20:18,540 --> 00:20:22,140
i sort of describe this as though these things cl suppose that these these data

263
00:20:22,140 --> 00:20:25,690
structures exist now here's how we can use them but i haven't told you how

264
00:20:25,690 --> 00:20:30,120
these data structures or put together created and set up to begin with

265
00:20:30,140 --> 00:20:31,840
and i did this at the beginning

266
00:20:31,870 --> 00:20:36,250
but in order to do this what we're gonna need is some sort of a

267
00:20:36,250 --> 00:20:39,890
special supervisory module that's able to

268
00:20:39,900 --> 00:20:44,080
look at the page maps for all of these different that's able to create new

269
00:20:44,080 --> 00:20:47,600
page maps and look at the page maps for all the different modules that are

270
00:20:47,600 --> 00:20:51,430
within the system and then the supervisory models can be able to do things like

271
00:20:51,430 --> 00:20:55,650
add new memory page map or be able to do that or be able to

272
00:20:55,660 --> 00:21:01,880
destroyed delete a particular module and its associated page so we need some sort of

273
00:21:01,880 --> 00:21:04,120
thing that can manage all this infrastructure

274
00:21:04,170 --> 00:21:12,050
and so this supervisory module

275
00:21:12,070 --> 00:21:14,280
it's called the kernel

276
00:21:16,830 --> 00:21:20,460
the kernel is really it's it's going to be the thing is going to be

277
00:21:20,460 --> 00:21:23,180
in charge of managing all these data structures for us

278
00:21:34,860 --> 00:21:39,720
here's a microprocessor with its m r register on and we're going to do is

279
00:21:39,720 --> 00:21:44,060
we're going to extend the microprocessor with one additional piece of one additional piece of

280
00:21:44,060 --> 00:21:46,440
hardware and this is the

281
00:21:49,850 --> 00:21:51,040
kernel bit

282
00:21:51,800 --> 00:21:57,120
so this is just a bit that specifies whether we're whether we're currently running a

283
00:21:57,120 --> 00:22:00,350
user module that is one of these just just to programs running on a computer

284
00:22:00,360 --> 00:22:03,890
whether the kernel is currently executing

285
00:22:03,930 --> 00:22:07,340
and the idea with this kernel there is that when the kernel that is the

286
00:22:07,910 --> 00:22:11,350
code it's going to be there is running is going to have special privileges is

287
00:22:11,350 --> 00:22:15,500
going to be able to do so can be able to manipulate special things about

288
00:22:15,500 --> 00:22:19,750
the hardware processor and in particular

289
00:22:19,790 --> 00:22:23,770
we're going to have a rule that says the only

290
00:22:24,430 --> 00:22:28,210
the kernel

291
00:22:28,210 --> 00:22:30,200
and change the PML

292
00:22:32,590 --> 00:22:39,750
so the PM hours the thing that specifies which process is currently running

293
00:22:39,750 --> 00:22:45,000
and which selects which address space we want to be currently using and we're gonna

294
00:22:45,000 --> 00:22:48,810
use is we're going to use work and but so what we want what we're

295
00:22:48,810 --> 00:22:52,100
going to do is have the kernel beta the thing that's in charge of manipulating

296
00:22:52,100 --> 00:22:57,320
the value of the are registered to select which thing is currently being executed and

297
00:22:57,320 --> 00:23:00,470
we want to make it so that only the kernel can do this because if

298
00:23:00,470 --> 00:23:04,040
we for example allowed one of these other programs to be able to manipulate the

299
00:23:04,040 --> 00:23:08,450
PMA are right then that other programs might be able to do something unpleasant to

300
00:23:08,450 --> 00:23:13,730
the computer-aided changes the PMA are to point at some other programs memory and now

301
00:23:13,730 --> 00:23:18,260
suddenly all the memory addresses in the system are going to be really being resolved

302
00:23:18,540 --> 00:23:22,750
it then suddenly we're going to be sort of resolving memory addresses relative to some

303
00:23:22,750 --> 00:23:24,080
other modules

304
00:23:24,510 --> 00:23:27,770
page map and that's likely to be problematic thing is likely to cause the other

305
00:23:27,770 --> 00:23:33,040
modules to crash for example because the processor is set up to be executing instructions

306
00:23:33,040 --> 00:23:37,530
from the current programs to make sure that only something this kernel contains the PMA

307
00:23:37,540 --> 00:23:41,610
are in this kernel is going to be this sort of supervisory module that all

308
00:23:41,610 --> 00:23:44,250
of the other modules are going to have to trust to do the right thing

309
00:23:44,250 --> 00:23:46,490
and manage the computers execution for you

310
00:23:48,930 --> 00:23:53,340
this kernel in except for this one difference the kernel can change the PMA are

311
00:23:53,420 --> 00:23:57,840
the kernel is in all other respects essentially just going to be another module that's

312
00:23:57,840 --> 00:24:02,000
running on the computer system so the kernel in particular the kernel

313
00:24:02,000 --> 00:24:03,580
OK what they altered it

314
00:24:03,660 --> 00:24:07,490
you sort of previous slide it took some fights of music

315
00:24:07,500 --> 00:24:13,000
and classified them correctly and with respect to the composers so burgess together to parts

316
00:24:16,220 --> 00:24:21,680
i guess the classical music because the stylus sort of probably more

317
00:24:24,290 --> 00:24:28,880
so this applies a little bit of

318
00:24:30,610 --> 00:24:32,130
and this

319
00:24:32,180 --> 00:24:34,010
they're right in their paper that

320
00:24:34,010 --> 00:24:35,500
also people

321
00:24:35,510 --> 00:24:36,770
i feel that these

322
00:24:37,110 --> 00:24:42,400
preload is different from these three

323
00:24:43,160 --> 00:24:47,870
they did a lot of other applications of formby for optical character recognition for classification

324
00:24:47,870 --> 00:24:55,190
of galaxies clustering novels for authors and some of the datasets

325
00:24:55,240 --> 00:24:58,380
OK so i this was sort of integration a little bit but i want to

326
00:24:58,380 --> 00:24:59,770
show you that

327
00:24:59,770 --> 00:25:02,680
and this kolmogorov complexity is not only

328
00:25:02,730 --> 00:25:08,940
cute in theory but can also be applied in practice quite directly and with great

329
00:25:09,840 --> 00:25:13,660
we normally it's much more complicated to get from the theory to the application another

330
00:25:13,660 --> 00:25:18,550
one is the MDL principle the minimum description length principle which is an approximation to

331
00:25:18,550 --> 00:25:20,420
kolmogorov complexity

332
00:25:20,480 --> 00:25:22,720
i mean rissanen like this but

333
00:25:22,850 --> 00:25:25,810
he was

334
00:25:25,820 --> 00:25:27,510
according to

335
00:25:27,600 --> 00:25:32,630
doris definitely aware of kolmogorov complexity when he wrote in the paper but it doesn't

336
00:25:32,640 --> 00:25:36,520
i mentioned that in this paper

337
00:25:36,560 --> 00:25:41,240
so what you can do is what you almost approximate kolmogorov complexity and you and

338
00:25:42,290 --> 00:25:47,280
compute an upper bounds on the two part MDL what do is

339
00:25:47,650 --> 00:25:49,840
you take the shannon fano code

340
00:25:50,160 --> 00:25:53,790
so you have

341
00:25:53,800 --> 00:25:55,680
some probability class take

342
00:25:55,700 --> 00:25:57,570
can kind of code

343
00:25:57,610 --> 00:25:58,410
he new

344
00:25:58,420 --> 00:26:01,060
and then you cold somehow

345
00:26:01,100 --> 00:26:07,820
so what if code the distribution and then i think the distribution cosmic vision part

346
00:26:07,820 --> 00:26:08,820
of this case

347
00:26:10,130 --> 00:26:13,270
the complete called which is an upper bound on

348
00:26:13,450 --> 00:26:16,040
oracle model of complexity

349
00:26:16,090 --> 00:26:21,860
and MDL that minimizes the right hand side with respect to move

350
00:26:21,910 --> 00:26:29,480
to get the best possible

351
00:26:30,610 --> 00:26:33,300
so this clustering methods to summarize

352
00:26:33,340 --> 00:26:38,770
so based on the universal similarity metric which is itself based on kolmogorov complexity

353
00:26:38,770 --> 00:26:42,160
then they approximated by some standard compressed

354
00:26:42,190 --> 00:26:48,430
compute the similarity metrics and then represented the similar larity metrics as three

355
00:26:48,520 --> 00:26:49,470
and the three

356
00:26:49,480 --> 00:26:53,860
because there is a computationally hard problem they approximated by the so-called quartet

357
00:26:53,950 --> 00:27:00,720
which will grow into it and it leads to excellent classification in many domains

358
00:27:00,800 --> 00:27:05,050
so finally

359
00:27:05,100 --> 00:27:07,220
the OK and questions the

360
00:27:07,670 --> 00:27:11,810
similarity part

361
00:27:11,850 --> 00:27:18,070
OK finally so far the work was about prediction

362
00:27:18,080 --> 00:27:22,190
but critics is everything you want agent which acts and influences the environment and they

363
00:27:22,190 --> 00:27:24,580
come to that now

364
00:27:24,620 --> 00:27:26,630
so cooperation agencies

365
00:27:26,680 --> 00:27:30,870
sequential decision theory reinforcement learning value functions

366
00:27:30,920 --> 00:27:33,470
and then the bayes mixture in

367
00:27:33,510 --> 00:27:39,000
in the this general case and then the specific i model self optimizing somersaults

368
00:27:39,320 --> 00:27:43,550
so optimizes great optimality and some

369
00:27:44,360 --> 00:27:47,090
OK here's the down

370
00:27:47,140 --> 00:27:48,300
you agent

371
00:27:48,310 --> 00:27:52,080
environment if your actions which are called by

372
00:27:52,140 --> 00:27:56,840
and your observations which are split into regular observations and reward

373
00:27:56,910 --> 00:27:59,760
one of these are called a and b are called

374
00:28:03,280 --> 00:28:09,450
this picture sort of in terms of turing machine so there environment which is an

375
00:28:09,450 --> 00:28:11,200
internal state resources

376
00:28:12,450 --> 00:28:17,370
the output is written on the tape and output the data written to tape and

377
00:28:17,930 --> 00:28:21,740
reaction of environments with tape and then you know the size

378
00:28:21,800 --> 00:28:24,490
so what you have is

379
00:28:24,530 --> 00:28:27,530
four deterministic agents you have a policy p

380
00:28:27,550 --> 00:28:31,490
which takes the observation sequence and produces an action sequence

381
00:28:31,600 --> 00:28:34,850
given the observations up to k minus one it produces

382
00:28:34,890 --> 00:28:36,870
and action y k

383
00:28:36,930 --> 00:28:40,350
and p captures all these actions from

384
00:28:40,390 --> 00:28:41,720
one to k

385
00:28:41,890 --> 00:28:44,300
symmetric early the environment

386
00:28:44,300 --> 00:28:47,220
deterministic environments function for

387
00:28:47,240 --> 00:28:48,510
actions to

388
00:28:48,530 --> 00:28:51,120
observations to the agent written in this way

389
00:28:51,120 --> 00:28:53,160
as the some notation you want

390
00:28:53,220 --> 00:28:58,120
k means one two k and less came in one to k

391
00:28:58,160 --> 00:29:03,490
the input itself split in two parts the reebok in the regular observation parts of

392
00:29:03,490 --> 00:29:05,680
the observations a video camera image

393
00:29:05,740 --> 00:29:09,950
and the really what is the and

394
00:29:10,010 --> 00:29:15,030
i assume that the rewards are bounded on the negative

395
00:29:15,050 --> 00:29:18,120
the value in the deterministic case just from

396
00:29:18,140 --> 00:29:19,910
the k two m

397
00:29:19,970 --> 00:29:24,840
four policy p environment q is just the sum of rewards

398
00:29:24,840 --> 00:29:28,320
and optimal policy is just the policy which maximizes value

399
00:29:28,430 --> 00:29:30,100
and here assume that the light

400
00:29:30,140 --> 00:29:32,280
time of the agent

401
00:29:32,300 --> 00:29:36,280
i come to that later what to do with

402
00:29:37,450 --> 00:29:42,260
so they before deterministic environments if you go to probabilistic environments what you do is

403
00:29:42,260 --> 00:29:45,840
the observation xk is just you know

404
00:29:45,850 --> 00:29:50,850
because the some for probability that depends on all possible actions in all past observations

405
00:29:50,890 --> 00:29:51,780
so let's

406
00:29:51,800 --> 00:29:55,430
continuous distribution sigma for that

407
00:29:57,820 --> 00:30:02,470
if you multiply all this conditional probabilities together

408
00:30:02,510 --> 00:30:04,370
you get the probability

409
00:30:04,410 --> 00:30:05,680
of observing

410
00:30:05,700 --> 00:30:09,190
this observation sequence given the agent

411
00:30:09,240 --> 00:30:15,240
as provided in section six but remember there you know some

412
00:30:15,300 --> 00:30:16,870
there's some interaction i mean this

413
00:30:16,870 --> 00:30:18,550
x y two candy

414
00:30:18,800 --> 00:30:23,570
this action this observation x took independent action y one

415
00:30:23,620 --> 00:30:28,800
but not two that's also some structure on the left-hand side which is apparent here

416
00:30:28,850 --> 00:30:32,320
without the conditions that but there are generic

417
00:30:32,890 --> 00:30:37,320
product of conditional probabilities with these conditions some temporal

418
00:30:42,600 --> 00:30:45,760
so and then the problem is the case you just take expectations to take the

419
00:30:45,760 --> 00:30:47,820
river from one to n now say

420
00:30:47,840 --> 00:30:50,800
and you ask what is the probability of observing this sequence

421
00:30:50,800 --> 00:30:52,890
representation of course useless

422
00:30:52,900 --> 00:30:57,950
so we always limit ourselves to a certain number

423
00:30:57,960 --> 00:31:00,950
if you look very close to the minus one

424
00:31:01,010 --> 00:31:02,910
notice that all the field lines

425
00:31:02,920 --> 00:31:07,280
comment on the minus one we understand that of course because the positive charge want

426
00:31:07,280 --> 00:31:08,920
to go to the minus one

427
00:31:08,920 --> 00:31:11,390
a few very close to the plus

428
00:31:11,410 --> 00:31:17,370
and they all go away from the plus being repelled

429
00:31:17,420 --> 00:31:20,770
you can sort of thing is these field lines if you want to

430
00:31:20,800 --> 00:31:21,760
measure the

431
00:31:23,160 --> 00:31:24,670
but the plus

432
00:31:24,670 --> 00:31:26,970
charges blowout air

433
00:31:27,040 --> 00:31:28,630
like a hairdryer

434
00:31:28,650 --> 00:31:30,370
and that the miners

435
00:31:30,380 --> 00:31:32,320
suck in air

436
00:31:32,380 --> 00:31:34,190
like a vacuum cleaner

437
00:31:34,200 --> 00:31:37,510
and then you get the a feeling for those on the left side is

438
00:31:37,550 --> 00:31:40,190
hendry which wants to blow out stuff

439
00:31:40,210 --> 00:31:43,720
and then there is that little talk about wants suck something in

440
00:31:43,780 --> 00:31:50,380
and it succeeds with some degree is not as powerful as the plus three term

441
00:31:50,590 --> 00:31:55,320
we lost all information about field strength we had earlier with these arrows we had

442
00:31:55,330 --> 00:31:59,220
the length of the arrow the the magnitude of the field was represented

443
00:31:59,530 --> 00:32:03,630
you've lost that but there is still some information on field strength

444
00:32:03,690 --> 00:32:08,760
if the lines are closer together the density of the lines is high

445
00:32:08,810 --> 00:32:12,430
the electric field is stronger than when the density becomes low

446
00:32:12,460 --> 00:32:14,530
so if you look for instance here

447
00:32:15,180 --> 00:32:17,470
how many lines there are

448
00:32:18,520 --> 00:32:21,660
a few millimeters and where you go further out

449
00:32:21,720 --> 00:32:25,520
these lines spread out that tells you field is going down the strength of the

450
00:32:25,520 --> 00:32:31,070
field is going down to one of our square field of course

451
00:32:31,120 --> 00:32:33,450
if you want to make these drawings what you could do

452
00:32:33,450 --> 00:32:34,680
to make them look good

453
00:32:34,690 --> 00:32:38,140
you can make three times more field lines going out from the plus in this

454
00:32:39,150 --> 00:32:43,490
then return to the minus one

455
00:32:43,490 --> 00:32:45,880
so the field lines are very powerful

456
00:32:45,940 --> 00:32:47,380
and we will

457
00:32:47,440 --> 00:32:49,990
i often think in terms of electric fields

458
00:32:50,040 --> 00:32:52,090
and the line configurations

459
00:32:52,100 --> 00:32:53,750
and you will have

460
00:32:56,190 --> 00:32:57,710
problems that deal with

461
00:32:57,720 --> 00:32:59,250
electric fields

462
00:32:59,290 --> 00:33:05,960
and with the electric field lines

463
00:33:05,970 --> 00:33:08,500
if an electric field lines

464
00:33:08,540 --> 00:33:11,580
is straight

465
00:33:11,600 --> 00:33:14,310
so i have electric fields

466
00:33:14,320 --> 00:33:15,710
get some range

467
00:33:16,940 --> 00:33:18,340
so we have fields

468
00:33:18,430 --> 00:33:19,540
but i like this

469
00:33:19,560 --> 00:33:21,630
straight field lines

470
00:33:21,640 --> 00:33:25,290
and i release a charge there

471
00:33:25,320 --> 00:33:27,910
princes the positive charge

472
00:33:27,960 --> 00:33:32,070
then the positive charge would experience force exactly in the same direction as the field

473
00:33:32,070 --> 00:33:35,270
line because the tangential now is in the direction of the field lines

474
00:33:35,290 --> 00:33:36,370
it will become

475
00:33:36,500 --> 00:33:40,030
accelerated in this direction and would always stay on the field lines

476
00:33:40,080 --> 00:33:42,600
i release it with zero spin

477
00:33:42,610 --> 00:33:45,970
started accelerating would stay on the field

478
00:33:46,030 --> 00:33:47,740
in a similar way

479
00:33:47,750 --> 00:33:51,230
if we think the earth is having a gravitational field

480
00:33:51,240 --> 00:33:54,650
is a to one we may never have used that was gravitational field but in

481
00:33:54,650 --> 00:33:56,940
physics we think of the

482
00:33:57,000 --> 00:33:59,600
of gravity also being fields

483
00:33:59,690 --> 00:34:02,070
if i have here a piece of chalk

484
00:34:02,730 --> 00:34:06,320
the field lines the gravitational field lines

485
00:34:06,440 --> 00:34:09,070
june twenty six one hundred nicely parallel

486
00:34:09,080 --> 00:34:10,440
and straight

487
00:34:10,440 --> 00:34:13,470
and if i really this piece of chalk at zero speed

488
00:34:13,520 --> 00:34:16,570
it will begin to move in the direction of the field lines and it will

489
00:34:17,410 --> 00:34:20,210
on the field line

490
00:34:20,210 --> 00:34:22,780
so now you can ask yourself the question

491
00:34:22,790 --> 00:34:25,270
if i release a charge

492
00:34:25,280 --> 00:34:28,040
would always follow the field lines

493
00:34:28,060 --> 00:34:29,840
and the answer is no

494
00:34:29,890 --> 00:34:32,590
only in this very special case

495
00:34:32,640 --> 00:34:34,740
but suppose now that the field lines

496
00:34:34,740 --> 00:34:36,570
a curves

497
00:34:36,760 --> 00:34:39,300
you feel line

498
00:34:39,320 --> 00:34:40,700
as you see in those

499
00:34:40,700 --> 00:34:43,510
configurations very common

500
00:34:43,650 --> 00:34:46,230
it now i release a

501
00:34:46,260 --> 00:34:48,520
charging here

502
00:34:48,540 --> 00:34:50,880
so point charge here

503
00:34:50,900 --> 00:34:54,080
it will experience a force in this direction

504
00:34:54,130 --> 00:34:57,990
so we'll will get an acceleration in this direction so it will immediately abandoned that

505
00:34:57,990 --> 00:34:59,560
field one

506
00:34:59,610 --> 00:35:01,820
so now you ask me what is the

507
00:35:01,830 --> 00:35:03,730
trajectory of that

508
00:35:03,730 --> 00:35:06,980
charge well could become very complicated i really don't know

509
00:35:06,990 --> 00:35:09,280
maybe going like this

510
00:35:09,350 --> 00:35:11,980
by the time it reaches this point

511
00:35:11,980 --> 00:35:14,040
what i do know that then the force

512
00:35:14,090 --> 00:35:16,430
will be tangential to this field lines

513
00:35:16,430 --> 00:35:18,240
will be in this direction

514
00:35:18,290 --> 00:35:21,300
so as it marches out and picks up speed

515
00:35:21,310 --> 00:35:23,950
locally it will experience forces

516
00:35:23,990 --> 00:35:26,070
representative of those field lines

517
00:35:26,090 --> 00:35:27,620
and so the trajectory

518
00:35:27,630 --> 00:35:30,010
can be rather complicated

519
00:35:30,010 --> 00:35:31,600
so field lines

520
00:35:31,620 --> 00:35:34,240
i'm not trajectories and not even when you

521
00:35:34,270 --> 00:35:35,730
release the charge

522
00:35:36,740 --> 00:35:38,160
with zero speed

523
00:35:38,220 --> 00:35:39,630
only in case

524
00:35:39,650 --> 00:35:41,110
that the field lines

525
00:35:41,120 --> 00:35:44,380
i straight lines

526
00:35:44,400 --> 00:35:47,760
let's now look at a field configuration

527
00:35:47,800 --> 00:35:53,050
with maxwell himself the great maestro in some of his publications

528
00:35:53,100 --> 00:35:53,950
but there

529
00:35:53,950 --> 00:35:55,370
the ratio

530
00:35:55,390 --> 00:35:58,120
one two four

531
00:35:58,140 --> 00:35:59,540
and whether

532
00:35:59,590 --> 00:36:03,300
this plus four plus one or minus four minus one

533
00:36:03,310 --> 00:36:04,890
is not important

534
00:36:06,330 --> 00:36:09,140
that's just matter of the direction of the arrows

535
00:36:10,730 --> 00:36:12,800
next all the input errors in

536
00:36:12,820 --> 00:36:15,910
so i leave it up appealing plus four plus one you have to put arrows

537
00:36:15,910 --> 00:36:18,360
going outwards

538
00:36:18,370 --> 00:36:22,200
and what you see now uses and lower effect think of them as both being

539
00:36:23,320 --> 00:36:26,980
so there's a plus for trying to blow around like a hairdryer

540
00:36:27,020 --> 00:36:29,190
and the prose one is trying to do his own thing

541
00:36:29,280 --> 00:36:30,580
so we get

542
00:36:30,640 --> 00:36:34,540
a field configuration field lines which are sort of

543
00:36:34,580 --> 00:36:37,550
not perhaps easy that you can sort of imagine

544
00:36:37,590 --> 00:36:38,990
why it has this

545
00:36:38,990 --> 00:36:42,130
peculiar shape

546
00:36:42,200 --> 00:36:43,790
if you

547
00:36:43,800 --> 00:36:44,870
put it

548
00:36:44,930 --> 00:36:46,800
plus test charge

549
00:36:46,810 --> 00:36:49,780
in between one and four

550
00:36:49,810 --> 00:36:51,970
then the four will repel it

551
00:36:52,060 --> 00:36:54,320
but the one that also repellent

552
00:36:54,380 --> 00:36:58,190
and so it is going to be a point somewhere probably close to one

553
00:36:58,190 --> 00:37:00,700
by the forces exactly cancel out

554
00:37:00,710 --> 00:37:05,040
therefore he will be zero there

555
00:37:05,060 --> 00:37:07,810
in a similar way between the moon and the earth

556
00:37:07,820 --> 00:37:10,720
that is the point is not too far away from the moon

557
00:37:10,720 --> 00:37:13,270
we to gravitational attraction from the earth

558
00:37:13,290 --> 00:37:17,880
and the gravitational attraction from the moon exactly cancel each other out

559
00:37:17,900 --> 00:37:20,600
it's not too dissimilar from this situation

560
00:37:20,610 --> 00:37:23,030
so when you have

561
00:37:23,030 --> 00:37:25,370
charges of the same polarity

562
00:37:25,430 --> 00:37:28,290
you always find in between somewhere point

563
00:37:28,300 --> 00:37:30,430
twenty the electric field

564
00:37:30,490 --> 00:37:33,080
is zero

565
00:37:33,190 --> 00:37:35,750
that's not going to very special case

566
00:37:35,880 --> 00:37:37,360
well by i make

567
00:37:37,410 --> 00:37:39,220
the two charges

568
00:37:39,220 --> 00:37:41,060
equal in magnitude

569
00:37:41,120 --> 00:37:42,350
but opposite

570
00:37:42,370 --> 00:37:43,760
in sign

571
00:37:43,800 --> 00:37:45,520
and we have a name for that

572
00:37:45,580 --> 00:37:50,940
we call that the dipole

573
00:37:50,980 --> 00:37:53,820
plus charges he minus charges there

574
00:37:53,870 --> 00:37:58,690
situation is extremely symmetric as you would expect because they have equal power

575
00:37:58,730 --> 00:38:00,540
there's one upstairs

576
00:38:00,560 --> 00:38:04,190
and one vacuum cleaner downstairs

577
00:38:04,190 --> 00:38:10,470
being were behaving as a particle beam only so this validates

578
00:38:11,490 --> 00:38:20,450
the broad and it also validates the whole concept of wave-particle duality wave-particle duality had

579
00:38:20,450 --> 00:38:22,320
been applied the photons

580
00:38:22,340 --> 00:38:26,290
they've already very comparable type individual photon but now

581
00:38:26,320 --> 00:38:31,780
we can talk about waves of matter and so the bride has often been the

582
00:38:31,780 --> 00:38:37,260
same Abreu his name many physicists the tag line they associated with the is matter

583
00:38:39,250 --> 00:38:42,810
matter waves as distinct from light waves

584
00:38:42,850 --> 00:38:44,930
waves of light we have ways of matter

585
00:38:47,140 --> 00:38:51,870
that's important so that's number 1 caricature over to where we were earlier this morning

586
00:38:52,310 --> 00:38:58,850
right number 2 we're going to have Werner Heisenberg Werner Heisenberg Heisenberg what did he

587
00:38:58,850 --> 00:39:04,490
do well the study with poly in poly were bodies in graduate school they work

588
00:39:04,530 --> 00:39:09,740
for Sommerfeld your Sommerfeld never won a Nobel Prize but we we should count how

589
00:39:09,740 --> 00:39:13,870
many Nobel Prize winners he educated in his group

590
00:39:13,890 --> 00:39:20,470
there's something very very important going on in the mentoring process that so

591
00:39:21,070 --> 00:39:26,250
Heisenberg gets his PhD with Sommerfeld in Munich and then he goes to Copenhagen work

592
00:39:26,250 --> 00:39:27,010
with born

593
00:39:28,490 --> 00:39:34,630
and he was is getting burned out the dissenting vacation went up the north from

594
00:39:34,630 --> 00:39:39,270
Denmark and we have to work on a deserted island off the coast of Norway

595
00:39:39,270 --> 00:39:42,790
and stayed there for about 3 weeks and he came back

596
00:39:43,090 --> 00:39:46,530
and with within brought the uncertainty principle

597
00:39:47,170 --> 00:39:52,770
the uncertainty principle and what he said was that when you get down to the

598
00:39:52,830 --> 00:39:59,390
atomic length scales there are limits there are constraints on our ability to observe

599
00:39:59,430 --> 00:40:02,330
constraints on our ability to observe

600
00:40:02,350 --> 00:40:08,950
at atomic length scales atomic scale

601
00:40:09,870 --> 00:40:23,010
limits on our ability to and this is important because it means that since we

602
00:40:23,010 --> 00:40:25,270
can observe

603
00:40:25,370 --> 00:40:29,850
we can talk about individual electrons we have to start doing that because we can't

604
00:40:29,850 --> 00:40:36,950
find an individual electron every observation we make involves the exchange of energy when looking

605
00:40:36,950 --> 00:40:42,260
at that wall it's photons hitting the wall coming back to me and so the

606
00:40:42,260 --> 00:40:45,350
photons hitting the wall actually put pressure on the wall

607
00:40:45,430 --> 00:40:47,370
but the pressure so

608
00:40:47,410 --> 00:40:49,080
who cares

609
00:40:50,120 --> 00:40:55,370
you might say well what was that make any difference well I suppose I want

610
00:40:55,430 --> 00:41:01,530
look at something like an electron in orbit here we know that this dimension is

611
00:41:01,530 --> 00:41:06,870
roughly 1 angstrom unit right the ball radius for hydrogen is . 5 2 9

612
00:41:06,870 --> 00:41:11,890
angstroms around numbers this is 1 extra while following look at that someone have very

613
00:41:13,230 --> 00:41:18,120
capabilities in terms of the energy that I use the light that I use so

614
00:41:18,140 --> 00:41:22,040
I would choose something that has a wavelength of roughly

615
00:41:23,330 --> 00:41:27,990
right you wouldn't you wouldn't measure the dimension of a human hair with the yardstick

616
00:41:28,010 --> 00:41:33,120
so I need something on this order so if I take lambda equals 1 angstrom

617
00:41:33,140 --> 00:41:39,620
go through HC overland you'll discover that the energy of a photon with 1 as

618
00:41:39,640 --> 00:41:46,470
its wavelength is on the order of 12 thousand 400 electron volts

619
00:41:46,470 --> 00:41:51,310
while I'm looking for the selection was the binding energy the ground state electron hydrogen

620
00:41:51,350 --> 00:41:55,990
it's 13 . 6 electron volts so I'm going go and I'm gonna look for

621
00:41:55,990 --> 00:42:01,430
something that's got an energy of 13 . 6 electron volts holding it in place

622
00:42:01,540 --> 00:42:06,430
and I'm going to use a flashlight with 12 thousand 400 electron volts

623
00:42:06,490 --> 00:42:09,930
so I think I'm liable to disturb the very thing I'm going to measure

624
00:42:10,040 --> 00:42:14,390
so you say well why you doing something so foolish why don't you something that

625
00:42:14,390 --> 00:42:20,470
has much lower energy OK so let's choose something that has a 100 times less

626
00:42:20,470 --> 00:42:27,160
than that so this be 13 . 6 over 100 electron volts well if I

627
00:42:27,160 --> 00:42:31,900
go through that the wavelength of such a photon is going to be so big

628
00:42:31,900 --> 00:42:36,180
that I'm back to measuring the dimension of the human here with the yardstick so

629
00:42:36,200 --> 00:42:38,490
now you see the dilemma

630
00:42:38,490 --> 00:42:42,280
if I want to get down to very very fine dimensions I have to bring

631
00:42:42,280 --> 00:42:47,280
huge energy and disturb the very thing going to measure so this is the uncertainty

632
00:42:47,280 --> 00:42:52,830
principle it leads to uncertainty in the measure suppose I ask you to take the

633
00:42:52,830 --> 00:42:58,580
time of 100 meters spread in the olympics by give a stopwatch that's measured in

634
00:42:58,580 --> 00:43:01,120
units of 15 seconds

635
00:43:01,140 --> 00:43:06,750
what these people are blasting through at 9 seconds and and change general ledger runners

636
00:43:06,750 --> 00:43:10,530
of 5 above all run under 15 seconds guess the whole batch is 1 another

637
00:43:10,530 --> 00:43:15,810
15 seconds and all you've got is either 0 or 15

638
00:43:15,810 --> 00:43:17,410
because that's resolution

639
00:43:18,370 --> 00:43:23,550
so what does that tell you tells you can't resolve what's going on at that

640
00:43:24,620 --> 00:43:27,120
In something else you can result

641
00:43:27,180 --> 00:43:30,260
you can't resolve causality

642
00:43:30,280 --> 00:43:33,560
when inserting it's that big you know which came 1st

643
00:43:34,570 --> 00:43:38,110
cause and effect below a certain level of definitions

644
00:43:42,870 --> 00:43:49,930
cause and effect cause and effect that becomes uncertain to

645
00:43:51,150 --> 00:43:52,670
this is philosophical

646
00:43:54,230 --> 00:43:56,430
has a lot of this is that should be

647
00:43:57,110 --> 00:44:01,550
so what does that mean it means that we better get away from these deterministic

648
00:44:01,550 --> 00:44:05,990
those sentences they usually long with each of the worlds

649
00:44:05,990 --> 00:44:11,550
better summarize the use of all so that

650
00:44:12,200 --> 00:44:16,160
coming back to the data i to this

651
00:44:16,180 --> 00:44:17,360
google cluster

652
00:44:17,380 --> 00:44:18,550
good news clusters

653
00:44:18,590 --> 00:44:24,320
two sentences since this is unlikely to summarise the content of each of the articles

654
00:44:24,340 --> 00:44:26,240
and this is going

655
00:44:26,950 --> 00:44:29,780
so it's likely that

656
00:44:29,860 --> 00:44:31,740
so sentences are also

657
00:44:31,740 --> 00:44:34,050
about the same thing so this is the intuition

658
00:44:34,070 --> 00:44:34,950
and that's what

659
00:44:34,950 --> 00:44:38,570
what's interesting about this data is those sentences that

660
00:44:38,700 --> 00:44:39,530
why not

661
00:44:39,570 --> 00:44:45,430
so you can do itself get news articles in russian english and you can measure

662
00:44:45,430 --> 00:44:51,570
the average length of the sentence is written in the long sentences and you traditionally

663
00:44:51,630 --> 00:44:53,380
find significant difference

664
00:44:53,490 --> 00:44:59,300
so for example for english which is about twenty eight thirty words which is can

665
00:44:59,320 --> 00:45:02,320
be a very charming all along

666
00:45:02,340 --> 00:45:05,260
new sentences about of twenty one

667
00:45:05,260 --> 00:45:06,970
so it's very interesting for me

668
00:45:07,260 --> 00:45:11,150
for this task because it really means that you need to start fresh

669
00:45:11,150 --> 00:45:13,490
make more challenging

670
00:45:13,510 --> 00:45:16,510
and it's also interesting because even the sentences

671
00:45:16,530 --> 00:45:19,990
it's very likely that pass if you want to use that

672
00:45:20,160 --> 00:45:25,820
which is suspected information the two processes to make mistakes the data

673
00:45:27,380 --> 00:45:28,410
so we

674
00:45:28,430 --> 00:45:30,660
go through the information society

675
00:45:30,680 --> 00:45:32,280
two if you're going to publish that

676
00:45:32,300 --> 00:45:37,780
o point it's not enough to just think about and

677
00:45:37,780 --> 00:45:38,650
i think of

678
00:45:38,660 --> 00:45:40,880
five examples

679
00:45:40,890 --> 00:45:45,820
put them into the system get the results and then invented it looks to me

680
00:45:45,820 --> 00:45:48,050
and it's not same

681
00:45:48,070 --> 00:45:53,360
first you always need to do is to think of the proper operation subproperty relations

682
00:45:54,610 --> 00:46:01,240
kind what it should be just she wants clean appearance of the centre

683
00:46:02,780 --> 00:46:08,590
different from these means that you need to think about ways you can adjust present

684
00:46:08,610 --> 00:46:10,660
results of system and tell

685
00:46:11,280 --> 00:46:12,990
reduce the readers

686
00:46:13,010 --> 00:46:14,590
we measure

687
00:46:14,630 --> 00:46:18,240
the quality of our system on the from

688
00:46:18,240 --> 00:46:21,760
twenty five we've got four point one which is a

689
00:46:21,840 --> 00:46:26,760
so this is meaningless because it's not the point of comparison

690
00:46:26,760 --> 00:46:29,320
we need to base

691
00:46:30,860 --> 00:46:33,470
it just claim which

692
00:46:33,510 --> 00:46:35,390
can convince anyone

693
00:46:35,410 --> 00:46:37,950
so what i tried to do that

694
00:46:37,950 --> 00:46:44,360
to prove what just to see what makes sense as i is the based that

695
00:46:44,530 --> 00:46:50,070
sense and also very so i think this is what most people think of in

696
00:46:50,090 --> 00:46:53,450
the context this task so it would have cost

697
00:46:53,470 --> 00:46:55,660
similar sentences twenty

698
00:46:55,780 --> 00:46:58,570
the building model running one

699
00:46:58,590 --> 00:47:04,260
out only after sentences and what want to

700
00:47:04,280 --> 00:47:06,950
get the space is the sequence of

701
00:47:06,990 --> 00:47:11,300
c eight words because this is one of our approach is frame to produce

702
00:47:11,320 --> 00:47:15,530
which is the highest probability with respect to this simple language models so this is

703
00:47:15,530 --> 00:47:19,160
a very simple thing to it

704
00:47:19,160 --> 00:47:21,660
this is still fail the

705
00:47:21,660 --> 00:47:24,340
except baseline because it's not

706
00:47:24,380 --> 00:47:26,220
so it's not entirely different words

707
00:47:26,240 --> 00:47:28,610
random order still has some

708
00:47:28,630 --> 00:47:30,470
motivation concepts in

709
00:47:34,030 --> 00:47:38,240
the configuration which is the

710
00:47:38,280 --> 00:47:39,030
first it

711
00:47:39,050 --> 00:47:43,510
very simple configuration which is referred each edge

712
00:47:43,570 --> 00:47:48,530
which is very and then the final formula which is all those adjustments so here

713
00:47:49,030 --> 00:47:53,840
it is that it's not good to say he was very successful

714
00:47:53,840 --> 00:47:58,410
i spent so much time thinking about how to see the result right so should

715
00:47:58,430 --> 00:48:01,700
always see where this system where you didn't

716
00:48:01,800 --> 00:48:06,380
i always complete suggests that it's not that you need to show that it's not

717
00:48:06,380 --> 00:48:09,090
good enough really needed all the other things

718
00:48:09,160 --> 00:48:16,050
and i did experiments to the initial state and also cycle for languages like russian

719
00:48:16,050 --> 00:48:17,590
job and

720
00:48:17,610 --> 00:48:18,950
french tell

721
00:48:20,360 --> 00:48:23,260
very competent to judge the quality just as

722
00:48:23,280 --> 00:48:24,220
because my

723
00:48:25,840 --> 00:48:27,530
do it myself

724
00:48:27,740 --> 00:48:31,280
it's not the simply thing people just looked OK

725
00:48:31,780 --> 00:48:34,490
but the john results of which is also

726
00:48:34,510 --> 00:48:36,240
creation of this language and all the

727
00:48:36,240 --> 00:48:41,760
has very strict structure so you need to have the second position

728
00:48:41,820 --> 00:48:44,110
if you have a small sentences

729
00:48:44,130 --> 00:48:46,050
and access to the shortest path

730
00:48:46,260 --> 00:48:47,780
the graph

731
00:48:47,800 --> 00:48:49,150
you cannot enforce

732
00:48:49,150 --> 00:48:53,570
so what you get there and it's very likely to be correct

733
00:48:53,590 --> 00:48:57,590
so it took eighteen years english just

734
00:48:57,590 --> 00:49:02,130
a new state considered classes which were not too small to be

735
00:49:02,130 --> 00:49:08,130
something like between ten fifteen years out the average from the rest

736
00:49:10,240 --> 00:49:12,780
i'm prepared to you

737
00:49:14,930 --> 00:49:17,700
you know what

738
00:49:56,240 --> 00:50:00,430
one of the weight so controlling the judgement on this is that we need to

739
00:50:00,430 --> 00:50:02,180
make sure that people

740
00:50:02,200 --> 00:50:07,360
read the output and they agreed that the results

741
00:50:07,360 --> 00:50:11,380
there is also a good of that see some agreement that's why i used for

742
00:50:11,380 --> 00:50:13,990
me speakers for every single august

743
00:50:14,010 --> 00:50:18,800
OK so what we should look like this is one of the things they just

744
00:50:18,800 --> 00:50:20,760
i come to light so this is

745
00:50:21,030 --> 00:50:25,380
this is the one is the bag of whiskers heuristic right so this one when

746
00:50:25,380 --> 00:50:29,220
i just got this small pieces that are on the edge of the network and

747
00:50:29,220 --> 00:50:33,820
then when you come to sell picking community on thousand nodes i just collect the

748
00:50:33,820 --> 00:50:37,880
appropriate basis for them in the back and say this is the clustering right in

749
00:50:37,880 --> 00:50:39,940
the interesting thing is the

750
00:50:39,960 --> 00:50:43,400
this disconnected clusters of clusters that this

751
00:50:43,860 --> 00:50:47,050
from you know data analysis don't have much says

752
00:50:47,050 --> 00:50:51,190
i have a better score better community score than something that is you know nice

753
00:50:52,090 --> 00:50:53,590
coherent connected

754
00:50:54,210 --> 00:50:56,440
so this is a bag of words

755
00:50:57,510 --> 00:50:59,380
and and

756
00:50:59,380 --> 00:51:00,710
the other method here is

757
00:51:00,760 --> 00:51:04,440
if i'm using mac and i post process so you

758
00:51:04,440 --> 00:51:07,030
matt also be disconnected clusters

759
00:51:07,090 --> 00:51:11,260
that but they have a much better for the company

760
00:51:12,420 --> 00:51:17,070
so if all i care about is you know the cluster the partitions or the

761
00:51:17,070 --> 00:51:20,210
number of edges icon then this is the method i should use it but if

762
00:51:21,110 --> 00:51:23,630
what about

763
00:51:24,400 --> 00:51:27,760
let's call the semantics of the clusters then maybe this is not the method i

764
00:51:27,760 --> 00:51:31,940
should use it because it's giving disconnected clusters so what you know exactly what can

765
00:51:31,940 --> 00:51:33,570
i say

766
00:51:33,570 --> 00:51:35,840
so that's the first and

767
00:51:36,400 --> 00:51:37,510
just to complete

768
00:51:37,510 --> 00:51:38,590
basically what is this

769
00:51:39,400 --> 00:51:41,170
if i take the same network

770
00:51:41,190 --> 00:51:45,780
but i my basically fighting the network with the same degree distribution as the network

771
00:51:45,820 --> 00:51:49,490
but i said just and so i think the number of edges in every node

772
00:51:49,490 --> 00:51:50,630
keeps the

773
00:51:50,630 --> 00:51:51,760
and you can see that

774
00:51:51,860 --> 00:51:56,170
then i just basically like an experiment i get something that's flat

775
00:51:56,190 --> 00:52:01,220
unless for this can be used just sort of as a result of its so

776
00:52:01,220 --> 00:52:06,650
what he says that he never much more community structure and if they would be

777
00:52:06,650 --> 00:52:08,320
that's what

778
00:52:08,800 --> 00:52:12,440
before i was trying to make the point that spectral clustering

779
00:52:12,440 --> 00:52:15,460
has issues online

780
00:52:15,460 --> 00:52:19,510
you know on the confuses long path so here is what i mean by that

781
00:52:19,610 --> 00:52:22,340
this is the the graph i want to cut

782
00:52:22,420 --> 00:52:24,690
and if i was the spectral clustering methods

783
00:52:24,710 --> 00:52:29,460
what i mean got basically this is the spectral embedding so the got be like

784
00:52:29,490 --> 00:52:33,070
the hyperplane in this in this space right and you can see that this is

785
00:52:33,070 --> 00:52:37,650
not what i want to like it it confuses the basically the random walk is

786
00:52:37,650 --> 00:52:38,900
the problem of like

787
00:52:39,050 --> 00:52:43,460
mixing over long chains so i don't mix well so i don't think that so

788
00:52:43,460 --> 00:52:47,380
whatever i whenever i can i will cut many edges on the other hand if

789
00:52:47,380 --> 00:52:50,400
i was like some kind of flow based embedding

790
00:52:50,420 --> 00:52:54,710
you know i actually i do i do you think i can't somewhere here and

791
00:52:54,710 --> 00:52:59,260
i just basically this corresponds to that so i'm going to

792
00:52:59,880 --> 00:53:02,800
i just want to go a bit more into

793
00:53:02,800 --> 00:53:04,650
what kind of got to give

794
00:53:05,900 --> 00:53:10,130
to make to make this point more precise so he's he's one one idea so

795
00:53:10,130 --> 00:53:14,510
this is the the methods this graph partitioning of body basically you can see that

796
00:53:14,610 --> 00:53:18,110
is said the better conductance so red is that

797
00:53:18,130 --> 00:53:20,990
right so low over here the better

798
00:53:21,010 --> 00:53:27,940
while using these local spectral that i'm using actually even worse cuts writing you

799
00:53:27,960 --> 00:53:30,510
clusters that when i have to more edges

800
00:53:30,530 --> 00:53:31,800
but the funny thing is

801
00:53:31,820 --> 00:53:35,610
it's actually the real thing gives me

802
00:53:35,630 --> 00:53:38,760
if the clusters that are internally that connects

803
00:53:39,570 --> 00:53:44,720
this is what plot here is the average shortest path length between the all the

804
00:53:44,720 --> 00:53:46,110
nodes in the cluster

805
00:53:46,110 --> 00:53:47,880
and here you can see that

806
00:53:47,920 --> 00:53:50,510
you know blue is below the red meaning that

807
00:53:50,570 --> 00:53:52,800
these blue clusters are more compact

808
00:53:52,840 --> 00:53:55,590
similarly if i measure

809
00:53:55,650 --> 00:54:00,150
the ratio of external internal conductance where the idea is i have the ground i

810
00:54:00,150 --> 00:54:04,510
cut away the cluster and now whatever i cut away i tried to cut it

811
00:54:04,510 --> 00:54:05,690
it's a great honor to be here.

812
00:54:06,860 --> 00:54:10,990
I I I think I might be able to compress some of the introduction to my talk but

813
00:54:11,370 --> 00:54:14,860
because I I think you you summed up very well what what I want why i'm here and what

814
00:54:14,860 --> 00:54:15,660
i'm trying to achieve.

815
00:54:16,180 --> 00:54:18,100
let me start off just by acknowledging

816
00:54:18,920 --> 00:54:22,390
the people who who did the work and thought, many if not

817
00:54:22,890 --> 00:54:25,480
most, are even close to all of the really good ideas here.

818
00:54:25,960 --> 00:54:29,430
these are res junior researchers i've worked with who, you know, over the years are now

819
00:54:29,960 --> 00:54:34,980
gonna become rather senior themselves. Tom Griffiths and Charles Kemp were two of the first students who work with me.

820
00:54:35,410 --> 00:54:39,540
they're now a faculty of Bekeley and CMU respectively, and their work is

821
00:54:39,540 --> 00:54:42,960
sort of very very much the first part of this talk and their thinking

822
00:54:42,960 --> 00:54:44,200
is deeply interlaced throughout it.

823
00:54:44,640 --> 00:54:47,890
Noah Goodman and Vikash Mansinghka are more recent alumni of our of our

824
00:54:48,760 --> 00:54:52,830
group. Noah is now a professor at Stanford, Vikash is a research scientist at MIT,

825
00:54:53,390 --> 00:54:56,210
and that they played the lead role on on the key idea that i'm most

826
00:54:56,210 --> 00:54:59,600
excited about, sort of what I hope will be the climax of this notion of probabilistic

827
00:55:00,740 --> 00:55:04,790
and then I want highlight a few other researchers, Dan Roy and Cameron Freer, who've been

828
00:55:04,790 --> 00:55:08,810
doing some some really interesting theoretical work and helped me think about so the connections

829
00:55:08,810 --> 00:55:14,310
between probalistic programs, their role in cognitive modeling and Turning's original vision for AI because

830
00:55:14,310 --> 00:55:16,370
of course in this year that's what everybody has to think about.

831
00:55:16,960 --> 00:55:17,450
rightfully so.

832
00:55:17,870 --> 00:55:20,180
and then researchers who are currently in our group who've done

833
00:55:21,200 --> 00:55:28,650
the the taken the lead on the main modeling and experimental projects Peter Battaglia, Chris Baker, Tomer Ullman and Steve Piantadosi, actually Steve recently graduated.

834
00:55:30,200 --> 00:55:32,950
so the the goal here, the the way I like to think of what i'm trying

835
00:55:32,950 --> 00:55:36,230
to do, is reverse engineering the mind. i'm trying to understand

836
00:55:36,730 --> 00:55:40,400
human cognition. if I had to choose am I a cognitive scientist or an AI researcher,

837
00:55:40,750 --> 00:55:44,150
i'm a cognitive scientist first and foremost. but the kind of science we're trying to do

838
00:55:44,200 --> 00:55:48,390
is a reverse engineering enterprise. we want to understand how the mind works in the

839
00:55:48,390 --> 00:55:52,360
same kinds of terms that you would use to build an intelligent system, some some kind of

840
00:55:52,360 --> 00:55:53,900
human-like intelligence in a machine.

841
00:55:54,490 --> 00:55:59,180
and we do AI research and other, you know, related work in machine learning, partly for its

842
00:55:59,210 --> 00:56:03,240
practical benefit but partly as that maybemost deeply as a proof-of-principle to show

843
00:56:03,760 --> 00:56:08,060
these ideas, these engineering ideas that we think explain how natural intelligence works actually work.

844
00:56:08,460 --> 00:56:13,500
it's it's rather surprising if you look at how many models, theories, informal and formal

845
00:56:13,500 --> 00:56:17,110
accounts that have been proposed over the years in in cognitive science and psychology don't

846
00:56:17,110 --> 00:56:19,590
actually work as engineering solutions, and that's central to what

847
00:56:20,020 --> 00:56:21,660
we think counts as adequate science here.

848
00:56:23,640 --> 00:56:24,770
as Barr mentioned the

849
00:56:25,210 --> 00:56:29,640
these fields of cognitive science AI have grew up together, starting in the nineteen fifties

850
00:56:30,260 --> 00:56:35,540
and I would say by the nineteen eighties were largely growing apart. cu cognitive scientists

851
00:56:35,540 --> 00:56:39,210
looked at AI and saw a field of people who were increasingly just focused on practical

852
00:56:39,210 --> 00:56:41,130
applications, had given up the big vision of

853
00:56:42,050 --> 00:56:47,580
of real and human-like intelligence and were using a lot of math they didn't understand. and AI researchers looked at

854
00:56:47,580 --> 00:56:52,730
cognitive scientists an and other psychologist and saw them obsessed with strange peculiar processing details that

855
00:56:52,730 --> 00:56:55,480
didn't really seem to illuminate those big questions anyway and

856
00:56:55,990 --> 00:56:58,490
you kno were very fuzzy and didn't understand any of the math that they should be

857
00:56:59,840 --> 00:57:02,900
now these fields are starting to come back together. it's a very exciting time

858
00:57:03,300 --> 00:57:06,030
driven by the shared sense that actually they might have something to say to each other

859
00:57:06,030 --> 00:57:09,830
again and a common language, really that's quite fundamental, a common language in which to say

860
00:57:11,340 --> 00:57:14,980
if you had to say where does this start from, let's start from success.

861
00:57:15,510 --> 00:57:16,960
AI has been a success.

862
00:57:18,070 --> 00:57:22,300
or we're living in an era of real AI success is maybe that's a better way to put it.

863
00:57:22,300 --> 00:57:25,170
certainly this conft had highlighted some of these successes

864
00:57:25,800 --> 00:57:28,710
we're all familiar with them in our daily lives, you know, even those even even if

865
00:57:28,710 --> 00:57:33,910
we weren't all AI professionals here, you can actually read about working AI technologies

866
00:57:33,910 --> 00:57:36,840
in the newspaper, see them on TV, use them in your daily life,

867
00:57:37,410 --> 00:57:41,100
and these are technologies that the early founders of the field, whatever the big vision

868
00:57:42,050 --> 00:57:47,710
they had, they would have to have been impressed, right. clearly these are successes these these systems,

869
00:57:47,850 --> 00:57:51,750
you know, whether it's Google, face recognition, pedestrian recognition systems,

870
00:57:53,350 --> 00:57:56,070
various kinds recommendation systems, things like,

871
00:57:56,490 --> 00:57:56,840
you know, things,

872
00:57:57,330 --> 00:58:01,540
Siri, their the new voice interface to the iphone, which works at least some of the time,

873
00:58:02,400 --> 00:58:05,200
maybe most prominently, the news IBM's Watson

874
00:58:05,650 --> 00:58:10,460
jeopardy playing system, or, you know, after this morning I definitely have to add Google's self driving cars.

875
00:58:10,900 --> 00:58:12,070
I mean, who couldn't've

876
00:58:12,750 --> 00:58:14,380
fail to be impressed by these amazing

877
00:58:15,700 --> 00:58:18,940
and if we have to say well what's what's driving them, maybe the the the

878
00:58:19,710 --> 00:58:24,190
sort of one word, one phrase story is something like statistics. statistics on a grand

879
00:58:24,190 --> 00:58:28,920
scale. massive data coming in through all kinds of sensors and then various kinds of

880
00:58:28,920 --> 00:58:34,590
sophisticated ways of finding patterns in that data. clusters correlations, low dimensional structure maybe even something

881
00:58:34,590 --> 00:58:35,310
a little bit causal.

882
00:58:36,830 --> 00:58:38,500
and that's has not

883
00:58:38,880 --> 00:58:42,570
how could that fail to get the attention of people in many fields, in particular cognitive scientists,

884
00:58:42,980 --> 00:58:45,270
you know, the people like me. I I got into the field in the mid

885
00:58:45,270 --> 00:58:50,360
nineties and well that was well before these successes were hitting the front pages.

886
00:58:50,900 --> 00:58:53,510
you know, there were much smaller scale versions that we also

887
00:58:54,350 --> 00:58:55,920
clearly saw where these were coming from.

888
00:58:56,640 --> 00:59:00,420
you know, the whole probabilistic revolution in AI that was that was the time I was getting in the field.

889
00:59:01,340 --> 00:59:04,820
so that's got attention. at the same time the gaps are also clear. while each

890
00:59:04,820 --> 00:59:09,160
of these systems achieves some human level, maybe even expert level, you know, as good or

891
00:59:09,270 --> 00:59:14,010
better than the experts' performance in some particular aspect of intelligent behavior,

892
00:59:14,410 --> 00:59:18,920
none of them we would say is truly intelligent. some actually intelligent engineers had to built them and

893
00:59:18,920 --> 00:59:21,600
each one of them does one of these things, but compared with with you, you know,

894
00:59:21,710 --> 00:59:23,200
no no one had to engineer you

895
00:59:23,710 --> 00:59:25,370
to solve these problems, to

896
00:59:25,910 --> 00:59:29,880
to, you know, learn how to drive, learn how to play jeopardy. may not play jeopardy as well as Watson

897
00:59:29,890 --> 00:59:32,890
but you only have to watch the game for a couple of minutes, maybe maybe

898
00:59:32,890 --> 00:59:35,000
not even be told the rules, or just be told the rules and,

899
00:59:35,510 --> 00:59:39,140
you know, just a couple of sentences, and then you get it. and you can perform an

900
00:59:39,190 --> 00:59:42,780
endless number of tasks without having to be programmed. you program yourself.

901
00:59:43,320 --> 00:59:47,650
we want to understand that gap. so I want to talk about the the successes that have driven

902
00:59:47,650 --> 00:59:53,240
the reconvergence, largely thinking about various kinds statistical approaches. but how do we make these

903
00:59:53,240 --> 00:59:56,730
kinds of approaches more sophisticated to actually get at the remaining gap?

904
00:59:58,320 --> 01:00:01,350
now let me shift over to the cognitive science side. and so here's the big question that

905
01:00:01,390 --> 01:00:04,750
animates me as a cognitive scientist and a lot of a lot of other work

906
01:00:04,750 --> 01:00:08,450
in our field and we we'll see how this starts to come back to where this

907
01:00:08,450 --> 01:00:08,890
gap is.

908
01:00:09,320 --> 01:00:12,040
wh when we look at at cognition, we see throughout,

909
01:00:12,790 --> 01:00:14,310
whether we're talking about perception,

910
01:00:17,640 --> 01:00:18,020
we see

911
01:00:18,460 --> 01:00:22,780
this this fundamental gap between the data coming into your senses and what you do

912
01:00:22,780 --> 01:00:28,270
with it. our minds build models, abstractions, generalizations but seem to go so far beyond the

913
01:00:28,270 --> 01:00:29,850
data that's there. how are they able to do that?

914
01:00:30,430 --> 01:00:34,900
here's a little demo that is is something that that we've been studying in the lab.

915
01:00:34,900 --> 01:00:36,980
it's it's meant to abstract in a form that we can study

916
01:00:37,390 --> 01:00:41,560
with adults the problem that the child faces when they're learning a word. as anybody who's ever

917
01:00:41,560 --> 01:00:43,830
been a child or or had a child might remember,

918
01:00:45,750 --> 01:00:50,140
a remarkable ability, you see a remarkable ability in children to learn concepts even just,

919
01:00:50,320 --> 01:00:51,520
you know, to label a concept like

920
01:00:51,940 --> 01:00:53,950
table, chair, cup, dog

921
01:00:54,560 --> 01:00:58,990
from just a very small number of positive examples. contrast this with the situation of,

922
01:00:58,990 --> 01:01:02,660
you know, most traditional machine learning, where classification is seen as something that requires many

923
01:01:02,990 --> 01:01:06,920
positive and negative examples. so to remind you of of this phenomenon and how much fun it is

924
01:01:06,920 --> 01:01:09,590
we'll just a little demo here to wake everyone up after lunch.

925
01:01:10,630 --> 01:01:15,070
i've i've shown you these novel objects we made in an object generating program and i've

926
01:01:15,070 --> 01:01:18,310
highlighted a few examples of tufas. and now you tell me which are the

927
01:01:18,310 --> 01:01:21,070
other tufas. we'll just go through this with a pointer. so tufa, yes or

928
01:01:24,550 --> 01:01:27,890
right it's too far for my pointer, my shaky hands so i'll you it like this.

929
01:01:27,890 --> 01:01:30,120
if you can see. OK, yes no. this one.

930
01:01:37,660 --> 01:01:37,740
wake up.

931
01:01:38,400 --> 01:01:39,420
yes or no, tufa or no?

932
01:01:40,890 --> 01:01:41,600
OK this one?

933
01:01:43,050 --> 01:01:43,360
this one?

934
01:01:44,390 --> 01:01:44,720
this one?

935
01:01:44,720 --> 01:01:46,010
forested land

936
01:01:46,030 --> 01:01:47,680
and all three

937
01:01:47,710 --> 01:01:54,180
ways of being can on top of on the of entity images and you can

938
01:01:54,180 --> 01:01:56,170
see that you have to do with that

939
01:01:56,190 --> 01:02:00,180
my former sit eleven percent to seventy six percent

940
01:02:00,180 --> 01:02:03,710
so is the typical example where you're going to be not that much

941
01:02:03,730 --> 01:02:08,920
but again a few percent by considering a dual representation in you know in the

942
01:02:09,070 --> 01:02:12,110
nice carnival kind

943
01:02:13,580 --> 01:02:14,260
and i

944
01:02:14,280 --> 01:02:20,280
what i presented so far is just like in the kind design in the simple

945
01:02:20,280 --> 01:02:21,840
setting of course

946
01:02:21,880 --> 01:02:28,200
o canal designs it is a bit and maybe a bit long to do find

947
01:02:28,260 --> 01:02:31,530
new problem in you will have to be able to design the kernel automatically from

948
01:02:31,530 --> 01:02:33,380
data OK let's be lazy

949
01:02:33,390 --> 01:02:37,460
it if you can learn everything from data OK not having to go through all

950
01:02:38,620 --> 01:02:42,100
we should have some nice links with sparsity inducing norms

951
01:02:42,110 --> 01:02:46,100
OK so want to learn the kinds from data

952
01:02:46,150 --> 01:02:47,630
OK so here

953
01:02:47,640 --> 01:02:53,470
kernels there are is not based only constraint i put on kernels that a is

954
01:02:53,610 --> 01:02:54,860
positive definite

955
01:02:54,960 --> 01:03:00,230
so if we restrict the set of that allow the kind of consider so since

956
01:03:00,230 --> 01:03:05,540
cystatin i consider iconic representation of my of my problem so you would assume i

957
01:03:05,540 --> 01:03:11,120
know in advance i can be used in different kernels from my problem so get

958
01:03:11,190 --> 01:03:14,110
images was supposed to be doing kind texture

959
01:03:14,120 --> 01:03:17,650
the shape of a kind of course and i want to combine them in that

960
01:03:19,180 --> 01:03:24,390
so i had to do before summing kernels is equivalent to concatenating feature spaces so

961
01:03:24,390 --> 01:03:29,360
when i some kernels essentially considering one feature space documents

962
01:03:29,370 --> 01:03:37,640
OK and in predicting some of of functions which now functions depend on the thing

963
01:03:37,650 --> 01:03:38,880
so what i do

964
01:03:38,890 --> 01:03:43,920
when you some kernels take x map it separated too few one feature in human

965
01:03:45,140 --> 01:03:48,310
consider the effect of which is also

966
01:03:48,320 --> 01:03:51,630
but in eighteen and you get this prediction function

967
01:03:51,680 --> 01:03:54,010
this is just exactly concatenating the

968
01:03:54,030 --> 01:03:55,420
feature space

969
01:03:55,430 --> 01:04:00,170
so logic is what organisation should i put him

970
01:04:00,180 --> 01:04:05,540
so first what if i put just as the sum of squared norms i exactly

971
01:04:05,540 --> 01:04:08,380
get some of the of the kind of why because

972
01:04:08,380 --> 01:04:12,260
summing the norms describes equivalent to

973
01:04:12,290 --> 01:04:14,710
i think the norm of the concatenation

974
01:04:14,760 --> 01:04:17,340
so now i would like to be able to do something different

975
01:04:17,340 --> 01:04:21,170
and now we consider the sum of the norms without the square

976
01:04:21,180 --> 01:04:24,340
so if this is where we go back to

977
01:04:24,340 --> 01:04:25,590
two sparsity

978
01:04:25,600 --> 01:04:28,660
if that image imagine if is one-dimensional

979
01:04:28,710 --> 01:04:30,500
it is and one not

980
01:04:30,510 --> 01:04:33,270
and now we have is not one-dimensional everybody

981
01:04:33,650 --> 01:04:37,850
so what i want to mention and even sometimes infinite dimensional

982
01:04:37,870 --> 01:04:40,430
so when you do that when you sum of the norms

983
01:04:40,460 --> 01:04:45,690
OK another square the norms you get sparsity at the group level you want to

984
01:04:45,690 --> 01:04:50,220
put the and they have yet to see all and not particularly inside

985
01:04:50,240 --> 01:04:52,680
goal is all of all edges the all

986
01:04:52,680 --> 01:04:56,500
and if not you don't really care about what's happening is if g

987
01:04:56,510 --> 01:04:58,910
so what is we do you essentially

988
01:04:58,990 --> 01:05:03,120
some some of the images and what this means we select good kernels

989
01:05:03,530 --> 01:05:05,420
we can prove that

990
01:05:05,510 --> 01:05:11,360
in fact this is equivalent to learning exactly what i wanted to connect combination of

991
01:05:11,770 --> 01:05:16,830
and the mattresses so this just much involved by the end by using a sparsity

992
01:05:16,830 --> 01:05:22,080
inducing norms which is some of the of the genomes you get what you want

993
01:05:22,080 --> 01:05:25,220
to get sparse in combination of kernels

994
01:05:25,320 --> 01:05:30,780
of course it's a bit harder to estimate than the good anyone on problems because

995
01:05:30,870 --> 01:05:35,840
you have so you have a lot of work trying to do that and finally

996
01:05:36,090 --> 01:05:40,490
of you also have some more between two and analyse when you give guide part

997
01:05:40,490 --> 01:05:41,620
of the kernels

998
01:05:41,620 --> 01:05:47,370
so in most of the work in in this sparsity inducing norm business we tell

999
01:05:47,370 --> 01:05:48,670
you get both

1000
01:05:48,750 --> 01:05:52,170
organisation and good this team and then with this selection

1001
01:05:52,180 --> 01:05:53,690
but do you actually get

1002
01:05:53,710 --> 01:05:54,920
the good model

1003
01:05:54,920 --> 01:05:58,090
and you can prove that in the case of the one norm in this case

1004
01:05:58,090 --> 01:05:58,790
to get the more

1005
01:05:59,280 --> 01:06:02,510
if you only you have a certain condition which is satisfy

1006
01:06:02,560 --> 01:06:07,350
and this has led to a lot of work in statistics and the musicians

1007
01:06:07,360 --> 01:06:09,710
so what you want to you to use it for

1008
01:06:09,740 --> 01:06:11,040
of course

1009
01:06:11,680 --> 01:06:15,460
using bioinformatics so stealing between bioinformatics division

1010
01:06:15,460 --> 01:06:18,450
and then they had to retrieve it and analyse the data

1011
01:06:18,450 --> 01:06:22,900
nowadays she has about on the order of ten high performance

1012
01:06:22,920 --> 01:06:26,550
they go in in the spring they have a huge hard disk on they record

1013
01:06:26,550 --> 01:06:30,180
twenty four hours continuously until the fall

1014
01:06:30,230 --> 01:06:34,240
so now every fall she gets like on the order of five years' worth of

1015
01:06:34,240 --> 01:06:36,160
audio recordings

1016
01:06:36,180 --> 01:06:38,740
and she has three months to analyse them

1017
01:06:38,770 --> 01:06:44,660
so now you know you really have the situation you cannot even look at the

1018
01:06:44,660 --> 01:06:46,660
data ones

1019
01:06:46,690 --> 01:06:49,410
and this is a typical situation nowadays

1020
01:06:49,430 --> 01:06:55,460
so you need some fast method to just scan through for interesting portions and so

1021
01:06:55,460 --> 01:06:57,890
on and that's of course in machine learning problem

1022
01:06:57,900 --> 01:07:00,380
because it's hard to tell what's interesting

1023
01:07:00,430 --> 01:07:04,620
especially if there is the potential for things that you didn't look for

1024
01:07:04,980 --> 01:07:06,940
new things

1025
01:07:07,090 --> 01:07:11,460
there are some some species of whale out in the simple lawrence where it's not

1026
01:07:11,460 --> 01:07:15,350
even known if they have localizations and what they are

1027
01:07:15,370 --> 01:07:18,870
so you know if you throw away too much missing that

1028
01:07:18,890 --> 01:07:23,470
so this is the typical problem nowadays and that's really the the current problems for

1029
01:07:23,480 --> 01:07:25,200
machine learning

1030
01:07:26,800 --> 01:07:29,620
these are some of the random examples

1031
01:07:29,630 --> 01:07:34,930
one pulsar survey at a receiver with a huge radio dish right in

1032
01:07:34,950 --> 01:07:39,870
costa rica produces one tera bytes of data a day

1033
01:07:39,880 --> 01:07:43,670
and that's one survey out of half a dozen other running concurrently

1034
01:07:43,680 --> 01:07:45,510
on the dish

1035
01:07:45,540 --> 01:07:51,840
OK the website over a hundred page requests a second probably google would be on

1036
01:07:51,880 --> 01:07:54,440
two orders of magnitude more than that but

1037
01:07:54,460 --> 01:07:55,910
google doesn't do

1038
01:07:55,910 --> 01:08:00,780
as much adaptation to a customer as other sites do amazonas maybe a good example

1039
01:08:00,780 --> 01:08:02,600
because amazon actually

1040
01:08:02,610 --> 01:08:07,210
remember suing customize the pages for you and they probably get on the order of

1041
01:08:07,210 --> 01:08:09,820
a thousand page requests the second

1042
01:08:09,860 --> 01:08:13,530
london has over five hundred thousand security cameras

1043
01:08:13,550 --> 01:08:18,900
and again nobody has time to watch all the video

1044
01:08:18,900 --> 01:08:23,440
so we clearly need something to cope with that

1045
01:08:23,440 --> 01:08:27,960
we need machine learning algorithms but not just any algorithm we need algorithms that can

1046
01:08:28,320 --> 01:08:31,300
handle large complex nonlinear models

1047
01:08:31,320 --> 01:08:34,700
because the datasets are large complex and nonlinear

1048
01:08:36,630 --> 01:08:40,650
by that i mean up to millions of degrees of freedom so i'm i'm happily

1049
01:08:40,650 --> 01:08:46,220
consider in problems where n here is on the order of ten million

1050
01:08:46,240 --> 01:08:51,760
OK and now this is something ten years ago that would have been considered insane

1051
01:08:51,760 --> 01:08:55,650
it's still a bit out of limb but it can actually be done

1052
01:08:57,010 --> 01:09:01,590
we need to be able to deal with large volumes of very low quality data

1053
01:09:01,590 --> 01:09:08,360
we may have noisy correlated data nonstationary data outliers all kinds of nasty things in

1054
01:09:10,420 --> 01:09:15,340
and we need real time online adaptation we want the answer is basically as the

1055
01:09:15,340 --> 01:09:17,030
data rules in

1056
01:09:17,050 --> 01:09:19,840
that's the typical scenario nowadays

1057
01:09:19,840 --> 01:09:22,470
so we cannot actually sort of

1058
01:09:22,490 --> 01:09:26,010
collective training set you clean it up

1059
01:09:26,010 --> 01:09:27,440
preprocess it

1060
01:09:27,490 --> 01:09:30,010
run the algorithm for a couple of days

1061
01:09:30,030 --> 01:09:31,880
and then published the result

1062
01:09:32,440 --> 01:09:36,880
the modalities is data comes in answers go out

1063
01:09:36,900 --> 01:09:42,670
the learning has to happen online in between

1064
01:09:42,690 --> 01:09:45,200
now my claim is that

1065
01:09:45,220 --> 01:09:49,400
a lot of the current machine learning techniques have difficulty with that

1066
01:09:49,420 --> 01:09:55,780
i should maybe say the current optimisation techniques tend to have difficulty with

1067
01:09:55,820 --> 01:10:00,090
and here's the problem so classical optimizer

1068
01:10:00,110 --> 01:10:04,190
is this iterative process right you the loop over

1069
01:10:04,260 --> 01:10:09,380
and in at each iteration and the loop you take function and gradient measurements

1070
01:10:09,400 --> 01:10:13,840
and that measurement consists also of the loop

1071
01:10:13,900 --> 01:10:17,440
namely over all your data

1072
01:10:17,440 --> 01:10:21,650
so you have two nested loops and if you dataset gets big this becomes very

1073
01:10:21,650 --> 01:10:24,550
very slow

1074
01:10:24,550 --> 01:10:29,690
what you need is online learning here so you need to use stochastic approximation so

1075
01:10:29,690 --> 01:10:34,440
that your interleaving your optimize with the sampling of the training data and it all

1076
01:10:34,440 --> 01:10:36,070
happens in a single

1077
01:10:39,470 --> 01:10:43,280
this is also the scenario that people in signal processing

1078
01:10:43,300 --> 01:10:46,470
you know adaptive filtering and so on consider

1079
01:10:46,490 --> 01:10:50,590
so far as far as i'm concerned is all near synonyms

1080
01:10:50,590 --> 01:10:55,360
the adaptive control adaptive signal processing filtering

1081
01:10:55,360 --> 01:10:58,090
stochastic approximation online learning

1082
01:10:58,110 --> 01:11:03,990
this old people from different backgrounds talking about more or less the same thing

1083
01:11:04,010 --> 01:11:06,940
but bringing different tools and techniques

1084
01:11:07,070 --> 01:11:11,400
to the enterprise

1085
01:11:11,990 --> 01:11:16,920
here's a little bit what i've already elaborated what have drawn on a whiteboard

1086
01:11:16,940 --> 01:11:18,630
you want to

1087
01:11:18,650 --> 01:11:22,670
typically minimize with respect to some parameter vector theta

1088
01:11:22,690 --> 01:11:26,670
now this is the

1089
01:11:26,690 --> 01:11:32,720
a scenario where i'm making a statistical assumptions talking about the expectation over the data

1090
01:11:32,760 --> 01:11:36,940
of some loss function that is defined over the data so this is actually the

1091
01:11:37,630 --> 01:11:42,460
risk and we approximate that by the empirical risk where we have some sort of

1092
01:11:42,470 --> 01:11:45,550
training set from which we sample the data

1093
01:11:45,550 --> 01:11:48,820
you can use

1094
01:12:27,940 --> 01:12:33,230
and what we

1095
01:13:08,280 --> 01:13:15,340
what you

1096
01:13:15,360 --> 01:13:17,480
because of the

1097
01:13:40,550 --> 01:13:49,750
so think

1098
01:13:49,800 --> 01:13:58,900
or you can

1099
01:13:58,920 --> 01:14:02,020
what really

1100
01:14:20,520 --> 01:14:24,690
trying to use

1101
01:14:44,730 --> 01:14:51,630
of course this is

1102
01:14:51,710 --> 01:14:55,230
one of my life and

1103
01:14:58,230 --> 01:15:05,090
you need to show you know

1104
01:15:05,210 --> 01:15:09,590
we are always always always

1105
01:15:11,390 --> 01:15:14,420
with the way

1106
01:15:21,770 --> 01:15:26,460
i i

1107
01:15:26,480 --> 01:15:32,210
try to look like or or

1108
01:15:39,570 --> 01:15:40,880
because he

1109
01:15:40,900 --> 01:15:42,630
or or

1110
01:16:24,460 --> 01:16:29,070
one of them

1111
01:16:30,110 --> 01:16:36,550
the problem

1112
01:16:36,650 --> 01:16:39,000
even more

1113
01:16:40,170 --> 01:16:41,190
he was

1114
01:16:58,670 --> 01:17:07,480
the one was the thing that we

1115
01:17:08,420 --> 01:17:13,820
you will be when you're

1116
01:17:13,860 --> 01:17:20,750
and for a long time and that everybody uses more

1117
01:17:55,670 --> 01:17:57,050
you might want

1118
01:18:01,380 --> 01:18:05,190
the way

1119
01:18:11,210 --> 01:18:18,880
if you might

1120
01:18:25,020 --> 01:18:27,610
one of the

1121
01:18:27,610 --> 01:18:37,680
proof theory serials then i minister stand still so that the videotape look completely silly

1122
01:18:37,680 --> 01:18:38,980
with me just

1123
01:18:38,990 --> 01:18:42,820
going back and forth

1124
01:18:42,830 --> 01:18:46,000
so we we where we left off was with the

1125
01:18:47,270 --> 01:18:50,070
the proof theory and

1126
01:18:50,090 --> 01:18:53,970
what i'd like and we talked a bit about the difference between

1127
01:18:54,090 --> 01:18:59,570
intuitions logic and minimal launch four we and up talk about about the difference between

1128
01:18:59,590 --> 01:19:01,130
intuitionist logic

1129
01:19:01,130 --> 01:19:06,840
in classical logic don't talk a little bit about them it's program for using intuitionist

1130
01:19:06,840 --> 01:19:11,280
logic is the way of interpreting natural language and then move on from there to

1131
01:19:11,280 --> 01:19:13,630
relevant logic

1132
01:19:18,560 --> 01:19:23,450
the difference as i said before has to do with the negation rules

1133
01:19:23,590 --> 01:19:31,460
so the asian roles

1134
01:19:47,780 --> 01:19:55,230
we're effectively those two that gives us minimal logic

1135
01:19:55,250 --> 01:19:58,890
so the introduction and elimination rules for negation

1136
01:19:58,940 --> 01:20:01,780
those give us minimal logic class

1137
01:20:01,970 --> 01:20:10,690
and as elimination rule

1138
01:20:10,700 --> 01:20:15,860
that gives us costs intuitionist logic now to get classical logic

1139
01:20:15,890 --> 01:20:18,050
these are all that i was calling

1140
01:20:18,090 --> 01:20:19,700
that's a

1141
01:20:19,720 --> 01:20:24,890
another negation elimination rule a kind of looks like negation introduction rule

1142
01:20:35,500 --> 01:20:39,360
and i'm calling negation elimination two

1143
01:20:39,420 --> 01:20:42,050
i think called that its

1144
01:20:42,070 --> 01:20:48,870
so we have this distinction between negation elimination one in the nation nomination two it'll

1145
01:20:48,890 --> 01:20:52,390
come back to that we only get to relevant logic because it has

1146
01:20:52,430 --> 01:20:54,870
both of these rules site

1147
01:20:54,890 --> 01:20:56,610
with slight modifications

1148
01:20:56,620 --> 01:21:02,460
now what is that this rule does for us this rule gives us

1149
01:21:02,520 --> 01:21:09,010
very direct play

1150
01:21:09,020 --> 01:21:14,420
a proof of that

1151
01:21:21,800 --> 01:21:34,490
so in the hypothesis not not a

1152
01:21:34,510 --> 01:21:38,550
we take another hypothesis at a

1153
01:21:47,390 --> 01:21:50,790
we reiterate our p the first hypothesis again

1154
01:21:50,800 --> 01:21:54,020
some of the various systems where you don't have to do that i like to

1155
01:21:54,020 --> 01:21:58,390
do that explicitly because some of the week substructural logics that we're not going to

1156
01:21:58,390 --> 01:22:00,040
discuss in in

1157
01:22:00,100 --> 01:22:03,580
in these natural deduction terms here but some of them don't allow you to do

1158
01:22:09,710 --> 01:22:11,120
then we get to have

1159
01:22:11,140 --> 01:22:14,550
one that because of these two two and three

1160
01:22:14,570 --> 01:22:17,420
gives us

1161
01:22:17,450 --> 01:22:25,830
as from the negation elimination

1162
01:22:25,890 --> 01:22:29,050
now what we get from as well anything we want

1163
01:22:29,050 --> 01:22:32,670
and then we have to think what we want we start with this is hypothesis

1164
01:22:32,700 --> 01:22:37,670
and we wanted to prove that

1165
01:22:38,700 --> 01:22:40,230
we don't

1166
01:22:40,240 --> 01:22:41,170
the that

1167
01:22:41,210 --> 01:22:44,060
we from not a we got have

1168
01:22:44,190 --> 01:22:49,500
from now on we get at so we can conclude a

1169
01:22:49,510 --> 01:22:53,000
that's what we wanted to check that all

1170
01:22:53,190 --> 01:22:59,550
so that's

1171
01:22:59,610 --> 01:23:02,000
two through four

1172
01:23:04,780 --> 01:23:06,780
the nation two

1173
01:23:06,790 --> 01:23:09,100
and one through

1174
01:23:09,110 --> 01:23:10,110
you know

1175
01:23:10,220 --> 01:23:13,120
in this account

1176
01:23:13,130 --> 01:23:15,170
one to five

1177
01:23:15,220 --> 01:23:16,550
book introduction

1178
01:23:16,570 --> 01:23:19,300
OK so that's how we get

1179
01:23:19,420 --> 01:23:24,800
what's often known as double then the double negation road obligation elimination rule

1180
01:23:24,810 --> 01:23:27,570
that we can eliminate double negations

1181
01:23:27,590 --> 01:23:32,710
so not a get a that's not intuitionistic a valid

1182
01:23:33,630 --> 01:23:41,680
this also gives us someone mentioned earlier the law of excluded middle

1183
01:23:41,710 --> 01:23:45,930
which i had the true of that year and you know it's but

1184
01:23:45,990 --> 01:23:53,280
we can do that

1185
01:23:53,300 --> 01:23:59,480
excluded middle

1186
01:24:00,760 --> 01:24:04,490
so this allows us is a form of reductio argument

1187
01:24:04,550 --> 01:24:06,610
we assume the negation

1188
01:24:06,610 --> 01:24:10,280
and we try to prove and we prove a contradiction so we

1189
01:24:10,340 --> 01:24:11,720
take a right

1190
01:24:11,730 --> 01:24:16,420
intuition is often say they don't like reductio argument that's what they mean

1191
01:24:16,430 --> 01:24:20,420
the former doctor using a and you get as many can conclude not if they

1192
01:24:20,430 --> 01:24:23,160
like that just fine

1193
01:24:24,120 --> 01:24:31,210
this this form of reductio they don't like

1194
01:24:39,730 --> 01:24:42,980
OK so this thing here is called the law of excluded middle

1195
01:24:42,990 --> 01:24:44,560
or land

1196
01:24:44,660 --> 01:24:46,010
o experimental

1197
01:24:46,660 --> 01:24:55,360
and we can prove it like that

1198
01:24:55,360 --> 01:24:59,990
so we take this hypothesis you're looking at this thing what what what what

1199
01:25:00,050 --> 01:25:02,210
are you doing here

1200
01:25:07,750 --> 01:25:12,500
proving things with a focus main connective very easy proving things where the

1201
01:25:12,550 --> 01:25:16,980
negation is the main connective will you know you're going to have to do

1202
01:25:16,990 --> 01:25:20,620
something out of the negation is the main connected is here

1203
01:25:20,660 --> 01:25:28,130
disjunction as it establishes the main connective negations or something else the disjunction particular disjunction

1204
01:25:28,130 --> 01:25:31,000
is main connective that what you want to prove

1205
01:25:31,010 --> 01:25:33,380
and this is what we want to prove

1206
01:25:33,410 --> 01:25:36,480
it then

1207
01:25:39,120 --> 01:25:44,240
do using reductio this form is often a good idea in classical logic intuitionistic logic

1208
01:25:44,250 --> 01:25:45,420
can do it so

1209
01:25:45,420 --> 01:25:47,800
you have to find another route

1210
01:25:47,810 --> 01:25:52,990
disjunctions are actually quite difficult in intuitionist logic is you really do

1211
01:25:53,050 --> 01:25:58,420
i need to prove one of the districts

1212
01:25:59,070 --> 01:26:01,550
OK from here we get by right

1213
01:26:01,780 --> 01:26:08,010
from two and disjunction introduction we get that but that may contradict each other and

1214
01:26:08,010 --> 01:26:15,220
that's a good thing

1215
01:26:15,230 --> 01:26:16,800
we reiterate one

1216
01:26:16,820 --> 01:26:21,790
and we bring that now so that these and we can conclude from those at

1217
01:26:24,760 --> 01:26:25,910
so three

1218
01:26:27,360 --> 01:26:30,800
and negation elimination

1219
01:26:32,030 --> 01:26:33,670
what i we want to conclude

1220
01:26:33,670 --> 01:26:35,490
well then

1221
01:26:35,540 --> 01:26:39,960
my negation introduction

1222
01:26:40,130 --> 01:26:42,470
we get not a

1223
01:26:42,510 --> 01:26:46,280
OK what we want and what we wanted a non right

1224
01:26:46,320 --> 01:26:57,440
o from monday we can get a an on air

1225
01:27:01,530 --> 01:27:03,350
from these two

1226
01:27:03,350 --> 01:27:05,600
so this is really expensive

1227
01:27:05,690 --> 01:27:10,230
you can also use click through data but then you need to worry about that

1228
01:27:10,290 --> 01:27:15,080
so for instance you could otherwise then somebody could actually script bought differences let's say

1229
01:27:16,060 --> 01:27:20,690
really keen on my name coming up in your favourite search engine on top

1230
01:27:20,790 --> 01:27:21,940
and maybe

1231
01:27:21,960 --> 01:27:26,310
you know it doesn't so what i could do is i could just hire botnet

1232
01:27:27,270 --> 01:27:31,540
well have query for my name and then click on the link

1233
01:27:31,580 --> 01:27:34,960
and that way i can completely skew things

1234
01:27:36,400 --> 01:27:41,630
this is why sometimes you might actually want to ground truth

1235
01:27:42,190 --> 01:27:43,120
one way

1236
01:27:43,130 --> 01:27:49,020
of actually looking at how well i'm doing is what's called the discounted cumulative gain

1237
01:27:49,060 --> 01:27:50,170
OK so i need to

1238
01:27:50,190 --> 01:27:51,850
explain what this is

1239
01:27:51,900 --> 01:27:54,480
basically what you care about is

1240
01:27:54,500 --> 01:27:57,310
that the most relevant pages come up on top

1241
01:27:57,370 --> 01:27:58,940
and so you will get

1242
01:27:59,000 --> 01:28:02,350
a decrease in game if you relevant pages

1243
01:28:02,400 --> 01:28:04,870
lies somewhere in the forest in

1244
01:28:04,920 --> 01:28:07,960
so is the position where the page is placed

1245
01:28:08,000 --> 01:28:09,270
why i

1246
01:28:09,290 --> 01:28:12,350
is the score of that because of that

1247
01:28:12,370 --> 01:28:14,710
document imply i j

1248
01:28:14,730 --> 01:28:18,520
is simply a permutation matrix which places document i

1249
01:28:18,520 --> 01:28:20,040
at position j

1250
01:28:20,150 --> 01:28:22,960
why i j i and j is one

1251
01:28:23,730 --> 01:28:25,580
the combination of i j

1252
01:28:25,600 --> 01:28:26,540
and here

1253
01:28:26,560 --> 01:28:31,100
for any other towns in that row and column

1254
01:28:32,150 --> 01:28:37,810
this is a fairly complicated scoring measure there's also something called normalized discounted cumulative gains

1255
01:28:37,830 --> 01:28:42,600
we take this quantity and divided by the maximum value

1256
01:28:42,690 --> 01:28:45,750
there's about five other scores you could think of

1257
01:28:45,790 --> 01:28:48,130
this means the reciprocal rank

1258
01:28:48,170 --> 01:28:52,790
as winner takes all those expected rank utilities

1259
01:28:52,850 --> 01:28:55,940
the cool thing is they all look like this

1260
01:28:56,000 --> 01:29:00,560
basically you have some term which depends on the position

1261
01:29:00,670 --> 01:29:03,210
in another term which depends on the document

1262
01:29:03,230 --> 01:29:06,830
and then you just take the inner product between those

1263
01:29:06,960 --> 01:29:12,250
this is going to be very useful later on when we discuss ranking

1264
01:29:14,000 --> 01:29:15,390
this function here

1265
01:29:16,210 --> 01:29:18,500
i guess for psychologists come

1266
01:29:18,540 --> 01:29:19,730
i came up with it

1267
01:29:19,790 --> 01:29:22,940
there's another variant of this namely the ECG

1268
01:29:22,960 --> 01:29:23,870
at k

1269
01:29:23,890 --> 01:29:25,630
or NDCG k

1270
01:29:25,700 --> 01:29:28,390
and that just as

1271
01:29:28,400 --> 01:29:30,060
witness to the fact that well

1272
01:29:30,080 --> 01:29:30,830
when you

1273
01:29:30,850 --> 01:29:34,400
look for spring summer school on gold

1274
01:29:34,420 --> 01:29:38,540
you will probably look at the first ten documents retrieved or maybe twenty documents if

1275
01:29:38,540 --> 01:29:40,770
you set your browser this way

1276
01:29:40,790 --> 01:29:43,560
and you're probably not going to look at the next page

1277
01:29:43,600 --> 01:29:47,900
and probably even within those links you're going to look at the ones on topmost

1278
01:29:47,940 --> 01:29:50,580
and for this you want to consider so much

1279
01:29:50,580 --> 01:29:53,020
so then it would actually truncate the sum

1280
01:29:53,060 --> 01:29:54,870
at twenty

1281
01:29:54,920 --> 01:29:57,940
you just look at the first thing to positions and not care about the rest

1282
01:30:00,630 --> 01:30:03,400
of course we don't need to perform in school we need the loss

1283
01:30:03,440 --> 01:30:08,080
so what we do is we just use performance relative to the base losses four

1284
01:30:08,120 --> 01:30:09,920
so they just

1285
01:30:09,940 --> 01:30:12,920
have delta of y one and y

1286
01:30:12,980 --> 01:30:15,850
being the the city of one in one minus the DCG

1287
01:30:15,870 --> 01:30:17,920
of online

1288
01:30:17,920 --> 01:30:20,040
so i'm assuming that one

1289
01:30:20,060 --> 01:30:24,190
the identity permutation will leave everything unchanged

1290
01:30:24,190 --> 01:30:27,690
and i'm assuming that my documents are sort in the right way

1291
01:30:27,730 --> 01:30:30,130
causation design loss function

1292
01:30:30,190 --> 01:30:33,310
the feature map that actually exploits explicit order

1293
01:30:33,350 --> 01:30:34,980
because otherwise i'll get

1294
01:30:35,000 --> 01:30:39,170
stupid results but that's easily taken care of

1295
01:30:39,290 --> 01:30:42,100
all i'm doing is i'm just looking at the regret

1296
01:30:42,100 --> 01:30:44,350
of coming up with permutation pi

1297
01:30:44,350 --> 01:30:50,210
rather than leaving the documents or that they are

1298
01:30:51,960 --> 01:30:55,750
doesn't even have questions so far

1299
01:30:55,790 --> 01:30:59,270
but what we've done is we've now construct the loss

1300
01:30:59,310 --> 01:31:06,290
the next thing we need to do is we need to construct the feature map

1301
01:31:07,230 --> 01:31:11,500
of course what you want is you would like to design something like pagerank right

1302
01:31:11,600 --> 01:31:13,350
that for a query

1303
01:31:13,350 --> 01:31:16,170
and the document maybe some user feature

1304
01:31:16,210 --> 01:31:19,420
this inner product gives us the score

1305
01:31:19,460 --> 01:31:22,730
and this is what you do is you just look at all the web pages

1306
01:31:22,730 --> 01:31:24,900
you just computers scores

1307
01:31:24,920 --> 01:31:29,540
and then you sort you pick the top ten to fifteen guys to actually retrieve

1308
01:31:32,390 --> 01:31:34,390
doing something that at

1309
01:31:34,440 --> 01:31:35,850
deployment time

1310
01:31:35,900 --> 01:31:40,310
is much more than a simple sort or find that top ten guys operation

1311
01:31:40,310 --> 01:31:42,470
reasoning such that provable

1312
01:31:42,650 --> 01:31:46,170
using x

1313
01:31:46,180 --> 01:31:49,220
the derivation five

1314
01:31:49,270 --> 01:31:53,220
and this runs up against the little interesting problem that we discussed way back the

1315
01:31:53,220 --> 01:31:56,180
beginning yesterday

1316
01:31:56,190 --> 01:31:59,600
it might turn out that way

1317
01:31:59,650 --> 01:32:01,240
things like that are true

1318
01:32:01,250 --> 01:32:05,000
even though no particular name number

1319
01:32:07,210 --> 01:32:09,680
makes the truth

1320
01:32:09,720 --> 01:32:13,960
and we were discussing the possibility that for some particular predicate

1321
01:32:14,980 --> 01:32:15,960
you might have

1322
01:32:15,970 --> 01:32:17,840
a b

1323
01:32:19,400 --> 01:32:22,890
but somehow rather you didn't have

1324
01:32:22,940 --> 01:32:29,510
but for every particular

1325
01:32:29,520 --> 01:32:32,510
o thing in the universe that you have to nine four

1326
01:32:32,540 --> 01:32:34,120
p is true

1327
01:32:34,250 --> 01:32:36,320
might turn out less

1328
01:32:36,330 --> 01:32:37,740
this thing files

1329
01:32:37,750 --> 01:32:41,430
and why would that be well presumably because there are some things that didn't get

1330
01:32:43,590 --> 01:32:46,620
recall that was i think consistency

1331
01:32:46,670 --> 01:32:51,080
there's i you can restate all in terms of existential quantifiers as well

1332
01:32:51,120 --> 01:32:53,450
it's an existential quantifier form

1333
01:32:53,490 --> 01:32:55,300
will come out this way

1334
01:32:55,310 --> 01:32:59,320
it might turn out to certain complex thing is true

1335
01:32:59,340 --> 01:33:01,620
like this

1336
01:33:03,350 --> 01:33:05,970
this is something is a

1337
01:33:06,000 --> 01:33:08,520
but no particular numeral

1338
01:33:08,580 --> 01:33:10,480
not zero not

1339
01:33:10,550 --> 01:33:14,220
success is zero not success its successors zero

1340
01:33:14,220 --> 01:33:18,260
no particular numeral makes this thing truth

1341
01:33:19,190 --> 01:33:23,720
i was surprised to discover additional assumption that we had to make about arithmetic

1342
01:33:23,820 --> 01:33:25,450
about formal systems

1343
01:33:26,940 --> 01:33:29,830
omega complete in this kind of way

1344
01:33:29,840 --> 01:33:30,690
does the

1345
01:33:30,740 --> 01:33:35,760
the language have resources to name everything that we intend them to tonight

1346
01:33:35,770 --> 01:33:39,040
now in our ordinary life thinking about numbers

1347
01:33:40,120 --> 01:33:41,940
don't give any thought

1348
01:33:41,960 --> 01:33:44,090
we think would the numbers i just

1349
01:33:44,140 --> 01:33:47,650
zero one two three et cetera at any other numbers

1350
01:33:47,790 --> 01:33:49,240
don't get nine by

1351
01:33:49,250 --> 01:33:50,970
rather it

1352
01:33:51,000 --> 01:33:54,020
well it turned out that when people started to worry about this and make this

1353
01:33:54,020 --> 01:33:55,790
assumptions explicit

1354
01:33:55,800 --> 01:33:58,890
but i found what actually is the possibility of

1355
01:33:58,940 --> 01:34:00,430
models of arithmetic

1356
01:34:00,450 --> 01:34:02,190
in which these things for

1357
01:34:02,200 --> 01:34:04,730
so it significant assumption

1358
01:34:05,760 --> 01:34:07,440
there are as many

1359
01:34:08,570 --> 01:34:09,500
as many

1360
01:34:09,510 --> 01:34:12,070
constructing the language

1361
01:34:12,150 --> 01:34:16,760
needed to name any particular number that you one night

1362
01:34:16,850 --> 01:34:21,270
OK so that's a little thing i'm saying don't want here

1363
01:34:21,320 --> 01:34:26,220
right to left in general requires the assumption that there is only reach number

1364
01:34:26,940 --> 01:34:28,170
so that's

1365
01:34:28,180 --> 01:34:31,740
that's all little bit harder to select that to one side

1366
01:34:31,750 --> 01:34:32,930
OK so

1367
01:34:32,940 --> 01:34:38,330
the general concept here is proof provability got expressed by some recursive functions

1368
01:34:38,350 --> 01:34:41,980
probably by turing machine then just asserted through that there

1369
01:34:42,040 --> 01:34:43,890
there must be a recursive function

1370
01:34:43,950 --> 01:34:45,460
his recursive function

1371
01:34:45,470 --> 01:34:47,490
we can express the first order logic

1372
01:34:47,500 --> 01:34:49,190
so somehow or other

1373
01:34:49,220 --> 01:34:52,750
if you look through the details of the plan that you find

1374
01:34:52,760 --> 01:34:58,350
how the provability predicate would get defined in a particular case

1375
01:34:58,360 --> 01:35:01,220
using that we define provability

1376
01:35:01,270 --> 01:35:04,080
and that to

1377
01:35:07,080 --> 01:35:09,460
just as free for that

1378
01:35:12,890 --> 01:35:14,590
it doesn't make it so

1379
01:35:14,630 --> 01:35:17,370
still building up some mechanisms

1380
01:35:17,440 --> 01:35:19,810
the next thing we actually have to say about

1381
01:35:23,550 --> 01:35:25,750
simulating this

1382
01:35:25,770 --> 01:35:28,940
effect of the stuff here struggle

1383
01:35:28,940 --> 01:35:32,330
it's a list each one of which is either an axiom one

1384
01:35:32,350 --> 01:35:35,480
and it turns out that when you start to examine the thing

1385
01:35:35,490 --> 01:35:36,280
you have to

1386
01:35:36,300 --> 01:35:38,400
figure out what properties it's going to have

1387
01:35:38,420 --> 01:35:39,390
here three

1388
01:35:39,400 --> 01:35:41,810
typical properties provability

1389
01:35:41,850 --> 01:35:44,090
which might not have thought about

1390
01:35:45,990 --> 01:35:48,590
there's some things the theorem

1391
01:35:48,600 --> 01:35:50,770
then it's going to have to proof

1392
01:35:50,770 --> 01:35:54,030
and we want that proved to be expressed

1393
01:35:54,070 --> 01:35:57,520
that was the left half thing i put up for left to right

1394
01:35:58,600 --> 01:36:01,610
so that proof itself will be expressed in t

1395
01:36:01,620 --> 01:36:04,520
we need a little bit more

1396
01:36:04,680 --> 01:36:08,070
t itself and i was used to prove anything

1397
01:36:08,120 --> 01:36:10,660
that's provability predicate

1398
01:36:10,750 --> 01:36:14,520
then then that itself needs to be provable

1399
01:36:14,520 --> 01:36:16,510
there is this whole process

1400
01:36:16,590 --> 01:36:19,910
this turned to these things all turn out to be

1401
01:36:19,960 --> 01:36:25,180
properties of provability that needed to make theorems goes through another come from

1402
01:36:25,360 --> 01:36:28,100
from people thinking about the priori

1403
01:36:28,110 --> 01:36:29,750
they came from the people

1404
01:36:29,750 --> 01:36:32,700
i arguing about this capability stuff

1405
01:36:32,700 --> 01:36:41,480
only one

1406
01:36:46,390 --> 01:36:47,620
that's all

1407
01:37:02,260 --> 01:37:03,360
thank you

1408
01:37:03,380 --> 01:37:08,590
so i'm going to present this talk link analysis with bias

1409
01:37:08,600 --> 01:37:15,510
i would like to say that the most well actually the whole pike programme was

1410
01:37:15,510 --> 01:37:21,110
developed by ludmila but again in the of one that's why i also put those

1411
01:37:22,470 --> 01:37:30,320
web world emails here because if i won't be able to to answer your questions

1412
01:37:30,340 --> 01:37:31,200
they will

1413
01:37:31,220 --> 01:37:36,430
and of course materials elastic is also one of

1414
01:37:36,510 --> 01:37:38,220
of the three that

1415
01:37:38,230 --> 01:37:41,800
developed this bike program

1416
01:37:42,590 --> 01:37:49,320
why is there spider on the left side it's because like in slovenian means spider

1417
01:37:49,390 --> 01:37:53,480
that's why and by the next are some kind

1418
01:37:53,490 --> 01:37:57,380
networks so that's why everything was called

1419
01:37:58,110 --> 01:38:01,670
so far the outline of my talk

1420
01:38:01,680 --> 01:38:04,380
i'm going to first

1421
01:38:04,390 --> 01:38:06,150
explain how

1422
01:38:06,160 --> 01:38:11,260
the network is going to be denoted in this talk what kind of networks are

1423
01:38:11,260 --> 01:38:13,680
used in program pike

1424
01:38:13,690 --> 01:38:19,860
what kind of data will how do we have to prepare

1425
01:38:19,870 --> 01:38:23,680
the data for using this software

1426
01:38:23,690 --> 01:38:26,700
then some some statistics

1427
01:38:26,720 --> 01:38:34,740
well how some somehow the statistics can be computed by a and with these

1428
01:38:34,800 --> 01:38:36,230
this part

1429
01:38:36,250 --> 01:38:40,050
the big part is going to be about how

1430
01:38:40,570 --> 01:38:47,000
important part of networks can be extracted from

1431
01:38:47,010 --> 01:38:52,130
the whole network using different algorithms then

1432
01:38:52,140 --> 01:38:56,460
almost at the end of the pattern searching also how it

1433
01:38:56,580 --> 01:39:02,730
this is this can be done program pike and well this is just an example

1434
01:39:02,740 --> 01:39:09,680
well known example of temporal network and what kind of analysis we did

1435
01:39:09,930 --> 01:39:17,070
and the last thing here this is a new program that was developed by a

1436
01:39:17,090 --> 01:39:24,250
and october provide it's also one of the statistics students in slovenia and it's supposed

1437
01:39:25,650 --> 01:39:29,470
produced animation for temporal networks

1438
01:39:29,480 --> 01:39:31,910
so visualisation

1439
01:39:31,920 --> 01:39:36,200
OK and then there are some further readings of course with links

1440
01:39:36,220 --> 01:39:39,270
so this is quite useful for

1441
01:39:39,320 --> 01:39:41,500
the people who want to look

1442
01:39:41,520 --> 01:39:42,660
some more

1443
01:39:42,680 --> 01:39:45,980
in this bike or

1444
01:39:46,000 --> 01:39:48,200
network analysis

1445
01:39:48,220 --> 01:39:50,470
OK so the networks

1446
01:39:50,860 --> 01:39:54,980
the networks can be presented and will be presented here as

1447
01:39:56,060 --> 01:40:00,980
four sets the first two are present

1448
01:40:01,940 --> 01:40:07,190
actually from grad so the set of the v is the set of vertices and

1449
01:40:07,670 --> 01:40:13,430
a set of lines of course lines can be lines are directed can be directed

1450
01:40:13,760 --> 01:40:20,550
or undirected and these are edges undirected and are self-directed

1451
01:40:20,560 --> 01:40:22,800
and what we do

1452
01:40:22,820 --> 01:40:28,270
also used is the small and will be the number of

1453
01:40:28,280 --> 01:40:33,360
vertices in the network and the small and the number of lines in the network

1454
01:40:33,390 --> 01:40:36,750
there are two more sense this is

1455
01:40:36,760 --> 01:40:41,120
p which is the set of cortex value functions

1456
01:40:41,130 --> 01:40:44,570
and we call the properties so

1457
01:40:44,740 --> 01:40:47,320
that means that each vertex

1458
01:40:47,340 --> 01:40:50,220
you can have a number

1459
01:40:50,240 --> 01:40:52,310
attached to read and this is

1460
01:40:52,350 --> 01:40:59,870
the this vertex property and for the same for lines so line many functions wise

1461
01:40:59,900 --> 01:41:03,140
is the set because we can have more than one

1462
01:41:03,660 --> 01:41:07,080
function so more than one different

1463
01:41:07,130 --> 01:41:08,420
value offline

1464
01:41:08,440 --> 01:41:10,440
for a single line

1465
01:41:10,450 --> 01:41:12,350
and they are going to be

1466
01:41:12,360 --> 01:41:14,930
well we're going to call them weights

1467
01:41:15,010 --> 01:41:19,000
OK so this is the transformation

1468
01:41:19,010 --> 01:41:20,510
the lines

1469
01:41:20,530 --> 01:41:21,620
go to

1470
01:41:21,630 --> 01:41:23,730
some real numbers

1471
01:41:23,740 --> 01:41:25,690
are you

1472
01:41:25,710 --> 01:41:27,060
so by

1473
01:41:27,060 --> 01:41:27,880
go on

1474
01:41:27,900 --> 01:41:30,670
why it really

1475
01:41:30,690 --> 01:41:35,090
a few people have the right to OK let's go on

1476
01:41:35,110 --> 01:41:42,270
so the second thing i would like to talk about this is about a discriminative

1477
01:41:43,630 --> 01:41:49,880
so typically people used to hidden markov models here and you using again one of

1478
01:41:49,880 --> 01:41:53,200
the learning for improve gene finding

1479
01:41:53,210 --> 01:41:59,670
OK so this a big interest in annotating genes so genomes so since nineteen ninety

1480
01:41:59,670 --> 01:42:03,800
five have been like one hundred eighty genome sequence

1481
01:42:05,100 --> 01:42:09,830
actually already quite a few compassionate towards for gene prediction

1482
01:42:09,880 --> 01:42:14,360
however the gene predictions are still very in

1483
01:42:14,370 --> 01:42:19,520
and here we would like to contribute something so for instance they were told what

1484
01:42:19,520 --> 01:42:23,570
the c elegans this has been sequence in nineteen eighty eight so

1485
01:42:23,590 --> 01:42:29,840
the only thing twenty five percent of all the genes in this organism are pretty

1486
01:42:29,840 --> 01:42:36,560
good so i experimentally analyzed insufficiently to know the gene structure OK but this fifty

1487
01:42:36,560 --> 01:42:42,740
percent which partially analyse this twenty five percent which are completely i mean i'm what

1488
01:42:42,780 --> 01:42:48,600
it is purely based on prediction and this is like one of the best analysed

1489
01:42:48,740 --> 01:42:51,890
model maybe you would be better

1490
01:42:52,060 --> 01:42:58,140
OK so so that's why i mean that's the rest sequenced the other about what

1491
01:42:58,140 --> 01:43:03,970
it is and now the question is which gene finding problem which invited program should

1492
01:43:03,970 --> 01:43:07,010
be used for annotating that's why there was competition

1493
01:43:07,090 --> 01:43:13,600
in get competition and there were some control competition conditions and they call that what

1494
01:43:13,610 --> 01:43:18,140
i did my best so i'm going to show you that

1495
01:43:19,300 --> 01:43:21,750
it submission for

1496
01:43:21,760 --> 01:43:26,150
however tools stages the first stage was and he said well i

1497
01:43:26,170 --> 01:43:28,600
so we still haven't got the fisheries

1498
01:43:32,850 --> 01:43:34,160
OK so

1499
01:43:34,170 --> 01:43:39,960
it's in this picture before being so you would like to model like the different

1500
01:43:39,960 --> 01:43:43,060
parts of the gene and we can see this

1501
01:43:43,080 --> 01:43:47,900
essentially is the label sequence of so if given the DNA sequence we would like

1502
01:43:47,910 --> 01:43:54,650
to predict the correct sequence of labels and these labels are exon intron

1503
01:43:54,700 --> 01:43:59,160
excellent OK

1504
01:43:59,270 --> 01:44:00,620
so and

1505
01:44:00,630 --> 01:44:04,450
we consider a city block model which i think what

1506
01:44:04,500 --> 01:44:09,750
and we have a state model which describes the signals in the end of the

1507
01:44:10,320 --> 01:44:14,650
so the first part of the transcription start that's just the

1508
01:44:14,670 --> 01:44:16,020
thirteen stars

1509
01:44:16,030 --> 01:44:21,430
then at some point this the translation start then series by science

1510
01:44:21,660 --> 01:44:23,750
and at some point the called

1511
01:44:23,770 --> 01:44:27,880
and there's a few other signals which are related to to the which we have

1512
01:44:28,940 --> 01:44:31,330
this is the simplest model

1513
01:44:31,420 --> 01:44:33,780
so each

1514
01:44:33,790 --> 01:44:40,820
state here corresponds to a signal on each transition between states corresponds to a segment

1515
01:44:40,860 --> 01:44:47,340
the segment here point is between the stations started building that exon and between the

1516
01:44:47,490 --> 01:44:49,020
except the intro

1517
01:44:49,030 --> 01:44:52,170
so we have to labels associated with the

1518
01:44:59,060 --> 01:45:03,750
a more realistic model looked like this we also have a splicing once the

1519
01:45:03,770 --> 01:45:07,260
UTR of community

1520
01:45:07,270 --> 01:45:13,050
and we have some additional information which pick up here regarding reading frame rate much

1521
01:45:13,050 --> 01:45:14,470
very very much

1522
01:45:14,620 --> 01:45:17,890
OK so

1523
01:45:17,900 --> 01:45:23,140
we follow a kind of a two layered approach that so essentially we could simply

1524
01:45:23,150 --> 01:45:31,970
take the sequence and run label sequence i wouldn't very happy hour pi that sequence

1525
01:45:31,970 --> 01:45:37,670
with some string kernels each and the label some some labelled can

1526
01:45:37,720 --> 01:45:39,450
but this is going to be to expensive

1527
01:45:39,470 --> 01:45:43,730
so what we do here is to be one of the different parts separately

1528
01:45:44,200 --> 01:45:48,980
so we want to win since the splice sites in model the translation start the

1529
01:45:48,980 --> 01:45:54,060
transcription start and site so on so predictors for each of these different parts then

1530
01:45:54,070 --> 01:45:55,940
we tried to combine

1531
01:45:56,000 --> 01:45:58,480
OK what kind of things can be one

1532
01:45:58,680 --> 01:46:05,030
so it's good to start events translation stop and the acceptor splice site and he

1533
01:46:06,140 --> 01:46:12,630
positive examples of training set and a negative sample essentially all the other

1534
01:46:12,650 --> 01:46:16,360
so this is a very unbalanced problem

1535
01:46:16,380 --> 01:46:21,100
you can also come up the census for content

1536
01:46:21,110 --> 01:46:26,600
OK so so i mean x has a different kind of composition that intro so

1537
01:46:26,640 --> 01:46:29,750
we also have classifier which can recognise questions

1538
01:46:29,930 --> 01:46:36,950
so the typical approach would be to use a position specific corresponding matrix matrices what

1539
01:46:36,950 --> 01:46:38,760
i all chains

1540
01:46:38,780 --> 01:46:43,340
and here we want to use it to support vector machines you have seen the

1541
01:46:43,340 --> 01:46:45,270
results of splicing

1542
01:46:45,460 --> 01:46:50,390
the results are quite a bit of markov chains

1543
01:46:50,470 --> 01:46:52,760
so there i mean

1544
01:46:52,930 --> 01:46:58,770
one benefit of this machine is that what it and they continue to improve when

1545
01:46:58,770 --> 01:47:00,120
you use more data

1546
01:47:00,130 --> 01:47:06,840
the typically markov chains you've just housing examples and they don't much

1547
01:47:06,900 --> 01:47:09,960
so what's

1548
01:47:09,970 --> 01:47:12,810
against SVM is that originally

1549
01:47:12,820 --> 01:47:18,320
large scale to really deal with large amounts of data so it seems some techniques

1550
01:47:18,430 --> 01:47:19,630
to solve this

1551
01:47:19,650 --> 01:47:22,730
and SVM still produced

1552
01:47:23,490 --> 01:47:28,910
probability score so that we can really easy to use it with lot what's so

1553
01:47:28,910 --> 01:47:34,630
that's why we use spectral of learning to combine this one and this one

1554
01:47:34,730 --> 01:47:38,490
the idea is as follows so we have this tool a approach and this is

1555
01:47:38,490 --> 01:47:41,100
now simplified setting

1556
01:47:41,110 --> 01:47:44,290
we only have a splice site

1557
01:47:44,790 --> 01:47:46,280
so this is the

1558
01:47:46,480 --> 01:47:50,820
in next on the donor splice site and this is the start of in exile

1559
01:47:51,060 --> 01:47:56,480
which is the best place to be predicted for every appearance of GT four the

1560
01:47:56,480 --> 01:48:01,370
don't that every period AG in the for except

1561
01:48:01,390 --> 01:48:04,130
the sequence predict the score

1562
01:48:04,970 --> 01:48:11,340
now we consider again true supplies is true splicing of the sequence and wrong splicing

1563
01:48:11,460 --> 01:48:15,580
so this might be true but i like to see

1564
01:48:15,600 --> 01:48:21,000
so true segmentation into exonintron it is all about

1565
01:48:21,020 --> 01:48:25,850
and for every segment i mean every every segment boundary has to be the little

1566
01:48:25,850 --> 01:48:29,220
respite one except that some people

1567
01:48:29,260 --> 01:48:35,990
OK so we don't like that of every change of thickness think where every segment

1568
01:48:35,990 --> 01:48:39,090
boundary we take these by site associated with it

1569
01:48:39,110 --> 01:48:45,590
consider the score and see how much it contributes to the cumulative score so like

1570
01:48:45,590 --> 01:48:48,220
with the first thing here we have the blues

1571
01:48:48,230 --> 01:48:52,850
score here so we get vision here and segment and he

1572
01:48:53,860 --> 01:48:57,270
so is a bronze splicing and they have an additional

1573
01:48:57,270 --> 01:49:01,710
the documents let's see if you want to see whether part of one document this

1574
01:49:01,710 --> 01:49:02,760
copy of

1575
01:49:04,050 --> 01:49:06,190
a lot of copy of

1576
01:49:06,200 --> 01:49:12,640
another part of some other document again this presentation would be enough

1577
01:49:12,650 --> 01:49:18,970
well for any deeper semantic tasks this representation is too weak so we'll just go

1578
01:49:18,970 --> 01:49:24,110
further on so the next very typical representation would be that the document is basically

1579
01:49:24,110 --> 01:49:27,040
a sequence of words not the sequence of

1580
01:49:27,050 --> 01:49:29,210
a character sequence of words

1581
01:49:32,430 --> 01:49:34,520
this is a set of the most

1582
01:49:34,540 --> 01:49:40,710
common representation for text and most of the basically used these

1583
01:49:43,470 --> 01:49:47,880
so the first step is when you have the document b

1584
01:49:47,880 --> 01:49:51,720
try to split it in the

1585
01:49:51,750 --> 01:49:53,140
set of

1586
01:49:53,140 --> 01:49:58,190
works and the this is what we call the book tokenisation and so we

1587
01:49:58,210 --> 01:50:00,190
you can do this in various ways

1588
01:50:00,610 --> 01:50:05,800
so there's lot of different software packages for this so this is not too hard

1589
01:50:06,520 --> 01:50:12,870
maybe it's just interesting to note that the word is quite well defined in this

1590
01:50:12,870 --> 01:50:17,830
western languages while it's a already in chinese this is not because the work consists

1591
01:50:17,830 --> 01:50:24,180
from a couple of symbols and they don't have the blank as we know is

1592
01:50:24,180 --> 01:50:26,210
for no it in our languages so

1593
01:50:26,260 --> 01:50:32,140
the chinese is a be different but still this concept is quite important so

1594
01:50:32,140 --> 01:50:35,260
delete it works what's

1595
01:50:35,320 --> 01:50:38,160
interesting when when we deal with words

1596
01:50:40,190 --> 01:50:41,810
the first problem which we

1597
01:50:41,830 --> 01:50:53,970
see when dealing with words is this relationships between surface forms of the words

1598
01:50:54,020 --> 01:50:55,660
so first of need

1599
01:50:55,880 --> 01:51:02,990
so the first

1600
01:51:03,000 --> 01:51:07,780
is relationship between the forms of the words and their actions sense

1601
01:51:07,970 --> 01:51:15,200
so and this these are for the most common relationship so one is homonymy where

1602
01:51:15,200 --> 01:51:18,410
we have the same form but different meaning so

1603
01:51:18,450 --> 01:51:20,350
for instance for the world bank

1604
01:51:20,400 --> 01:51:25,020
this can mean means either river bank or financial institution so

1605
01:51:25,120 --> 01:51:27,510
the world bank needs to be

1606
01:51:27,600 --> 01:51:29,850
disambiguate in that sense

1607
01:51:29,930 --> 01:51:30,960
so this is

1608
01:51:31,000 --> 01:51:32,460
often the problem

1609
01:51:32,470 --> 01:51:35,600
so just seeing the world bank doesn't mean that we know

1610
01:51:35,600 --> 01:51:37,040
what sense was used

1611
01:51:37,130 --> 01:51:41,970
or another problem is policy where we have the same form of word but related

1612
01:51:41,970 --> 01:51:46,470
meaning it's again bank would be like the bank or financial institution so

1613
01:51:46,880 --> 01:51:49,990
but sort of banks what

1614
01:51:50,000 --> 01:51:54,650
what's meetings school something what's different types of

1615
01:51:55,660 --> 01:52:01,510
they have different form of word but same meaning like singer vocalist so these are

1616
01:52:01,510 --> 01:52:04,100
two different surface forms but

1617
01:52:04,120 --> 01:52:10,540
completely the same ink again usually for efficient text analysis we need to disambiguate this

1618
01:52:11,800 --> 01:52:13,460
one meaning people

1619
01:52:13,540 --> 01:52:18,210
or hypernymy so this is another

1620
01:52:18,220 --> 01:52:23,630
issue which we need to with is one word denotes a subclass of another letter

1621
01:52:24,750 --> 01:52:29,410
it is part of all sub of the

1622
01:52:30,670 --> 01:52:36,960
having this type of relationships usually we don't know this in advance occasionally have vocabularies

1623
01:52:36,970 --> 01:52:37,790
for this

1624
01:52:37,810 --> 01:52:39,500
which helped but

1625
01:52:39,520 --> 01:52:43,600
usually we don't we don't know this in advance so you just need to be

1626
01:52:43,600 --> 01:52:49,380
aware that this problem exists and we try to resolve these problems with

1627
01:52:52,630 --> 01:52:56,010
proper analytic techniques space

1628
01:52:56,070 --> 01:53:00,290
it was also very important which we would see a little bit later that word

1629
01:53:00,300 --> 01:53:06,080
frequencies in texts of the so-called power distribution so power law is

1630
01:53:06,160 --> 01:53:11,520
something which i would describe and later when we talk about link analysis

1631
01:53:11,600 --> 01:53:17,720
it's very fundamental and wallace all techniques in text rely on this so-called power distribution

1632
01:53:19,150 --> 01:53:24,800
we see this power law in in the in this simple observation so that so

1633
01:53:24,820 --> 01:53:29,000
we have a small number of very frequent words and big number of low frequency

1634
01:53:29,000 --> 01:53:30,920
words so this is this

1635
01:53:30,930 --> 01:53:33,560
diagram which

1636
01:53:33,580 --> 01:53:37,350
is that we see a little bit later on and

1637
01:53:37,360 --> 01:53:41,240
i won't go now into the details but let's say this is a very fundamental

1638
01:53:41,240 --> 01:53:43,690
property of the text and the never

1639
01:53:43,800 --> 01:53:45,980
you see the data with the same

1640
01:53:45,990 --> 01:53:48,780
power law distribution you can more less say that's

1641
01:53:48,800 --> 01:53:52,980
all the techniques which are used for text mining or text analysis are also applicable

1642
01:53:52,980 --> 01:53:59,450
for that particular in image recognition in link analysis in

1643
01:53:59,530 --> 01:54:04,020
maybe in some other areas you can you can spot this kind of distribution center

1644
01:54:04,020 --> 01:54:05,740
immediately more less you can use

1645
01:54:05,790 --> 01:54:07,770
a set of techniques from this area

1646
01:54:07,790 --> 01:54:11,660
on that day

1647
01:54:13,300 --> 01:54:14,680
i was keep this

1648
01:54:14,680 --> 01:54:16,300
the word sentence

1649
01:54:16,350 --> 01:54:20,370
stemming so these are just some

1650
01:54:20,370 --> 01:54:24,030
technicalities when dealing on the word level

1651
01:54:24,140 --> 01:54:27,070
the next presentation would be phrases

1652
01:54:27,190 --> 01:54:30,200
so words are just a single units but

1653
01:54:30,210 --> 01:54:31,960
location be would be

1654
01:54:31,970 --> 01:54:34,390
interested in a sequence of words

1655
01:54:34,400 --> 01:54:36,970
which means something let's say

1656
01:54:37,000 --> 01:54:38,870
if you have

1657
01:54:38,870 --> 01:54:43,200
in the text written in machine learning so if you

1658
01:54:43,240 --> 01:54:46,770
if you know the machine learning is one phrase denoting one

1659
01:54:46,790 --> 01:54:48,260
particular concept this

1660
01:54:48,330 --> 01:54:53,370
this is a valuable piece of information back to the fact that we would consider

1661
01:54:53,370 --> 01:55:00,820
this as a separate machine and separate machine and separate words not so machine learning

1662
01:55:00,830 --> 01:55:04,210
carries a little bit more information

1663
01:55:06,780 --> 01:55:08,420
phrases are

1664
01:55:11,600 --> 01:55:18,400
frequent contiguous word sequences so this is something which which we need to know here

1665
01:55:18,410 --> 01:55:21,960
and what's interesting

1666
01:55:21,980 --> 01:55:23,540
but this is

1667
01:55:23,550 --> 01:55:26,280
it's recently so this was

1668
01:55:26,330 --> 01:55:27,260
one year ago

1669
01:55:29,680 --> 01:55:31,440
this he released

1670
01:55:31,490 --> 01:55:33,940
n gram corpus so if you go to this

1671
01:55:36,970 --> 01:55:38,310
internet address

1672
01:55:39,170 --> 01:55:41,180
you can think or the

1673
01:55:41,210 --> 01:55:42,270
the whole corpus

1674
01:55:42,280 --> 01:55:44,940
is it you get the package of five DVD's

1675
01:55:44,940 --> 01:55:47,480
the impact this sixty giga bytes

1676
01:55:47,500 --> 01:55:49,750
and what it is

1677
01:55:51,610 --> 01:55:55,770
the event over the whole

1678
01:55:55,790 --> 01:55:59,260
english part of their index

1679
01:55:59,270 --> 01:56:01,380
basically all the web documents they

1680
01:56:01,710 --> 01:56:04,880
from that and they

1681
01:56:04,930 --> 01:56:06,480
identified as

1682
01:56:07,810 --> 01:56:10,990
phrases all the sequences of words for

1683
01:56:10,990 --> 01:56:16,870
dental the standpoint denclue two point zero that has nothing to do with the web

1684
01:56:16,980 --> 01:56:21,610
two point there it's just the second version of this algorithm and i thought b

1685
01:56:21,650 --> 01:56:22,470
got some

1686
01:56:22,480 --> 01:56:30,130
interesting enhancement of the origin of class grows which was published on ninety eight

1687
01:56:30,150 --> 01:56:37,790
in KDD conference because of workers collaborative work was intending gabriel just from

1688
01:56:37,810 --> 01:56:42,630
diploma diploma student of mine and w

1689
01:56:43,290 --> 01:56:51,110
like this first introduced density density based clustering and denclue one point zero

1690
01:56:51,110 --> 01:56:58,110
i layout the problem the problem which is seen climbing and reformulated climbing as an

1691
01:56:58,110 --> 01:57:05,010
expectation maximisation algorithm then there are some minor issues also identification of local maxima it

1692
01:57:05,020 --> 01:57:07,710
has to do this identify the clusterings

1693
01:57:07,790 --> 01:57:13,750
and then there's some some broader issues coming like you could apply general techniques to

1694
01:57:13,750 --> 01:57:16,440
this new algorithms and i

1695
01:57:16,460 --> 01:57:18,720
present some experiments

1696
01:57:18,740 --> 01:57:23,620
so when we talk about density based clustering we have the assumption that the clusters

1697
01:57:23,620 --> 01:57:25,070
are regions of high

1698
01:57:25,080 --> 01:57:31,110
density in the data and we assume the attributes in this case numeric attributes from

1699
01:57:31,110 --> 01:57:36,900
a d dimensional space and we have an of those dimensional data points

1700
01:57:36,910 --> 01:57:41,520
and the question is how to estimate density there are several answers to this first

1701
01:57:41,520 --> 01:57:43,900
you can say we too parametric models

1702
01:57:43,910 --> 01:57:46,770
and like mixture models and things like this

1703
01:57:46,820 --> 01:57:47,550
this is

1704
01:57:48,010 --> 01:57:53,440
something i'm not going to talk about than they are nonparametric models like histograms or

1705
01:57:53,440 --> 01:57:57,020
is it more advanced stuff like kernel density estimation

1706
01:57:57,040 --> 01:58:00,820
the idea of concept kind of density estimation is to

1707
01:58:00,820 --> 01:58:06,160
to model the influence of the data by point by canoe or

1708
01:58:06,180 --> 01:58:12,150
in this setting you could also say by generalisation you call a membership function and

1709
01:58:12,150 --> 01:58:15,770
the density then its enormous sum of all these kernels

1710
01:58:15,850 --> 01:58:24,720
and then there's the parameter coming into play this smoothing parameter called h and depends

1711
01:58:24,720 --> 01:58:25,800
on the with

1712
01:58:25,810 --> 01:58:35,080
depends on the on the value of this parameter all these comments

1713
01:58:35,180 --> 01:58:40,730
small data fusion to the very small then you have basically peaks and

1714
01:58:40,740 --> 01:58:44,180
you have many local maxima in the future to be bigger

1715
01:58:44,210 --> 01:58:51,300
then you moves datapoints together and you get less local maxima

1716
01:58:52,530 --> 01:58:57,150
this is from a song from the textbook in this case the are i'm talking

1717
01:58:57,150 --> 01:58:58,620
about goshen college

1718
01:58:58,680 --> 01:59:04,430
they are written like this and casey some of the data points and you put

1719
01:59:05,020 --> 01:59:09,520
and you apply the kind of function and you have these

1720
01:59:09,590 --> 01:59:10,710
data points

1721
01:59:10,730 --> 01:59:13,390
going into the kind of functions

1722
01:59:13,400 --> 01:59:18,860
so the clustering dental one point here are defined by the local maxima

1723
01:59:18,870 --> 01:59:22,480
so clusters

1724
01:59:22,490 --> 01:59:24,240
these are the points which

1725
01:59:24,250 --> 01:59:27,180
correspond to the same local maxima

1726
01:59:27,240 --> 01:59:32,710
so basically these two points in one cluster these are another cluster and the origin

1727
01:59:32,710 --> 01:59:40,310
if had an additional part XII which is used to separate between outliers but this

1728
01:59:40,310 --> 01:59:45,370
is not of main importance you can choose imagine this to be to set zero

1729
01:59:45,960 --> 01:59:53,370
originally we sort we did the following approach to find local maxima climbing the computer

1730
01:59:53,430 --> 01:59:57,140
gradients of this density function like this year

1731
01:59:57,150 --> 02:00:00,640
and then we started in iteration

1732
02:00:01,020 --> 02:00:06,610
starting on the data point computing the gradient and moving a bit into in that

1733
02:00:06,610 --> 02:00:11,930
direction of the gradient and we choose to to be constant is constant stepsize and

1734
02:00:11,930 --> 02:00:13,740
this created some problems

1735
02:00:13,750 --> 02:00:19,390
the one problem is that is not very efficient you make many and this is

1736
02:00:19,390 --> 02:00:23,680
this is a small steps in the beginning you see here need many steps to

1737
02:00:23,680 --> 02:00:25,330
approach to local maxima

1738
02:00:25,390 --> 02:00:30,090
and the second thing is that it does not converge to the local maxima in

1739
02:00:30,090 --> 02:00:30,840
the end

1740
02:00:30,850 --> 02:00:35,560
if you zoom in here you jump around and you don't you don't converge to

1741
02:00:36,840 --> 02:00:41,450
so you just come close to this is something some things which are not too

1742
02:00:41,450 --> 02:00:46,090
nice and basically there's other problems we just solve for his new approach

1743
02:00:46,100 --> 02:00:47,610
the new approach

1744
02:00:47,620 --> 02:00:48,700
can be in

1745
02:00:48,720 --> 02:00:53,800
if you don't want to whole series you can mention like this that you take

1746
02:00:54,510 --> 02:00:57,170
the density function and

1747
02:00:57,180 --> 02:01:00,880
you basically differentiate and said it to zero

1748
02:01:01,360 --> 02:01:07,340
you can cancel these constants here and rearranged and you get this equation

1749
02:01:07,450 --> 02:01:11,090
but this is not very good this is not the solution because x still here

1750
02:01:11,460 --> 02:01:17,360
is kind kernel but you can sink whom i not have a solution but it

1751
02:01:17,360 --> 02:01:19,370
can iterate until

1752
02:01:19,550 --> 02:01:24,850
do it like considerations and you can iterative to form and this does not seem

1753
02:01:25,090 --> 02:01:29,030
what we want it automatically adjust step size

1754
02:01:29,050 --> 02:01:32,180
basically at no extra cost if you look

1755
02:01:32,740 --> 02:01:35,410
thank you have the same sum over

1756
02:01:35,450 --> 02:01:41,610
the data points which we had before here the gradient that the sun appears twice

1757
02:01:41,610 --> 02:01:47,220
doesn't matter at the same computation so on the same cost

1758
02:01:47,370 --> 02:01:51,170
and yet you have much fewer steps and you really converge

1759
02:01:52,800 --> 02:01:55,340
now also question why is this going to work

1760
02:01:55,360 --> 02:01:56,470
so this

1761
02:01:56,480 --> 02:01:59,000
it's likely this and

1762
02:01:59,440 --> 02:02:02,690
first introduction there's no proof but

1763
02:02:02,700 --> 02:02:05,440
you can imagine that you

1764
02:02:05,550 --> 02:02:08,840
cast the problem of maximizing kernel density function

1765
02:02:08,860 --> 02:02:11,610
by rewriting it you

1766
02:02:11,630 --> 02:02:14,280
just put

1767
02:02:14,290 --> 02:02:17,540
in here the kernel function and in u

1768
02:02:17,610 --> 02:02:22,450
reformulated and you can sing it looks like a mixture model humans and components

1769
02:02:22,560 --> 02:02:31,110
and what you have something some constraints to piece of fixed and the emus are

1770
02:02:31,380 --> 02:02:35,430
set to the original data points so you can think of the density is the

1771
02:02:35,430 --> 02:02:42,290
kind of likelihood of mixture model and now the difference deficit that we don't maximises

1772
02:02:42,290 --> 02:02:46,000
likelihood of the parameters mu and sigma and p pi

1773
02:02:46,050 --> 02:02:48,500
but we maximize over them

1774
02:02:51,560 --> 02:02:56,060
but this is this doesn't make a big problem because here in this formula

1775
02:02:56,090 --> 02:03:02,660
you can see that you can places xt by mu t this is interchangeable because

1776
02:03:02,660 --> 02:03:04,190
of this guy clear here

